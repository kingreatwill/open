[TOC]
## ç¬¬ 10 ç«  éšé©¬å°”å¯å¤«æ¨¡å‹

**éšé©¬å°”å¯å¤«æ¨¡å‹**ï¼ˆ[Hidden Markov Model,HMM](https://en.jinzhao.wiki/wiki/Hidden_Markov_model)ï¼‰æ˜¯å¯ç”¨äº**æ ‡æ³¨é—®é¢˜**çš„ç»Ÿè®¡å­¦ä¹ æ¨¡å‹ï¼Œæè¿°ç”±éšè—çš„é©¬å°”å¯å¤«é“¾éšæœºç”Ÿæˆè§‚æµ‹åºåˆ—çš„è¿‡ç¨‹ï¼Œå±äºç”Ÿæˆæ¨¡å‹ã€‚

é©¬å°”å¯å¤«æ¨¡å‹æ˜¯å…³äºæ—¶åºçš„æ¦‚ç‡æ¨¡å‹ï¼Œæè¿°ç”±ä¸€ä¸ªéšè—çš„é©¬å°”å¯å¤«é“¾éšæœºç”Ÿæˆ**ä¸å¯è§‚æµ‹çš„çŠ¶æ€éšæœºåºåˆ—**ï¼Œå†ç”±å„ä¸ªçŠ¶æ€ç”Ÿæˆä¸€ä¸ªè§‚æµ‹è€Œäº§ç”Ÿè§‚æµ‹éšæœºåºåˆ—çš„è¿‡ç¨‹ã€‚
éšè—çš„é©¬å°”å¯å¤«é“¾éšæœºç”Ÿæˆçš„çŠ¶æ€çš„åºåˆ—ï¼Œç§°ä¸º**çŠ¶æ€åºåˆ—**ï¼ˆstate sequenceï¼‰ï¼›æ¯ä¸ªçŠ¶æ€ç”Ÿæˆä¸€ä¸ªè§‚æµ‹ï¼Œè€Œç”±æ­¤äº§ç”Ÿçš„è§‚æµ‹çš„éšæœºåºåˆ—ï¼Œç§°ä¸º**è§‚æµ‹åºåˆ—**ï¼ˆobservation sequenceï¼‰ã€‚åºåˆ—çš„æ¯ä¸€ä¸ªä½ç½®åˆå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ—¶åˆ»ã€‚

éšé©¬å°”å¯å¤«æ¨¡å‹ç”±åˆå§‹æ¦‚ç‡åˆ†å¸ƒã€çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒä»¥åŠè§‚æµ‹æ¦‚ç‡åˆ†å¸ƒç¡®å®šã€‚
éšé©¬å°”å¯å¤«æ¨¡å‹çš„å½¢å¼å®šä¹‰å¦‚ä¸‹ï¼š
è®¾ Q æ˜¯æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€çš„é›†åˆï¼ŒN æ˜¯å¯èƒ½çš„çŠ¶æ€æ•°ï¼›V æ˜¯æ‰€æœ‰å¯èƒ½çš„è§‚æµ‹çš„é›†åˆï¼ŒM æ˜¯å¯èƒ½çš„è§‚æµ‹æ•°ã€‚
$$Q = \{q_1,q_2,...,q_N\} , V= \{v_1,v_2,...,v_M\}$$
é•¿åº¦ä¸º T çš„çŠ¶æ€åºåˆ—$I = (i_1,i_2,...,i_T)$ä»¥åŠä¸çŠ¶æ€åºåˆ—å¯¹åº”çš„é•¿åº¦ä¸º T çš„è§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$

**çŠ¶æ€è½¬ç§»çŸ©é˜µ(çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒ)**ï¼šï¼ˆå°±æ˜¯åˆå§‹åŒ–å‚æ•°[transmat_prior](https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn-hmm)ï¼Œä¹Ÿå¯ä»¥ç”¨ params å’Œæ±‚å‡ºçš„å±æ€§ transmat_ï¼‰
$$A=[a_{ij}]_{N\times N}$$
å…¶ä¸­$a_{ij} = P(i\_{t+1} = q_j | i_t = q_i) ,ä¸‹æ ‡ i,j = 1,...,N$è¡¨ç¤ºåœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_i$çš„æ¡ä»¶ä¸‹ åœ¨æ—¶åˆ»$t+1$è½¬ç§»åˆ°çŠ¶æ€$q_j$çš„æ¦‚ç‡

**è§‚æµ‹çŸ©é˜µ(è§‚æµ‹æ¦‚ç‡åˆ†å¸ƒ)**ï¼šï¼ˆå¯¹äº MultinomialHMM ç”¨ params å’Œæ±‚å‡ºçš„å±æ€§ emissionprob_ï¼Œå«å‘ç”Ÿæ¦‚ç‡çŸ©é˜µï¼›å¯¹äº GMMHMM æœ‰ n_mix ã€means_priorã€covars_prior ï¼›å¯¹äº GaussianHMM æœ‰ means_priorã€covars_prior ï¼‰
$$B = [b_j(k)]_{N \times M}$$
å…¶ä¸­$b_j(k) = P(o_t = v_k | i_t = q_j) ,k = 1,...,M,j = 1,...,N$è¡¨ç¤ºåœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_j$çš„æ¡ä»¶ä¸‹ç”Ÿæˆè§‚æµ‹$v_k$çš„æ¦‚ç‡

**åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡ï¼ˆåˆå§‹æ¦‚ç‡åˆ†å¸ƒï¼‰**ï¼šï¼ˆå°±æ˜¯åˆå§‹åŒ–å‚æ•°[startprob_prior](https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn-hmm)å’Œæ±‚å‡ºçš„å±æ€§ startprob\_ ï¼‰
$$\pi = (\pi_i)$$
å…¶ä¸­$\pi_i = P(i_1 =q_i) ,ä¸‹æ ‡i = 1,...,N$è¡¨ç¤ºæ—¶åˆ»$t=1$æ—¶ å¤„äºçŠ¶æ€$q_i$çš„æ¦‚ç‡

éšé©¬å°”å¯å¤«æ¨¡å‹ç”±åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡$\pi$ã€çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ$A$å’Œè§‚æµ‹æ¦‚ç‡çŸ©é˜µ$B$å†³å®šã€‚
$\pi$å’Œ$A$å†³å®šçŠ¶æ€åºåˆ—ï¼Œ$B$å†³å®šè§‚æµ‹åºåˆ—ã€‚
å› æ­¤ï¼Œ**ä¸€ä¸ªéšé©¬å°”å¯å¤«æ¨¡å‹**å¯ä»¥ç”¨ä¸‰å…ƒç¬¦å·è¡¨ç¤ºï¼Œå³
$$\lambda = (A,B,\pi)$$
ç§°ä¸ºéšé©¬å°”å¯å¤«æ¨¡å‹çš„ä¸‰è¦ç´ ã€‚

çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ$A$ä¸åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡$\pi$ç¡®å®šäº†éšè—çš„é©¬å°”å¯å¤«é“¾ï¼Œç”Ÿæˆä¸å¯è§‚æµ‹çš„çŠ¶æ€åºåˆ—ã€‚è§‚æµ‹æ¦‚ç‡çŸ©é˜µ$B$ç¡®å®šäº†å¦‚ä½•ä»çŠ¶æ€ç”Ÿæˆè§‚æµ‹ï¼Œä¸çŠ¶æ€åºåˆ—ç»¼åˆç¡®å®šäº†å¦‚ä½•äº§ç”Ÿè§‚æµ‹åºåˆ—ã€‚

ä»å®šä¹‰å¯çŸ¥ï¼Œéšé©¬å°”å¯å¤«æ¨¡å‹ä½œäº†**ä¸¤ä¸ªåŸºæœ¬å‡è®¾**ï¼š

1. **é½æ¬¡é©¬å°”å¯å¤«æ€§å‡è®¾**ï¼Œå³å‡è®¾éšè—çš„é©¬å°”å¯å¤«é“¾åœ¨ä»»æ„æ—¶åˆ» t çš„çŠ¶æ€åªä¾èµ–äºå…¶å‰ä¸€æ—¶åˆ»çš„çŠ¶æ€ï¼Œä¸å…¶ä»–æ—¶åˆ»çš„çŠ¶æ€åŠè§‚æµ‹æ— å…³ï¼Œä¹Ÿä¸æ—¶åˆ» t æ— å…³ã€‚
   $$P(i_{t}|i_{t-1},o_{t-1},...,i_{1},o_{1}) = P(i_{t}|i_{t-1}), t=1,2,...,T$$
1. **è§‚æµ‹ç‹¬ç«‹æ€§å‡è®¾**ï¼Œå³å‡è®¾ä»»æ„æ—¶åˆ»çš„è§‚æµ‹åªä¾èµ–äºè¯¥æ—¶åˆ»çš„é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€ï¼Œä¸å…¶ä»–è§‚æµ‹åŠçŠ¶æ€æ— å…³ã€‚
   $$P(o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_{t},o_{t},i_{t-1},o_{t-1},...,i_{1},o_{1}) = P(o_{t}|i_{t})$$

éšé©¬å°”å¯å¤«æ¨¡å‹çš„**ä¸‰ä¸ªåŸºæœ¬é—®é¢˜**ï¼š

1. æ¦‚ç‡è®¡ç®—é—®é¢˜ã€‚ç»™å®šæ¨¡å‹$\lambda = (A,B,\pi)$å’Œè§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$ï¼Œè®¡ç®—åœ¨æ¨¡å‹$\lambda$ä¸‹è§‚æµ‹åºåˆ—$O$å‡ºç°çš„æ¦‚ç‡$P(O|\lambda)$ã€‚

1. å­¦ä¹ é—®é¢˜ã€‚å·²çŸ¥è§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$ï¼Œä¼°è®¡æ¨¡å‹$\lambda = (A,B,\pi)$å‚æ•°ï¼Œä½¿å¾—åœ¨è¯¥æ¨¡å‹ä¸‹è§‚æµ‹åºåˆ—æ¦‚ç‡$P(O|\lambda)$æœ€å¤§ã€‚å³ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ä¼°è®¡å‚æ•°ã€‚ï¼ˆ$\lambda_{MLE}=\argmax_{\lambda}P(O|\lambda)$ï¼Œä½¿ç”¨ EM ç®—æ³•æ±‚è§£ã€‚ï¼‰

1. é¢„æµ‹é—®é¢˜ï¼Œä¹Ÿç§°ä¸ºè§£ç ï¼ˆdecodingï¼‰é—®é¢˜ã€‚å·²çŸ¥æ¨¡å‹$\lambda = (A,B,\pi)$å’Œè§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$ï¼Œæ±‚å¯¹ç»™å®šè§‚æµ‹åºåˆ—æ¡ä»¶æ¦‚ç‡$P(I|O)$æœ€å¤§çš„çŠ¶æ€åºåˆ—$I = (i_1,i_2,...,i_T)$ã€‚å³ç»™å®šè§‚æµ‹åºåˆ—ï¼Œæ±‚æœ€æœ‰å¯èƒ½çš„å¯¹åº”çš„çŠ¶æ€åºåˆ—ã€‚ï¼ˆViterbi ç®—æ³•æ±‚$\hat{I}=\argmax_{I}P(I|O,\lambda)$ï¼‰

**æ¦‚ç‡è®¡ç®—é—®é¢˜**ï¼š
å¼•å…¥éšå˜é‡ï¼Œå¯¹å®Œå…¨æ•°æ®å»ºæ¨¡(è¿™é‡Œè¿˜æ˜¯ä¸€æ ·$P(O|\lambda),P(O;\lambda)$æ˜¯ä¸€æ ·çš„ï¼Œ$\lambda$æ˜¯å‚æ•°)
$$P(O|\lambda) = \sum_{I}P(O,I|\lambda)= \sum_{I}P(O|I,\lambda)P(I|\lambda)$$
æ ¹æ®ä¹˜æ³•è§„åˆ™ï¼ˆæ¦‚ç‡è®ºåŸºç¡€æ•™ç¨‹ 51 é¡µï¼Œæ³¨æ„$P(i_1|\lambda) = P(i_1)$ï¼‰ä»¥åŠé©¬å°”å¯å¤«å‡è®¾æœ‰ï¼š
$$P(I|\lambda) = P(i_1,i_2,...,i_T|\lambda)=P(i_1).P(i_2|i_1,\lambda).P(i_3|i_1,i_2,\lambda)...P(i_T|i_1,i_2,...,i_{T-1},\lambda) \\= P(i_1)\prod_{t=2}^T P(i_t|i_1,i_2,...,i_{t-1},\lambda) \\= P(i_1)\prod_{t=2}^T P(i_t|i_{t-1},\lambda) \\= \pi_{i_1}\prod_{t=2}^T a_{i_{t-1}i_{t}}$$
æ ¹æ®ä¹˜æ³•è§„åˆ™ä»¥åŠè§‚æµ‹ç‹¬ç«‹æ€§å‡è®¾æœ‰ï¼š
$$P(O|I,\lambda) = P(o_1,o_2,...,o_T|i_1,i_2,...,i_{T},\lambda) \\= P(o_1|i_1,i_2,...,i_{T},\lambda).P(o_2|o_1,i_1,i_2,...,i_{T},\lambda).P(o_3|o_1,o_2,i_1,i_2,...,i_{T},\lambda)...P(o_T|o_1,o_2,...,o_{T-1},i_1,i_2,...,i_{T},\lambda) \\ = P(o_1|i_1,\lambda).P(o_2|i_2,\lambda)...P(o_T|i_T,\lambda) \\= \prod_{t=1}^Tb_{i_t}(o_t)$$
é‚£ä¹ˆ

$$
P(O,I|\lambda) = P(O|I,\lambda)P(I|\lambda) = \pi_{i_1}\prod_{t=2}^T a_{i_{t-1}i_{t}}\prod_{t=1}^Tb_{i_t}(o_t)
\\= \pi_{i_1}b_{i_1}(o_1) .a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T) = \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t)
$$

**æ¦‚ç‡è®¡ç®—é—®é¢˜- ç›´æ¥ç”±ä¸Šé¢è®¡ç®—æ¦‚ç‡**å¯å¾—
$$P(O|\lambda) = \sum_{I}P(O,I|\lambda)= \sum_{I}P(O|I,\lambda)P(I|\lambda) \\= \sum_{i_1,i_2,...,i_T} \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t) \\= \sum_{i_1 \in N}...\sum_{i_T\in N} \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t) $$
æ—¶é—´å¤æ‚åº¦$O(TN^{T})$ï¼Œæ‰€ä»¥ä¸å¯è¡Œã€‚

ä¸Šé¢è¯´è¿‡ç›´æ¥æ±‚ä¸å¥½æ±‚ï¼Œæœ‰ä»¥ä¸‹æ–¹æ³•å¯æ±‚å¾—ï¼š
**æ¦‚ç‡è®¡ç®—é—®é¢˜- å‰å‘è®¡ç®—**ï¼š
é¦–å…ˆæˆ‘ä»¬å®šä¹‰**å‰å‘æ¦‚ç‡**$\alpha_t(i) = P(o_1,o_2,...,o_t,i_t=q_i | \lambda)$ï¼Œè¡¨ç¤ºæ—¶åˆ»$t$éƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸º$o_1,o_2,...,o_t$ä¸”çŠ¶æ€ä¸º$q_i$çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆ
$$P(O|\lambda) = \sum_{i=1}^N P(O,i_T=q_i|\lambda) = \sum_{i=1}^N P(o_1,...,o_T,i_T=q_i|\lambda) = \sum_{i=1}^N \alpha_T(i)$$

> å…¶å®$P(O|\lambda) = \sum_{j=1}^N P(O,i_1=q_j|\lambda) =...= \sum_{j=1}^N P(O,i_t=q_j|\lambda) = \sum_{i=1}^N\sum_{j=1}^N P(O,i_1=q_i,i_2=q_j|\lambda)$ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å° tï¼Œåªä¸è¿‡æˆ‘ä»¬å®šä¹‰äº†å‰å‘æ¦‚ç‡ï¼Œå¹¶ä¸”$O=(o_1,...,o_T)$

æ‰€ä»¥æˆ‘ä»¬åªè¦æ±‚å‡º$\alpha_T(i)$ï¼Œå¦‚ä½•æ±‚ï¼Ÿä¾æ¬¡$\alpha_1(i) ... \alpha_{t+1}(i) ... \alpha_T(i)$

$$\alpha_1(i) = P(o_1,i_1=q_i | \lambda) =P(i_1=q_i | \lambda)P(o_1|i_1=q_i , \lambda) = \pi_ib_i(o_1) \\  \vdots\\ \alpha_{t+1}(i) = P(o_1,o_2,...,o_t,o_{t+1},i_{t+1}=q_i | \lambda)  \\=\sum_{j=1}^N P(o_1,o_2,...,o_t,o_{t+1},i_{t+1}=q_i,i_{t}=q_j | \lambda) \\ =\sum_{j=1}^NP(o_{t+1}|o_1,..,o_t,i_{t+1}=q_i,i_{t}=q_j,\lambda)P(o_1,o_2,...,o_t,i_{t+1}=q_i,i_{t}=q_j | \lambda) \\=\sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(o_1,o_2,...,o_t,i_{t+1}=q_i,i_{t}=q_j | \lambda)  \\= \sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(i_{t+1}=q_i | o_1,o_2,...,o_t,i_{t}=q_j,\lambda)P(o_1,o_2,...,o_t,i_{t}=q_j | \lambda)  \\=\sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(i_{t+1}=q_i | i_{t}=q_j,\lambda)P(o_1,o_2,...,o_t,i_{t}=q_j | \lambda) \\=\sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(i_{t+1}=q_i | i_{t}=q_j,\lambda)\alpha_t(j) \\= P(o_{t+1}|i_{t+1}=q_i)\sum_{j=1}^NP(i_{t+1}=q_i | i_{t}=q_j,\lambda)\alpha_t(j) \\= \bigg[\sum_{j=1}^N\alpha_t(j)a_{ji} \bigg]  b_i(o_{t+1})$$

**æ¦‚ç‡è®¡ç®—é—®é¢˜- åå‘è®¡ç®—**ï¼š
é¦–å…ˆæˆ‘ä»¬å®šä¹‰**åå‘æ¦‚ç‡**$\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=q_i , \lambda)$ï¼Œè¡¨ç¤ºæ—¶åˆ»çŠ¶æ€ä¸º$q_i$çš„æ¡ä»¶ä¸‹ï¼Œä»$t+1$åˆ°$T$çš„éƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸º$o_{t+1},o_{t+2},...,o_T$æ¦‚ç‡ï¼Œé‚£ä¹ˆ
$$P(O|\lambda) = \sum_{i=1}^N P(O,i_1=q_i|\lambda) = \sum_{i=1}^N P(o_1,...,o_T,i_1=q_i|\lambda) \\= \sum_{i=1}^N P(o_1,...,o_T|i_1=q_i,\lambda)P(i_1=q_i|\lambda) \\= \sum_{i=1}^N P(o_1|o_2,...,o_T,i_1=q_i,\lambda)P(o_2,...,o_T|i_1=q_i,\lambda)P(i_1=q_i|\lambda) \\ = \sum_{i=1}^N P(o_1|i_1=q_i,\lambda)P(o_2,...,o_T|i_1=q_i,\lambda)P(i_1=q_i|\lambda) \\= \sum_{i=1}^N b_i(o_1)\beta_1(i)\pi_i$$
æ‰€ä»¥æˆ‘ä»¬åªè¦æ±‚å‡º$\beta_1(i)$ï¼Œå¦‚ä½•æ±‚ï¼Ÿä¾æ¬¡$\beta_T(i) ... \beta_1{t-1}(i) ... \beta_1(i)$

$$\beta_T(i) = P(i_T = q_i,\lambda) = 1 \\ \vdots \\ \beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=q_i , \lambda) \\= \sum_{j=1}^N P(o_{t+1},o_{t+2},...,o_T,i_{t+1}=q_j|i_t=q_i , \lambda) \\= \sum_{j=1}^N P(o_{t+1},o_{t+2},...,o_T|i_{t+1}=q_j,i_t=q_i , \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda) \\ æ¡ä»¶å‰é¢æ²¡æœ‰o_t(æ ¹æ®æ¦‚ç‡å›¾ä¹Ÿèƒ½å¾—å‡ºç»™å®ši_{t+1}æ—¶ï¼Œi_tä¸o_{t+1},...,o_Tæ— å…³) \\= \sum_{j=1}^N P(o_{t+1},o_{t+2},...,o_T|i_{t+1}=q_j, \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda) \\= \sum_{j=1}^N P(o_{t+1}|o_{t+2},...,o_T,i_{t+1}=q_j, \lambda)P(o_{t+2},...,o_T|i_{t+1}=q_j, \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda)\\= \sum_{j=1}^N P(o_{t+1}|i_{t+1}=q_j, \lambda)P(o_{t+2},...,o_T|i_{t+1}=q_j, \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda) \\ =\sum_{j=1}^N b_j(o_{t+1}) \beta_{t+1}(j) a_{ij}$$

- **æ¨¡å‹**ï¼š
  $$P(O|\lambda) = \sum_{I}P(O,I|\lambda) = \sum_{I}P(O|I,\lambda)P(I|\lambda)$$

- **ç­–ç•¥**ï¼š
  $$\argmax_{\lambda} P(O|\lambda)$$

- **ç®—æ³•**ï¼š
  Baum-Welch ç®—æ³•ï¼Œå…¶å®å°±æ˜¯ EM ç®—æ³•çš„ä¸€ä¸ªå®ç°
  æ ¹æ® EM ç®—æ³•å¾— Q å‡½æ•°
  $$Q(\lambda,\={\lambda}) = \sum_{I} \log P(O,I|\lambda) P(I|O,\={\lambda}) = \sum_{I} \log P(O,I|\lambda) P(I,O|\={\lambda}).\frac{1}{P(O|\={\lambda})}$$
  å› ä¸ºæˆ‘ä»¬è¦æ±‚$\lambda$,è€Œ$1/{P(O|\={\lambda})}$å¯¹äº$\lambda$è€Œè¨€ï¼Œå¯ä»¥çœ‹ä½œå¸¸æ•°ï¼Œæ‰€ä»¥
  $$Q(\lambda,\={\lambda}) =\sum_{I} \log P(O,I|\lambda) P(I,O|\={\lambda})$$
  å› ä¸º
  $$P(O,I|\lambda) = \sum_{I} \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t)  = \sum_{I} \pi_{i_1}\prod_{t=2}^T a_{i_{t-1}i_{t}}\prod_{t=1}^T b_{i_t}(o_t) $$
  æ‰€ä»¥
  $$Q(\lambda,\={\lambda}) = \sum_{I}\bigg[ \log\pi_{i_1}+ \sum_{t=2}^T\log a_{i_{t-1}i_{t}} + \sum_{t=1}^T \log b_{i_t}(o_t) \bigg]P(I,O|\={\lambda})$$
  è¿™é‡Œæˆ‘ä»¬ä»¥æ±‚$\pi$ï¼ˆæ¦‚ç‡å‘é‡ï¼‰ä¸ºä¾‹å­ï¼ˆA,Bå°±ä¸æ¨å¯¼äº†,å‚è§[ä¸€ç«™å¼è§£å†³ï¼šéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å…¨è¿‡ç¨‹æ¨å¯¼åŠå®ç°](https://zhuanlan.zhihu.com/p/85454896)ï¼‰ï¼Œå‘ç°åªæœ‰ä¸€é¡¹ä¸$\pi$æœ‰å…³ç³»
  $$\pi^{(t+1)} = \argmax_{\pi} Q(\lambda,\lambda^{(t)}) \\= \argmax_{\pi} \sum\_{I}\bigg[ \log\pi_{i_1}P(I,O|\lambda^{(t)})\bigg] \\= \argmax_{\pi} \sum_{i_1}\sum_{i_2}...\sum_{i_T}\bigg[ \log\pi_{i_1}P(i_1,i_2,...,i_T,O|\lambda^{(t)})\bigg] \\ æˆ‘ä»¬è§‚å¯Ÿä¸€ä¸‹ï¼Œå‘ç°è¾¹ç¼˜åˆ†å¸ƒ å¯ä»¥åªä¿ç•™ä¸€é¡¹æ¥è®¡ç®—\\ =\argmax_{\pi} \sum_{i_1}\bigg[ \log\pi_{i_1}P(i_1,O|\lambda^{(t)})\bigg] \\ æˆ‘ä»¬æŠŠ i_1 æ›¿æ¢æ‰ \\ =\argmax_{\pi} \sum_{j=1}^N \bigg[ \log\pi_{j}P(i_1 = q_j,O|\lambda^{(t)})\bigg]$$
  æˆ‘ä»¬çŸ¥é“$\pi = (\pi_1,..,\pi_N)$æ˜¯æ¦‚ç‡å‘é‡ï¼Œ$ \sum_{j=1}^N \pi_{j} =1$ï¼Œåˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼Œå†™å‡ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š
  $$L(\pi,\gamma) = \sum_{j=1}^N \bigg[ \log\pi_{j}P(i_1 = q_j,O|\lambda^{(t)})\bigg] + \gamma\bigg(\sum_{j=1}^N \pi_{j} -1\bigg)$$
  æ±‚å…¶ä¸­ä¸€ä¸ªåˆ†é‡$\pi_j$ï¼Œåˆ™å¯¹å…¶æ±‚åå¯¼ï¼Œä»¤åå¯¼æ•°ä¸º0å¾—ï¼š
  $$\frac{\partial L}{\partial \pi_j} = \frac{1}{\pi_j}P(i_1 = q_j,O|\lambda^{(t)})+\gamma = 0$$
  å¾—
  $$P(i_1 = q_j,O|\lambda^{(t)}) + \gamma\pi_j=0$$
  é‚£ä¹ˆ
   $$\sum_{j=1}^N\bigg[P(i_1 = q_j,O|\lambda^{(t)}) + \gamma\pi_j \bigg]=0 \\ \Darr \\ P(O|\lambda^{(t)}) + \gamma = 0$$
   å¸¦å…¥ä¸Šé¢å¾—åˆ°çš„å¼å­ä¸­ï¼Œæœ‰ï¼š
   $$\pi_j^{(t+1)} = \frac{P(i_1 = q_j,O|\lambda^{(t)})}{P(O|\lambda^{(t)})}$$
  å¼å­ä¸­åˆ†æ¯$P\left(X|\lambda^{\left(t\right)}\right)$å¯ä»¥æ ¹æ®å‰å‘ç®—æ³•å’Œåå‘ç®—æ³•æ±‚è§£å‡ºæ¥ï¼Œå°±æ˜¯å½“å‰å‚æ•°ä¸‹è§‚æµ‹æ•°æ®çš„æ¦‚ç‡ã€‚
  å¦å¤–ï¼Œåˆ©ç”¨å®šä¹‰çš„å‰å‘æ¦‚ç‡å’Œåå‘æ¦‚ç‡ï¼Œæœ‰ï¼š
  $$\begin{align} &\alpha_t\left(i\right)\beta_t\left(i\right)\\ &=P\left(x_i,x_2,\ldots,x_t,z_t=q_i|\lambda\right)P\left(x_T,x_{T-1},\ldots,x_{t+1}|z_t=q_i,\lambda\right)\\ &=P\left(x_i,x_2,\ldots,x_t|z_t=q_i,\lambda\right)P\left(x_T,x_{T-1},\ldots,x_{t+1}|z_t=q_i,\lambda\right)P\left(z_t=q_i|\lambda\right)\\ &=P\left(x_i,x_2,\ldots,x_T|z_t=q_i,\lambda\right)P\left(z_t=q_i|\lambda\right)\\ &=P\left(X,z_t=q_i|\lambda\right) \end{align}$$
  é‚£ä¹ˆ
  $$\pi_i^{\left(t+1\right)}=\frac{\alpha_1\left(i\right)\beta_1\left(i\right)}{P\left(X|\lambda^{\left(t\right)}\right)}$$



**é¢„æµ‹é—®é¢˜ï¼Œä¹Ÿç§°ä¸ºè§£ç ï¼ˆdecodingï¼‰é—®é¢˜**ï¼š
ç»´ç‰¹æ¯”ç®—æ³•ï¼ˆ[Viterbi algorithm](https://en.jinzhao.wiki/wiki/Viterbi_algorithm)ï¼‰å®é™…æ˜¯ç”¨åŠ¨æ€è§„åˆ’è§£éšé©¬å°”å¯å¤«æ¨¡å‹é¢„æµ‹é—®é¢˜ï¼Œå³ç”¨åŠ¨æ€è§„åˆ’ï¼ˆ[dynamic programming](https://en.jinzhao.wiki/wiki/Dynamic_programming)ï¼‰æ±‚æ¦‚ç‡æœ€å¤§è·¯å¾„ï¼ˆæœ€ä¼˜è·¯å¾„ï¼‰ï¼Œè¿™é‡Œçš„æœ€ä¼˜è·¯å¾„å°±æ˜¯æœ€ä¼˜çŠ¶æ€åºåˆ—$I$ã€‚

> è¯·å‚è€ƒä¹¦ç±å’Œ[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åå››)-éšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼ˆHidden Markov Modelï¼‰](https://www.bilibili.com/video/BV1MW41167Rf?p=6)

è¿™ä¸€ç±»æ¨¡å‹éœ€è¦æ±‚è§£çš„é—®é¢˜çš„å¤§ä½“æ¡†æ¶ä¸ºï¼š
å…¶ä¸­$X$ä»£è¡¨è§‚æµ‹åºåˆ—ï¼Œ$Z$ä»£è¡¨éšå˜é‡åºåˆ—ï¼Œ$\lambda$ä»£è¡¨å‚æ•°ã€‚

$$
\begin{cases}
   Representation &  \text{Probabilistic graphical model} \\
   Learning & \lambda_{MLE}=arg \underset{\lambda}{\max} P(X|\lambda)  \boxed{\text{Baum Welch Algorithm(EM)}}\\
   Inference & \begin{cases} Decoding & Z=arg\underset{Z}{\max}P(Z|X,\lambda) or P(z_1,z_2,\cdots,z_t|x_1,x_2,\cdots,x_t,\lambda) \boxed{\text{Viterbi Algorithm}}\\ \text{Prob of evidence} & P(X|\lambda)  \boxed{\text{Forward Algorithm,Backward Algorithm}} \\ Filtering & P(z_t|x_1,x_2,\cdots,x_t,\lambda) \boxed{\text{(online)Forward Algorithm}}\\ Smothing & P(z_t|x_1,x_2,\cdots,x_T,\lambda) \boxed{\text{(offline)Forward-Backward Algorithm}}\\Prediction & \begin{Bmatrix} P(z_{t+1},z_{t+2},...|x_1,x_2,\cdots,x_t,\lambda) \\ P(x_{t+1},x_{t+2},...|x_1,x_2,\cdots,x_t,\lambda) \end{Bmatrix} \boxed{\text{Forward Algorithm}} \end{cases}\\
\end{cases}
$$

[Filtering problem (stochastic processes)](<https://en.jinzhao.wiki/wiki/Filtering_problem_(stochastic_processes)>)ï¼š
[Smoothing problem (stochastic processes)](<https://en.jinzhao.wiki/wiki/Smoothing_problem_(stochastic_processes)>)

### é™„åŠ çŸ¥è¯†

#### éšæœºè¿‡ç¨‹

**éšæœºè¿‡ç¨‹**ï¼ˆ[Stochastic process](https://en.jinzhao.wiki/wiki/Stochastic_process)ï¼‰ï¼š

è®¾$(\Omega ,{\mathcal {F}},P)$ä¸ºä¸€ä¸ª**æ¦‚ç‡ç©ºé—´**ï¼ˆ[Probability space](https://en.jinzhao.wiki/wiki/Probability_space)ï¼‰,$\Omega$ ä¸º**æ ·æœ¬ç©ºé—´**ï¼ˆ[sample space](https://en.jinzhao.wiki/wiki/Sample_space)ï¼‰ï¼Œ $\mathcal {F}$ æ˜¯ï¼ˆ[Sigma-algebra](https://en.jinzhao.wiki/wiki/%CE%A3-algebra)ï¼‰ï¼Œ$P$ æ˜¯ï¼ˆ[Probability measure](https://en.jinzhao.wiki/wiki/Probability_measure)ï¼‰ï¼›
è®¾$(S,\Sigma )$ä¸ºå¯æµ‹é‡çš„ç©ºé—´ï¼ˆmeasurable spaceï¼‰ï¼Œ$S$ä¸ºéšæœºå˜é‡çš„é›†åˆ
$${\displaystyle \{X(t):t\in T\}} or {\displaystyle \{X(t,\omega ):t\in T\}}$$

å…¶ä¸­$X(t)$æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œï¼ˆåœ¨è‡ªç„¶ç§‘å­¦çš„è®¸å¤šé—®é¢˜ä¸­$t$è¡¨ç¤ºæ—¶é—´ï¼Œé‚£ä¹ˆ$X(t)$è¡¨ç¤ºåœ¨æ—¶åˆ»$t$è§‚å¯Ÿåˆ°çš„å€¼ï¼‰ï¼›$\omega \in \Omega$ï¼›$T$æ˜¯æŒ‡æ ‡é›† or å‚æ•°é›†ï¼ˆindex set or parameter setï¼‰ï¼Œä¸€èˆ¬è¡¨ç¤ºæ—¶é—´æˆ–ç©ºé—´ï¼Œå¦‚ï¼šç¦»æ•£$T=\{0,1,2,...\}$ä¸€èˆ¬ç§°ä¸ºéšæœºåºåˆ—æˆ–æ—¶é—´åºåˆ—ï¼Œè¿ç»­$T=[a,b] ,aå¯ä»¥å–0æˆ–è€… -\infty,bå¯ä»¥å–+\infty$

æ˜ å°„$X(t,\omega):T \times \Omega \to R$ï¼Œå³$X(.,.)$æ˜¯å®šä¹‰åœ¨$T \times \Omega$ä¸Šçš„äºŒå…ƒå€¼å‡½æ•°;
$\forall t \in T$ï¼ˆå›ºå®š$t \in T$ï¼‰,$ X(t,.)$æ˜¯å®šä¹‰åœ¨æ ·æœ¬ç©ºé—´$\Omega $ä¸Šçš„å‡½æ•°ï¼Œç§°ä¸º**éšæœºå˜é‡**; 
$\forall \omega \in \Omega$,æ˜ å°„$X(.,\omega):T \to S$ï¼ˆå…¶å®å°±æ˜¯å›ºå®š$\omega \in \Omega $ï¼Œå˜æˆå…³äºTçš„å‡½æ•°ï¼‰,è¢«ç§°ä¸º**æ ·æœ¬å‡½æ•°**ï¼ˆsample functionï¼‰,ç‰¹åˆ«æ˜¯å½“$T$è¡¨ç¤ºæ—¶é—´æ—¶ï¼Œç§°ä¸ºéšæœºè¿‡ç¨‹${\displaystyle \{X(t,\omega ):t\in T\}}$çš„**æ ·æœ¬è·¯å¾„**ï¼ˆsample pathï¼‰ã€‚

#### å‚»å‚»åˆ†ä¸æ¸…æ¥šçš„é©¬å°”å¯å¤«

**é©¬å°”å¯å¤«æ€§è´¨**ï¼ˆ[Markov property](https://en.jinzhao.wiki/wiki/Markov_property)ï¼‰ï¼š
å¦‚æœ**éšæœºè¿‡ç¨‹**ï¼ˆ[Stochastic process](https://en.jinzhao.wiki/wiki/Stochastic_process)ï¼‰çš„æœªæ¥çŠ¶æ€çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼ˆä»¥è¿‡å»å’Œç°åœ¨å€¼ä¸ºæ¡ä»¶ï¼‰ä»…å–å†³äºå½“å‰çŠ¶æ€ï¼Œåˆ™éšæœºè¿‡ç¨‹å…·æœ‰é©¬å°”å¯å¤«æ€§è´¨ï¼›ä¸æ­¤å±æ€§çš„è¿‡ç¨‹è¢«è®¤ä¸ºæ˜¯**é©¬æ°**æˆ–**é©¬å°”å¯å¤«è¿‡ç¨‹**ï¼ˆ[Markov process](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚æœ€è‘—åçš„é©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov chain](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚**å¸ƒæœ—è¿åŠ¨**ï¼ˆ[Brownian motion](https://en.jinzhao.wiki/wiki/Brownian_motion)ï¼‰æ˜¯å¦ä¸€ä¸ªè‘—åçš„é©¬å°”å¯å¤«è¿‡ç¨‹ã€‚é©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯ä¸å…·å¤‡è®°å¿†ç‰¹è´¨çš„ï¼ˆ[Memorylessness](https://en.jinzhao.wiki/wiki/Memorylessness)ï¼‰

**ä¸€é˜¶ ç¦»æ•£**
$${\displaystyle P(X_{n}=x_{n}\mid X_{n-1}=x_{n-1},\dots ,X_{0}=x_{0})=P(X_{n}=x_{n}\mid X_{n-1}=x_{n-1}).}$$

**m é˜¶ ç¦»æ•£**
$${\displaystyle {\begin{aligned}&\Pr(X_{n}=x_{n}\mid X_{n-1}=x_{n-1},X_{n-2}=x_{n-2},\dots ,X_{1}=x_{1})\\={}&\Pr(X_{n}=x_{n}\mid X_{n-1}=x_{n-1},X_{n-2}=x_{n-2},\dots ,X_{n-m}=x_{n-m}){\text{ for }}n>m\end{aligned}}}$$

**æ—¶é—´é½æ¬¡**ï¼ˆTime-homogeneousï¼‰
$${\displaystyle \Pr(X_{n+1}=x\mid X_{n}=y)=\Pr(X_{n}=x\mid X_{n-1}=y)}$$

**é©¬å°”å¯å¤«æ¨¡å‹**ï¼ˆ[Markov model](https://en.jinzhao.wiki/wiki/Markov_model)ï¼‰ï¼š
.|System state is fully observable |System state is partially observable
---|---|---
System is autonomous |[Markov chain](https://en.jinzhao.wiki/wiki/Markov_chain) | [Hidden Markov model](https://en.jinzhao.wiki/wiki/Hidden_Markov_model)
System is controlled |[Markov decision process](https://en.jinzhao.wiki/wiki/Markov_decision_process) |[Partially observable Markov decision process](https://en.jinzhao.wiki/wiki/Partially_observable_Markov_decision_process)

é©¬å°”å¯å¤«æ¨¡å‹æ˜¯å…·æœ‰é©¬å°”å¯å¤«æ€§å‡è®¾çš„éšæœºè¿‡ç¨‹ï¼Œæœ€ç®€å•çš„é©¬å°”å¯å¤«æ¨¡å‹æ˜¯**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov chain](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰

| .                                        | Countable state spaceï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„ç¦»æ•£$\Omega$ï¼‰               | Continuous or general state spaceï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„è¿ç»­$\Omega$ï¼‰                              |
| ---------------------------------------- | ----------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| Discrete-timeï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„ç¦»æ•£$T$ï¼‰   | (discrete-time) Markov chain on a countable or finite state space | Markov chain on a measurable state space (for example, Harris chain)                         |
| Continuous-timeï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„è¿ç»­$T$ï¼‰ | Continuous-time Markov process or Markov jump process             | Any continuous stochastic process with the Markov property (for example, the Wiener process) |

ä»¤$\{X_n|n=1,2,\cdots\}$æ˜¯æœ‰é™ä¸ªæˆ–å¯æ•°ä¸ªå¯èƒ½å€¼çš„éšæœºè¿‡ç¨‹ã€‚é™¤éç‰¹åˆ«æé†’ï¼Œè¿™ä¸ªéšæœºè¿‡ç¨‹çš„å¯èƒ½å€¼çš„é›†åˆéƒ½å°†è®°ä¸ºéè´Ÿæ•´æ•°é›†$\{0,1,2,\cdots\}$ ã€‚
å¦‚æœ $X_n=i$ï¼Œé‚£ä¹ˆç§°è¯¥è¿‡ç¨‹åœ¨æ—¶åˆ» $t$ åœ¨çŠ¶æ€ $i$ ï¼Œå¹¶å‡è®¾$P_{i,j}$ ç§°ä¸º(å•æ­¥(one-step))**è½¬ç§»æ¦‚ç‡**(transition probability)ï¼Œè¡¨ç¤ºå¤„åœ¨$i$çŠ¶æ€çš„éšæœºå˜é‡ä¸‹ä¸€æ—¶åˆ»å¤„åœ¨$j$ çŠ¶æ€çš„æ¦‚ç‡ï¼Œå¦‚æœå¯¹æ‰€æœ‰çš„çŠ¶æ€ $i_0,i_1,\cdots,i_{n-1},i,j$åŠä»»æ„$n\ge 0$ ï¼Œ$P\{X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},\cdots,X_1=i_1,X_0=i_0\}=P_{i,j}$ï¼Œè¿™æ ·çš„è¿‡ç¨‹ç§°ä¸º**é©¬å°”å¯å¤«é“¾**(Markov chain)ã€‚

> ä¸€ä¸ªéšæœºè¿‡ç¨‹$\{X(t):t \geq 0\}$å¦‚æœ$t \in \mathbb{R}_+$åˆ™ç§°ä¸ºè¿ç»­æ—¶é—´çš„é©¬å°”ç§‘å¤«é“¾ï¼Œå¦‚æœ$t \in \mathbb{N}_0$åˆ™ç§°ä¸ºç¦»æ•£æ—¶é—´çš„é©¬å°”ç§‘å¤«é“¾

æ ¹æ® $P_{i,j}$çš„å®šä¹‰æ˜¾ç„¶æœ‰$P_{i,j}\ge0,\;i,j\ge0;\;\;\sum_{j=0}^\infty P_{i,j}=1,\;i=0,1,\cdots$,
ç”¨ $P_{i,j}$ è®°å½•ä» $i$ åˆ° $j$ çš„(å•æ­¥)**è½¬ç§»ï¼ˆæ¦‚ç‡ï¼‰çŸ©é˜µ**ï¼ˆ[transition matrix](https://en.jinzhao.wiki/wiki/Stochastic_matrix)ï¼‰ä¹Ÿç§°ä¸º**éšæœºçŸ©é˜µã€æ¦‚ç‡çŸ©é˜µã€è½¬ç§»çŸ©é˜µã€æ›¿ä»£çŸ©é˜µæˆ–é©¬å°”å¯å¤«çŸ©é˜µ**ï¼š
$$\mathbf{P}_{i,j}=(P_{i_{n},i_{n+1}}) =\begin{bmatrix}P_{0,0}&P_{0,1}&P_{0,2}&\cdots\\P_{1,0}&P_{1,1}&P_{1,2}&\cdots\\\vdots&\vdots&\vdots\\P_{i,0}&P_{i,1}&P_{i,2}&\cdots\\\vdots&\vdots&\vdots\end{bmatrix}$$
ç°åœ¨å®šä¹‰ n æ­¥(n-step)**è½¬ç§»æ¦‚ç‡**$P_{i,j}^n$ ï¼š$P_{i,j}^n=P\{X_{n+k=j}|X_k=i\},\;n\ge 0,i,j\ge 0$

**å³éšæœºçŸ©é˜µ**æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°çš„æ–¹é˜µï¼Œæ¯ä¸ªè¡Œæ€»å’Œä¸º 1ã€‚
**å·¦éšæœºçŸ©é˜µ**æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°çš„æ–¹é˜µï¼Œæ¯ä¸ªåˆ—çš„æ€»å’Œä¸º 1ã€‚
**åŒéšæœºçŸ©é˜µ**æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°çš„æ–¹é˜µï¼Œæ¯ä¸ªè¡Œå’Œæ¯ä¸ªåˆ—çš„æ€»å’Œä¸º 1ã€‚

å‡è®¾$A$æ˜¯é©¬å°”å¯å¤«çŸ©é˜µï¼Œå…¶æ€§è´¨æœ‰ï¼š

1. çŸ©é˜µ$A$çš„ k æ¬¡å¹‚$A^k$ä¹Ÿæ˜¯é©¬å°”å¯å¤«çŸ©é˜µã€‚
1. è‡³å°‘æœ‰ä¸€ä¸ªç‰¹å¾å€¼ä¸º 1ï¼Œå…¶ç‰¹å¾å€¼åœ¨[-1,1]åŒºé—´ï¼Œç‰¹å¾å€¼ä¸º 1 å¯¹åº”çš„ç‰¹å¾å‘é‡$\pi$ç§°ä¸º**å¹³ç¨³æ¦‚ç‡å‘é‡**ï¼ˆstationary probability vectorï¼‰ã€‚
1. å¯¹äºä»»æ„**æ¦‚ç‡å‘é‡**ï¼ˆ[Probability vector](https://en.jinzhao.wiki/wiki/Probability_vector)ï¼‰æˆ–è€…**éšæœºå‘é‡**$\pi_0$æœ‰$\lim_{k \to \infty} A^k \pi_0 = \pi$ ï¼ˆè¿™é‡Œæ˜¯åœ¨æ²¡æœ‰-1 ç‰¹å¾å€¼çš„æƒ…å†µä¸‹ï¼‰ã€‚
1. å¯¹äºä»»æ„**æ¦‚ç‡å‘é‡**$\mu_0$æœ‰$\mu_1 = A \mu_0$ä¹Ÿæ˜¯æ¦‚ç‡å‘é‡ã€‚

ç‰¹å¾å€¼çš„æ±‚è§£ï¼š$\det(A-\lambda I)=0$

ç”±äº $A$ çš„æ¯ä¸€åˆ—ç›¸åŠ ç­‰äº 1ï¼Œæ‰€ä»¥ $Aâˆ’I$ çš„æ¯ä¸€åˆ—ç›¸åŠ ç­‰äº 0ï¼Œè¿™ä¹Ÿå°±æ˜¯è¯´ $Aâˆ’I$ çš„è¡Œæ˜¯ç›¸å…³çš„ï¼Œå…¶è¡Œåˆ—å¼$\det(A-I)=0$ä¸ºé›¶ï¼Œæ‰€ä»¥ $Aâˆ’I$å¥‡å¼‚çŸ©é˜µï¼Œæ‰€ä»¥ 1 æ˜¯ $A$ çš„ä¸€ä¸ªç‰¹å¾å€¼ã€‚

[å¯¹è§’åŒ–](https://en.jinzhao.wiki/wiki/Diagonalizable_matrix#Diagonalization) $A = P \Lambda P^{-1} $ ï¼ˆå‚è§çº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨279é¡µï¼Œç‰¹å¾å€¼ç›¸åŒç‰¹å¾å‘é‡ä¸ä¸€å®šç›¸åŒï¼‰,å…¶ä¸­$\Lambda$æ˜¯ç”±$A$çš„ç‰¹å¾å€¼ç»„æˆçš„å¯¹è§’çŸ©é˜µ
$$\mu_k = A^k \mu_0 = (P \Lambda P^{-1})^k \mu_0 = P \Lambda^k P^{-1} \mu_0$$
ä¸å¦¨è®¾ $A$çš„ç‰¹å¾å‘é‡å’Œç›¸åº”çš„ç‰¹å¾å€¼åˆ†åˆ«ä¸º ${x_1},...,{x_n}$å’Œ $\lambda_1,...,\lambda_n$ï¼Œå¯ä»¥ç”¨ç‰¹å¾å‘é‡æ¥åšä¸€ç»„åŸºï¼Œå¯ä»¥æŠŠç©ºé—´ä¸­ä»»ä½•å‘é‡å†™æˆå®ƒçš„çº¿æ€§ç»„åˆï¼š$\mu_0 = c_1{x_1} + \cdots + c_n{x_n}$
é‚£ä¹ˆï¼š
$$A^k\mathbf{\mu_0} = A^kc_1\mathbf{x_1} + \cdots + A^kc_n\mathbf{x_n}\\ = c_1A^k\mathbf{x_1} + \cdots + c_nA^k\mathbf{x_n} \\= c_1A^{k-1}A\mathbf{x_1} + \cdots + c_nA^{k-1}A\mathbf{x_n} \\= c_1A^{k-1}\lambda_1\mathbf{x_1} + \cdots + c_nA^{k-1}\lambda_n\mathbf{x_n}\\=c_1\lambda_1^k\mathbf{x_1} + \cdots + c_n\lambda_n^k\mathbf{x_n}\\=\sum_{i=1}^n{c_i\lambda_i^k\bm{x_i}}$$

ä¸å¦¨ä»¤$\lambda_1=1$, æœ‰$|\lambda_i|\leq 1$,é‚£ä¹ˆï¼š
$$\bm{u_\infty}=\lim_{k\to\infty}{A^k\bm{u_0}}=\lim_{k\to\infty}{\sum_{i=1}^k{c_i\lambda_i^k\bm{x_i}}}=c_1\bm{x_1}$$

å› ä¸º$u_\infty$æ˜¯æ¦‚ç‡å‘é‡ï¼Œè€Œç‰¹å¾å€¼ä¸º 1 å¯¹åº”çš„ç‰¹å¾å‘é‡ä¹Ÿæ˜¯æ¦‚ç‡å‘é‡ï¼Œæ‰€ä»¥$c_1=1$ï¼Œå¾—åˆ°$\bm{u_\infty}=\bm{x_1}$

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé™¤åŒ…å« $\lambda=1$ çš„æƒ…å½¢è¿˜åŒ…å« $\lambda=-1$ çš„æƒ…å½¢ã€‚
ä¸Šå¼å¦‚æœé™¤$\lambda_1=1$è¿˜æœ‰$\lambda_2=-1$ï¼Œé‚£ä¹ˆå°±æœ‰ï¼š
$$\bm{u_\infty}=\lim_{k\to\infty}{\sum_{i=1}^k{c_i\lambda_i^k\bm{x_i}}}=c_1\bm{x_1}+(-1)^k c_2\bm{x_2}$$

å¾—$\bm{u_\infty}=\bm{x_1}+(-1)^k\bm{x_2}$,æ­¤æ—¶$k$ä¸ºå¥‡æ•°å’Œå¶æ•°ç»“æœæ˜¯ä¸åŒçš„ï¼Œé€ æˆçš„ç»“æœå°±æ˜¯åœ¨ä¸¤ç§ç»“æœä¹‹é—´åå¤æ¨ªè·³ï¼Œæ°¸è¿œè¾¾ä¸åˆ°ç¨³æ€ã€‚

å¦‚ï¼š
$$A=\begin{bmatrix}0&1\\1&0\\\end{bmatrix}$$
å…¶ç‰¹å¾å€¼ä¸º$\lambda_1=1ï¼Œ\lambda_2=-1$

> ä¹Ÿå¯ä»¥å‚è€ƒç¬¬ 21 ç«  PageRank ç®—æ³•

#### è§„åˆ’è®º

è§„åˆ’è®ºåˆç§°æ•°å­¦è§„åˆ’,è¿ç­¹å­¦ï¼ˆ[Operations research](https://en.jinzhao.wiki/wiki/Category:Operations_research)ï¼‰çš„ä¸€ä¸ªåˆ†æ”¯ã€‚ è§„åˆ’è®ºæ˜¯æŒ‡åœ¨æ—¢å®šæ¡ä»¶ï¼ˆçº¦æŸæ¡ä»¶ï¼‰ä¸‹ï¼ŒæŒ‰ç…§æŸä¸€è¡¡é‡æŒ‡æ ‡ï¼ˆç›®æ ‡å‡½æ•°ï¼‰åœ¨å¤šç§ æ–¹æ¡ˆä¸­å¯»æ±‚æœ€ä¼˜æ–¹æ¡ˆï¼ˆå–æœ€å¤§æˆ–æœ€å°å€¼ï¼‰ã€‚è§„åˆ’è®ºåŒ…æ‹¬çº¿æ€§è§„åˆ’ã€éçº¿æ€§è§„åˆ’å’ŒåŠ¨æ€è§„åˆ’ç­‰ï¼Œæ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•æˆ–æ–¹æ³•ï¼ˆ[Optimization algorithms and methods](https://en.jinzhao.wiki/wiki/Category:Optimization_algorithms_and_methods)ï¼‰

æ•°å­¦ä¼˜åŒ–ï¼ˆ[Mathematical optimization](https://en.jinzhao.wiki/wiki/Category:Mathematical_optimization)ï¼‰

[ä¼˜åŒ–æŠ€æœ¯](https://en.jinzhao.wiki/wiki/Mathematical_optimization#Computational_optimization_techniques)ï¼š

- ä¼˜åŒ–ç®—æ³• Optimization algorithms
  [ä¼˜åŒ–ç®—æ³•åˆ—è¡¨](https://en.jinzhao.wiki/wiki/List_of_algorithms#Optimization_algorithms)

- è¿­ä»£æ–¹æ³• Iterative methods
  [Iterative method](https://en.jinzhao.wiki/wiki/Iterative_method)

- å…¨å±€æ”¶æ•› Global convergence
- å¯å‘å¼ Heuristics
  [Heuristic algorithm](<https://en.jinzhao.wiki/wiki/Heuristic_(computer_science)>)

**çº¿æ€§è§„åˆ’**ï¼š

å½“ç›®æ ‡å‡½æ•°ä¸çº¦æŸæ¡ä»¶éƒ½æ˜¯çº¿å½¢çš„ï¼Œåˆ™ç§°ä¸ºçº¿æ€§è§„åˆ’ï¼ˆ[Linear programming](https://en.jinzhao.wiki/wiki/Linear_programming)â€ï¼‰ã€‚

æ±‚è§£æ–¹æ³•ï¼šå›¾è§£æ³•(graphical method)ã€å•çº¯å½¢æ³•ï¼ˆ[simplex algorithm](https://en.jinzhao.wiki/wiki/Simplex_algorithm)ï¼‰ã€å¯¹å¶å•çº¯å½¢æ³•ç­‰

**éçº¿æ€§è§„åˆ’**ï¼š

é™¤å»çº¿æ€§è§„åˆ’ï¼Œåˆ™ä¸ºéçº¿æ€§è§„åˆ’ï¼ˆ[Nonlinear programming](https://en.jinzhao.wiki/wiki/Nonlinear_programming)ï¼‰ã€‚å…¶ä¸­ï¼Œå‡¸è§„åˆ’ï¼ˆå‰é¢çš„ç« èŠ‚æœ‰è®²åˆ°å‡¸ä¼˜åŒ–ï¼‰ã€äºŒæ¬¡è§„åˆ’ï¼ˆ[Quadratic programming](https://en.jinzhao.wiki/wiki/Quadratic_programming)ï¼‰ã€å‡ ä½•è§„åˆ’éƒ½æ˜¯ä¸€ç§ç‰¹æ®Šçš„éçº¿æ€§è§„åˆ’ã€‚

æ±‚è§£æ–¹æ³•ï¼šæ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ã€å¯è¡Œæ–¹å‘æ³•ã€åˆ¶çº¦å‡½æ•°æ³•(constrained function method )ç­‰ã€‚

å†…ç‚¹æ³•([Interior point methods](https://en.jinzhao.wiki/wiki/Interior-point_method))æ˜¯ä¸€ç§æ±‚è§£çº¿æ€§è§„åˆ’æˆ–éçº¿æ€§å‡¸ä¼˜åŒ–é—®é¢˜çš„ç®—æ³•ã€‚

**æ— çº¦æŸä¼˜åŒ–é—®é¢˜**ï¼š

å»é™¤å¸¦çº¦æŸçš„è§„åˆ’é—®é¢˜ï¼Œåˆ™ä¸ºæ— çº¦æŸä¼˜åŒ–é—®é¢˜ï¼ˆUnconstrained convex optimizationï¼Œå¯¹åº”çš„æœ‰çº¦æŸä¼˜åŒ–ï¼ˆ[Constrained optimization](https://en.jinzhao.wiki/wiki/Constrained_optimization)ï¼‰ï¼‰ã€‚

æ±‚è§£æ–¹æ³•ï¼š 1ã€ æœ€é€Ÿä¸‹é™æ³•(ä¹Ÿå«æ¢¯åº¦ä¸‹é™) 2ã€ å…±è½­æ¢¯åº¦ä¸‹é™ 3ã€ ç‰›é¡¿æ³• 4ã€ æ‹Ÿç‰›é¡¿æ³•

**åŠ¨æ€è§„åˆ’**ï¼š

è‹¥è§„åˆ’é—®é¢˜ä¸æ—¶é—´æœ‰å…³ï¼Œåˆ™ç§°ä¸ºåŠ¨æ€è§„åˆ’ï¼ˆ[Dynamic programmingâ€](https://en.jinzhao.wiki/wiki/Dynamic_programming)ï¼‰ï¼›

> æŠŠå¤šé˜¶æ®µè¿‡ç¨‹è½¬åŒ–ä¸ºä¸€ç³»åˆ—å•é˜¶æ®µé—®é¢˜ï¼Œé€ä¸ªæ±‚è§£ï¼Œè§£å†³è¿™ç±»é—®é¢˜çš„æ–¹æ³•ç§°ä¸ºåŠ¨æ€è§„åˆ’ï¼Œå®ƒæ˜¯ä¸€ç§æ–¹æ³•ã€è€ƒå¯Ÿé—®é¢˜çš„ä¸€ç§é€”å¾„ï¼Œä½†ä¸æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç®—æ³•ã€‚ æ²¡æœ‰ç»Ÿä¸€çš„æ ‡å‡†æ¨¡å‹ï¼Œä¹Ÿæ²¡æœ‰æ„é€ æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼Œç”šè‡³è¿˜æ²¡æœ‰åˆ¤æ–­ä¸€ä¸ªé—®é¢˜èƒ½å¦æ„é€ åŠ¨æ€è§„åˆ’æ¨¡å‹çš„å‡†åˆ™ã€‚è¿™æ ·å°±åªèƒ½å¯¹æ¯ç±»é—®é¢˜è¿›è¡Œå…·ä½“åˆ†æï¼Œæ„é€ å…·ä½“çš„æ¨¡å‹ã€‚å¯¹äºè¾ƒå¤æ‚çš„é—®é¢˜åœ¨é€‰æ‹©çŠ¶æ€ã€å†³ç­–ã€ç¡®å®šçŠ¶æ€è½¬ç§»è§„å¾‹ç­‰æ–¹é¢éœ€è¦ä¸°å¯Œçš„æƒ³è±¡åŠ›å’Œçµæ´»çš„æŠ€å·§æ€§ï¼Œè¿™å°±å¸¦æ¥äº†åº”ç”¨ä¸Šçš„å±€é™æ€§ã€‚

åŠ¨æ€è§„åˆ’ä¸€èˆ¬å¯åˆ†ä¸ºçº¿æ€§åŠ¨è§„ï¼ŒåŒºåŸŸåŠ¨è§„ï¼Œæ ‘å½¢åŠ¨è§„ï¼ŒèƒŒåŒ…åŠ¨è§„ï¼ˆ[Knapsack problem](https://en.jinzhao.wiki/wiki/Knapsack_problem)ï¼‰å››ç±»ã€‚
çº¿æ€§åŠ¨è§„ï¼šæ‹¦æˆªå¯¼å¼¹ï¼Œåˆå”±é˜Ÿå½¢ï¼ŒæŒ–åœ°é›·ï¼Œå»ºå­¦æ ¡ï¼Œå‰‘å®¢å†³æ–—ç­‰ï¼›
åŒºåŸŸåŠ¨è§„ï¼šçŸ³å­åˆå¹¶ï¼Œ åŠ åˆ†äºŒå‰æ ‘ï¼Œç»Ÿè®¡å•è¯ä¸ªæ•°ï¼Œç‚®å…µå¸ƒé˜µç­‰ï¼›
æ ‘å½¢åŠ¨è§„ï¼šè´ªåƒçš„ä¹å¤´é¾™ï¼ŒäºŒåˆ†æŸ¥æ‰¾æ ‘ï¼Œèšä¼šçš„æ¬¢ä¹ï¼Œæ•°å­—ä¸‰è§’å½¢ç­‰ï¼›
èƒŒåŒ…é—®é¢˜ï¼šèƒŒåŒ…é—®é¢˜ï¼Œå®Œå…¨èƒŒåŒ…é—®é¢˜ï¼Œåˆ†ç»„èƒŒåŒ…é—®é¢˜ï¼ŒäºŒç»´èƒŒåŒ…ï¼Œè£…ç®±é—®é¢˜ï¼ŒæŒ¤ç‰›å¥¶

**éšæœºè§„åˆ’**ï¼š

è‹¥è§„åˆ’é—®é¢˜ä¸éšæœºå˜é‡æœ‰å…³ï¼Œåˆ™ç§°ä¸ºéšæœºè§„åˆ’ï¼ˆ[Stochastic programming](https://en.jinzhao.wiki/wiki/Stochastic_programming)ï¼‰ã€‚

**éšæœºåŠ¨æ€è§„åˆ’**ï¼š

[Stochastic dynamic programming](https://en.jinzhao.wiki/wiki/Stochastic_dynamic_programming)

**ç»„åˆè§„åˆ’**ï¼š

è‹¥è§„åˆ’é—®é¢˜ä¸æœ‰é™ä¸ªäº‹ç‰©çš„æ’åˆ—ç»„åˆæœ‰å…³ï¼Œåˆ™ç§°ä¸ºç»„åˆè§„åˆ’([combinatorial optimization](https://en.jinzhao.wiki/wiki/Combinatorial_optimization))

### å‚è€ƒæ–‡çŒ®

[10-1] Rabiner L,Juang B. [An introduction to hidden markov Models](http://ai.stanford.edu/~pabbeel/depth_qual/Rabiner_Juang_hmms.pdf). IEEE ASSPMagazine,January 1986

[10-2] Rabiner L. [A tutorial on hidden Markov models and selected applications in speechrecognition](https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf). Proceedings of IEEE,1989

[10-3] Baum L,et al. [A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177697196). Annals of Mathematical Statistics,1970,41: 164â€“171

[10-4] Bilmes JA. [A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models](https://people.ece.uw.edu/bilmes/p/mypubs/bilmes1997-em.pdf).

[10-5] Lari K,Young SJ. Applications of stochastic context-free grammars using the Inside-Outside algorithm,Computer Speech & Language,1991,5(3): 237â€“257

[10-6] Ghahramani Z. [Learning Dynamic Bayesian Networks](https://courses.cs.duke.edu//compsci662/current/pdf/ghahramani.1998.pdf). Lecture Notes in ComputerScience,Vol. 1387,1997,168â€“197

ä»¥ä¸‹æ¥è‡ª[éšé©¬å°”å¯å¤«æ¨¡å‹](http://infolab.stanford.edu/~jiali/hmm.html)

[10-7] J. Li, A. Najmi, R. M. Gray, `Image classification by a two dimensional hidden Markov model`,IEEE Transactions on Signal Processing , 48(2):517-33, February 2000. [2-D HMM] ([download](http://www.stat.psu.edu/~jiali/pub/sp00.pdf))

[10-8] J. Li, R. M. Gray, R. A. Olshen, `Multiresolution image classification by hierarchical modeling with two dimensional hidden Markov models`, IEEE Transactions on Information Theory , 46(5):1826-41, August 2000. [2-D MHMM] ([download](http://www.stat.psu.edu/~jiali/pub/it00.pdf))

[10-9] J. Li, W. Miller, `Significance of inter-species matches when evolutionary rate varies`, Journal of Computational Biology , 10(3-4):537-554, 2003. [HMMMO] ([download](http://www.stat.psu.edu/~jiali/pub/jcb03.pdf))

[10-10] J. Li, J. Z. Wang, `Studying digital imagery of ancient paintings by mixtures of stochastic models`, IEEE Transactions on Image Processing, 12(3):340-353, 2004. [Mixture of 2-D MHMMs] ([download](http://www-db.stanford.edu/~wangz/project/imsearch/ART/TIP03/li_ip.pdf))

## ç¬¬ 11 ç«  æ¡ä»¶éšæœºåœº
**æ¡ä»¶éšæœºåœº**ï¼ˆ[Conditional random field, CRF](https://en.jinzhao.wiki/wiki/Conditional_random_field)ï¼‰æ¡ä»¶éšæœºåœº(CRFs)æ˜¯ä¸€ç±»å¸¸ç”¨çš„ç»Ÿè®¡å»ºæ¨¡æ–¹æ³•ï¼ˆ[statistical modeling methods](https://en.jinzhao.wiki/wiki/Statistical_model)ï¼‰ï¼Œå¸¸ç”¨äºæ¨¡å¼è¯†åˆ«ï¼ˆ[pattern recognition](https://en.jinzhao.wiki/wiki/Pattern_recognition)ï¼‰å’Œæœºå™¨å­¦ä¹ ï¼Œå¹¶ç”¨äºç»“æ„é¢„æµ‹ï¼ˆ[structured prediction](https://en.jinzhao.wiki/wiki/Structured_prediction)ï¼‰ã€‚

ç›¸å…³çš„æœºå™¨å­¦ä¹ åº“æœ‰[PyStruct](https://github.com/pystruct/pystruct)å’Œ[python-crfsuite](https://github.com/scrapinghub/python-crfsuite)

è¿™é‡Œæ¨èå­¦ä¹ ï¼š[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åä¸ƒ)-æ¡ä»¶éšæœºåœºCRFï¼ˆConditional Random Fieldï¼‰](https://www.bilibili.com/video/BV19t411R7QU) ä»¥åŠè®ºæ–‡[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)

æ¡ä»¶éšæœºåœºæ˜¯åœ¨æ— å‘å›¾ä¸Šçš„åˆ¤åˆ«æ¨¡å‹ã€‚

æ¡ä»¶éšæœºåœºæ˜¯ç»™å®šä¸€ç»„è¾“å…¥éšæœºå˜é‡æ¡ä»¶ä¸‹å¦ä¸€ç»„è¾“å‡ºéšæœºå˜é‡çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹ï¼Œå…¶ç‰¹ç‚¹æ˜¯å‡è®¾è¾“å‡ºéšæœºå˜é‡æ„æˆé©¬å°”å¯å¤«éšæœºåœºã€‚
æ¡ä»¶éšæœºåœºå¯ä»¥ç”¨äºä¸åŒçš„é¢„æµ‹é—®é¢˜ï¼Œæœ¬ä¹¦ä»…è®ºåŠå®ƒåœ¨æ ‡æ³¨é—®é¢˜çš„åº”ç”¨ã€‚å› æ­¤ä¸»è¦è®²è¿°çº¿æ€§é“¾ï¼ˆlinear   chainï¼‰æ¡ä»¶éšæœºåœºï¼Œè¿™æ—¶ï¼Œé—®é¢˜å˜æˆäº†ç”±è¾“å…¥åºåˆ—å¯¹è¾“å‡ºåºåˆ—é¢„æµ‹çš„åˆ¤åˆ«æ¨¡å‹ï¼Œå½¢å¼ä¸ºå¯¹æ•°çº¿æ€§æ¨¡å‹ï¼Œå…¶å­¦ä¹ æ–¹æ³•é€šå¸¸æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚


**æ¡ä»¶éšæœºåœº**ï¼ˆconditional  random  fieldï¼‰æ˜¯ç»™å®šéšæœºå˜é‡Xæ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡Yçš„é©¬å°”å¯å¤«éšæœºåœºã€‚
è®¾$X$ä¸$Y$æ˜¯éšæœºå˜é‡ï¼Œ$P(Y|X)$æ˜¯åœ¨ç»™å®šXçš„æ¡ä»¶ä¸‹$Y$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚è‹¥éšæœºå˜é‡$Y$æ„æˆä¸€ä¸ªç”±æ— å‘å›¾$Gï¼(V,E)$è¡¨ç¤ºçš„é©¬å°”å¯å¤«éšæœºåœºï¼Œå³
$$p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \neq v) = p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \sim v)$$
å¯¹ä»»æ„ç»“ç‚¹$v$æˆç«‹ï¼Œåˆ™ç§°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(Y|X)$ä¸ºæ¡ä»¶éšæœºåœºã€‚å¼ä¸­$w \sim v$è¡¨ç¤ºåœ¨å›¾$Gï¼(V,E)$ä¸­ä¸ç»“ç‚¹$v$æœ‰è¾¹è¿æ¥çš„æ‰€æœ‰ç»“ç‚¹$w$ï¼Œ$w \neq v$è¡¨ç¤ºç»“ç‚¹$v$ä»¥å¤–çš„æ‰€æœ‰ç»“ç‚¹ï¼Œ$Y_v$ï¼Œ$Y_u$ä¸$Y_w$ä¸ºç»“ç‚¹$v$ï¼Œ$u$ä¸$w$å¯¹åº”çš„éšæœºå˜é‡ã€‚


**çº¿æ€§é“¾æ¡ä»¶éšæœºåœº**ï¼ˆlinear chain conditional random fieldï¼‰å‡è®¾Xå’ŒYæœ‰ç›¸åŒçš„å›¾ç»“æ„ã€‚
> æ¡ä»¶éšæœºåœºåœ¨å®šä¹‰ä¸­å¹¶æ²¡æœ‰è¦æ±‚Xå’ŒYå…·æœ‰ç›¸åŒçš„ç»“æ„ã€‚ç°å®ä¸­ï¼Œä¸€èˆ¬å‡è®¾Xå’ŒYæœ‰ç›¸åŒçš„å›¾ç»“æ„ã€‚

è®¾$Xï¼(X_1,X_2,...,X_n)ï¼ŒYï¼(Y_1ï¼ŒY_2,...,Y_n)$å‡ä¸ºçº¿æ€§é“¾è¡¨ç¤ºçš„éšæœºå˜é‡åºåˆ—ï¼Œè‹¥åœ¨ç»™å®šéšæœºå˜é‡åºåˆ—$X$çš„æ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡åºåˆ—$Y$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(Y|X)$æ„æˆæ¡ä»¶éšæœºåœºï¼Œå³æ»¡è¶³é©¬å°”å¯å¤«æ€§
$$P(Y_i|X,Y_1,...,Y_{i-1},Y_{i+1},...,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})\\ i=1,2,...,n(å½“i=1å’Œnæ—¶åªè€ƒè™‘å•è¾¹)$$
åˆ™ç§°$P(Y|X)$ä¸ºçº¿æ€§é“¾æ¡ä»¶éšæœºåœºã€‚

```mermaid
graph LR
    Y1(("Yâ‚"))
    Y2(("Yâ‚‚"))
    Yi(("Yáµ¢"))
    Yn(("Yâ‚™"))
    Xg(("Xâ‚:â‚™"))
    Y1---Y2-.-Yi-.-Yn
    Y1---Xg
    Y2---Xg
    Yi---Xg
    Xg---Yn
    style Y1 fill:#fff
    style Y2 fill:#fff
    style Yi fill:#fff
    style Yn fill:#fff
    style Xg fill:#f96
```

çº¿æ€§é“¾æ¡ä»¶éšæœºåœºå¯ä»¥ç”¨äºæ ‡æ³¨ç­‰é—®é¢˜ã€‚
åœ¨æ ‡æ³¨é—®é¢˜ä¸­ï¼Œ$X$è¡¨ç¤ºè¾“å…¥è§‚æµ‹åºåˆ—ï¼Œ$Y$è¡¨ç¤ºå¯¹åº”çš„è¾“å‡ºæ ‡è®°åºåˆ—æˆ–çŠ¶æ€åºåˆ—ã€‚

è¿™æ—¶ï¼Œåœ¨æ¡ä»¶æ¦‚ç‡æ¨¡å‹P(Y|X)ä¸­ï¼ŒYæ˜¯è¾“å‡ºå˜é‡ï¼Œè¡¨ç¤ºæ ‡è®°åºåˆ—ï¼ŒXæ˜¯è¾“å…¥å˜é‡ï¼Œè¡¨ç¤ºéœ€è¦æ ‡æ³¨çš„è§‚æµ‹åºåˆ—ã€‚ä¹ŸæŠŠæ ‡è®°åºåˆ—ç§°ä¸ºçŠ¶æ€åºåˆ—ã€‚
å­¦ä¹ æ—¶ï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®é›†é€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°æ¡ä»¶æ¦‚ç‡æ¨¡å‹$\hat{P}(Y|X)$ï¼›
é¢„æµ‹æ—¶ï¼Œå¯¹äºç»™å®šçš„è¾“å…¥åºåˆ—xï¼Œæ±‚å‡ºæ¡ä»¶æ¦‚ç‡$\hat{P}(Y|X)$æœ€å¤§çš„è¾“å‡ºåºåˆ—$\hat{y}$ã€‚


æ ¹æ®**æ— å‘å›¾çš„å› å­åˆ†è§£**ï¼Œå¾—ï¼š
$$P(Y|X) = \frac{1}{Z} exp\sum_{i=1}^K F_i(x_{ci})$$
æ ¹æ®CRFçš„æ¦‚ç‡æ— å‘å›¾è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å®é™…çš„èŠ‚ç‚¹å¸¦å…¥è¿›å»ï¼ˆæœ€å¤§å›¢$y_{t-1},y_t,x_{1:T}$ï¼‰ï¼Œï¼ˆä¸ºäº†è¡¨è¾¾çš„æ–¹ä¾¿ï¼Œå‡‘ä¸€ä¸ªy0ï¼‰æœ‰ï¼š
$$P(Y|X) = \frac{1}{Z} exp\sum_{t=1}^T F(y_{t-1},y_t,x_{1:T})$$
å°†$F(y_{t-1},y_t,x_{1:T})$åˆ†è§£ä¸º2ä¸ªéƒ¨åˆ†ï¼Œå³ï¼š$x_{1:T}$ï¼ˆå·²çŸ¥çš„ï¼‰å¯¹$y_t$çš„å½±å“ä»¥åŠ$y_{t-1},y_t$é—´çš„å½±å“ã€‚æ•°å­¦åŒ–è¡¨ç¤ºä¸ºï¼š
$$F(y_{t-1},y_t,x_{1:T})=\triangle y_{t},x_{1:T} + \triangle y_{t-1},y_{t},x_{1:T}$$
å…¶å®è¿˜æœ‰ä¸ª$\triangle y_{t-1},x_{1:T}$ï¼Œå› ä¸ºè¿™æ˜¯ä¸Šä¸€ä¸ªçš„çŠ¶æ€ï¼Œå¯¹äºtæ—¶åˆ»æ˜¯å·²çŸ¥çš„ï¼Œè¿™é‡Œå¿½ç•¥äº†ã€‚
å…¶ä¸­ï¼Œ$\triangle y_t,x_{1:T}$ä¸º**çŠ¶æ€å‡½æ•°**ï¼Œå³è¡¨ç¤ºä¸ºåœ¨$t$ä½ç½®ä¸Šçš„èŠ‚ç‚¹$y_t$çŠ¶æ€ï¼›
$\triangle y_{t-1},y_t,x_{1:T}$ä¸º**è½¬ç§»å‡½æ•°**ï¼Œå³è¡¨ç¤ºå½“å‰èŠ‚ç‚¹$y_t$ä¸ä¸Šä¸€ä¸ªèŠ‚ç‚¹$y_{t-1}$çš„ç›¸å…³æ€§ã€‚
å®šä¹‰åœ¨ğ‘Œä¸Šä¸‹æ–‡çš„**å±€éƒ¨ç‰¹å¾å‡½æ•°**$t_k$ï¼Œè¿™ç±»ç‰¹å¾å‡½æ•°åªå’Œå½“å‰èŠ‚ç‚¹å’Œä¸Šä¸€ä¸ªèŠ‚ç‚¹æœ‰å…³ï¼Œå³ä¸ºä¸Šé¢çš„
$$\triangle y_{t-1},y_t,x_{1:T} =\sum_{k=1}^K \lambda_k t_k(y_{i-1},y_i,X,i),k=1,2,..,K$$
å…¶ä¸­$K$æ˜¯å®šä¹‰åœ¨è¯¥èŠ‚ç‚¹çš„å±€éƒ¨ç‰¹å¾å‡½æ•°çš„æ€»ä¸ªæ•°ï¼Œ$i$æ˜¯å½“å‰èŠ‚ç‚¹åœ¨åºåˆ—çš„ä½ç½®ã€‚$\lambda_k$ä¸ºç‰¹å¾å‡½æ•°çš„ä¿¡ä»»åº¦ã€‚
å®šä¹‰åœ¨ğ‘ŒèŠ‚ç‚¹ä¸Šçš„**èŠ‚ç‚¹ç‰¹å¾å‡½æ•°**ï¼Œè¿™ç±»ç‰¹å¾å‡½æ•°åªå’Œå½“å‰èŠ‚ç‚¹æœ‰å…³ï¼Œå³ä¸ºä¸Šé¢çš„
$$\triangle y_t,x_{1:T} =\sum_{l=1}^L \mu_l s_l(y_i,X,i),l=1,2,â€¦,L$$
å…¶ä¸­$L$æ˜¯å®šä¹‰åœ¨è¯¥èŠ‚ç‚¹çš„èŠ‚ç‚¹ç‰¹å¾å‡½æ•°çš„æ€»ä¸ªæ•°ï¼Œ$i$æ˜¯å½“å‰èŠ‚ç‚¹åœ¨åºåˆ—çš„ä½ç½®ã€‚$\mu_l$ä¸ºç‰¹å¾å‡½æ•°çš„ä¿¡ä»»åº¦ã€‚
ä¸ºäº†ä½¿ç‰¹å¾å‡½æ•°make senseï¼ˆæœ‰é“ç†ï¼Œåˆä¹æƒ…ç†; å¯ä»¥ç†è§£;è®²å¾—é€šï¼‰ï¼Œä¸€èˆ¬æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œå³å–å€¼é0å³1ã€‚æ— è®ºæ˜¯èŠ‚ç‚¹ç‰¹å¾å‡½æ•°è¿˜æ˜¯å±€éƒ¨ç‰¹å¾å‡½æ•°ï¼Œå®ƒä»¬çš„å–å€¼åªèƒ½æ˜¯0æˆ–è€…1ã€‚å³æ»¡è¶³ç‰¹å¾æ¡ä»¶æˆ–è€…ä¸æ»¡è¶³ç‰¹å¾æ¡ä»¶ã€‚
å¦‚ï¼š
$$t_k\{y_{t-1}=åè¯, y_t=åŠ¨è¯, x_{1:T}\} = 1 \\ t_k\{y_{t-1}=åè¯, y_t=åŠ©è¯, x_{1:T}\} = 0$$
æ‰€ä»¥linear-chain-CRFçš„å‚æ•°åŒ–å½¢å¼ä¸ºï¼š
$$P(Y|X)=\frac{1}{Z(X)}exp \sum_{i=1}^ T   \bigg (\sum_{k=1}^K \lambda_k t_k (y_{i-1},y_i,X,i)  +\sum_{l=1}^L \mu_l s_l (y_i,X,i)\bigg )$$
$Y$è¡¨ç¤ºçš„æ˜¯æ ‡æ³¨åºåˆ—ï¼Œæ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œé•¿åº¦ä¸º$T$ï¼›$X = x_{1:T}$è¡¨ç¤ºçš„è¯è¯­åºåˆ—ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œé•¿åº¦ä¹Ÿä¸º$T$ã€‚
å…¶ä¸­ï¼Œ$Z(X)$ä¸ºè§„èŒƒåŒ–å› å­ï¼š
$$Z(X)=\sum_Y exp \sum_{i=1}^T \bigg(\sum_{k=1}^K\lambda_k t_k (y_{i-1},y_i,X,i) +\sum_{l}^L\mu_l s_l (y_i,X,i)\bigg)$$
**æ¨¡å‹çš„ç®€åŒ–è¡¨ç¤º-æ•°å€¼è¡¨ç¤º**
å‡è®¾ï¼Œå…±æœ‰$K=K_1+K_2$ä¸ªç‰¹å¾å‡½æ•°ï¼Œå…¶ä¸­ï¼Œ$K_1$ä¸ªå±€éƒ¨ç‰¹å¾å‡½æ•°$t_k$ï¼Œ$K_2$ä¸ªèŠ‚ç‚¹ç‰¹å¾å‡½æ•°$s_l$ã€‚æˆ‘ä»¬ç”¨1ä¸ªç‰¹å¾å‡½æ•°$f_k(y_{i-1},y_i,X,i)$æ¥ç»Ÿä¸€è¡¨ç¤ºï¼š
$$\begin{aligned}f_k(y_{i-1},y_i,X,i)=\left\{\begin{aligned} & t_k (y_{i-1},y_i,X,i)  \qquad k = 1,2,..,K_1 \\ &  s_l (y_i,X,i)  \qquad k = K_1+l,l=1,2,â€¦,K_2    \end{aligned}\right.\end{aligned}$$
å¯¹$f_k(y_{i-1},y_i,X,i)$åœ¨å„ä¸ªåºåˆ—ä½ç½®æ±‚å’Œå¾—åˆ°ï¼š
$$\begin{aligned}f_k(Y,X)=\sum_{i=1}^T f_k(y_{i-1},y_i,X,i)\end{aligned}$$
åŒæ—¶ä¹Ÿç»Ÿä¸€$f_k(y_{i-1},y_i,x,i)$å¯¹åº”çš„æƒé‡ç³»æ•°$w_k$ï¼š
$$\begin{aligned}w_k=\left\{   \begin{aligned} & \lambda_k \qquad k = 1,2,..,K_1 \\ & \mu_l   \qquad k = K_1+l,l=1,2,â€¦,K_2    \end{aligned}\right.\end{aligned}$$
è¿™æ ·ï¼ŒLinear-chain-CRFçš„ç®€åŒ–å·¥ä½œå°±åˆ°è¿™é‡Œç»“æŸå•¦ï¼š
$$\begin{aligned}P(Y|X)=\frac{1}{Z(X)}exp\sum_{k=1}^K w_kf_k(Y,X)\end{aligned}$$
å…¶ä¸­ï¼Œè§„èŒƒåŒ–å› å­ï¼š
$$\begin{aligned}Z(X)=\sum_Y exp\sum_{k=1}^Kw_kf_k(Y,X)\end{aligned}$$
**æ¨¡å‹çš„ç®€åŒ–è¡¨ç¤º-å‘é‡è¡¨ç¤º**
å¦‚æœå¯¹$f_k(Y,X)$å’Œ$w_k$è¿›è¡Œå‘é‡åŒ–è¡¨ç¤ºï¼Œ$F(Y,X)$å’Œ$W$éƒ½æ˜¯$K \times 1$çš„åˆ—å‘é‡ï¼š
$$\begin{aligned}W  =\left [ \begin{aligned}    w_1\\    w_2\\    â€¦\\    w_K \end{aligned}\right]\end{aligned}$$
$$\begin{aligned}F(Y,X) =\left[    \begin{aligned}   f_1(Y,X)\\   f_2(Y,X)\\   â€¦â€¦â€¦\\   f_K(Y,X)    \end{aligned}\right]\end{aligned}$$
é‚£ä¹ˆLinear-chain-CRFçš„å‘é‡å†…ç§¯å½¢å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$\begin{aligned}P_W(Y|X) = \frac{exp(W \bullet F(Y,X))}{Z(X,W)}\\   = \frac{exp(W \bullet F(Y,X))}{\sum_Y exp(W \bullet F(Y,X))}\end{aligned}$$
å‘é‡åŒ–çš„æ„ä¹‰ï¼š
å°±æ˜¯ä¸ºäº†å¹²æ‰è¿åŠ çš„å½¢å¼ï¼Œä¸ºåé¢çš„è®­ç»ƒæä¾›æ›´åŠ åˆç†çš„è®¡ç®—æ”¯æŒã€‚
**è¦è§£å†³çš„ä¸‰ä¸ªé—®é¢˜**
1. Inferenceï¼ˆæ¦‚ç‡è®¡ç®—é—®é¢˜ï¼‰ï¼šè®¡ç®—æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå³ç»™å®šXåºåˆ—ï¼Œç®—å‡ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ‰€å¯¹åº”æ ‡æ³¨çš„æ¦‚ç‡ï¼Œå³ï¼š$P(y_t|X)$
1. Learningï¼šæŠŠå‚æ•°å­¦ä¹ å‡ºæ¥ï¼ˆparameter estimationï¼‰ï¼Œä¹Ÿå°±æ˜¯ç»™å®š$N$ä¸ªè®­ç»ƒæ•°æ®ï¼Œæ±‚ä¸Šé¢å‘é‡è¡¨ç¤ºçš„$W$çš„å‚æ•°å€¼ï¼Œå³ï¼š$\hat{W}=argmax\prod_{i=1}^N P(Y^{(i)}|X^{(i)})$
1. Decodingï¼šç»™å®šXåºåˆ—ï¼Œæ‰¾åˆ°ä¸€ä¸ªæœ€æœ‰å¯èƒ½çš„æ ‡æ³¨åºåˆ—ï¼Œå³ï¼š$\hat{Y}=argmax P(Y|X)$ï¼Œå…¶ä¸­ï¼Œ$Y=y_1y_2..y_T$

**Inferenceï¼šæ¡ä»¶æ¦‚ç‡(å‰å‘-åå‘)**
**Learning(å‚æ•°ä¼°è®¡)**
**Decoding(Vitebi)**

- **æ¨¡å‹**ï¼š
å‘é‡å†…ç§¯å½¢å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$\begin{aligned}P_W(Y|X) = \frac{exp(W \bullet F(Y,X))}{Z(X,W)}\\ = \frac{exp(W \bullet F(Y,X))}{\sum_Y exp(W \bullet F(Y,X))}\end{aligned}$$
- **ç­–ç•¥**ï¼š
$$\hat{W}=\mathop{\arg\max}\limits_{W} \prod_{i=1}^N P(Y^{(i)}|X^{(i)})$$
- **ç®—æ³•**ï¼š
æ”¹è¿›çš„è¿­ä»£å°ºåº¦ç®—æ³•ã€æ¢¯åº¦ä¸‹é™æ³•ã€æ‹Ÿç‰›é¡¿æ³•

> å‚è€ƒ[ã€NLPã€‘ä»éšé©¬å°”ç§‘å¤«åˆ°æ¡ä»¶éšæœºåœº](https://anxiang1836.github.io/2019/11/05/NLP_From_HMM_to_CRF/) ä»¥åŠè§†é¢‘[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åä¸ƒ)-æ¡ä»¶éšæœºåœºCRFï¼ˆConditional Random Fieldï¼‰](https://www.bilibili.com/video/BV19t411R7QU)

### é™„åŠ çŸ¥è¯†

#### éšæœºåœº

**éšæœºåœº**ï¼ˆ[Random field, RF](https://en.jinzhao.wiki/wiki/Random_field)ï¼‰æ˜¯ç”±è‹¥å¹²ä¸ªä½ç½®ç»„æˆçš„æ•´ä½“ï¼Œå½“ç»™æ¯ä¸€ä¸ªä½ç½®ä¸­æŒ‰ç…§æŸç§åˆ†å¸ƒï¼ˆæˆ–è€…æ˜¯æŸç§æ¦‚ç‡ï¼‰éšæœºèµ‹äºˆä¸€ä¸ªå€¼ä¹‹åï¼Œå…¶å…¨ä½“å°±å«åšéšæœºåœºã€‚

ä»¥è¯æ€§æ ‡æ³¨ä¸ºä¾‹ï¼š

å‡å¦‚æˆ‘ä»¬æœ‰10ä¸ªè¯å½¢æˆçš„å¥å­éœ€è¦åšè¯æ€§æ ‡æ³¨ã€‚è¿™10ä¸ªè¯æ¯ä¸ªè¯çš„è¯æ€§å¯ä»¥åœ¨æˆ‘ä»¬å·²çŸ¥çš„è¯æ€§é›†åˆï¼ˆåè¯ï¼ŒåŠ¨è¯â€¦ï¼‰ä¸­å»é€‰æ‹©ã€‚å½“æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯é€‰æ‹©å®Œè¯æ€§åï¼Œè¿™å°±å½¢æˆäº†ä¸€ä¸ªéšæœºåœºã€‚

**é©¬å°”ç§‘å¤«éšæœºåœº**ï¼ˆ[Markov random field, MRF](https://en.jinzhao.wiki/wiki/Markov_random_field)ï¼‰æ˜¯éšæœºåœºçš„ç‰¹ä¾‹ï¼Œå®ƒå‡è®¾éšæœºåœºä¸­æŸä¸€ä¸ªä½ç½®çš„èµ‹å€¼ä»…ä»…ä¸å’Œå®ƒç›¸é‚»çš„ä½ç½®çš„èµ‹å€¼æœ‰å…³ï¼Œå’Œä¸å…¶ä¸ç›¸é‚»çš„ä½ç½®çš„èµ‹å€¼æ— å…³ã€‚
æ¢ä¸€ç§è¡¨ç¤ºæ–¹å¼ï¼ŒæŠŠé©¬å°”ç§‘å¤«éšæœºåœºæ˜ å°„åˆ°æ— å‘å›¾ä¸­ã€‚æ­¤æ— å‘å›¾ä¸­çš„èŠ‚ç‚¹éƒ½ä¸æŸä¸ªéšæœºå˜é‡ç›¸å…³ï¼Œè¿æ¥ç€èŠ‚ç‚¹çš„è¾¹ä»£è¡¨ä¸è¿™ä¸¤ä¸ªèŠ‚ç‚¹æœ‰å…³çš„éšæœºå˜é‡ä¹‹é—´çš„å…³ç³»ã€‚
ç»§ç»­è¯æ€§æ ‡æ³¨ä¸ºä¾‹ï¼šï¼ˆè¿˜æ˜¯10ä¸ªè¯çš„å¥å­ï¼‰
å¦‚æœæˆ‘ä»¬å‡è®¾æ‰€æœ‰è¯çš„è¯æ€§ä»…ä¸å’Œå®ƒç›¸é‚»çš„è¯çš„è¯æ€§æœ‰å…³æ—¶ï¼Œè¿™ä¸ªéšæœºåœºå°±ç‰¹åŒ–æˆä¸€ä¸ªé©¬å°”ç§‘å¤«éšæœºåœºã€‚
æ¯”å¦‚ç¬¬3ä¸ªè¯çš„è¯æ€§é™¤äº†ä¸è‡ªå·±æœ¬èº«çš„ä½ç½®æœ‰å…³å¤–ï¼Œåªä¸ç¬¬2ä¸ªè¯å’Œç¬¬4ä¸ªè¯çš„è¯æ€§æœ‰å…³ã€‚

**æ¡ä»¶éšæœºåœº**(CRF)æ˜¯é©¬å°”ç§‘å¤«éšæœºåœºçš„ç‰¹ä¾‹ï¼Œå®ƒå‡è®¾é©¬å°”ç§‘å¤«éšæœºåœºä¸­åªæœ‰ğ‘‹å’Œğ‘Œä¸¤ç§å˜é‡ï¼Œğ‘‹ä¸€èˆ¬æ˜¯ç»™å®šçš„ï¼Œè€Œğ‘Œä¸€èˆ¬æ˜¯åœ¨ç»™å®šğ‘‹çš„æ¡ä»¶ä¸‹æˆ‘ä»¬çš„è¾“å‡ºã€‚è¿™æ ·é©¬å°”ç§‘å¤«éšæœºåœºå°±ç‰¹åŒ–æˆäº†æ¡ä»¶éšæœºåœºã€‚

åœ¨æˆ‘ä»¬10ä¸ªè¯çš„å¥å­è¯æ€§æ ‡æ³¨çš„ä¾‹å­ä¸­ï¼Œğ‘‹æ˜¯è¯ï¼Œğ‘Œæ˜¯è¯æ€§ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾å®ƒæ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«éšæœºåœºï¼Œé‚£ä¹ˆå®ƒä¹Ÿå°±æ˜¯ä¸€ä¸ªCRFã€‚
å¯¹äºCRFï¼Œæˆ‘ä»¬ç»™å‡ºå‡†ç¡®çš„æ•°å­¦è¯­è¨€æè¿°ï¼š
è®¾ğ‘‹ä¸ğ‘Œæ˜¯éšæœºå˜é‡ï¼ŒP(ğ‘Œ|ğ‘‹)æ˜¯ç»™å®šğ‘‹æ—¶ğ‘Œçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œè‹¥éšæœºå˜é‡ğ‘Œæ„æˆçš„æ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«éšæœºåœºï¼Œåˆ™ç§°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(ğ‘Œ|ğ‘‹)æ˜¯æ¡ä»¶éšæœºåœºã€‚

**çº¿æ€§é“¾æ¡ä»¶éšæœºåœº**(Linear-CRF)
æ³¨æ„åœ¨CRFçš„å®šä¹‰ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¦æ±‚ğ‘‹å’Œğ‘Œæœ‰ç›¸åŒçš„ç»“æ„ã€‚å½“ğ‘‹å’Œğ‘Œæœ‰ç›¸åŒç»“æ„ï¼Œå³ï¼š
$$X=(x_1,x_2,â€¦,x_T),Y=(y_1,y_2,â€¦,y_T)$$
è¿™ä¸ªæ—¶å€™ï¼Œğ‘‹å’Œğ‘Œæœ‰ç›¸åŒçš„ç»“æ„çš„CRFå°±æ„æˆäº†çº¿æ€§é“¾æ¡ä»¶éšæœºåœºã€‚

#### MEMM(Maximum Entropy Markov Model)
åˆ¤åˆ«æ¨¡å‹
[Maximum Entropy Markov Models for Information Extraction and Segmentation](http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf)
[Maximum Entropy Markov Models](http://www.cs.cornell.edu/courses/cs778/2006fa/lectures/05-memm.pdf)
[Hidden Markov Model and Naive Bayes relationship](https://aman.ai/primers/ai/hmm-and-naive-bayes/)
[Maximum Entropy Markov Models and Logistic Regression](https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/)

[Maximum-Entropy Markov Model](https://devopedia.org/maximum-entropy-markov-model)

![](https://devopedia.org/images/article/225/8864.1570601314.png)
MEMMä¸HMM
![](https://devopedia.org/images/article/225/6824.1570601351.png)

#### æ¦‚ç‡å›¾æ¨¡å‹
ä»‹ç»æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆProbabilistic Graphical Modelï¼‰ä¹‹å‰ï¼Œå…ˆç®€å•äº†è§£ä¸‹**ç»“æ„å­¦ä¹ **ï¼ˆ[Structured Learning](https://en.jinzhao.wiki/wiki/Structured_prediction)ï¼‰ï¼Œç›¸æ¯”äºå›å½’ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡æˆ–è€…é¢„æµ‹ï¼Œè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œç»“æ„åŒ–å­¦ä¹ çš„è¾“å‡ºæ›´åŠ å¤æ‚ï¼Œå¯ä»¥æ˜¯å›¾åƒï¼Œå¯ä»¥æ˜¯è¯­å¥ï¼Œå¯ä»¥æ˜¯æ ‘ç»“æ„ï¼Œç­‰ã€‚
é‚£ä¹ˆä¸æ¦‚ç‡å›¾æ¨¡å‹æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ
æ¦‚ç‡å›¾å½¢æ¨¡å‹å½¢æˆäº†å¤§é‡çš„ç»“æ„åŒ–é¢„æµ‹æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œè´å¶æ–¯ç½‘ç»œå’Œéšæœºåœºå¾ˆå—æ¬¢è¿ã€‚[å‚è§](https://en.jinzhao.wiki/wiki/Structured_prediction#Techniques)


[ä»€ä¹ˆæ˜¯ç»“æ„åŒ–å­¦ä¹ ï¼ŸWhat is structured learning?](https://pystruct.github.io/intro.html#intro)
ç»“æ„åŒ–é¢„æµ‹æ˜¯ç›‘ç£å­¦ä¹ ã€åˆ†ç±»å’Œå›å½’æ ‡å‡†èŒƒå¼çš„æ¦‚æ‹¬ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°æ¥æœ€å°åŒ–è®­ç»ƒé›†ä¸Šçš„ä¸€äº›æŸå¤±ã€‚åŒºåˆ«åœ¨äºä½¿ç”¨çš„å‡½æ•°ç±»å‹å’ŒæŸå¤±ã€‚
åœ¨åˆ†ç±»ä¸­ï¼Œç›®æ ‡åŸŸæ˜¯ç¦»æ•£çš„ç±»æ ‡ç­¾ï¼ŒæŸå¤±é€šå¸¸æ˜¯0-1çš„æŸå¤±ï¼Œå³å¯¹è¯¯åˆ†ç±»è¿›è¡Œè®¡æ•°ã€‚åœ¨å›å½’ä¸­ï¼Œç›®æ ‡åŸŸæ˜¯å®æ•°ï¼ŒæŸå¤±é€šå¸¸æ˜¯å‡æ–¹è¯¯å·®ã€‚åœ¨ç»“æ„åŒ–é¢„æµ‹ä¸­ï¼Œç›®æ ‡åŸŸå’ŒæŸå¤±æˆ–å¤šæˆ–å°‘éƒ½æ˜¯ä»»æ„çš„ã€‚è¿™æ„å‘³ç€ç›®æ ‡ä¸æ˜¯é¢„æµ‹æ ‡ç­¾æˆ–æ•°å­—ï¼Œè€Œæ˜¯å¯èƒ½æ›´å¤æ‚çš„å¯¹è±¡ï¼Œå¦‚åºåˆ—æˆ–å›¾å½¢ã€‚


**æ¦‚ç‡å›¾æ¨¡å‹**ï¼ˆ[Probabilistic Graphical Modelï¼ŒPGM](https://en.jinzhao.wiki/wiki/Graphical_model)ï¼‰ï¼Œç®€ç§°å›¾æ¨¡å‹ï¼ˆGraphical Modelï¼ŒGMï¼‰ï¼Œæ˜¯æŒ‡ä¸€ç§ç”¨å›¾ç»“æ„æ¥æè¿°å¤šå…ƒéšæœºå˜é‡ä¹‹é—´æ¡ä»¶ç‹¬ç«‹å…³ç³»çš„æ¦‚ç‡æ¨¡å‹ï¼Œä»è€Œç»™ç ”ç©¶é«˜ç»´ç©ºé—´ä¸­çš„æ¦‚ç‡æ¨¡å‹å¸¦æ¥äº†å¾ˆå¤§çš„ä¾¿æ·æ€§ã€‚
å¾ˆå¤šæœºå™¨å­¦ä¹ æ¨¡å‹éƒ½å¯ä»¥å½’ç»“ä¸ºæ¦‚ç‡æ¨¡å‹ï¼Œå³å»ºæ¨¡è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼å› æ­¤ï¼Œå›¾æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„è§’åº¦æ¥è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸”è¿™ç§è§’åº¦æœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œæ¯”å¦‚äº†è§£ä¸åŒæœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„è”ç³»ï¼Œæ–¹ä¾¿è®¾è®¡æ–°æ¨¡å‹ï¼ˆDeveloping Bayesian networksï¼‰ç­‰ï¼åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå›¾æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°ç”¨æ¥è®¾è®¡å’Œåˆ†æå„ç§å­¦ä¹ ç®—æ³•ï¼


**å›¾æ¨¡å‹æœ‰ä¸‰ä¸ªåŸºæœ¬é—®é¢˜**ï¼š
1. è¡¨ç¤ºï¼ˆRepresentationï¼‰é—®é¢˜ï¼šå¯¹äºä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œå¦‚ä½•é€šè¿‡å›¾ç»“æ„æ¥æè¿°å˜é‡ä¹‹é—´çš„ä¾
èµ–å…³ç³»ï¼
1. å­¦ä¹ ï¼ˆLearningï¼‰é—®é¢˜ï¼šå›¾æ¨¡å‹çš„å­¦ä¹ åŒ…æ‹¬å›¾ç»“æ„çš„å­¦ä¹ å’Œå‚æ•°çš„å­¦ä¹ ï¼åœ¨æœ¬ç« ä¸­ï¼Œ
æˆ‘ä»¬åªå…³æ³¨åœ¨ç»™å®šå›¾ç»“æ„æ—¶çš„å‚æ•°å­¦ä¹ ï¼Œå³å‚æ•°ä¼°è®¡é—®é¢˜ï¼
1. æ¨æ–­ï¼ˆInferenceï¼‰é—®é¢˜ï¼šåœ¨å·²çŸ¥éƒ¨åˆ†å˜é‡æ—¶ï¼Œè®¡ç®—å…¶ä»–å˜é‡çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ

$$
\begin{cases}
   Representation(è¡¨ç¤º) &  \begin{cases} \text{æœ‰å‘å›¾ Bayesian Network} \\ \text{æ— å‘å›¾ Markov Network} \end{cases} \\
   Learning(å­¦ä¹ ) & \begin{cases} \text{å‚æ•°å­¦ä¹ } & \begin{cases} \text{å®Œå¤‡æ•°æ®} \\ \text{éšå˜é‡} \to EM \end{cases} \\ \text{ç»“æ„å­¦ä¹ } \end{cases}\\
   Inference(æ¨æ–­) & \begin{cases} \text{ç²¾ç¡®æ¨æ–­} \\ \text{è¿‘ä¼¼æ¨æ–­} & \begin{cases} \text{ç¡®å®šæ€§è¿‘ä¼¼} \to å˜åˆ†æ¨æ–­ \\ \text{éšæœºè¿‘ä¼¼} \to MCMC \end{cases} \end{cases} \\
\end{cases}
$$


**å›¾çš„è¡¨ç¤º**ï¼š
å›¾å¯ä»¥ç”¨$G=(V,E)$è¡¨ç¤ºï¼Œ$V$æ˜¯é¡¶ç‚¹vertices(nodes or points)é›†åˆï¼Œ
${\displaystyle E\subseteq \{(x,y)\mid (x,y)\in V^{2}\;{\textrm {and}}\;x\neq y\}}$æ˜¯è¾¹çš„é›†åˆedges;å¯¹äºæœ‰å‘å›¾è€Œè¨€ï¼Œè¾¹æ˜¯æœ‰å‘çš„ï¼ˆdirected edges, directed links, directed lines, arrows or arcsï¼‰å®ƒä»¬æ˜¯æœ‰åºçš„é¡¶ç‚¹å¯¹ï¼Œä»£è¡¨ç€æ–¹å‘;å¯¹äºæ— å‘å›¾è€Œè¨€ï¼Œè¾¹æ˜¯æ— å‘çš„ã€‚

ä¹Ÿæœ‰äº›åœ°æ–¹æœ‰å‘è¾¹ä¸€èˆ¬ç”¨å°–æ‹¬å·è¡¨ç¤º<>ï¼›è€Œæ— å‘è¾¹ä¸€èˆ¬ç”¨å¼§å½¢æ‹¬å·è¡¨ç¤ºï¼ˆï¼‰ï¼›å¦‚ï¼š
æœ‰å‘å›¾ï¼š
$$G1=(V,E) \\ V(G1)=\{v1,v2,v3\}\\  E(G1)=\{\braket{v1,v2},\braket{v1,v3},\braket{v2,v3}\}$$

```mermaid
graph LR
    v1(("v1"))-->v2(("v2"))
    v1-->v3(("v3"))
    v2-->v3
```

æ— å‘å›¾ï¼š
$$G2=(V,E) \\ V(G2)=\{v1,v2,v3,v4\} \\ E(G2)=\{(vl,v2),(v1,v3),(v1,v4),(v2,v3),(v2,v4),(v3,v4)\}$$
```mermaid
graph LR
    v1(("v1"))---v2(("v2"))
    v1---v3(("v3"))
    v1---v4(("v4"))
    v2---v3
    v2---v4
    v3---v4
```
##### ï¼ˆæ¦‚ç‡ï¼‰æœ‰å‘å›¾æ¨¡å‹
æœ‰å‘å›¾æ¨¡å‹ï¼ˆDirected Graphical Modelï¼‰åˆç§°è´å¶æ–¯ç½‘ç»œï¼ˆ[Bayesian Network](https://en.jinzhao.wiki/wiki/Bayesian_network)ï¼‰æˆ–ä¿¡å¿µç½‘ç»œï¼ˆBelief Networkï¼ŒBNï¼‰æˆ–ï¼ˆcausal networksï¼‰æ˜¯ä¸€ç±»ç”¨æœ‰å‘å›¾ï¼ˆ[Directed Graphical](https://en.jinzhao.wiki/wiki/Graph_(discrete_mathematics)#Directed_graph)ï¼‰æ¥æè¿°éšæœºå‘é‡æ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å‹ï¼

> è¿™é‡Œæ˜¯ æœ‰å‘æ— ç¯å›¾(DAG)

å®šä¹‰å’Œæ¦‚ç‡ Definitions and conceptsï¼š
> parent çˆ¶èŠ‚ç‚¹
> descendants åä»£
> non-descendants éåä»£ï¼ˆä¸åŒ…æ‹¬çˆ¶ä»£ï¼Œä¹Ÿå°±æ˜¯all-parent-descendantsï¼‰

- **æ¦‚ç‡åˆ†å¸ƒçš„åˆ†è§£ï¼ˆFactorization definitionï¼‰**ï¼š
$X$æ˜¯ä¸€ä¸ªå…³äº$G$çš„è´å¶æ–¯ç½‘ç»œï¼Œå¦‚æœ$X$çš„è”åˆæ¦‚ç‡åˆ†å¸ƒ(è”åˆæ¦‚ç‡å¯†åº¦å‡½æ•°)å¯ä»¥å†™æˆã€å•ä¸ªå¯†åº¦å‡½æ•°çš„ä¹˜ç§¯ï¼Œæ¡ä»¶æ˜¯å®ƒä»¬çš„çˆ¶å˜é‡ã€‘ä¹Ÿå°±æ˜¯å±€éƒ¨æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼ˆLocal Conditional Probability Distributionï¼‰çš„è¿ä¹˜å½¢å¼ï¼ˆå¹¿ä¹‰çš„ä¸€é˜¶é©¬å¯å¤«æ€§è´¨ï¼‰:
$$p(X)=\prod _{v\in V}p\left(x_{v}\,{\big |}\,x_{\operatorname {pa} (v)}\right)$$
å…¶ä¸­$x_{\operatorname {pa} (v)}$è¡¨ç¤º$x_{v}$çš„çˆ¶äº²èŠ‚ç‚¹é›†åˆã€‚
å¦‚ï¼š
```mermaid
graph LR
    x1(("xâ‚"))-->x2(("xâ‚‚"))-->x4(("xâ‚„"))
    x1-->x3(("xâ‚ƒ"))
    x2-->x3
    x3-->x5(("xâ‚…"))
```
$X=x_1,x_2,x_3,x_4,x_5$
$V=\{x_1,x_2,x_3,x_4,x_5\}$
$E=\{\braket{x_1,x_2},\braket{x_1,x_3},\braket{x_2,x_3},\braket{x_2,x_4}\},\braket{x_3,x_5}$
$G=(V,E)$
æœ‰å‘å›¾å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸º
$$p(X) = p(x_1,x_2,x_3,x_4,x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_2)p(x_5|x_3)$$

- **å› æœç½‘ç»œ(Causal networks)**ï¼š
åœ¨è´å¶æ–¯ç½‘ç»œä¸­ï¼Œå¦‚æœä¸¤ä¸ªèŠ‚ç‚¹æ˜¯ç›´æ¥è¿æ¥çš„ï¼Œå®ƒä»¬è‚¯å®šæ˜¯éæ¡ä»¶ç‹¬ç«‹çš„ï¼Œæ˜¯ç›´æ¥å› æœå…³ç³»ï¼çˆ¶èŠ‚ç‚¹æ˜¯â€œå› â€(tail)ï¼Œå­èŠ‚ç‚¹æ˜¯â€œæœâ€ï¼ˆä¹Ÿå°±æ˜¯ç®­å¤´æŒ‡å‘çš„ï¼Œä¹Ÿç§°headï¼‰$tail \rightarrow head (å› \rightarrow æœ)$ã€‚å¦‚æœä¸¤ä¸ªèŠ‚ç‚¹ä¸æ˜¯ç›´æ¥è¿æ¥çš„ï¼Œä½†å¯ä»¥ç”±ä¸€æ¡ç»è¿‡å…¶ä»–èŠ‚ç‚¹çš„è·¯å¾„æ¥è¿æ¥ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„**æ¡ä»¶ç‹¬ç«‹æ€§**å°±æ¯”è¾ƒå¤æ‚ã€‚
ä»¥ä¸‰ä¸ªèŠ‚ç‚¹çš„è´å¶æ–¯ç½‘ç»œä¸ºä¾‹

Pattern|Model | æ¡ä»¶ç‹¬ç«‹æ€§
---|---|---
Chainï¼ˆé—´æ¥å› æœå…³ç³»/tail to headï¼‰	|$X\rightarrow Y\rightarrow Z$ | å·²çŸ¥Yæ—¶,Xå’ŒZä¸ºæ¡ä»¶ç‹¬ç«‹ï¼Œå³ $X \perp \!\!\!\perp Z\mid Y$
Forkï¼ˆå…±å› å…³ç³»/tail to tailï¼‰|$X\leftarrow Y\rightarrow Z$ | å·²çŸ¥Yæ—¶,Xå’ŒZä¸ºæ¡ä»¶ç‹¬ç«‹ï¼Œå³ $X \perp \!\!\!\perp Z \mid Y$ ï¼ˆYæœªçŸ¥æ—¶ï¼ŒXå’ŒZä¸ºä¸ç‹¬ç«‹ï¼‰
Colliderï¼ˆå…±æœå…³ç³»/head to headï¼‰	|$X\rightarrow Y\leftarrow Z$ | å·²çŸ¥Yæ—¶,Xå’ŒZä¸ºä¸ç‹¬ç«‹ï¼Œå³ $X \perp \!\!\!\perp \!\!\!\!\!\!/ \;\; Z \mid Y$ï¼ˆYæœªçŸ¥æ—¶ï¼ŒXå’ŒZä¸ºç‹¬ç«‹ï¼‰


- **å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨ï¼ˆLocal Markov propertyï¼‰**ï¼š
å¯¹ä¸€ä¸ªæ›´ä¸€èˆ¬çš„è´å¶æ–¯ç½‘ç»œï¼Œå…¶å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨ä¸ºï¼šæ¯ä¸ªéšæœºå˜é‡åœ¨ç»™å®šçˆ¶èŠ‚ç‚¹çš„æƒ…å†µä¸‹ï¼Œæ¡ä»¶ç‹¬ç«‹äºå®ƒçš„éåä»£èŠ‚ç‚¹ï¼ 
$${\displaystyle X_{v}\perp \!\!\!\perp X_{V\,\smallsetminus \,\operatorname {de} (v)}\mid X_{\operatorname {pa} (v)}\quad {\text{for all }}v\in V}$$
å…¶ä¸­$X_{V\,\smallsetminus \,\operatorname {de} (v)}$è¡¨ç¤ºéåä»£é›†åˆ

- **é©¬å°”å¯å¤«æ¯¯**ï¼ˆ[Markov blanket](https://en.jinzhao.wiki/wiki/Markov_blanket)ï¼‰ï¼š
åœ¨éšæœºå˜é‡çš„å…¨é›†U UUä¸­ï¼Œå¯¹äºç»™å®šçš„å˜é‡$X\in U$å’Œå˜é‡é›†$MB\subset U(X\notin MB)$ï¼Œè‹¥æœ‰
$$X\perp \!\!\!\perp\{U-MB-\{X\}\}|MB$$
åˆ™ç§°èƒ½æ»¡è¶³ä¸Šè¿°æ¡ä»¶çš„æœ€å°å˜é‡é›†$MB$ä¸º$X$çš„é©¬å°”å¯å¤«æ¯¯(Markov Blanket)ã€‚


- **Dåˆ’åˆ†ï¼ˆd-separationï¼‰**ï¼š
dè¡¨ç¤ºæ–¹å‘ï¼ˆdirectionalï¼‰ã€‚pæ˜¯u to vçš„å»é™¤æ–¹å‘çš„è·¯å¾„ã€‚pè¢«ä¸€ç»„èŠ‚ç‚¹Zåˆ†éš”ã€‚
  - å¦‚æœpæ˜¯è¿™æ ·çš„è·¯å¾„ ${\displaystyle u\cdots \leftarrow m\leftarrow \cdots v}$ or ${\displaystyle u\cdots \rightarrow m\rightarrow \cdots v}$ å¹¶ä¸”$m \in Z$
  - å¦‚æœpæ˜¯è¿™æ ·çš„è·¯å¾„ ${\displaystyle u\cdots \leftarrow m\rightarrow \cdots v}$ å¹¶ä¸”$m \in Z$
  - å¦‚æœpæ˜¯è¿™æ ·çš„è·¯å¾„ ${\displaystyle u\cdots \rightarrow m\leftarrow \cdots v}$ å¹¶ä¸”$m \notin Z$
$$X_{u}\perp \!\!\!\perp X_{v}\mid X_{Z}$$


- **å¸¸è§çš„æœ‰å‘å›¾æ¨¡å‹**ï¼š
å¦‚æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€éšé©¬å°”å¯å¤«æ¨¡å‹ã€æ·±åº¦ä¿¡å¿µç½‘ç»œç­‰
æœ´ç´ è´å¶æ–¯ï¼šå‡è®¾è¾“å…¥Xæœ‰ä¸‰ä¸ªç‰¹å¾
```mermaid
graph TD
    y(("y"))
    y-->x1(("xâ‚"))
    y-->x2(("xâ‚‚"))
    y-->x3(("xâ‚ƒ"))
    style y fill:#fff
    style x1 fill:#f96
    style x2 fill:#f96
    style x3 fill:#f96
```
ç”±å›¾å¯å¾—
$$P(y,x_1,x_2,x_3) = P(y)P(x_1|y)P(x_2|y)P(x_3|y) = P(x_1,x_2,x_3|y)P(y) \\ \Darr\\  P(x_1,x_2,x_3|y)=P(x_1|y)P(x_2|y)P(x_3|y)$$
è¿™ä¸å°±æ˜¯æœ´ç´ è´å¶æ–¯çš„æ¡ä»¶ç›¸äº’ç‹¬ç«‹çš„å‡è®¾ä¹ˆ?$P(X|y) = \prod_{i=1}^n P(x_i|y)$
è€Œè¿™ä¸ªç‹¬ç«‹å‡è®¾å¤ªå¼ºäº†ï¼Œæ¯ä¸ªç‰¹å¾ä¹‹é—´æ²¡æœ‰ä»»ä½•å…³ç³»ï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒi.i.d.ï¼‰ï¼›
é‚£ä¹ˆæˆ‘ä»¬å‡è®¾å½“å‰åªä¸å‰ä¸€æ—¶åˆ»æœ‰å…³ï¼Œä¸å…¶å®ƒæ— å…³ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æœ‰äº†Markovå‡è®¾ï¼Œå¦‚éšé©¬å°”å¯å¤«æ¨¡å‹ï¼š
å…¶ä¸­yä¸ºéšå˜é‡ï¼Œxä¸ºè§‚æµ‹å˜é‡
```mermaid
graph LR
    y1(("yâ‚"))
    y1-->x1(("xâ‚"))
    y1-->y2(("yâ‚‚"))
    y2-->x2(("xâ‚‚"))
    y2-->y3(("yâ‚ƒ"))
    y3-->x3(("xâ‚ƒ"))
    y3-->y4(("yâ‚„"))
    y4-->x4(("xâ‚„"))
    style y1 fill:#fff
    style y2 fill:#fff
    style y3 fill:#fff
    style y4 fill:#fff
    style x1 fill:#f96
    style x2 fill:#f96
    style x3 fill:#f96
    style x4 fill:#f96
```
æˆ‘ä»¬èƒ½ä»å›¾ä¸­ç›´æ¥å¾—åˆ°
$P(y_t|y_{t-1},...,y_1,x_{t-1},...,x_1) = P(y_t|y_{t-1})$ï¼Œå³Markovå‡è®¾ï¼›
$P(x_t|x_{T},...,x_{t+1},x_{t-1},...,x_1,Y) = P(x_t|y_{t})$ï¼Œå³è§‚æµ‹ç‹¬ç«‹æ€§å‡è®¾ï¼›

##### ï¼ˆæ¦‚ç‡ï¼‰æ— å‘å›¾æ¨¡å‹
æ— å‘å›¾æ¨¡å‹ï¼ˆUndirected Graphical Modelï¼‰åˆç§°é©¬å°”å¯å¤«éšæœºåœºï¼ˆ[Markov random field, MRF](https://en.jinzhao.wiki/wiki/Markov_random_field)ï¼‰æˆ–é©¬å°”å¯å¤«ç½‘ç»œï¼ˆMarkov networkï¼‰æ˜¯ä¸€ç±»ç”¨æ— å‘å›¾ï¼ˆ[Undirected Graphical](https://en.jinzhao.wiki/wiki/Graph_(discrete_mathematics)#Undirected_graph)ï¼‰æ¥æè¿°ä¸€ç»„å…·æœ‰å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨çš„éšæœºå‘é‡ğ‘¿çš„è”åˆæ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å‹ï¼
ä»¥ä¸‹å®šä¹‰æ˜¯ç­‰ä»·çš„
$$\text{Global Markov} \iff \text{Local Markov}\iff\text{Pair Markov}\xLeftrightarrow{Hammesleyâˆ’Clifford } å› å­åˆ†è§£$$



- **å›¢åˆ†è§£ï¼Œå› å­åˆ†è§£**ï¼ˆClique factorizationï¼‰ï¼š
æ— å‘å›¾Gä¸­ä»»ä½•ä¸¤ä¸ªç»“ç‚¹å‡æœ‰è¾¹è¿æ¥çš„ç»“ç‚¹å­é›†ç§°ä¸º**å›¢**ï¼ˆcliqueï¼‰ã€‚è‹¥Cæ˜¯æ— å‘å›¾Gçš„ä¸€ä¸ªå›¢ï¼Œå¹¶ä¸”ä¸èƒ½å†åŠ è¿›ä»»ä½•ä¸€ä¸ªGçš„ç»“ç‚¹ä½¿å…¶æˆä¸ºä¸€ä¸ªæ›´å¤§çš„å›¢ï¼Œåˆ™ç§°æ­¤Cä¸º**æœ€å¤§å›¢**ï¼ˆmaximal cliqueï¼‰ã€‚
å°†æ¦‚ç‡æ— å‘å›¾æ¨¡å‹çš„è”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºä¸ºå…¶æœ€å¤§å›¢ä¸Šçš„éšæœºå˜é‡çš„å‡½æ•°çš„ä¹˜ç§¯å½¢å¼çš„æ“ä½œï¼Œç§°ä¸ºæ¦‚ç‡æ— å‘å›¾æ¨¡å‹çš„å› å­åˆ†è§£ï¼ˆfactorizationï¼‰ã€‚
ç»™å®šæ¦‚ç‡æ— å‘å›¾æ¨¡å‹ï¼Œè®¾å…¶æ— å‘å›¾ä¸ºGï¼Œéšæœºå˜é‡${\displaystyle X=(X_{v})_{v\in V}}$ï¼ŒCä¸ºGä¸Šçš„æœ€å¤§å›¢ï¼Œ$X_C$è¡¨ç¤ºCå¯¹åº”çš„éšæœºå˜é‡ã€‚é‚£ä¹ˆæ¦‚ç‡æ— å‘å›¾æ¨¡å‹çš„è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X)$å¯å†™ä½œå›¾ä¸­æ‰€æœ‰æœ€å¤§å›¢Cä¸Šçš„å‡½æ•°$\phi_C (x_C)$çš„ä¹˜ç§¯å½¢å¼ï¼Œå³
$$P(X) =\frac{1}{Z} \prod_{C \in \operatorname{cl}(G)} \phi_C (X_C) $$
$$Z=\sum_{X}\prod_{C \in \operatorname{cl}(G)} \phi_C (X_C)$$
Zæ˜¯è§„èŒƒåŒ–å› å­ï¼ˆnormalization factorï¼‰æˆ–å½’ä¸€åŒ–å› å­ä¹Ÿè¢«ç§°ä¸ºé…åˆ†å‡½æ•°ï¼ˆpartition functionï¼‰;
$\phi_C (X_C)$ç§°ä¸ºåŠ¿å‡½æ•°ï¼ˆpotential function or factor potentials or clique potentialsï¼‰ï¼ŒåŠ¿å‡½æ•°è¦æ±‚æ˜¯ä¸¥æ ¼æ­£çš„ï¼Œé€šå¸¸å®šä¹‰ä¸ºæŒ‡æ•°å‡½æ•°ï¼š
$$\phi_C (X_C) = \exp\{-E(X_C)\}$$
å…¶ä¸­Eä¸ºèƒ½é‡å‡½æ•°ï¼ˆenergy functionï¼‰ã€‚
å®é™…ä¸Šç”¨è¿™ç§å½¢å¼è¡¨è¾¾çš„p(x)ï¼Œä¸ºGibbs Distributionï¼Œæˆ–è€…åˆè¢«ç§°ä¹‹ä¸ºBoltzman Distributionã€‚å¯ä»¥å†™æˆï¼š
$$P(x) = \frac{1}{Z} \prod_{i=1}^K \phi (x_{C_{i}}) = \frac{1}{Z} \prod_{i=1}^K \exp\{-E(x_{C_{i}})\} =  \frac{1}{Z}\exp\{-\sum_{i=1}^K E(x_{C_{i}})\} = \frac{1}{Z}\exp\sum_{i=1}^K F_i(x_{ci})ï¼Œx \in \mathbb{R}^{p}$$
$x \in \mathbb{R}^p$æ˜¯ä¸ªè”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒçš„ç»´åº¦æ˜¯$p$ç»´ï¼›$\phi$è¡¨ç¤ºåŠ¿å‡½æ•°ï¼›$E$è¡¨ç¤ºèƒ½é‡å‡½æ•°ï¼›$K$è¡¨ç¤ºæœ€å¤§å›¢çš„ä¸ªæ•°ï¼›$C_i$è¡¨ç¤ºç¬¬$i$ä¸ªæœ€å¤§å›¢ã€‚
æˆ‘ä»¬å°†æŒ‡æ•°æ—åˆ†å¸ƒå’ŒåŠ¿å‡½æ•°è”ç³»èµ·æ¥ï¼š
$${\displaystyle p(x\mid {\boldsymbol {\eta }})=h(x)\,\exp {\Big (}{\boldsymbol {\eta^T }}\cdot \mathbf {T} (x)-A({\boldsymbol {\eta }}){\Big )}} = h(x) \frac{1}{Z(\eta)}\exp\{\eta^T \cdot \mathbf {T} (x)\}$$
å‘ç°åŠ¿å‡½æ•°(Gibbs Distribution)æ˜¯ä¸€ä¸ªæŒ‡æ•°æ—åˆ†å¸ƒã€‚Gibbsæ˜¯æ¥è‡ªç»Ÿè®¡ç‰©ç†å­¦ï¼Œå½¢å¼ä¸Šå’ŒæŒ‡æ•°æ—åˆ†å¸ƒæ—¶ä¸€æ ·çš„ã€‚è€ŒæŒ‡æ•°æ—åˆ†å¸ƒå®é™…ä¸Šæ˜¯ç”±æœ€å¤§ç†µåˆ†å¸ƒå¾—åˆ°çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç†è§£Gibbsåˆ†å¸ƒä¹Ÿæ˜¯æœ‰æœ€å¤§ç†µåŸç†å¾—åˆ°çš„ã€‚è€Œé©¬å°”å¯å¤«éšæœºåœº(Markov Random Field)å®é™…ä¸Šç­‰ä»·äºGibbsåˆ†å¸ƒã€‚å³ï¼š
æœ€å¤§ç†µåŸç† â‡’ æŒ‡æ•°æ—åˆ†å¸ƒ(Gibbsåˆ†å¸ƒ).
Markov Random Field â‡” Gibbs Distribution.

- **æˆå¯¹é©¬å°”å¯å¤«æ€§**ï¼ˆPairwise Markov propertyï¼‰ï¼š
ä»»æ„ä¸¤ä¸ªä¸ç›¸é‚»çš„å˜é‡åœ¨ç»™å®šå…¶ä»–å˜é‡çš„æ¡ä»¶ä¸‹æ˜¯ç‹¬ç«‹çš„:${\displaystyle X_{u}\perp \!\!\!\perp X_{v}\mid X_{V\setminus \{u,v\}}}$
- **å±€éƒ¨é©¬å°”å¯å¤«æ€§**ï¼ˆLocal Markov propertyï¼‰ï¼š
ä¸€ä¸ªå˜é‡åœ¨ç»™å®šå…¶ç›¸é‚»å˜é‡çš„æ¡ä»¶ä¸‹æ˜¯ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–å˜é‡:${\displaystyle X_{v}\perp \!\!\!\perp X_{V\setminus \operatorname {N} [v]}\mid X_{\operatorname {N} (v)}}$
å…¶ä¸­$\operatorname {N} (v)$æ˜¯vçš„é‚»å±…ï¼ˆneighborï¼‰èŠ‚ç‚¹ï¼›${\displaystyle \operatorname {N} [v]=v\cup \operatorname {N} (v)}$
- **å…¨å±€é©¬å°”å¯å¤«æ€§**ï¼ˆGlobal Markov propertyï¼‰ï¼š
ç»™å®šä¸€ä¸ªåˆ†ç¦»å­é›†ï¼ˆseparating subsetï¼‰ï¼Œä»»æ„ä¸¤ä¸ªå˜é‡å­é›†éƒ½æ˜¯æ¡ä»¶ç‹¬ç«‹çš„:$X_A \perp\!\!\!\perp X_B \mid X_S$
Aä¸­çš„èŠ‚ç‚¹åˆ°Bä¸­çš„èŠ‚ç‚¹éƒ½è¦ç»è¿‡Sï¼›

- **é“å¾·å›¾**ï¼ˆMoral graphï¼‰ï¼š
æœ‰å‘å›¾å’Œæ— å‘å›¾å¯ä»¥ç›¸äº’è½¬æ¢ï¼Œä½†å°†æ— å‘å›¾è½¬ä¸ºæœ‰å‘å›¾é€šå¸¸æ¯”è¾ƒå›°éš¾ï¼åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå°†æœ‰å‘å›¾è½¬ä¸ºæ— å‘å›¾æ›´åŠ é‡è¦ï¼Œè¿™æ ·å¯ä»¥åˆ©ç”¨æ— å‘å›¾ä¸Šçš„ç²¾ç¡®æ¨æ–­ç®—æ³•ï¼Œæ¯”å¦‚è”åˆæ ‘ç®—æ³•ï¼ˆJunction Tree Algorithmï¼‰ï¼
æœ‰å‘å›¾è½¬åŒ–æˆæ— å‘å›¾çš„è¿‡ç¨‹ç§°ä¸ºé“å¾·åŒ–ï¼ˆMoralizationï¼‰ï¼Œè½¬åŒ–åçš„æ— å‘å›¾ç§°ä¸ºé“å¾·å›¾ï¼ˆ[Moral graph](https://en.jinzhao.wiki/wiki/Moral_graph)ï¼‰ã€‚
æ¯ä¸ªæœ‰å‘å›¾åˆ†è§£çš„å› å­è¦å¤„äºä¸€ä¸ªæœ€å¤§å›¢ä¸­ï¼Œå¦‚ï¼š
$$P(X) = p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)$$
å…¶ä¸­$p(x_4|x_1,x_2,x_3)$æœ‰å››ä¸ªå˜é‡ï¼Œé‚£ä¹ˆï¼š
```mermaid
graph TD
    x1(("xâ‚"))
    x2(("xâ‚‚"))
    x3(("xâ‚ƒ"))
    x4(("xâ‚„"))
    x1-->x4
    x2-->x4
    x3-->x4
    y1(("xâ‚"))
    y2(("xâ‚‚"))
    y3(("xâ‚ƒ"))
    y4(("xâ‚„"))
    y1---y2
    y1---y3
    y1---y4
    y2---y3
    y2---y4
    y3---y4
```
> é“å¾·åŒ–çš„è¿‡ç¨‹ä¸­ï¼ŒåŸæœ‰çš„ä¸€äº›æ¡ä»¶ç‹¬ç«‹æ€§ä¼šä¸¢å¤±ã€‚

- **å› å­å›¾**ï¼ˆFactor graphï¼‰ï¼š
è¿™é‡Œä¸ä½œä»‹ç»ï¼Œç›®å‰ä¸å¤ªæ˜ç™½ç”¨å¤„ã€‚


- **å¸¸è§çš„æœ‰å‘å›¾æ¨¡å‹**ï¼š
å¯¹æ•°çº¿æ€§æ¨¡å‹ï¼ˆæœ€å¤§ç†µæ¨¡å‹ï¼‰ã€æ¡ä»¶éšæœºåœºã€ç»å°”å…¹æ›¼æœºã€å—é™ç»å°”å…¹æ›¼æœºç­‰ï¼

> ä»¥ä¸Šå†…å®¹åªæ˜¯è®²åˆ°äº†æ¦‚ç‡å›¾çš„è¡¨ç¤ºã€‚

### å‚è€ƒæ–‡çŒ®

[11-1] Bishop M. Pattern Recognition and Machine Learning. Springer-Verlag,2006

[11-2] Koller D,Friedman N. Probabilistic Graphical Models: Principles and Techniques.MIT Press,2009

[11-3] Lafferty J,McCallum A,Pereira F. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In: International Conference on Machine Learning,2001

[11-4] Sha F,Pereira F. Shallow parsing with conditional random fields. In: Proceedings ofthe 2003 Conference of the North American Chapter of Association for ComputationalLinguistics on Human Language Technology,Vol.1,2003

[11-5] McCallum A,Freitag D,Pereira F. Maximum entropy Markov models for informationextraction and segmentation. In: Proc of the International Conference on Machine Learning,2000

[11-6] Taskar B,Guestrin C,Koller D. Max-margin Markov networks. In: Proc of the NIPS2003,2003

[11-7] Tsochantaridis I,Hofmann T,Joachims T. Support vector machine learning forinterdependent and structured output spaces. In: ICML,2004

## ç¬¬ 12 ç«  ç›‘ç£å­¦ä¹ æ–¹æ³•æ€»ç»“
