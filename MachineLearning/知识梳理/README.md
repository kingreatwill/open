[TOC]

# ç»Ÿè®¡å­¦ä¹ æ–¹æ³•

[ç¬¬ä¸€ç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-first_edition)

[ç¬¬äºŒç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-second_edition)

## ç¬¬ 1 ç«  ç»Ÿè®¡å­¦ä¹ åŠç›‘ç£å­¦ä¹ æ¦‚è®º

**ç»Ÿè®¡å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹æ˜¯**ï¼š

1. ç»Ÿè®¡å­¦ä¹ ä»¥è®¡ç®—æœºåŠç½‘ç»œä¸ºå¹³å°ï¼Œæ˜¯å»ºç«‹åœ¨è®¡ç®—æœºåŠç½‘ç»œä¹‹ä¸Šçš„ï¼›
2. ç»Ÿè®¡å­¦ä¹ ä»¥æ•°æ®ä¸ºç ”ç©¶å¯¹è±¡ï¼Œæ˜¯æ•°æ®é©±åŠ¨çš„å­¦ç§‘ï¼›
3. ç»Ÿè®¡å­¦ä¹ çš„ç›®çš„æ˜¯å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
4. ç»Ÿè®¡å­¦ä¹ ä»¥æ–¹æ³•ä¸ºä¸­å¿ƒï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•æ„å»ºæ¨¡å‹å¹¶åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
5. ç»Ÿè®¡å­¦ä¹ æ˜¯æ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€è®¡ç®—ç†è®ºã€æœ€ä¼˜åŒ–ç†è®ºåŠè®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªé¢†åŸŸçš„äº¤å‰å­¦ç§‘ï¼Œå¹¶ä¸”åœ¨å‘å±•ä¸­é€æ­¥å½¢æˆç‹¬è‡ªçš„ç†è®ºä½“ç³»ä¸æ–¹æ³•è®ºã€‚

**å‡è®¾ç©ºé—´(hypothesis space)**ï¼š
$$\mathcal H = \{ f(x;\theta) | \theta \in \mathbb{R}^D\} \\ or \quad \mathcal F = \{P|P(Y|X;\theta),\theta \in \mathbb{R}^D\}$$
å…¶ä¸­$f(x; \theta)$æ˜¯å‚æ•°ä¸º$\theta$ çš„å‡½æ•°ï¼ˆ**å†³ç­–å‡½æ•°**ï¼‰ï¼Œä¹Ÿç§°ä¸ºæ¨¡å‹ï¼ˆModelï¼‰ï¼Œå‚æ•°å‘é‡$\theta$å–å€¼ä¸$D$ç»´æ¬§å¼ç©ºé—´$\mathbb{R}^D$,ä¹Ÿç§°ä¸ºå‚æ•°ç©ºé—´(parameter space)ï¼Œ$D$ ä¸ºå‚æ•°çš„æ•°é‡(ç»´åº¦)

æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæˆ–å†³ç­–å‡½æ•°

**ç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰**ï¼š
æ¯ä¸ªå…·ä½“çš„è¾“å…¥æ˜¯ä¸€ä¸ªå®ä¾‹ï¼ˆinstanceï¼‰ï¼Œé€šå¸¸ç”±ç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰è¡¨ç¤ºã€‚è¿™
æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾å‘é‡å­˜åœ¨çš„ç©ºé—´ç§°ä¸ºç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰ã€‚ç‰¹å¾ç©ºé—´çš„æ¯ä¸€ç»´å¯¹åº”äº
ä¸€ä¸ªç‰¹å¾ã€‚

> è¾“å…¥ç©ºé—´ä¸­çš„ä¸€ä¸ªè¾“å…¥å‘é‡$x = (x_1,x_2)$ï¼Œåœ¨å¤šé¡¹å¼æ¨¡å‹ä¸­ç‰¹å¾å‘é‡æ˜¯($x_1^2,x_1x_2,x_2^2,...$)
> ä¸€èˆ¬è¯´çš„çº¿æ€§æ¨¡å‹ï¼ŒæŒ‡çš„æ˜¯ç‰¹å¾å‘é‡çš„çº¿æ€§ç»„åˆï¼Œè€Œä¸æ˜¯æŒ‡è¾“å…¥å‘é‡ï¼Œæ‰€ä»¥è¯´æ¨¡å‹éƒ½æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„

**ç»Ÿè®¡å­¦ä¹ çš„ä¸‰è¦ç´ **ï¼š

1. æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)ï¼Œç®€ç§°ï¼šæ¨¡å‹(model)
2. æ¨¡å‹é€‰æ‹©çš„å‡†åˆ™(evaluation criterion)ï¼Œç®€ç§°ï¼šç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™
3. æ¨¡å‹å­¦ä¹ çš„ç®—æ³•(algorithm)ï¼Œç®€ç§°ï¼šç®—æ³•(algorithm)

> ä»¥çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ä¸ºä¾‹ï¼š
> æ¨¡å‹ï¼š $f(x;w,b) = w^Tx +b$
> ç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™: å¹³æ–¹æŸå¤±å‡½æ•° $\mathcal L(y,\hat{y}) = (y-f(x,\theta))^2$
> ç®—æ³•ï¼šè§£æè§£ analytical solution(é—­å¼è§£ closed-form solution)å’Œæ•°å€¼è§£ numerical solutionï¼Œå¦‚ï¼šclosed-form çš„æœ€å°äºŒä¹˜çš„è§£ä»¥åŠæ¢¯åº¦ä¸‹é™æ³•

**æœºå™¨å­¦ä¹ çš„å®šä¹‰**ï¼š

```mermaid
graph LR;
    F(["æœªçŸ¥çš„ç›®æ ‡å‡½æ•°(ç†æƒ³ä¸­å®Œç¾çš„å‡½æ•°)ï¼šğ‘“: ğ’™âŸ¶ğ‘¦"])-->D["è®­ç»ƒæ ·æœ¬D:{(ğ’™Â¹,ğ‘¦Â¹),...,(ğ’™â¿,ğ‘¦â¿)}"];
    D-->A{{"ç®—æ³•"}}
    H{{"å‡è®¾ç©ºé—´"}}-->A
    A-->G["æ¨¡å‹ gâ‰ˆf"]
```

ä½¿ç”¨è®­ç»ƒæ•°æ®æ¥è®¡ç®—æ¥è¿‘ç›®æ ‡ ğ‘“ çš„å‡è®¾ï¼ˆhypothesis ï¼‰g ï¼ˆæ¥è‡ªï¼š[Machine Learning Foundationsï¼ˆæœºå™¨å­¦ä¹ åŸºçŸ³ï¼‰- the learning problem,25 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/01_handout.pdf)ï¼‰

**ç›‘ç£å­¦ä¹ **ï¼š
ç›‘ç£å­¦ä¹ (supervised learning)æ˜¯æŒ‡ä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„çš„ç»Ÿè®¡è§„å¾‹**ã€‚

è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºè¿ç»­å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**å›å½’é—®é¢˜**ï¼›
è¾“å‡ºå˜é‡ä¸ºæœ‰é™ä¸ªç¦»æ•£å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**åˆ†ç±»é—®é¢˜**ï¼›
è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºå˜é‡åºåˆ—çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**æ ‡æ³¨é—®é¢˜**(åˆ†ç±»é—®é¢˜çš„æ¨å¹¿ï¼Œå¦‚ï¼šéšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼Œæ¡ä»¶éšæœºåœº CRF)ã€‚

ç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥æ˜¯æ¦‚ç‡æ¨¡å‹æˆ–éæ¦‚ç‡æ¨¡å‹ï¼Œç”±**æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ**$P(Y|X)$æˆ–**å†³ç­–å‡½æ•°ï¼ˆdecision functionï¼‰**$Y=f(X)$è¡¨ç¤ºï¼Œéšå…·ä½“å­¦ä¹ æ–¹æ³•è€Œå®šã€‚å¯¹å…·ä½“çš„è¾“å…¥è¿›è¡Œç›¸åº”çš„è¾“å‡ºé¢„æµ‹æ—¶ï¼Œå†™ä½œ$P(y|x)$æˆ–$Y=f(x)$ã€‚
$$y =\displaystyle\argmax_{y}  P(y|x)$$

**è”åˆæ¦‚ç‡åˆ†å¸ƒ**ï¼š
ç›‘ç£å­¦ä¹ å‡è®¾è¾“å…¥ä¸è¾“å‡ºçš„éšæœºå˜é‡ X å’Œ Y éµå¾ªè”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ã€‚$P(X,Y)$è¡¨ç¤ºåˆ†å¸ƒå‡½æ•°ï¼Œæˆ–åˆ†å¸ƒå¯†åº¦å‡½æ•°ã€‚æ³¨æ„ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå‡å®šè¿™ä¸€è”åˆæ¦‚ç‡åˆ†å¸ƒå­˜åœ¨ï¼Œä½†å¯¹å­¦ä¹ ç³»ç»Ÿæ¥è¯´ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å…·ä½“å®šä¹‰æ˜¯æœªçŸ¥çš„ã€‚**è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®è¢«çœ‹ä½œæ˜¯ä¾è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ç‹¬ç«‹åŒåˆ†å¸ƒäº§ç”Ÿçš„**ã€‚
ç»Ÿè®¡å­¦ä¹ å‡è®¾æ•°æ®å­˜åœ¨ä¸€å®šçš„ç»Ÿè®¡è§„å¾‹ï¼Œ$X$å’Œ$Y$å…·æœ‰è”åˆæ¦‚ç‡åˆ†å¸ƒçš„å‡è®¾å°±æ˜¯ç›‘ç£å­¦ä¹ å…³äºæ•°æ®çš„åŸºæœ¬å‡è®¾ã€‚

**éç›‘ç£å­¦ä¹ **ï¼š
éç›‘ç£å­¦ä¹ (unsupervised learning)æ˜¯æŒ‡ä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ æ•°æ®ä¸­çš„ç»Ÿè®¡è§„å¾‹æˆ–æ½œåœ¨ç»“æ„**ã€‚

éç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•°$z = g(x)$æˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(z|x)$ ï¼ˆè¾“å‡º$z$å¯ä»¥æ˜¯**èšç±»**æˆ–è€…**é™ç»´**ï¼‰
$$z =\displaystyle\argmax_{z}  P(z|x)$$
ä»¥åŠ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x|z)$ ï¼ˆç”¨æ¥åš**æ¦‚ç‡å¯†åº¦ä¼°è®¡**ï¼Œæ¯”å¦‚ GMM ä¸­$P(x|z)$å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå¦‚æœå‡è®¾çŸ¥é“æ•°æ®æ¥è‡ªå“ªä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå³çŸ¥é“$z$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼°è®¡ç›¸å…³å‚æ•°ï¼‰ã€‚

[æ ¸å¯†åº¦ä¼°è®¡ Kernel Density Estimation.](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html)

**æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰ä¸éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰æˆ–è€…ç¡®å®šæ€§æ¨¡å‹ï¼ˆdeterministic modelï¼‰**ï¼š

æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰- æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ P(y|x)å’Œ éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰ - å‡½æ•° y=f(x)å¯ä»¥**ç›¸äº’è½¬åŒ–**ï¼Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæœ€å¤§åŒ–åå¾—åˆ°å‡½æ•°ï¼Œå‡½æ•°å½’ä¸€åŒ–åå¾—åˆ°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚æ‰€ä»¥æ¦‚ç‡æ¨¡å‹ä¸éæ¦‚ç‡æ¨¡å‹çš„åŒºåˆ«ä¸åœ¨äºè¾“å…¥è¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œè€Œåœ¨äºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼šæ¦‚ç‡æ¨¡å‹ä¸€å®šå¯ä»¥è¡¨ç¤ºä¸ºè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œè€Œéæ¦‚ç‡æ¨¡å‹åˆ™ä¸ä¸€å®šå­˜åœ¨è¿™æ ·çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚

æ¦‚ç‡æ¨¡å‹çš„ä»£è¡¨æ˜¯**æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆprobabilistic graphical modelï¼‰**$^{å‚è€ƒæ–‡çŒ®[1-3]}$ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒå¯ä»¥æ ¹æ®å›¾çš„ç»“æ„åˆ†è§£ä¸ºå› å­ä¹˜ç§¯çš„å½¢å¼ï¼Œå¯ä»¥ç”¨æœ€åŸºæœ¬çš„åŠ æ³•è§„åˆ™å’Œä¹˜æ³•è§„åˆ™è¿›è¡Œæ¦‚ç‡æ¨ç†ï¼š
$$P(x) = \sum_yP(x,y) \\ P(x,y) = P(x)P(y|x)$$

**å‚æ•°åŒ–æ¨¡å‹ï¼ˆparametric modelï¼‰å’Œéå‚æ•°åŒ–æ¨¡å‹ï¼ˆnon-parametric modelï¼‰**ï¼š

å‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„ç»´åº¦å›ºå®šï¼Œæ¨¡å‹å¯ä»¥ç”±æœ‰é™ç»´å‚æ•°å®Œå…¨åˆ»ç”»ã€‚(å¦‚ï¼šæ„ŸçŸ¥æœºã€GMM)
éå‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„å”¯ç‹¬ä¸å›ºå®šæˆ–è€…è¯´æ— ç©·å¤§ï¼Œéšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ è€Œä¸æ–­å¢å¤§ã€‚(å¦‚ï¼šå†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœº)

**åœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰å’Œæ‰¹é‡å­¦ä¹ ï¼ˆbatch learningï¼‰**ï¼š

åœ¨çº¿å­¦ä¹ æ¯æ¬¡æ¥å—ä¸€ä¸ªæ ·æœ¬ï¼Œé¢„æµ‹åå­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸æ–­é‡å¤è¯¥æ“ä½œã€‚
æ‰¹é‡å­¦ä¹ ä¸€æ¬¡æ¥å—æ‰€æœ‰æ•°æ®ï¼Œå­¦ä¹ æ¨¡å‹ä¹‹åè¿›è¡Œé¢„æµ‹ã€‚

åœ¨çº¿å­¦ä¹ æ¯”æ‰¹é‡å­¦ä¹ æ›´éš¾ï¼Œå› ä¸ºæ¯æ¬¡æ¨¡å‹æ›´æ–°ä¸­å¯åˆ©ç”¨çš„æ•°æ®æœ‰é™ã€‚

**è´å¶æ–¯å­¦ä¹ ï¼ˆBayesian learningï¼‰/ è´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰**ï¼š
$$\mathrm{Bayes \; Rule:} \\ \underbrace{P(X|Y)}_{\mathrm{posterior}} = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{P(Y)}_{\mathrm{evidence}}}   = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{\sum_{x}P(Y|X)P(X)}_{\mathrm{evidence}}}$$

**æ ¸æŠ€å·§ï¼ˆkernel trickï¼‰/ æ ¸æ–¹æ³•ï¼ˆkernel methodï¼‰**ï¼š

**æ ¸æ–¹æ³•**æ˜¯ä¸€ç±»æŠŠä½ç»´ç©ºé—´çš„éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œè½¬åŒ–ä¸ºé«˜ç»´ç©ºé—´çš„çº¿æ€§å¯åˆ†é—®é¢˜çš„æ–¹æ³•ã€‚
**æ ¸æŠ€å·§**æ˜¯ä¸€ç§åˆ©ç”¨æ ¸å‡½æ•°ç›´æ¥è®¡ç®— $\lang \phi(x),\phi(z) \rang$ ï¼Œä»¥é¿å¼€åˆ†åˆ«è®¡ç®— $\phi(x)$ å’Œ $\phi(z)$ ï¼Œä»è€ŒåŠ é€Ÿæ ¸æ–¹æ³•è®¡ç®—çš„æŠ€å·§ã€‚

**æ ¸å‡½æ•°**ï¼š[Kernel function](https://en.jinzhao.wiki/wiki/Positive-definite_kernel)
è®¾ $\mathcal X$ æ˜¯è¾“å…¥ç©ºé—´ï¼ˆå³ $x_i \in \mathcal X $ ï¼Œ $\mathcal X$ æ˜¯ $\mathbb R^n$ çš„å­é›†æˆ–ç¦»æ•£é›†åˆ ï¼‰ï¼Œåˆè®¾ $\mathcal H$ ä¸ºç‰¹å¾ç©ºé—´ï¼ˆâ€‹ å¸Œå°”ä¼¯ç‰¹ç©ºé—´$^{é™„åŠ çŸ¥è¯†:å„ç§ç©ºé—´ä»‹ç»}$ï¼‰ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªä» $\mathcal X$ åˆ° $\mathcal H$ çš„æ˜ å°„

$$\phi(x) : \mathcal X \to \mathcal H$$

ä½¿å¾—å¯¹æ‰€æœ‰ $x,z \in \mathcal X$ ï¼Œå‡½æ•° $K(x,z)$ æ»¡è¶³æ¡ä»¶

$$K(x,z) = \phi(x).\phi(z) = \lang \phi(x),\phi(z) \rang$$

åˆ™ç§° $K(x,z)$ ä¸ºæ ¸å‡½æ•°ã€‚å…¶ä¸­ $\phi(x) $ ä¸ºæ˜ å°„å‡½æ•°ï¼Œ $\lang \phi(x),\phi(z) \rang$ ä¸ºå†…ç§¯ã€‚

[æ ¸æŠ€å·§](https://en.jinzhao.wiki/wiki/Kernel_method)çš„æƒ³æ³•æ˜¯ï¼Œåœ¨å­¦ä¹ å’Œé¢„æµ‹ä¸­åªå®šä¹‰æ ¸å‡½æ•° $K(x,z)$ ï¼Œè€Œä¸æ˜¾å¼åœ°å®šä¹‰æ˜ å°„å‡½æ•° $\phi $ã€‚é€šå¸¸ç›´æ¥è®¡ç®—$K(x,z)$æ¯”è¾ƒå®¹æ˜“ï¼Œè€Œé€šè¿‡$\phi(x) $å’Œ$\phi(z) $è®¡ç®—$K(x,z)$å¹¶ä¸å®¹æ˜“ã€‚

> æ³¨æ„ï¼š$\phi $æ˜¯è¾“å…¥ç©ºé—´$\mathbb{R}^n$åˆ°ç‰¹å¾ç©ºé—´$\mathcal H$çš„æ˜ å°„ï¼Œç‰¹å¾ç©ºé—´$\mathcal H$ä¸€èˆ¬æ˜¯é«˜ç»´çš„ï¼Œç”šè‡³æ˜¯æ— ç©·ç»´çš„ã€‚æ‰€ä»¥$\phi$ä¸å¥½è®¡ç®—ï¼Œç”šè‡³ä¼šå¸¦æ¥**ç»´åº¦ç¾éš¾**åˆç§°**ç»´åº¦è¯…å’’ï¼ˆCurse of Dimensionalityï¼‰**$^{é™„åŠ çŸ¥è¯†:ç»´åº¦è¯…å’’}$ã€‚

### é™„åŠ çŸ¥è¯†

#### æ­£åˆ™åŒ–

æ­£åˆ™åŒ–ç¬¦åˆå¥¥å¡å§†å‰ƒåˆ€ï¼ˆOccam's razorï¼‰åŸç†ã€‚

å‚è€ƒï¼š[L1L2 æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)

#### æ¨¡å‹é€‰æ‹©

å‚è€ƒï¼š[æ¨¡å‹é€‰æ‹©](../Model-Selection.md)

#### ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹

å‚è€ƒï¼š[ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹](../ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹.md)

#### å„ç§ç©ºé—´ä»‹ç»

**çº¿æ€§ç©ºé—´**å°±æ˜¯å®šä¹‰äº†**åŠ æ³•å’Œæ•°ä¹˜**çš„ç©ºé—´(ç©ºé—´é‡Œçš„ä¸€ä¸ªå…ƒç´ å°±å¯ä»¥ç”±å…¶ä»–å…ƒç´ çº¿æ€§è¡¨ç¤º)ã€‚

---

**åº¦é‡ç©ºé—´**å°±æ˜¯å®šä¹‰äº†**è·ç¦»**çš„ç©ºé—´ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼Œæ¬§æ°è·ç¦»ï¼Œé—µå¯å¤«æ–¯åŸºè·ç¦»ï¼Œé©¬æ°è·ç¦»ï¼Œåˆ‡æ¯”é›ªå¤«è·ç¦»ï¼‰ã€‚
å®šä¹‰è·ç¦»æ—¶ï¼Œæœ‰ä¸‰æ¡å…¬ç†å¿…é¡»éµå®ˆï¼š

1. éè´Ÿæ€§ã€åŒä¸€æ€§ï¼š$dist(x_i,x_j) \geq 0$(éè´Ÿæ€§)ï¼Œ$dist(x_i,x_j) = 0$å½“ä¸”ä»…å½“$x_i=x_j$(åŒä¸€æ€§)
2. å¯¹ç§°æ€§ï¼š$dist(x_i,x_j) = dist(x_j,x_i)$
3. ä¸‰è§’ä¸ç­‰å¼(ä¹Ÿå«ç›´é€’æ€§)ï¼š$dist(x_i,x_j) \leq dist(x_i,x_k) + dist(x_k,x_j)$
   å¸Œå°”ä¼¯ç‰¹ç©ºé—´(Hilbert)
   > æ–‡å­—è§£é‡Šï¼šã€ä¸¤ç‚¹ä¹‹é—´è·ç¦»ä¸ä¸ºè´Ÿï¼›ä¸¤ä¸ªç‚¹åªæœ‰åœ¨ ç©ºé—´ ä¸Šé‡åˆæ‰å¯èƒ½è·ç¦»ä¸ºé›¶ï¼›a åˆ° b çš„è·ç¦»ç­‰äº b åˆ° a çš„è·ç¦»;a åˆ° c çš„è·ç¦»åŠ ä¸Š c åˆ° b çš„è·ç¦»å¤§äºç­‰äº a ç›´æ¥åˆ° b çš„è·ç¦»;ã€‘

---

**èµ‹èŒƒç©ºé—´**å°±æ˜¯å®šä¹‰äº†**èŒƒæ•°**çš„ç©ºé—´ã€‚
x çš„èŒƒæ•°||x||å°±æ˜¯ x çš„**é•¿åº¦**ã€‚é‚£ä¹ˆè¿™é‡Œçš„é•¿åº¦å’Œä¸Šä¸€èŠ‚ä¸­è¯´çš„è·ç¦»åˆ°åº•æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ã€‚**è·ç¦»çš„æ¦‚å¿µæ˜¯é’ˆå¯¹ä¸¤ä¸ªå…ƒç´ æ¥è¯´çš„**ï¼Œä¾‹å¦‚ d(x,y)æŒ‡çš„æ˜¯ x ä¸ y ä¸¤ä¸ªå…ƒç´ ä¹‹é—´çš„è·ç¦»ï¼Œè€Œ**èŒƒæ•°æ˜¯é’ˆå¯¹ä¸€ä¸ªå…ƒç´ æ¥è¯´çš„**ï¼Œæ¯ä¸€ä¸ªå…ƒç´ éƒ½å¯¹åº”ä¸€ä¸ªèŒƒæ•°ï¼Œå¯ä»¥å°†èŒƒæ•°ç†è§£ä¸ºä¸€ä¸ªå…ƒç´ åˆ°é›¶ç‚¹çš„è·ç¦»ï¼ˆè¿™åªæ˜¯ä¸€ç§ç†è§£ï¼Œå¹¶ä¸æ˜¯å®šä¹‰ï¼‰ï¼Œä¹Ÿå°±æ˜¯å®ƒè‡ªå·±çš„é•¿åº¦ã€‚
å®šä¹‰ï¼š
ç§° æ˜ å°„$||.|| : \mathbb{R}^n \to \mathbb{R}$ä¸º $\mathbb{R}^n$ ä¸Šçš„èŒƒæ•°ï¼Œå½“ä¸”ä»…å½“ï¼š

1. éè´Ÿæ€§ï¼š $\forall x \in \mathbb{R}^n ,||x|| \geq 0$ ,$||x|| = 0$å½“ä¸”ä»…å½“$x=0$
2. æ•°ä¹˜ï¼š$\forall x \in \mathbb{R}^n ,a \in \mathbb{R}^n, ||ax|| = |a|.||x||$
3. ä¸‰è§’ä¸ç­‰å¼: $\forall x,y \in \mathbb{R}^n ,||x+y|| \leq ||x|| + ||y||$

å¦‚æœæˆ‘ä»¬å®šä¹‰äº†èŒƒæ•°ï¼Œå¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šå®šä¹‰è·ç¦»ï¼šdist(x,y)=||x-y||ã€‚æ ¹æ®èŒƒæ•°çš„ä¸‰æ¡æ€§è´¨ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜æˆ‘ä»¬è¿™æ ·å®šä¹‰çš„è·ç¦»ä¹Ÿæ»¡è¶³è·ç¦»çš„å®šä¹‰ï¼Œèªæ˜çš„ä½ å¯ä»¥è‡ªå·±è¯æ˜ä¸€ä¸‹ï¼ˆå¯¹ç§°æ€§çš„è¯æ˜ï¼Œæä¸€ä¸ª-1 å‡ºæ¥ï¼Œä¸€åŠ ç»å¯¹å€¼å°±æ˜¯ 1 äº†ï¼‰ã€‚

ä¹Ÿå°±æ˜¯è¯´èŒƒæ•°å…¶å®æ˜¯ä¸€ä¸ªæ›´åŠ å…·ä½“çš„æ¦‚å¿µï¼Œ**æœ‰äº†èŒƒæ•°ä¸€å®šèƒ½åˆ©ç”¨èŒƒæ•°å®šä¹‰è·ç¦»ï¼Œä½†æ˜¯æœ‰è·ç¦»ä¸èƒ½å®šä¹‰èŒƒæ•°**ã€‚

ä¹Ÿè®¸ä½ ä¼šé—®ï¼Œä½ ä¸æ˜¯è¯´ç†è§£èŒƒæ•°å°±æ˜¯ä¸€ä¸ªå…ƒç´ åˆ°é›¶ç‚¹çš„è·ç¦»å—ï¼Œé‚£å®šä¹‰èŒƒæ•°ä¸º||x||=dist(x,0) ä¸å°±è¡Œäº†å—ã€‚è¿™æ ·çš„è¯ï¼Œå¯¹äºèŒƒæ•°çš„ç¬¬äºŒæ¡æ€§è´¨å°±ä¸ä¸€å®šä¼šæ»¡è¶³ï¼Œ||ax||=dist(ax,0)ï¼Œè€Œ dist(ax,0)ä¸ä¸€å®šç­‰äº|a|dist(x,0)ï¼Œå…·ä½“ç­‰ä¸ç­‰äºè¿˜è¦çœ‹ä½ çš„è·ç¦»æ˜¯æ€ä¹ˆå®šä¹‰çš„ã€‚

ä¾‹å¦‚ï¼šL<sub>p</sub>èŒƒæ•°
æ¬§å¼è·ç¦»å¯¹åº” L2 èŒƒæ•°
æ›¼å“ˆé¡¿è·ç¦»å¯¹åº” L1 èŒƒæ•°
åˆ‡æ¯”é›ªå¤«è·ç¦»å¯¹åº” Lâˆ èŒƒæ•°
L<sub>p</sub>èŒƒæ•°ï¼šå½“ p>=1 æ—¶ï¼Œå‘é‡çš„ L<sub>p</sub>èŒƒæ•°æ˜¯å‡¸çš„ã€‚(è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä¸€èˆ¬ä¸ç”¨ L0 èŒƒæ•°çš„åŸå› ä¹‹ä¸€)

---

**çº¿æ€§èµ‹èŒƒç©ºé—´**å°±æ˜¯å®šä¹‰äº†åŠ æ³•ã€æ•°ä¹˜å’ŒèŒƒæ•°çš„ç©ºé—´ã€‚

---

**å·´æ‹¿èµ«ç©ºé—´**å°±æ˜¯**å®Œå¤‡çš„èµ‹èŒƒçº¿æ€§ç©ºé—´**ã€‚(Banach space)
**å®Œå¤‡çš„ç©ºé—´**çš„å®šä¹‰ï¼šå¦‚æœä¸€ä¸ªç©ºé—´æ˜¯å®Œå¤‡çš„ï¼Œé‚£ä¹ˆè¯¥ç©ºé—´ä¸­çš„ä»»ä½•ä¸€ä¸ªæŸ¯è¥¿åºåˆ—éƒ½æ”¶æ•›åœ¨è¯¥ç©ºé—´ä¹‹å†…ã€‚

é¦–å…ˆæ¥è¯´ä¸€ä¸‹æŸ¯è¥¿åºåˆ—æ˜¯ä»€ä¹ˆï¼ŒæŸ¯è¥¿åºåˆ—å°±æ˜¯éšç€åºæ•°å¢åŠ ï¼Œå€¼ä¹‹é—´çš„è·ç¦»è¶Šæ¥è¶Šå°çš„åºåˆ—ã€‚æ¢ä¸€ç§è¯´æ³•æ˜¯ï¼ŒæŸ¯è¥¿åºåˆ—å¯ä»¥åœ¨å»æ‰æœ‰é™ä¸ªå€¼ä¹‹åï¼Œä½¿ä»»æ„ä¸¤ä¸ªå€¼ä¹‹é—´çš„$\underline{\mathrm{è·ç¦»}}$éƒ½å°äºä»»æ„ç»™å®šæ­£å¸¸æ•°ï¼ˆå…¶å®è¿™å°±æ˜¯å®šä¹‰äº†ä¸€ä¸ªæé™è€Œå·²ï¼‰ã€‚

é‚£ä¹ˆä»»æ„ä¸€ä¸ªæŸ¯è¥¿åºåˆ—éƒ½æ”¶æ•›åœ¨è¯¥ç©ºé—´å†…æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Œä¸¾ä¸ªä¾‹å­ä½ å°±æ˜ç™½äº†ã€‚

è®¾å®šä¹‰åœ¨æœ‰ç†æ•°ç©ºé—´ Q ä¸Šçš„åºåˆ—ï¼š$x_n = \frac{[\sqrt{2}n]}{n}$ï¼Œå…¶ä¸­[x]è¡¨ç¤º x å–æ•´æ•°éƒ¨åˆ†ã€‚
å¯¹äºè¿™ä¸ªæ•°åˆ—æ¥è¯´ï¼Œæ¯ä¸€ä¸ªå…ƒç´ çš„åˆ†å­åˆ†æ¯éƒ½æ˜¯æ•´æ•°ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ª$x_n$éƒ½åœ¨æœ‰ç†æ•°ç©ºé—´ Q ä¸Šï¼Œé‚£è¿™ä¸ªåºåˆ—çš„æé™å‘¢ï¼Œç¨æœ‰å¸¸è¯†çš„äººéƒ½èƒ½çœ‹å‡ºï¼Œè¿™ä¸ªåºåˆ—çš„æé™æ˜¯$\sqrt{2}$ï¼Œè€Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæœ‰ç†æ•°ï¼Œæ‰€ä»¥è¿™ä¸ªæŸ¯è¥¿åºåˆ—çš„æé™ä¸åœ¨è¯¥ç©ºé—´é‡Œé¢ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰ç†æ•°ç©ºé—´ Q æ˜¯ä¸å®Œå¤‡çš„ã€‚

æ‰€ä»¥å®Œå¤‡çš„æ„ä¹‰æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ï¼Œé‚£å°±æ˜¯**åœ¨ä¸€ä¸ªç©ºé—´ä¸Šæˆ‘ä»¬å®šä¹‰äº†æé™ï¼Œä½†æ˜¯ä¸è®ºä½ æ€ä¹ˆå–æé™ï¼Œå®ƒçš„æé™çš„å€¼éƒ½ä¸ä¼šè·‘å‡ºè¿™ä¸ªç©ºé—´ï¼Œé‚£ä¹ˆè¿™ä¸ªç©ºé—´å°±æ˜¯å®Œå¤‡ç©ºé—´**ã€‚

å¦å¤–ï¼Œä¸çŸ¥é“ä½ æœ‰æ²¡æœ‰å‘ç°ï¼Œä¸Šé¢åœ¨è§£é‡Šä»€ä¹ˆæ˜¯æŸ¯è¥¿åºåˆ—çš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªè¯æˆ‘åŠ äº†ä¸‹åˆ’çº¿ï¼Œé‚£å°±æ˜¯è·ç¦»ï¼Œä¹Ÿå°±è¯´è¯´åœ¨å®šä¹‰å®Œå¤‡ç©ºé—´ä¹‹å‰ï¼Œè¦å…ˆæœ‰è·ç¦»çš„æ¦‚å¿µã€‚æ‰€ä»¥**å®Œå¤‡ç©ºé—´ï¼Œå…¶å®ä¹Ÿæ˜¯å®Œå¤‡åº¦é‡ç©ºé—´**ã€‚

æ‰€ä»¥ï¼Œå·´æ‹¿èµ«ç©ºé—´æ»¡è¶³å‡ æ¡ç‰¹æ€§å‘¢ï¼šè·ç¦»ã€èŒƒæ•°ã€å®Œå¤‡ã€‚

---

**å†…ç§¯ç©ºé—´**å°±æ˜¯å®šä¹‰äº†å†…ç§¯çš„ç©ºé—´ã€‚[Inner product space](https://en.jinzhao.wiki/wiki/Inner_product_space)
æœ‰æ—¶ä¹Ÿç§°å‡†å¸Œå°”ä¼¯ç‰¹ç©ºé—´ã€‚
å†…ç§¯å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„ç‚¹ä¹˜ã€æ ‡ç§¯ï¼Œå®ƒçš„å®šä¹‰æ–¹å¼ä¹Ÿä¸æ˜¯å”¯ä¸€çš„ï¼Œä½†å¦‚åŒè·ç¦»èŒƒæ•°çš„å®šä¹‰ä¸€æ ·ï¼Œå†…ç§¯çš„å®šä¹‰ä¹Ÿè¦æ»¡è¶³æŸäº›æ¡ä»¶ï¼Œä¸èƒ½éšä¾¿å®šä¹‰ã€‚

å®šä¹‰æ˜ å°„$\lang .,. \rang : V \times V \to \mathbb{F}$, å…¶ä¸­$V$æ˜¯å‘é‡ï¼Œ$\mathbb{F}$æ˜¯æ ‡é‡
æœ‰$x,y,z \in V ,s \in \mathbb{F}$ï¼Œé‚£ä¹ˆå†…ç§¯æ»¡è¶³

1. ç¬¬ä¸€ä¸ªå‚æ•°ä¸­çš„çº¿æ€§:
   $$\lang sx,y \rang = s\lang x,y \rang \\ \lang x+y,z \rang = \lang x,z \rang + \lang y,z \rang \\ \lang 0,x \rang = 0$$

2. å…±è½­å¯¹ç§°:$\lang x,y \rang = \overline{\lang y,x \rang }$

3. æ­£å®šæ€§:$\lang x,x \rang > 0 \quad\mathrm{if}\; x \neq 0$

4. æ­£åŠå®šæ€§æˆ–éè´Ÿå®šæ€§:$\forall{x}, \lang x,x \rang \geq 0 $

5. ç¡®å®šæ€§ï¼š$\lang x,x \rang = 0 å¿…ç„¶æœ‰ x=0$

3ï¼Œ4ï¼Œ5 å¯ä»¥è·Ÿä¸Šé¢å®šä¹‰èŒƒæ•°å’Œè·ç¦»ä¸€æ ·å†™æˆä¸€ä¸ª

ä¾‹å­-æ¬§å‡ é‡Œå¾—å‘é‡ç©ºé—´:
$ x,y \in \mathbb{R}^n , \lang x,y \rang = x^Ty=\sum\_{i=1}^n{x_iy_i}$

**åªæœ‰å®šä¹‰äº†å†…ç§¯ï¼Œæ‰ä¼šæœ‰å¤¹è§’çš„æ¦‚å¿µï¼Œæ‰ä¼šæœ‰æ­£äº¤çš„æ¦‚å¿µï¼Œå¦å¤–å†…ç§¯ä¹Ÿå¯ä»¥å®šä¹‰èŒƒæ•°ï¼Œä¹Ÿå°±æ˜¯è¯´å†…ç§¯æ˜¯æ¯”èŒƒæ•°æ›´å…·ä½“çš„ä¸€ä¸ªæ¦‚å¿µã€‚**

---

**æ¬§å¼ç©ºé—´**å°±æ˜¯å®šä¹‰äº†å†…ç§¯çš„æœ‰é™ç»´å®çº¿æ€§ç©ºé—´ã€‚

---

**å¸Œå°”ä¼¯ç‰¹ç©ºé—´**å°±æ˜¯å®Œå¤‡çš„å†…ç§¯ç©ºé—´ã€‚(Hilbert space)
å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„å…ƒç´ ä¸€èˆ¬æ˜¯å‡½æ•°ï¼Œå› ä¸ºä¸€ä¸ªå‡½æ•°å¯ä»¥è§†ä¸ºä¸€ä¸ªæ— ç©·ç»´çš„å‘é‡ã€‚

```mermaid
graph LR;
    LS(("Linear Space"))-->NLS(("Normed Linear Space"));
    NLS-->BS(("Banach Space"))
    NLS-->IPS(("Inner Product Space"))
    IPS-->HS(("Hilbert Space"))
    IPS-->ES(("Euclid Space"))
```

![](https://pic2.zhimg.com/80/v2-be26b2ba1df2edc9636647a28b22238d_720w.jpg?source=1940ef5c)

å‚è€ƒï¼š[ä¸€ç‰‡æ–‡ç« å¸¦ä½ ç†è§£å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰ä»¥åŠå„ç§ç©ºé—´](https://blog.csdn.net/ChangHengyi/article/details/80577318)

#### ç»´åº¦è¯…å’’

ç»´åº¦è¯…å’’é€šå¸¸æ˜¯æŒ‡åœ¨æ¶‰åŠåˆ°å‘é‡çš„è®¡ç®—çš„é—®é¢˜ä¸­ï¼Œéšç€ç»´æ•°çš„å¢åŠ ï¼Œè®¡ç®—é‡å‘ˆæŒ‡æ•°å€å¢é•¿çš„ä¸€ç§ç°è±¡ã€‚é«˜ç»´åº¦æœ‰æ›´å¤§çš„ç‰¹å¾ç©ºé—´ï¼Œéœ€è¦æ›´å¤šçš„æ•°æ®æ‰å¯ä»¥è¿›è¡Œè¾ƒå‡†ç¡®çš„ä¼°è®¡ã€‚

> è‹¥ç‰¹å¾æ˜¯äºŒå€¼çš„ï¼Œåˆ™æ¯å¢åŠ ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€éœ€æ•°æ®é‡éƒ½åœ¨ä»¥ 2 çš„æŒ‡æ•°çº§è¿›è¡Œå¢é•¿ï¼Œæ›´ä½•å†µå¾ˆå¤šç‰¹å¾ä¸åªæ˜¯äºŒå€¼çš„ã€‚

å‡ ä½•è§’åº¦ 1ï¼š

<svg width="52" height="52" xmlns="http://www.w3.org/2000/svg">
 <!-- Created with Method Draw - http://github.com/duopixel/Method-Draw/ -->
 <g>
  <title>background</title>
  <rect fill="#fff" id="canvas_background" height="54" width="54" y="-1" x="-1"/>
  <g display="none" overflow="visible" y="0" x="0" height="100%" width="100%" id="canvasGrid">
   <rect fill="url(#gridpattern)" stroke-width="0" y="0" x="0" height="100%" width="100%"/>
  </g>
 </g>
 <g>
  <title>Layer 1</title>
  <rect stroke="#000" id="svg_1" height="50" width="50" y="1.134891" x="1.227186" stroke-width="1.5" fill="#fff"/>
  <ellipse stroke="#000" ry="25" rx="25" id="svg_2" cy="26.316708" cx="25.727185" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" fill="#fff"/>
  <line stroke-linecap="null" stroke-linejoin="null" id="svg_3" y2="26.363651" x2="49.090879" y1="26.363651" x1="23.636325" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="none"/>
  <text stroke="#000" transform="matrix(0.8454890517551235,0,0,0.38060957631270753,66.36433546231878,120.48066499237646) " xml:space="preserve" text-anchor="start" font-family="Helvetica, Arial, sans-serif" font-size="24" id="svg_4" y="-262.016546" x="-56.089448" fill-opacity="null" stroke-opacity="null" stroke-width="0" fill="#000000">0.5</text>
 </g>
</svg>

ä¸Šå›¾è¡¨ç¤ºä¸€ä¸ªå¤šç»´ç©ºé—´ï¼ˆä»¥äºŒç»´ä¸ºä¾‹ï¼‰ï¼Œè®¾æ­£æ–¹å½¢è¾¹é•¿ä¸º 1ï¼Œåˆ™å…¶å†…åˆ‡åœ†åŠå¾„ä¸º$r=0.5$ï¼Œåˆ™æ­£æ–¹å½¢é¢ç§¯ä¸º 1ï¼Œå†…åˆ‡åœ†é¢ç§¯ä¸º$\pi(0.5)^2$ ã€‚è‹¥å°†æ­¤å˜ä¸ºä¸‰ç»´æƒ…å†µä¸‹ï¼Œæ­£æ–¹ä½“ä½“ç§¯ä¸º 1ï¼Œå†…åˆ‡çƒä½“ç§¯ä¸º$\frac{4}{3}\pi(0.5)^3$ã€‚

å› æ­¤çƒä½“çš„ä½“ç§¯å¯ä»¥è¡¨ç¤ºä¸º$V(d) = \frac{\pi^{d/2}}{\varGamma(\frac{d}{2}+1)}0.5^d = k(0.5)^d$(d ä¸ºç»´åº¦),åˆ™ $\lim_{d \to \infty}k(0.5)^d = 0$ï¼Œå…¶å†…åˆ‡è¶…çƒä½“çš„ä½“ç§¯ä¸º 0ã€‚ç”±æ­¤å¯çŸ¥ï¼Œ**é«˜ç»´æƒ…å†µä¸‹ï¼Œæ•°æ®å¤§éƒ½åˆ†å¸ƒåœ¨å››è§’ï¼ˆæ­£æ–¹å½¢å†…ï¼Œå†…åˆ‡åœ†å¤–ï¼‰**ï¼Œç¨€ç–æ€§å¤ªå¤§ï¼Œä¸å¥½åˆ†ç±»ã€‚

> ç»´åº¦è¶Šå¤§ï¼Œè¶…çƒä½“ä½“ç§¯è¶Šå°ã€‚è¯´æ˜è½åœ¨è¶…çƒä½“å†…çš„æ ·æœ¬è¶Šå°‘ï¼Œå› ä¸ºè¶…çƒä½“æ˜¯è¶…ç«‹æ–¹ä½“çš„å†…åˆ‡çƒã€‚ä¸åœ¨çƒå†…,é‚£åªèƒ½åœ¨è§’è½ï¼

å‡ ä½•è§’åº¦ 2ï¼š

<svg width="52" height="52" xmlns="http://www.w3.org/2000/svg">
 <!-- Created with Method Draw - http://github.com/duopixel/Method-Draw/ -->
 <g>
  <title>background</title>
  <rect fill="#fff" id="canvas_background" height="54" width="54" y="-1" x="-1"/>
  <g display="none" overflow="visible" y="0" x="0" height="100%" width="100%" id="canvasGrid">
   <rect fill="url(#gridpattern)" stroke-width="0" y="0" x="0" height="100%" width="100%"/>
  </g>
 </g>
 <g>
  <title>Layer 1</title>
  <ellipse stroke="#000" ry="25" rx="25" id="svg_5" cy="25" cx="25" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" fill="#fff"/>
  <ellipse id="svg_6" cy="24.593763" cx="34.636353" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="#fff"/>
  <ellipse ry="20" rx="20" id="svg_7" cy="25" cx="25" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="#fff"/>
 </g>
</svg>

ä¸Šå›¾ä¹Ÿè¡¨ç¤ºä¸€ä¸ªå¤šç»´ç©ºé—´ï¼ˆä»¥äºŒç»´ä¸ºä¾‹ï¼‰ï¼Œåˆ™å…¶ä¸­å›¾å½¢çš„ä½“ç§¯æœ‰å¦‚ä¸‹å…³ç³»ï¼šå¤–åœ†åŠå¾„$r=1$ï¼Œå†…åœ†åŠå¾„ä¸º$râˆ’\varepsilon$ ã€‚åŒæ ·åœ¨é«˜ç»´æƒ…å†µä¸‹ï¼Œå¤–åœ†ä½“ç§¯ä¸º$V_{å¤–åœ†} = k.1^d = k$ï¼Œä¸­é—´çš„åœ†ç¯ä½“ç§¯ä¸º$V_{åœ†ç¯} = k - k(1-\varepsilon)^d$ï¼Œåˆ™ï¼š
$$\lim_{d \to \infty}\frac{V_{åœ†ç¯}}{V_{å¤–åœ†}} = \lim_{d \to \infty}\frac{ k - k(1-\varepsilon)^d}{k} = \lim_{d \to \infty}(1-(1-\varepsilon)^d) = 1$$

> é«˜ç»´æƒ…å†µä¸‹ï¼Œæ— è®º$\varepsilon$å¤šå°ï¼Œåªè¦ d è¶³å¤Ÿå¤§ï¼Œåœ†ç¯å‡ ä¹å æ®äº†æ•´ä¸ªå¤–åœ†ï¼Œå†…åœ†ä½“ç§¯è¶‹å‘äº 0ï¼Œå¯¼è‡´æ•°æ®**ç¨€ç–**ã€‚

å‚è€ƒï¼š
[The Curse of Dimensionality in classification](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(äº”)-é™ç»´ï¼ˆDimensionality Reductionï¼‰](https://www.bilibili.com/video/BV1vW411S7tH)

#### ä¸ç­‰å¼(Inequality)

[æ‰€æœ‰ä¸ç­‰å¼](https://en.jinzhao.wiki/wiki/Category:Inequalities) ä»¥åŠ[æ‰€æœ‰æ¦‚ç‡ï¼ˆProbabilisticï¼‰ä¸ç­‰å¼](https://en.jinzhao.wiki/wiki/Category:Probabilistic_inequalities)

- **[ç»å¯¹å€¼ä¸ç­‰å¼](https://chi.jinzhao.wiki/wiki/%E7%BB%9D%E5%AF%B9%E5%80%BC%E4%B8%8D%E7%AD%89%E5%BC%8F) - Absolute value inequality**

- **å¹‚å¹³å‡å€¼ä¸ç­‰å¼- [Power-Mean Inequality](https://artofproblemsolving.com/wiki/index.php/Power_Mean_Inequality)**

- **[ä¸‰è§’å½¢å†…è§’çš„åµŒå…¥ä¸ç­‰å¼](https://chi.jinzhao.wiki/wiki/%E4%B8%89%E8%A7%92%E5%BD%A2%E5%86%85%E8%A7%92%E7%9A%84%E5%B5%8C%E5%85%A5%E4%B8%8D%E7%AD%89%E5%BC%8F) - æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º Wolstenholme ä¸ç­‰å¼**

- **ä¼¯åŠªåˆ©ä¸ç­‰å¼ - [Bernoulli's inequality](https://en.jinzhao.wiki/wiki/Bernoulli%27s_inequality)**
- **æ’åºä¸ç­‰å¼ - [Rearrangement inequality](https://en.jinzhao.wiki/wiki/Rearrangement_inequality)**
- **å‡å€¼ä¸ç­‰å¼ - [Inequality of arithmetic and geometric means](https://en.jinzhao.wiki/wiki/Inequality_of_arithmetic_and_geometric_means)**

- **èˆ’å°”ä¸ç­‰å¼ - [Schur's inequality](https://en.jinzhao.wiki/wiki/Schur%27s_inequality)**

- **é—µå¯å¤«æ–¯åŸº (Minkowski) ä¸ç­‰å¼ - [Minkowski inequality](https://en.jinzhao.wiki/wiki/Minkowski_inequality)**

##### æ¦‚ç‡ä¸ç­‰å¼ Probabilistic inequalities

- **æŸ¯è¥¿-æ–½ç“¦èŒ¨ (Cauchyâ€“Schwarz) ä¸ç­‰å¼ - [Cauchyâ€“Schwarz inequality](https://en.jinzhao.wiki/wiki/Cauchy%E2%80%93Schwarz_inequality)**
  $$[\sum_{i=1}^{n}{a_ib_i}]^2  \leq [\sum_{i=1}^{n}a_i^2].[\sum_{i=1}^{n}b_i^2] ç­‰å¼æˆç«‹ï¼šb_i=ka_i \\ å‘é‡å½¢å¼ï¼š|\braket{u,v}| \leq ||u||.||v|| \\ æ¦‚ç‡ä¸­ï¼š|E(XY)|^2 \leq E(X^2)E(Y^2)$$
  è¯æ˜ï¼š
  $$\vec{A} = (a_1,...,a_n),  \vec{B} = (b_1,...,b_n) \\ \vec{A}.\vec{B} = (a_1b_1,...,a_nb_n) = ||\vec{A}||.||\vec{B}||\cos\theta \leq ||\vec{A}||.||\vec{B}|| = \sqrt{a_1^2+...+a_n^2}.\sqrt{b_1^2+...+b_n^2}$$
  åº”ç”¨:

  1. è¯æ˜ covariance inequalityï¼š$Var(Y) \geq \frac{Cov(Y,X)^2}{Var(X)}$,æœ‰$\braket{X,Y} := E(XY)$
     $$|Cov(Y,X)|^2 = |E((X-\mu)(Y-v))|^2 = |\braket{X-\mu,Y-v}|^2 \\ \leq \braket{X-\mu,X-\mu}\braket{Y-v,Y-v} = E((X-\mu)^2)E((Y-v)^2) = Var(X)Var(Y)$$

- **èµ«å°”å¾· (Holder) ä¸ç­‰å¼ - [HÃ¶lder's inequality](https://en.jinzhao.wiki/wiki/H%C3%B6lder%27s_inequality)**

- **ç´ç”Ÿ (Jensen) ä¸ç­‰å¼ - [Jensen's inequality](https://en.jinzhao.wiki/wiki/Jensen%27s_inequality)**
  $$f(tx_1 +(1-t)x_2) \leq tf(x_1) + (1-t)f(x_2), \text{f is convex function} \\ æ¨å¹¿ï¼šf(a_1x_1 +...+ a_nx_n) \leq a_1f(x_1) +...+ a_nf(x_n), a_1+...+a_n = 1 , a_i \geq 0 \\ or: f(\sum_{i=1}^n{a_ix_i}) \leq \sum_{i=1}^n{a_if(x_i)} , \sum_{i=1}^n{a_i} = 1, a_i \geq 0$$

  æ¦‚ç‡ä¸­ï¼šå¦‚æœ$X$æ˜¯éšæœºå˜é‡ï¼Œè€Œ$\varphi$æ˜¯å‡¸å‡½æ•°ï¼Œåˆ™:$\varphi(E[X]) \leq E[\varphi(X)]$,ä¸ç­‰å¼ä¸¤è¾¹çš„å·®ï¼Œ$ E[\varphi(X)] - \varphi(E[X]) $ç§°ä¸º Jensen gap(é—´éš™)ï¼›
  åº”ç”¨ï¼š

  1. EM ç®—æ³•ä¸­æœ‰ç”¨åˆ°(log å‡½æ•°æ˜¯å‡¹å‡½æ•°æ­£å¥½ä¸å‡¸å‡½æ•°ç›¸å);
  2. è¯æ˜ KL æ•£åº¦>=0;

- **é©¬å°”å¯å¤«ä¸ç­‰å¼ - [Markov's inequality](https://en.jinzhao.wiki/wiki/Markov%27s_inequality)**
  $$P(X \geq a) \leq \frac{E(X)}{a}$$
  å…¶ä¸­$X$ä¸ºéè´Ÿéšæœºå˜é‡ï¼Œ$\forall a>0$
  åº”ç”¨ï¼š

  1. ç”¨äºä¼°è®¡ä¸€ä¸ªæ¦‚ç‡çš„ä¸Šç•Œï¼Œæ¯”å¦‚å‡è®¾ä½ æ‰€åœ¨å…¬å¸çš„äººå‡å·¥èµ„æ˜¯ 1 ä¸‡ï¼Œé‚£ä¹ˆéšæœºé€‰ä¸€ä¸ªä½ å¸å‘˜å·¥ï¼Œå…¶å·¥èµ„è¶…è¿‡ 10 ä¸‡çš„æ¦‚ç‡ï¼Œä¸ä¼šè¶…è¿‡ 1/10ï¼›
  2. ç”¨äºå…¶ä»–æ¦‚ç‡ä¸ç­‰å¼çš„è¯æ˜ï¼Œæ¯”å¦‚éœå¤«ä¸ä¸ç­‰å¼ï¼›

- **åˆ‡æ¯”é›ªå¤« (Chebyshev) ä¸ç­‰å¼ - [Chebyshev's inequality](https://en.jinzhao.wiki/wiki/Chebyshev%27s_inequality)**
  $$P\{|X-\mu| \geq k\} \leq \frac{\sigma^2}{k^2}$$
  å…¶ä¸­$X$ä¸ºéšæœºå˜é‡ï¼Œ$\forall k>0$, $\mu$ä¸ºå‡å€¼ï¼Œ$\sigma^2$ä¸ºæ–¹å·®
  ï¼ˆè¯æ˜å¯ä»¥åˆ©ç”¨é©¬å°”å¯å¤«ä¸ç­‰å¼ï¼Œè§æ¦‚ç‡è®ºåŸºç¡€æ•™ç¨‹ 313 é¡µï¼‰

- **éœå¤«ä¸ä¸ç­‰å¼ - [Hoeffding's inequality](https://en.jinzhao.wiki/wiki/Hoeffding%27s_inequality)**
  åº”ç”¨ï¼š
  1. [Machine Learning Foundationsï¼ˆæœºå™¨å­¦ä¹ åŸºçŸ³ï¼‰- feasibility of learning,12,13,18 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/04_handout.pdf)
  2. ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼Œ26 é¡µï¼Œè¯æ˜æ³›åŒ–è¯¯å·®ä¸Šç•Œï¼ˆåœ¨[æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„å‡ ä¸ªæ¦‚ç‡ä¸ç­‰å¼åŠè¯æ˜](https://zhuanlan.zhihu.com/p/392348396)ä¸­ä¹Ÿæœ‰æåˆ°ï¼‰

å‚è€ƒï¼š[åˆç­‰æ•°å­¦å­¦ä¹ ç¬”è®°](https://github.com/zhcosin/elementary-math/blob/master/elementary-math-note.pdf)

### å‚è€ƒæ–‡çŒ®

[1-1] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[1-2] Bishop M. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). Springer,2006

[1-3] [Probabilistic Graphical Models: Principles and Techniques](https://djsaunde.github.io/read/books/pdfs/probabilistic%20graphical%20models.pdf) by Daphne Koller, Nir Friedman from The MIT Press

[1-4] [Deep Learning](https://raw.fastgit.org/Zhenye-Na/machine-learning-uiuc/master/docs/Deep%20Learning.pdf) (Ian Goodfellow, Yoshua Bengio, Aaron Courville)

[1-5] Tom M Michelle. [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html). McGraw-Hill Companies,Inc. 1997ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬ï¼šæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003ï¼‰

[1-6] [Bayesian Reasoning and Machine Learning by David Barber 2007â€“2020](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf) ,[other version](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/)

[1-7] [Reinforcement Learning:An Introduction (second edition 2020) by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020trimmed.pdf) ,[other version](http://incompleteideas.net/book/)

[1-8] å‘¨å¿—åï¼Œ[æœºå™¨å­¦ä¹ ](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)ï¼Œæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ ([æ‰‹æ¨ç¬”è®°](https://github.com/Sophia-11/Machine-Learning-Notes) ä»¥åŠ [å…¬å¼æ¨å¯¼è§£æ](https://github.com/datawhalechina/pumpkin-book))

[1-9] [Lecture Notes in MACHINE LEARNING](https://news.vidyaacademy.ac.in/wp-content/uploads/2018/10/NotesOnMachineLearningForBTech-1.pdf) Dr V N Krishnachandran

## ç¬¬ 2 ç«  æ„ŸçŸ¥æœº

æ„ŸçŸ¥æœº[Perceptron](https://en.jinzhao.wiki/wiki/Perceptron)æ˜¯**ç¥ç»ç½‘ç»œ**å’Œ**æ”¯æŒå‘é‡æœº**çš„åŸºç¡€ã€‚æœ€æ—©åœ¨ 1957 å¹´ç”± Rosenblatt æå‡º$^{å‚è€ƒæ–‡çŒ®[2-1]}$ã€‚Novikoff$^{å‚è€ƒæ–‡çŒ®[2-2]}$ï¼ŒMinsky ä¸ Papert$^{å‚è€ƒæ–‡çŒ®[2-3]}$ç­‰äººå¯¹æ„ŸçŸ¥æœºè¿›è¡Œäº†ä¸€ç³»åˆ—ç†è®ºç ”ç©¶ã€‚æ„ŸçŸ¥æœºçš„æ‰©å±•å­¦ä¹ æ–¹æ³•åŒ…æ‹¬å£è¢‹ç®—æ³•(pocket algorithm)$^{å‚è€ƒæ–‡çŒ®[2-4]}$ã€è¡¨å†³æ„ŸçŸ¥æœº(voted perceptron)$^{å‚è€ƒæ–‡çŒ®[2-5]}$ã€å¸¦è¾¹ç¼˜æ„ŸçŸ¥æœº(perceptron with margin)$^{å‚è€ƒæ–‡çŒ®[2-6]}$ç­‰ã€‚
[Brief History of Machine Learning](https://erogol.com/brief-history-machine-learning/)

è¦æ±‚ï¼šæ•°æ®é›†çº¿æ€§å¯åˆ†(linearly separable data set)

- **æ¨¡å‹**ï¼š
  $$f(x) = sign(w.x + b)$$
  å…¶ä¸­$x,w \in \mathbb{R}^n ,b \in \mathbb{R}$,$w$å«ä½œæƒå€¼ï¼ˆweightï¼‰æˆ–æƒå€¼å‘é‡ï¼ˆweight vectorï¼‰ï¼Œ$b$å«ä½œåç½®ï¼ˆbiasï¼‰ï¼Œsign æ˜¯ç¬¦å·å‡½æ•°
  $$
  sign(x) = \begin{cases}
     +1 & x \geq 0 \\
     -1 & x<0
  \end{cases}
  $$

æ„ŸçŸ¥æœºæ˜¯ä¸€ç§çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼Œå±äºåˆ¤åˆ«æ¨¡å‹ã€‚æ„ŸçŸ¥æœºæ¨¡å‹çš„å‡è®¾ç©ºé—´æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©º é—´ä¸­çš„æ‰€æœ‰çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼ˆlinear classification modelï¼‰æˆ–çº¿æ€§åˆ†ç±»å™¨(linear classifier)ï¼Œå³ å‡½æ•°é›†åˆ$\{f|f(x)ï¼wÂ·x+b\}$

è¶…å¹³é¢ Sï¼š$w.x+b = 0$,å…¶ä¸­$w$æ˜¯ S çš„æ³•å‘é‡ï¼Œ$b$æ˜¯ S çš„æˆªè·ï¼Œè¶…å¹³é¢ S ç§°ä¸ºåˆ†ç¦»è¶…å¹³é¢ï¼ˆseparating hyperplaneï¼‰

- **ç­–ç•¥**ï¼š
  $$L(w,b) = -\sum_{x_i \in M}{y_i(w.x_i + b)}$$
  å…¶ä¸­$M$ä¸ºè¯¯åˆ†ç±»ç‚¹çš„é›†åˆã€‚è¯¯åˆ†ç±»æ•°æ®$M = \{ (x_i,y_i)|-y_i(w.x_i +b) > 0\}$

å‡½æ•°é—´éš”ï¼š$y(w.x + b)$
å‡ ä½•é—´éš”ï¼š$\frac{1}{||w||}|w.x + b|$ (åœ¨ä¸Šé¢çš„ loss function ä¸­æ²¡æœ‰è€ƒè™‘$\frac{1}{||w||}$)

- **ç®—æ³•**ï¼š
  $$\min_{w,b} L(w,b) = -\sum_{x_i \in M}{y_i(w.x_i + b)}$$
  ä½¿ç”¨**éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆstochastic gradientï¼‰**:

1. åˆå§‹åŒ–å‚æ•°(éšæœºæ³•)ï¼š$w_0,b_0$
2. é€‰å–æ•°æ®$(x_i,y_i)$
3. å¦‚æœ$(x_i,y_i)$æ˜¯è¯¯åˆ†ç±»ç‚¹ï¼Œä¹Ÿå°±æ˜¯$y_i(w.x_i + b) \leq 0$ï¼Œåˆ™å¯¹$w,b$è¿›è¡Œæ›´æ–°
   $$åœ¨(x_i,y_i)ç‚¹å¤„æ¢¯åº¦ä¸ºï¼š\\ \nabla_wL(w,b) = -y_ix_i \\ \nabla_bL(w,b) = -y_i\\ æ›´æ–°wï¼šw_{k+1} \gets w_{k}+\eta y_ix_i \\ æ›´æ–°bï¼šb_{k+1} \gets b_{k}+\eta y_i \\å…¶ä¸­å­¦ä¹ ç‡\eta \in (0,1]$$
4. å¾ªç¯ 2-3ï¼Œç›´åˆ°è®­ç»ƒé›†ä¸­æ²¡æœ‰è¯¯åˆ†ç±»ç‚¹ã€‚

- ä¸Šè¿°**ç®—æ³•çš„æ”¶æ•›æ€§**ï¼š

Novikoff å®šç†ï¼š
è®¾è®­ç»ƒé›†$T = \{(x_1,y_1),...,(x_N,y_N)\}$æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œ

1. è®¾å®Œç¾è¶…å¹³é¢$\hat{w}_{opt}.\hat{x} = 0 , ||\hat{w}_{opt}||=1$ å°†è®­ç»ƒé›†å®Œå…¨æ­£ç¡®åˆ†å¼€ï¼ˆç®€åŒ–èµ·è§ $\hat{w}_{opt}.\hat{x} = w_{opt}.x +b$ï¼‰ï¼Œå­˜åœ¨$\gamma >0$ ,å¯¹æ‰€æœ‰ç‚¹æœ‰$y_i(\hat{w}_{opt}.\hat{x_i}) \geq \gamma$ï¼›

2. ä»¤$R = \max_{1\leq i\leq N}||\hat{x_i}||$,åˆ™ç®—æ³•ä¼šåœ¨æœ‰é™æ­¥ k æ»¡è¶³ä¸ç­‰å¼$k \leq (\frac{R}{\gamma})^2$

è¯æ˜(æ³¨æ„ï¼šå¸¦ hat çš„è¡¨ç¤ºæ‰©å……å‘é‡)ï¼š

1. å› ä¸ºæ•°æ®çº¿æ€§å¯åˆ†ï¼Œå¯¹äºæ‰€æœ‰ç‚¹$y_i(\hat{w}_{opt}.\hat{x_i}) > 0$,æ‰€ä»¥å­˜åœ¨
   $$\gamma = \min_i{y_i(\hat{w}_{opt}.\hat{x_i})} \leq {y_i(\hat{w}_{opt}.\hat{x_i})} \label{2-1}\tag{2-1}$$
   æ‰€ä»¥è¿™é‡Œçš„$\gamma$ä»£è¡¨äº†æ‰€æœ‰ç‚¹ç¦»å®Œç¾è¶…å¹³é¢çš„æœ€å°è·ç¦»ï¼›

2. ä¸ºäº†æ–¹ä¾¿è®¡ç®— è®¾ æ‰©å……å‘é‡$\hat{w} = (w^T,b)^T$ï¼Œ æœ‰
   $$\hat{w}_{k} = \hat{w}_{k-1}+\eta y_i\hat{x_i} \label{2-2}\tag{2-2}$$

3. æ¨å¯¼ä¸ç­‰å¼
   $$\hat{w}_{k}.\hat{w}_{opt} \geq k\eta\gamma \label{2-3}\tag{2-3}$$

ç”±$\eqref{2-1}$å’Œ$\eqref{2-2}$
$$\hat{w}_{k}.\hat{w}_{opt} = \hat{w}_{k-1}.\hat{w}_{opt} + \eta{y_i}\hat{w}_{opt}.\hat{x_i} \\ \geq \hat{w}_{k-1}.\hat{w}_{opt} + \eta\gamma \\ \geq \hat{w}_{k-2}.\hat{w}_{opt} + 2\eta\gamma \\ \geq k\eta\gamma$$

4. æ¨å¯¼ä¸ç­‰å¼
   $$||\hat{w}_{k}||^2 \leq k\eta^2R^2 \label{2-4}\tag{2-4}$$
   ç”±$\eqref{2-2}$
   $$||\hat{w}_{k}||^2=||\hat{w}_{k-1}+\eta y_i\hat{x_i}||^2 = ||\hat{w}_{k-1}||^2 + 2\eta{y_i}\hat{w}_{k-1}.\hat{x}_{i} + \eta^2||\hat{x}_{i}||^2$$
   å‡è®¾ k æ¬¡å®Œå…¨åˆ†å¯¹ï¼Œé‚£ä¹ˆ k-1 æ¬¡æœ‰è¯¯åˆ†ç±»ç‚¹ï¼Œåˆ™${y_i}\hat{w}_{k-1}.\hat{x}_{i} \leq 0$
   æ‰€ä»¥
   $$||\hat{w}_{k}||^2 =||\hat{w}_{k-1}||^2 + 2\eta{y_i}\hat{w}_{k-1}.\hat{x}_{i} + \eta^2||\hat{x}_{i}||^2 \\ \leq ||\hat{w}_{k-1}||^2 +  \eta^2||\hat{x}_{i}||^2 \\ \leq ||\hat{w}_{k-1}||^2 +  \eta^2R^2  \\ \leq ||\hat{w}_{k-2}||^2 +  2\eta^2R^2 \leq ... \\ \leq k\eta^2R^2$$

5. ç”±$\eqref{2-3}$å’Œ$\eqref{2-4}$

$$k\eta\gamma \leq \underbrace{\hat{w}_{k}.\hat{w}_{opt} \leq ||\hat{w}_{k}||.\underbrace{||\hat{w}_{opt}||}_{=1} }_{\text{æŸ¯è¥¿-æ–½ç“¦èŒ¨ (Cauchyâ€“Schwarz) ä¸ç­‰å¼}} \leq \sqrt{k} \eta R \\ \; \\ \Rightarrow k^2\gamma^2 \leq kR^2 \\ \Rightarrow k \leq (\frac{R}{\gamma})^2$$

ä¹Ÿå°±æ˜¯è¯´ k æ˜¯æœ‰ä¸Šç•Œçš„ã€‚

### å‚è€ƒæ–‡çŒ®

[2-1] Rosenblatt, F. (1958). [The perceptron: A probabilistic model for information storage and organization in the brain](http://homepages.math.uic.edu/~lreyzin/papers/rosenblatt58.pdf). Psychological Review, 65(6), 386â€“408.

[2-2] Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.

[2-3] Minsky M L and Papert S A 1969 Perceptrons (Cambridge, MA: MIT Press)

[2-4] Gallant, S. I. (1990). Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179-191.

[2-5] Freund, Y. and Schapire, R. E. 1998. Large margin classification using the perceptron algorithm. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT' 98). ACM Press.

[2-6] Li YY,Zaragoza H,Herbrich R,Shawe-Taylor J,Kandola J. The Perceptron algorithmwith uneven margins. In: Proceedings of the 19th International Conference on MachineLearning. 2002,379â€“386

[2-7] [Widrow, B.](https://en.jinzhao.wiki/wiki/Bernard_Widrow), Lehr, M.A., "[30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation,](http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/widrow.pdf)" Proc. IEEE, vol 78, no 9, pp. 1415-1442, (1990)ã€‚

[2-8] Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines and OtherKernelbased Learning Methods. Cambridge University Press,2000

## ç¬¬ 3 ç«  k è¿‘é‚»æ³•

k è¿‘é‚»æ³•ï¼ˆ[k-nearest neighborï¼Œk-NN](https://en.jinzhao.wiki/wiki/K-nearest_neighbors_algorithm)ï¼‰1968 å¹´ç”± Cover å’Œ Hart æå‡ºï¼Œæ˜¯ä¸€ç§åŸºæœ¬åˆ†ç±»ä¸å›å½’æ–¹æ³•ã€‚æœ¬ä¹¦åªè®¨è®ºåˆ†ç±»é—®é¢˜ä¸­çš„ k è¿‘é‚»æ³•ã€‚
k å€¼çš„é€‰æ‹©ã€è·ç¦»åº¦é‡åŠåˆ†ç±»å†³ç­–è§„åˆ™æ˜¯ k è¿‘é‚»æ³•çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ ã€‚
æœ€åè®²è¿° k è¿‘é‚»æ³•çš„ä¸€ä¸ªå®ç°æ–¹æ³•â€”â€”kd æ ‘ï¼Œä»‹ç»æ„é€  kd æ ‘å’Œæœç´¢ kd æ ‘çš„ç®—æ³•

**k è¿‘é‚»æ³•çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ **ï¼š
k å€¼çš„é€‰æ‹©ï¼šè¶…å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯æ³•æ¥é€‰å–æœ€ä¼˜ k å€¼
è·ç¦»åº¦é‡ï¼š$L_2$è·ç¦»/æ¬§æ°è·ç¦»ï¼Œ$L_p$è·ç¦»/Minkowski è·ç¦»
åˆ†ç±»å†³ç­–è§„åˆ™ï¼šå¤šæ•°è¡¨å†³ï¼ˆ0-1 æŸå¤±ä¹Ÿå°±æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼‰

- **æ¨¡å‹**ï¼š
  k è¿‘é‚»æ³•æ²¡æœ‰æ˜¾å¼çš„å­¦ä¹ è¿‡ç¨‹ï¼ˆä¸å­¦ä¹ ä¹Ÿèƒ½é¢„æµ‹ï¼‰ï¼Œå®ƒæœ¬èº«å¹¶æ²¡æœ‰å¯¹æ•°æ®è¿›è¡Œç†è®ºå»ºæ¨¡çš„è¿‡ç¨‹ï¼Œè€Œæ˜¯åˆ©ç”¨è®­ç»ƒæ•°æ®å¯¹ç‰¹å¾å‘é‡ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶å°†å…¶åˆ’åˆ†çš„ç»“æœä½œä¸ºå…¶æœ€ç»ˆçš„ç®—æ³•æ¨¡å‹ã€‚è¿™å°±å¥½æ¯”ï¼Œåœ¨ç°å®ä¸–ç•Œçš„ç»´åº¦ä¸­ï¼Œç»å¸¸æ¸¸èµ°äºç”·å•æ‰€çš„æˆ‘ä»¬å½’ä¸ºç”·æ€§ï¼Œè€Œç»å¸¸åœ¨å¥³å•æ‰€å‡ºæ²¡çš„äººæˆ‘ä»¬å½’ä¸ºå¥³æ€§æˆ–è€…æ˜¯å˜æ€ã€‚

- **ç­–ç•¥**ï¼š
  $$y = \argmin_{c_j} \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1- \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j) $$
  æœ€å¤§åŒ–ç±»åˆ«å±äº$c_j$ç±»çš„æ¦‚ç‡$\frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j)$
  æœ€å°åŒ–è¯¯åˆ†ç±»ç‡$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j)$
  $N_k(x)$è¡¨ç¤ºæ¶µç›– k ä¸ªç‚¹çš„ x çš„é‚»åŸŸ
- **ç®—æ³•**ï¼š
  ç›´æ¥è®¡ç®—ï¼ˆçº¿æ€§æ‰«æ linear scanï¼‰,å½“è®­ç»ƒé›†å¾ˆå¤§æ—¶ï¼Œè®¡ç®—å¾ˆè€—æ—¶ï¼ˆæ¯æ¬¡éƒ½è¦è®¡ç®—æ‰€æœ‰è·ç¦»ï¼Œç„¶åæ‰¾åˆ° k ä¸ªæœ€è¿‘è·ç¦»çš„ç‚¹ï¼‰ï¼Œå› ä¸ºæ²¡æœ‰å­¦ä¹ ã€‚
  ä¸ºäº†æé«˜ k è¿‘é‚»æœç´¢çš„æ•ˆç‡ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç‰¹æ®Šçš„ç»“æ„å­˜å‚¨è®­ç»ƒæ•°æ®ï¼Œä»¥å‡å°‘è®¡ç®—è·ç¦»çš„æ¬¡æ•°ã€‚
  å…·ä½“æ–¹æ³•å¾ˆå¤šï¼Œå¦‚ï¼š[kd_tree](https://en.jinzhao.wiki/wiki/K-d_tree)ï¼Œ[ball_tree](https://arxiv.org/pdf/1511.00628.pdf)ï¼Œbrute(è›®åŠ›å®ç°,ä¸ç®—ä¼˜åŒ–ï¼Œåªæ˜¯æŠŠ sklearn ä¸­çš„å‚æ•°æ‹¿è¿‡æ¥)
  ![](img/kd-tree-vs-ball-tree.png)ï¼Œä»¥åŠå…¶å®ƒ[æ ‘ç»“æ„](<https://en.jinzhao.wiki/wiki/Category:Trees_(data_structures)>)
  ä¸ºäº†æ”¹è¿› KDtree çš„äºŒå‰æ ‘æ ‘å½¢ç»“æ„ï¼Œå¹¶ä¸”æ²¿ç€ç¬›å¡å°”åæ ‡è¿›è¡Œåˆ’åˆ†çš„ä½æ•ˆç‡ï¼Œball tree å°†åœ¨ä¸€ç³»åˆ—åµŒå¥—çš„è¶…çƒä½“ä¸Šåˆ†å‰²æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼šä½¿ç”¨è¶…çƒé¢è€Œä¸æ˜¯è¶…çŸ©å½¢åˆ’åˆ†åŒºåŸŸã€‚è™½ç„¶åœ¨æ„å»ºæ•°æ®ç»“æ„çš„èŠ±è´¹ä¸Šå¤§è¿‡äº KDtreeï¼Œä½†æ˜¯åœ¨é«˜ç»´ç”šè‡³å¾ˆé«˜ç»´çš„æ•°æ®ä¸Šéƒ½è¡¨ç°çš„å¾ˆé«˜æ•ˆã€‚

  ä¸‹é¢ä»‹ç»å…¶ä¸­çš„ kd æ ‘ï¼ˆkd tree æ˜¯ä¸€ä¸ªäºŒå‰æ ‘ï¼‰æ–¹æ³•ï¼ˆkd æ ‘æ˜¯å­˜å‚¨ k ç»´ç©ºé—´æ•°æ®çš„æ ‘ç»“æ„ï¼Œè¿™é‡Œçš„ k ä¸ k è¿‘é‚»æ³•çš„ k æ„ä¹‰ä¸åŒï¼‰ã€‚
  æ•°æ®é›†$T = \{x_1,...,x_N\}$ï¼Œå…¶ä¸­$x_i$æ˜¯ k ç»´å‘é‡$x_i = (x_i^{(1)},...,x_i^{(k)})^T$

  - **æ„é€  kd æ ‘**ï¼š

  ```
  function kdtree (list of points pointList, int depth)
  {
      // Select axis based on depth so that axis cycles through all valid values
      var int axis := depth mod k;

      // Sort point list and choose median as pivot element
      select median by axis from pointList;

      // Create node and construct subtree
      node.location := median;
      node.leftChild := kdtree(points in pointList before median, depth+1);
      node.rightChild := kdtree(points in pointList after median, depth+1);
      return node;
  }
  ```

  1. æ ¹æ®ç¬¬(depth mod k)ç»´æŸ¥æ‰¾ä¸­ä½æ•°ï¼ˆä¸­ä½æ•°æ‰€åœ¨çš„ç‚¹ä½œä¸ºèŠ‚ç‚¹ï¼Œç¬¬ä¸€æ¬¡å°±æ˜¯ root èŠ‚ç‚¹ï¼‰ï¼Œå°†æ•°æ®åˆ’åˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸï¼Œå°äºä¸­ä½æ•°çš„åˆ’åˆ†åœ¨å·¦è¾¹ï¼Œå¤§äºä¸­ä½æ•°çš„åˆ’åˆ†åœ¨å³è¾¹
  2. é‡å¤ 1ï¼Œdepth++

  - **æœç´¢ kd æ ‘**ï¼š

  1. åœ¨ kd æ ‘ä¸­æ‰¾å‡ºåŒ…å«ç›®æ ‡ç‚¹ x çš„å¶ç»“ç‚¹ï¼šä»æ ¹ç»“ç‚¹å‡ºå‘ï¼Œé€’å½’åœ°å‘ä¸‹è®¿é—® kd æ ‘ã€‚è‹¥ç›®æ ‡ç‚¹ x å½“å‰ç»´çš„åæ ‡å°äºåˆ‡åˆ†ç‚¹çš„åæ ‡ï¼Œåˆ™ç§»åŠ¨åˆ°å·¦å­ç»“ç‚¹ï¼Œå¦åˆ™ç§»åŠ¨åˆ°å³å­ç»“ç‚¹ã€‚ç›´åˆ°å­ç»“ç‚¹ä¸ºå¶ç»“ç‚¹ä¸ºæ­¢ã€‚
  2. ä»¥æ­¤å¶ç»“ç‚¹ä¸ºâ€œå½“å‰æœ€è¿‘ç‚¹â€ã€‚
  3. é€’å½’åœ°å‘ä¸Šå›é€€ï¼Œåœ¨æ¯ä¸ªç»“ç‚¹è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
     a. å¦‚æœè¯¥ç»“ç‚¹ä¿å­˜çš„å®ä¾‹ç‚¹æ¯”å½“å‰æœ€è¿‘ç‚¹è·ç¦»ç›®æ ‡ç‚¹æ›´è¿‘ï¼Œåˆ™ä»¥è¯¥å®ä¾‹ç‚¹ä¸ºâ€œå½“å‰æœ€è¿‘ç‚¹â€ã€‚
     b. å½“å‰æœ€è¿‘ç‚¹ä¸€å®šå­˜åœ¨äºè¯¥ç»“ç‚¹ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸã€‚æ£€æŸ¥è¯¥å­ç»“ç‚¹çš„çˆ¶ç»“ç‚¹çš„å¦ä¸€å­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸæ˜¯å¦æœ‰æ›´è¿‘çš„ç‚¹ã€‚å…·ä½“åœ°ï¼Œæ£€æŸ¥å¦ä¸€å­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸæ˜¯å¦ä¸ä»¥ç›®æ ‡ç‚¹ä¸ºçƒå¿ƒã€ä»¥ç›®æ ‡ç‚¹ä¸â€œå½“å‰æœ€è¿‘ç‚¹â€é—´çš„è·ç¦»ä¸ºåŠå¾„çš„è¶…çƒä½“ç›¸äº¤ã€‚
     å¦‚æœç›¸äº¤ï¼Œå¯èƒ½åœ¨å¦ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸå†…å­˜åœ¨è·ç›®æ ‡ç‚¹æ›´è¿‘çš„ç‚¹ï¼Œç§»åŠ¨åˆ°å¦ä¸€ä¸ªå­ç»“ç‚¹ã€‚æ¥ç€ï¼Œé€’å½’åœ°è¿›è¡Œæœ€è¿‘é‚»æœç´¢ï¼›
     å¦‚æœä¸ç›¸äº¤ï¼Œå‘ä¸Šå›é€€ã€‚
  4. å½“å›é€€åˆ°æ ¹ç»“ç‚¹æ—¶ï¼Œæœç´¢ç»“æŸã€‚æœ€åçš„â€œå½“å‰æœ€è¿‘ç‚¹â€å³ä¸º x çš„æœ€è¿‘é‚»ç‚¹ã€‚
     å¦‚æœå®ä¾‹ç‚¹æ˜¯éšæœºåˆ†å¸ƒçš„ï¼Œkd æ ‘æœç´¢çš„å¹³å‡è®¡ç®—å¤æ‚åº¦æ˜¯ O(logN)ï¼Œè¿™é‡Œ N æ˜¯è®­ç»ƒå®ä¾‹æ•°ã€‚kd æ ‘æ›´é€‚ç”¨äºè®­ç»ƒå®ä¾‹æ•°è¿œå¤§äºç©ºé—´ç»´æ•°æ—¶çš„ k è¿‘é‚»æœç´¢ã€‚å½“ç©ºé—´ç»´æ•°æ¥è¿‘è®­ç»ƒå®ä¾‹æ•°æ—¶ï¼Œå®ƒçš„æ•ˆç‡ä¼šè¿…é€Ÿä¸‹é™ï¼Œå‡ ä¹æ¥è¿‘çº¿æ€§æ‰«æã€‚

  | ç®—æ³• | å¹³å‡        | æœ€å·®çš„æƒ…å†µ |
  | ---- | ----------- | ---------- |
  | ç©ºé—´ | $O(n)$      | $O(n)$     |
  | æœç´¢ | $O(\log n)$ | $O(n)$     |
  | æ’å…¥ | $O(\log n)$ | $O(n)$     |
  | åˆ é™¤ | $O(\log n)$ | $O(n)$     |

### é™„åŠ çŸ¥è¯†

#### è·ç¦»åº¦é‡

[Distance](https://en.jinzhao.wiki/wiki/Category:Distance)

[sklearn.neighbors.DistanceMetric](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)

[Distance computations(scipy.spatial.distance)](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)

[24 ç§è·ç¦»åº¦é‡å°ç»“](https://blog.csdn.net/weixin_43840403/article/details/89075759)

> å…ˆäº†è§£åº¦é‡ç©ºé—´å’Œèµ‹èŒƒç©ºé—´

å®å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Euclidean(æ¬§å‡ é‡Œå¾—è·ç¦»ä¹Ÿç§°æ¬§å¼è·ç¦») ${||u-v||}_2$ or $\sqrt{\sum_i{(u_i - v_i)^2}}$
- SEuclidean(æ ‡å‡†åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»)
- SqEuclidean(å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»)
- Mahalanobis(é©¬æ°è·ç¦») $\sqrt{ (u-v) \Sigma^{-1} (u-v)^T }$
- Manhattan/cityblock(åŸå¸‚è¡—åŒºï¼ˆæ›¼å“ˆé¡¿ï¼‰è·ç¦») $\sum_i{|u_i-v_i|}$
- Chebyshev(åˆ‡æ¯”é›ªå¤«è·ç¦») $L_\infty$åº¦é‡ $\max_i{|u_i-v_i|}$
- Minkowski(é—µå¯å¤«æ–¯åŸºè·ç¦») æ¬§å¼è·ç¦»çš„æ¨å¹¿ï¼Œp=1 æ—¶ç­‰ä»·äºæ›¼å“ˆé¡¿è·ç¦»ï¼Œp=2 æ—¶ç­‰ä»·äºæ¬§æ°è·ç¦»ï¼Œp=âˆ æ—¶ç­‰ä»·äºåˆ‡æ¯”é›ªå¤«è·ç¦»;$\sqrt[p]{\sum_i{(u_i - v_i)^p}}$
- WMinkowski(åŠ æƒ Minkowski)

å®å€¼å‘é‡ç©ºé—´çš„åº¦é‡(scipy)ï¼š

- Correlation(çš®å°”é€Šç›¸å…³ç³»æ•°(Pearson Correlation))
- Cosine(ä½™å¼¦è·ç¦»)
- JensenShannon(JS æ•£åº¦ä¹Ÿç§° JS è·ç¦»ï¼Œæ˜¯ KL æ•£åº¦çš„ä¸€ç§å˜å½¢)

æ•´æ•°å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Hamming(æ±‰æ˜è·ç¦»)
- Canberra(å ªåŸ¹æ‹‰è·ç¦»)
- BrayCurtis(å¸ƒé›·æŸ¯è’‚æ–¯è·ç¦»)

å¸ƒå°”å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Jaccard(Jaccard-Needham ä¸ç›¸ä¼¼åº¦)
- Matching(Hamming åŒä¹‰è¯)
- Dice(Dice ç³»æ•°)
- Kulsinski(Kulsinski ç›¸å¼‚åº¦)
- RogersTanimoto(Rogers-Tanimoto ç›¸å¼‚åº¦)
- RussellRao(Russell-Rao ç›¸å¼‚æ€§)
- SokalMichener(Sokal-Michener ç›¸å¼‚æ€§)
- SokalSneath(Sokal-Sneath ç›¸å¼‚æ€§)
- Yuleï¼ˆscipy ä¸­çš„ Yule ç›¸å¼‚åº¦ï¼‰

ç»çº¬åº¦è·ç¦»ï¼š

- Haversine(sklearn ä¸­çš„åŠæ­£çŸ¢è·ç¦»)

å…¶å®ƒï¼š

- ç›¸å¯¹ç†µåˆç§° KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰[scipy.special.kl_div](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.kl_div.html)
- äº¤å‰ç†µï¼ˆCross Entropyï¼‰ [scipy.stats.entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)

### å‚è€ƒæ–‡çŒ®

[3-1] Cover T,Hart P. Nearest neighbor pattern classification. IEEE Transactions onInformation Theory,1967

[3-2] Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[3-3] Friedman J. Flexible metric nearest neighbor classification. Technical Report,1994

[3-4] Weinberger KQ,Blitzer J,Saul LK. Distance metric learning for large margin nearestneighbor classification. In: Proceedings of the NIPS. 2005

[3-5] Samet H. The Design and Analysis of Spatial Data Structures. Reading,MA: Addison-Wesley,1990

## ç¬¬ 4 ç«  æœ´ç´ è´å¶æ–¯æ³•

æœ´ç´ è´å¶æ–¯ï¼ˆ[NaÃ¯ve Bayes](https://en.jinzhao.wiki/wiki/Naive_Bayes_classifier)ï¼‰æ³•æ˜¯åŸºäº**è´å¶æ–¯å®šç†**ä¸**ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼ˆNaive å¤©çœŸçš„ï¼‰çš„åˆ†ç±»æ–¹æ³•ã€‚
å¯¹äºç»™å®šçš„è®­ç»ƒæ•°æ®é›†ï¼Œé¦–å…ˆåŸºäºç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å­¦ä¹ è¾“å…¥/è¾“å‡ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼›ç„¶ååŸºäºæ­¤æ¨¡å‹ï¼Œå¯¹ç»™å®šçš„è¾“å…¥ xï¼Œåˆ©ç”¨è´å¶æ–¯å®šç†æ±‚å‡ºåéªŒæ¦‚ç‡æœ€å¤§çš„è¾“å‡º yã€‚
æœ´ç´ è´å¶æ–¯æ³•å®ç°ç®€å•ï¼Œå­¦ä¹ ä¸é¢„æµ‹çš„æ•ˆç‡éƒ½å¾ˆé«˜ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•ã€‚

æœ´ç´ è´å¶æ–¯æ³•æ˜¯å…¸å‹çš„**ç”Ÿæˆå­¦ä¹ æ–¹æ³•**ã€‚ç”Ÿæˆæ–¹æ³•ç”±è®­ç»ƒæ•°æ®å­¦ä¹ è”åˆæ¦‚ç‡åˆ†å¸ƒ P(X,Y)ï¼Œç„¶åæ±‚å¾—åéªŒæ¦‚ç‡åˆ†å¸ƒ P(Y|X)ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®å­¦ä¹  P(X|Y)å’Œ P(Y)çš„ä¼°è®¡ï¼Œå¾—åˆ°è”åˆæ¦‚ç‡åˆ†å¸ƒï¼šP(X,Y)ï¼ P(Y)P(X|Y) ï¼›æ¦‚ç‡ä¼°è®¡æ–¹æ³•å¯ä»¥æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–è´å¶æ–¯ä¼°è®¡ã€‚

**[è´å¶æ–¯å®šç†(Bayes' theorem)](https://en.jinzhao.wiki/wiki/Bayes%27_theorem)**ï¼š
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

- $P(A|B)$ æ˜¯æ¡ä»¶æ¦‚ç‡[conditional probability](https://en.jinzhao.wiki/wiki/Conditional_probability)ï¼šæ˜¯å·²çŸ¥ B å‘ç”Ÿåï¼ŒA çš„æ¦‚ç‡ï¼Œä¹Ÿè¢«ç§°ä¸º å·²çŸ¥ B çš„æƒ…å†µä¸‹ A çš„åéªŒæ¦‚ç‡[posterior probability](https://en.jinzhao.wiki/wiki/Posterior_probability)

- $P(B|A)$ ä¹Ÿæ˜¯ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ï¼šå·²çŸ¥ A æ—¶ï¼ŒB çš„ä¼¼ç„¶æ€§/å¯èƒ½æ€§([likelihood](https://en.jinzhao.wiki/wiki/Likelihood_function)), ä¸ºä»€ä¹ˆå« likelihoodï¼Ÿå› ä¸º$P(B|A) = L(A|B) ^{å‚è§ï¼šé™„åŠ çŸ¥è¯†-å‚æ•°ä¼°è®¡-æå¤§ä¼¼ç„¶ä¼°è®¡}$

- $P(A)$ å« A çš„è¾¹é™…æ¦‚ç‡([marginal probability](https://en.jinzhao.wiki/wiki/Marginal_probability))æˆ–å…ˆéªŒæ¦‚ç‡([prior probability](https://en.jinzhao.wiki/wiki/Prior_probability))

- $P(B)$ å« B çš„è¾¹é™…æ¦‚ç‡æˆ–å…ˆéªŒæ¦‚ç‡ï¼Œä¹Ÿç§°ä¸º evidence è¯æ®

**[ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾](https://en.jinzhao.wiki/wiki/Conditional_independence)**ï¼š
**æ¡ä»¶ç‹¬ç«‹**
$$(A\perp B|C) \iff P(A|B,C) = P(A|C) \\ (A\perp B|C) \iff P(A,B|C) = P(A|C)P(B|C)$$

ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å°±æ˜¯å·²çŸ¥ y çš„æƒ…å†µä¸‹ï¼Œx ä¸­æ¯ä¸ªç‰¹å¾ç›¸äº’ç‹¬ç«‹ã€‚

æ•°æ®é›†$T = \{(x_1,y_1),...,(x_N,y_N)\}$ï¼Œ$K$ä¸ºç±»åˆ«ä¸ªæ•°,å…¶ä¸­$x_i$æ˜¯ n ç»´å‘é‡$x_i = (x_i^{(1)},...,x_i^{(n)})^T$

- **æ¨¡å‹**ï¼š
  $$\underbrace{P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}}_{\text{è´å¶æ–¯å®šç†}} \varpropto P(Y=c_k) \underbrace{\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}}_{\text{ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾}}$$

  å…¶ä¸­
  $$P(X=x) = \sum_k{P(X=x|Y=c_k)P(Y=c_k)} = \sum_k{P(Y=c_k) \prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}}$$
  P(X)æ˜¯ evidence(å·²çŸ¥çš„,è§‚å¯Ÿå˜é‡)ï¼Œå¯çœ‹åšå¸¸æ•°(ä¹Ÿå¯ä»¥è¯´å¯¹$c_k$æ¥è¯´åˆ†æ¯ P(X)æ˜¯ç›¸åŒçš„ï¼Œæ±‚æœ€å¤§æœ€å°æ—¶å¯ä»¥å»æ‰)ï¼Œåˆ™ï¼š
  $$P(Y=c_k|X=x) \varpropto {P(X=x|Y=c_k)P(Y=c_k)}$$

- **ç­–ç•¥**ï¼š
  åéªŒæœ€å¤§åŒ–ï¼ˆç­‰ä»· 0-1 æŸå¤±ï¼‰ï¼š
  $$y = \argmax_{c_k} P(Y=c_k|X=x)= \argmax_{c_k}P(Y=c_k)\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}$$
  åŸç†ï¼ˆè¯æ˜ï¼‰ï¼šä½¿ç”¨ 0-1 æŸå¤±
  $$\argmin_{y}\sum_{k=1}^K P(y \neq c_k|X=x) \\= \argmin_{y}(1- P(y = c_k|X=x)) \\= \argmax_y P(y = c_k|X=x)$$

- **ç®—æ³•**ï¼šå‚æ•°ä¼°è®¡
  æˆ‘ä»¬éœ€è¦çŸ¥é“$P(Y=c_k)$ä»¥åŠ$\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}$
  **æå¤§ä¼¼ç„¶ä¼°è®¡**ï¼š

  1. å…ˆéªŒ$P(Y=c_k)$çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ï¼š
     $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k)}{N}$$
  2. ç¬¬$j$ä¸ªç‰¹å¾$x^{(j)}$çš„å–å€¼é›†åˆæ˜¯$\{a_{j1},...,a_{jS_j}\}$,([æ³¨æ„è¿™é‡Œç”¨çš„éƒ½æ˜¯é¢‘ç‡è®¡æ•°ï¼Œä¹Ÿå°±æ˜¯ç¦»æ•£ç‰¹å¾ï¼Œå¦‚æœæ˜¯è¿ç»­ç‰¹å¾å˜é‡ï¼Œåˆ™ä½¿ç”¨é«˜æ–¯æœ´ç´ è´å¶æ–¯](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes))
     æ¡ä»¶æ¦‚ç‡(likelihood)$P(X^{(j)}=x^{(j)}|Y=c_k)$çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ï¼š
     $$P(X^{(j)} = a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} =a_{jl} , y_i = c_k)}{\sum_{i=1}^N I(y_i=c_k)}$$
     å…¶ä¸­$j = 1,2,...N; \quad l=1,2...S_j ;\quad k=1,2,...K$ï¼Œ$x_i^{(j)}$æ˜¯ç¬¬$i$ä¸ªæ ·æœ¬çš„ç¬¬$j$ä¸ªç‰¹å¾ï¼›$a_{jl}$æ˜¯ç¬¬$j$ä¸ªç‰¹å¾å¯èƒ½å–å€¼çš„ç¬¬$l$ä¸ªå€¼ã€‚

  **è´å¶æ–¯ä¼°è®¡**ï¼š
  æå¤§ä¼¼ç„¶ä¼°è®¡æœ‰ä¸€ä¸ªé—®é¢˜å°±æ˜¯æ¡ä»¶æ¦‚ç‡$P(X^{(j)}=x^{(j)}|Y=c_k)$æœ‰ä¸€ä¸ªä¸º 0ï¼Œå°±ä¼šå‡ºç°æ— æ³•ä¼°è®¡çš„æƒ…å†µ(å°±æ˜¯æ¦‚ç‡ä¸º 0)ï¼Œä¹Ÿå°±æ˜¯ç»™å®šè¦é¢„æµ‹çš„ç‰¹å¾å‘é‡çš„ä¸€ä¸ªç‰¹å¾å‡ºç°äº†æ–°çš„ç±»åˆ«ï¼ˆå¦‚ï¼šç¬¬$j$ä¸ªç‰¹å¾$x^{(j)} = a_{jS_j+1}$ï¼‰ï¼Œé‚£ä¹ˆå°±ä¼šå¯¼è‡´æ¦‚ç‡ä¸º 0ï¼Œè¿™æ˜¯è¦ä¹ˆå¢åŠ æ ·æœ¬æ•°é‡ï¼Œè¦ä¹ˆä½¿ç”¨è´å¶æ–¯ä¼°è®¡

  > æ³¨æ„ï¼šæœ´ç´ è´å¶æ–¯æ³•ä¸è´å¶æ–¯ä¼°è®¡ï¼ˆBayesian estimationï¼‰æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚

  1. å…ˆéªŒ$P(Y=c_k)$çš„è´å¶æ–¯ä¼°è®¡æ˜¯ï¼š
     $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$$
  2. æ¡ä»¶æ¦‚ç‡(likelihood)çš„è´å¶æ–¯ä¼°è®¡æ˜¯ï¼š
     $$P(X^{(j)} = a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl} , y_i = c_k) + \lambda}{\sum_{i=1}^N I(y_i=c_k) + S_j\lambda}$$

  å…¶ä¸­$\lambda \geq 0$,å½“$\lambda = 0$æ—¶å°±ç­‰ä»·äºæå¤§ä¼¼ç„¶ä¼°è®¡ï¼›å½“$\lambda = 1$æ—¶ï¼Œç§°ä¸ºæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆ[Laplacian smoothing](https://en.jinzhao.wiki/wiki/Laplacian_smoothing)ï¼‰ï¼›å½“$\lambda < 1$æ—¶ä¸º Lidstone å¹³æ»‘

  > é«˜æ–¯æœ´ç´ è´å¶æ–¯:æ¡ä»¶æ¦‚ç‡(likelihood)
  > $$P(X^{(j)} = x^{(j)}|Y=c_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}} exp\bigg(-\frac{(x^{(j)}-\mu_k)^2}{2\sigma_k^2}\bigg) $$
  > å…¶ä¸­$\mu_k$ä¸ºæ ·æœ¬ä¸­ç±»åˆ«ä¸º$c_k$çš„ æ‰€æœ‰$x^{(j)}$çš„å‡å€¼ï¼›$\sigma_k^2$ä¸ºæ ·æœ¬ä¸­ç±»åˆ«ä¸º$c_k$çš„ æ‰€æœ‰$x^{(j)}$çš„æ–¹å·®ï¼ˆå…¶å®å°±æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡å‡å€¼å’Œæ–¹å·®ï¼‰ã€‚
  > sklearn ä¸­ GaussianNB ç±»çš„ä¸»è¦å‚æ•°ä»…æœ‰ä¸€ä¸ªï¼Œå³å…ˆéªŒæ¦‚ç‡ priors ï¼Œå¯¹åº” Y çš„å„ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡$P(Y=c_k)$ã€‚è¿™ä¸ªå€¼é»˜è®¤ä¸ç»™å‡ºï¼Œå¦‚æœä¸ç»™å‡ºæ­¤æ—¶$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$ã€‚å¦‚æœç»™å‡ºçš„è¯å°±ä»¥ priors ä¸ºå‡†ã€‚

### é™„åŠ çŸ¥è¯†

#### å‚æ•°ä¼°è®¡

å‚æ•°ä¼°è®¡([Parameter Estimation](https://en.jinzhao.wiki/wiki/Estimation_theory)) æœ‰ç‚¹ä¼°è®¡ï¼ˆ[point estimation](https://en.jinzhao.wiki/wiki/Point_estimation)ï¼‰å’ŒåŒºé—´ä¼°è®¡ï¼ˆ[interval estimation](https://en.jinzhao.wiki/wiki/Interval_estimation)ï¼‰ä¸¤ç§

**ç‚¹ä¼°è®¡æ³•ï¼š**

- **æå¤§ä¼¼ç„¶ä¼°è®¡([Maximum likelihood estimation, MLE](https://en.jinzhao.wiki/wiki/Maximum_likelihood_estimation))**
  æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯å…¸å‹çš„é¢‘ç‡å­¦æ´¾è§‚ç‚¹ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå¾…ä¼°è®¡å‚æ•°$\theta$ æ˜¯å®¢è§‚å­˜åœ¨çš„ï¼Œåªæ˜¯æœªçŸ¥è€Œå·²
  $$L(\theta|x) = f(x|\theta) = P(X|\theta) \\ \hat{\theta}_{MLE} = \argmax_{\theta} L(\theta|x)$$
  è¿™é‡Œç”¨ | å’Œ ; æ˜¯ç­‰ä»·çš„; è¦æœ€å¤§åŒ– Lï¼Œå¯¹ L æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º 0 å³å¯æ±‚è§£ã€‚
  $P(X|\theta)$å°±æ˜¯è´å¶æ–¯å…¬å¼ä¸­çš„likelihoodï¼Œ$\theta$å°±æ˜¯$c_k$
  log-likelihood:$\ell(\theta|x) = \log L(\theta|x)$

- **æœ€å¤§åéªŒä¼°è®¡([maximum a posteriori estimation, MAP](https://en.jinzhao.wiki/wiki/Maximum_a_posteriori_estimation))**
  è´å¶æ–¯å®šç†ï¼š
  $$f(\theta|x) = \frac{f(x|\theta)g(\theta)}{\int_\vartheta f(x|\vartheta)g(\vartheta)d\vartheta}$$
  $g$ æ˜¯$\theta $çš„å¯†åº¦å‡½æ•°ï¼ˆdensity functionï¼‰
  $$\hat{\theta}_{MAP} = \argmax_{\theta} f(\theta|x) \\= \argmax_{\theta} \frac{f(x|\theta)g(\theta)}{\int_\vartheta f(x|\vartheta)g(\vartheta)d\vartheta} \\= \argmax_{\theta}f(x|\theta)g(\theta)$$
  è¿™é‡Œåˆ†æ¯ä¸$\theta$æ— å…³ï¼Œå¯ä»¥çœç•¥
  æˆ‘ä»¬å°†likelihoodå˜æˆlog-likelihoodï¼š
  $$\hat{\theta}_{MAP} =  \argmax_{\theta}\log{f(x|\theta)g(\theta)} =  \argmax_{\theta} (\log{f(x|\theta)} + \log{g(\theta)})$$
  è¿™æ ·æˆ‘ä»¬å¯ä»¥å°†$\log{g(\theta)}$çœ‹ä½œæœºå™¨å­¦ä¹ ç»“æ„é£é™©ä¸­çš„**æ­£åˆ™åŒ–é¡¹**ï¼Œé‚£ä¹ˆå¸¦æœ‰æ­£åˆ™åŒ–é¡¹çš„æœ€å¤§ä¼¼ç„¶å­¦ä¹ å°±å¯ä»¥è¢«è§£é‡Šä¸ºMAPï¼ˆå¦‚ï¼š[Ridgeå›å½’å’ŒLassoå›å½’](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)ï¼‰ã€‚
  å½“ç„¶ï¼Œè¿™å¹¶ä¸æ˜¯æ€»æ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚ï¼Œæœ‰äº›æ­£åˆ™åŒ–é¡¹å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å¯¹æ•°ï¼Œè¿˜æœ‰äº›æ­£åˆ™åŒ–é¡¹ä¾èµ–äºæ•°æ®ï¼Œå½“ç„¶ä¹Ÿä¸ä¼šæ˜¯ä¸€ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒã€‚ä¸è¿‡ï¼ŒMAPæä¾›äº†ä¸€ä¸ªç›´è§‚çš„æ–¹æ³•æ¥è®¾è®¡å¤æ‚ä½†å¯è§£é‡Šçš„æ­£åˆ™åŒ–é¡¹ï¼Œä¾‹å¦‚ï¼Œæ›´å¤æ‚çš„æƒ©ç½šé¡¹å¯ä»¥é€šè¿‡æ··åˆé«˜æ–¯åˆ†å¸ƒä½œä¸ºå…ˆéªŒå¾—åˆ°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„é«˜æ–¯åˆ†å¸ƒã€‚
  > æœ€å¤§åéªŒä¼°è®¡å°±æ˜¯**è€ƒè™‘åéªŒåˆ†å¸ƒæå¤§åŒ–è€Œæ±‚è§£å‚æ•°**çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼›MAP = æœ€å¤§ä¼¼ç„¶ä¼°è®¡ + æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ­£åˆ™åŒ–ã€‚
  > è¦æœ€å¤§åŒ– Lï¼Œå¯¹ L æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º 0 å³å¯æ±‚è§£ã€‚

- **è´å¶æ–¯ä¼°è®¡([Bayes estimation](https://en.jinzhao.wiki/wiki/Bayes_estimator))**

> **å…±è½­å…ˆéªŒï¼ˆ[Conjugate prior](https://en.jinzhao.wiki/wiki/Conjugate_prior)ï¼‰**ï¼šå¦‚æœå…ˆéªŒåˆ†å¸ƒpriorå’ŒåéªŒåˆ†å¸ƒposteriorå±äºåŒä¸€åˆ†å¸ƒç°‡ï¼Œåˆ™priorç§°ä¸ºlikehoodçš„å…±è½­å…ˆéªŒ
> likehoodä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œpriorä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™posteriorä¹Ÿä¸ºé«˜æ–¯åˆ†å¸ƒã€‚
> likehoodä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹å¼åˆ†å¸ƒï¼‰ï¼Œpriorä¸ºbetaåˆ†å¸ƒï¼Œåˆ™posteriorä¹Ÿä¸ºbetaåˆ†å¸ƒã€‚
> likehoodä¸ºå¤šé¡¹å¼åˆ†å¸ƒï¼Œpriorä¸ºDirichletåˆ†å¸ƒï¼ˆbetaåˆ†å¸ƒçš„ä¸€ä¸ªæ‰©å±•ï¼‰ï¼Œåˆ™posteriorä¹Ÿä¸ºDirichletï¼ˆç‹„åˆ©å…‹é›·ï¼‰åˆ†å¸ƒã€‚betaåˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯dirichletåˆ†å¸ƒçš„ç‰¹æ®Šæƒ…å†µã€‚



æœ€å°äºŒä¹˜ä¼°è®¡([Least squares estimation, LSE](https://en.jinzhao.wiki/wiki/Least_squares))

çŸ©ä¼°è®¡(Method of moments estimators)

**åŒºé—´ä¼°è®¡æ³•ï¼š**
åŒºé—´ä¼°è®¡æœ€æµè¡Œçš„å½¢å¼æ˜¯ç½®ä¿¡åŒºé—´ [confidence intervals](https://en.jinzhao.wiki/wiki/Confidence_interval) ï¼ˆä¸€ç§[é¢‘ç‡è®ºæ–¹æ³•](https://en.jinzhao.wiki/wiki/Frequentism)ï¼‰å’Œ å¯ä¿¡åŒºé—´ [credible intervals](https://en.jinzhao.wiki/wiki/Credible_interval)ï¼ˆä¸€ç§[è´å¶æ–¯æ–¹æ³•](https://en.jinzhao.wiki/wiki/Bayesian_method)ï¼‰ï¼Œæ­¤å¤–è¿˜æœ‰é¢„æµ‹åŒºé—´ï¼ˆ[Prediction interval](https://en.jinzhao.wiki/wiki/Prediction_interval)ï¼‰ç­‰

**é‡‡æ ·æ³•ï¼š** è´å¶æ–¯ä¼°è®¡ï¼Œè¿‘ä¼¼æ¨æ–­
é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³• [Markov chain Monte Carlo, MCMC](https://en.jinzhao.wiki/wiki/Markov_chain_Monte_Carlo)

### å‚è€ƒæ–‡çŒ®

[4-1] Mitchell TM. Chapter 1: [Generative and discriminative classifiers: NaÃ¯ve Bayes andlogistic regression. In: Machine Learning.](http://www.cs.cmu.edu/~tom/mlbook/NBayeslogReg.pdf) Draft,2005.

[4-2] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning. DataMining,Inference,and Prediction. ](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf) Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[4-3] Bishop C. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf),Springer,2006

## ç¬¬ 5 ç«  å†³ç­–æ ‘
