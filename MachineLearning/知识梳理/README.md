[TOC]

# ç»Ÿè®¡å­¦ä¹ æ–¹æ³•

[ç¬¬ä¸€ç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-first_edition)

[ç¬¬äºŒç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-second_edition)

## ç¬¬ 1 ç«  ç»Ÿè®¡å­¦ä¹ åŠç›‘ç£å­¦ä¹ æ¦‚è®º

**ç»Ÿè®¡å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹æ˜¯**ï¼š

1. ç»Ÿè®¡å­¦ä¹ ä»¥è®¡ç®—æœºåŠç½‘ç»œä¸ºå¹³å°ï¼Œæ˜¯å»ºç«‹åœ¨è®¡ç®—æœºåŠç½‘ç»œä¹‹ä¸Šçš„ï¼›
2. ç»Ÿè®¡å­¦ä¹ ä»¥æ•°æ®ä¸ºç ”ç©¶å¯¹è±¡ï¼Œæ˜¯æ•°æ®é©±åŠ¨çš„å­¦ç§‘ï¼›
3. ç»Ÿè®¡å­¦ä¹ çš„ç›®çš„æ˜¯å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
4. ç»Ÿè®¡å­¦ä¹ ä»¥æ–¹æ³•ä¸ºä¸­å¿ƒï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•æ„å»ºæ¨¡å‹å¹¶åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
5. ç»Ÿè®¡å­¦ä¹ æ˜¯æ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€è®¡ç®—ç†è®ºã€æœ€ä¼˜åŒ–ç†è®ºåŠè®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªé¢†åŸŸçš„äº¤å‰å­¦ç§‘ï¼Œå¹¶ä¸”åœ¨å‘å±•ä¸­é€æ­¥å½¢æˆç‹¬è‡ªçš„ç†è®ºä½“ç³»ä¸æ–¹æ³•è®ºã€‚

**å‡è®¾ç©ºé—´(hypothesis space)**ï¼š
$$\mathcal H = \{ f(x;\theta) | \theta \in \mathbb{R}^D\}$$
å…¶ä¸­$f(x; \theta)$æ˜¯å‚æ•°ä¸º$\theta$ çš„å‡½æ•°ï¼Œä¹Ÿç§°ä¸ºæ¨¡å‹ï¼ˆModelï¼‰ï¼Œ$D$ ä¸ºå‚æ•°çš„æ•°é‡ï¼

**ç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰**ï¼š
æ¯ä¸ªå…·ä½“çš„è¾“å…¥æ˜¯ä¸€ä¸ªå®ä¾‹ï¼ˆinstanceï¼‰ï¼Œé€šå¸¸ç”±ç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰è¡¨ç¤ºã€‚è¿™
æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾å‘é‡å­˜åœ¨çš„ç©ºé—´ç§°ä¸ºç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰ã€‚ç‰¹å¾ç©ºé—´çš„æ¯ä¸€ç»´å¯¹åº”äº
ä¸€ä¸ªç‰¹å¾ã€‚

> è¾“å…¥ç©ºé—´ä¸­çš„ä¸€ä¸ªè¾“å…¥å‘é‡$x = (x_1,x_2)$ï¼Œåœ¨å¤šé¡¹å¼æ¨¡å‹ä¸­ç‰¹å¾å‘é‡æ˜¯($x_1^2,x_1x_2,x_2^2,...$)
> ä¸€èˆ¬è¯´çš„çº¿æ€§æ¨¡å‹ï¼ŒæŒ‡çš„æ˜¯ç‰¹å¾å‘é‡çš„çº¿æ€§ç»„åˆï¼Œè€Œä¸æ˜¯æŒ‡è¾“å…¥å‘é‡ï¼Œæ‰€ä»¥è¯´æ¨¡å‹éƒ½æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„

**ç»Ÿè®¡å­¦ä¹ çš„ä¸‰è¦ç´ **ï¼š

1. æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)ï¼Œç®€ç§°ï¼šæ¨¡å‹(model)
2. æ¨¡å‹é€‰æ‹©çš„å‡†åˆ™(evaluation criterion)ï¼Œç®€ç§°ï¼šç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™
3. æ¨¡å‹å­¦ä¹ çš„ç®—æ³•(algorithm)ï¼Œç®€ç§°ï¼šç®—æ³•(algorithm)

> ä»¥çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ä¸ºä¾‹ï¼š
> æ¨¡å‹ï¼š $f(x;w,b) = w^Tx +b$
> ç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™: å¹³æ–¹æŸå¤±å‡½æ•° $\mathcal L(y,\hat{y}) = (y-f(x,\theta))^2$
> ç®—æ³•ï¼šä¹Ÿç§°ä¸ºä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ï¼šæ¢¯åº¦ä¸‹é™æ³•

**æœºå™¨å­¦ä¹ çš„å®šä¹‰**ï¼š

```mermaid
graph LR;
    F(["æœªçŸ¥çš„ç›®æ ‡å‡½æ•°(ç†æƒ³ä¸­å®Œç¾çš„å‡½æ•°)ï¼šğ‘“: ğ’™âŸ¶ğ‘¦"])-->D["è®­ç»ƒæ ·æœ¬D:{(ğ’™Â¹,ğ‘¦Â¹),...,(ğ’™â¿,ğ‘¦â¿)}"];
    D-->A{{"ç®—æ³•"}}
    H{{"å‡è®¾ç©ºé—´"}}-->A
    A-->G["æ¨¡å‹ gâ‰ˆf"]
```

ä½¿ç”¨è®­ç»ƒæ•°æ®æ¥è®¡ç®—æ¥è¿‘ç›®æ ‡ ğ‘“ çš„å‡è®¾ï¼ˆhypothesis ï¼‰g [^1]

[^1]: [Machine Learning Foundations,25 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/01_handout.pdf)

**ç›‘ç£å­¦ä¹ **ï¼š
ç›‘ç£å­¦ä¹ (supervised learning)æ˜¯æŒ‡ä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„çš„ç»Ÿè®¡è§„å¾‹**ã€‚

è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºè¿ç»­å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**å›å½’é—®é¢˜**ï¼›
è¾“å‡ºå˜é‡ä¸ºæœ‰é™ä¸ªç¦»æ•£å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**åˆ†ç±»é—®é¢˜**ï¼›
è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºå˜é‡åºåˆ—çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**æ ‡æ³¨é—®é¢˜**(å¯ä»¥ç†è§£ä¸ºç‰¹æ®Šçš„åˆ†ç±»é—®é¢˜)ã€‚

ç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥æ˜¯æ¦‚ç‡æ¨¡å‹æˆ–éæ¦‚ç‡æ¨¡å‹ï¼Œç”±**æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ**$P(Y|X)$æˆ–**å†³ç­–å‡½æ•°ï¼ˆdecision functionï¼‰**$Y=f(X)$è¡¨ç¤ºï¼Œéšå…·ä½“å­¦ä¹ æ–¹æ³•è€Œå®šã€‚å¯¹å…·ä½“çš„è¾“å…¥è¿›è¡Œç›¸åº”çš„è¾“å‡ºé¢„æµ‹æ—¶ï¼Œå†™ä½œ$P(y|x)$æˆ–$Y=f(x)$ã€‚
$$y =\displaystyle\argmax_{y}  P(y|x)$$

**è”åˆæ¦‚ç‡åˆ†å¸ƒ**ï¼š
ç›‘ç£å­¦ä¹ å‡è®¾è¾“å…¥ä¸è¾“å‡ºçš„éšæœºå˜é‡ X å’Œ Y éµå¾ªè”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ã€‚$P(X,Y)$è¡¨ç¤ºåˆ†å¸ƒå‡½æ•°ï¼Œæˆ–åˆ†å¸ƒå¯†åº¦å‡½æ•°ã€‚æ³¨æ„ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå‡å®šè¿™ä¸€è”åˆæ¦‚ç‡åˆ†å¸ƒå­˜åœ¨ï¼Œä½†å¯¹å­¦ä¹ ç³»ç»Ÿæ¥è¯´ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å…·ä½“å®šä¹‰æ˜¯æœªçŸ¥çš„ã€‚**è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®è¢«çœ‹ä½œæ˜¯ä¾è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ç‹¬ç«‹åŒåˆ†å¸ƒäº§ç”Ÿçš„**ã€‚
ç»Ÿè®¡å­¦ä¹ å‡è®¾æ•°æ®å­˜åœ¨ä¸€å®šçš„ç»Ÿè®¡è§„å¾‹ï¼Œ$X$å’Œ$Y$å…·æœ‰è”åˆæ¦‚ç‡åˆ†å¸ƒçš„å‡è®¾å°±æ˜¯ç›‘ç£å­¦ä¹ å…³äºæ•°æ®çš„åŸºæœ¬å‡è®¾ã€‚

**éç›‘ç£å­¦ä¹ **ï¼š
éç›‘ç£å­¦ä¹ (unsupervised learning)æ˜¯æŒ‡ä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ æ•°æ®ä¸­çš„ç»Ÿè®¡è§„å¾‹æˆ–æ½œåœ¨ç»“æ„**ã€‚

éç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•°$z = g(x)$æˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(z|x)$ ï¼ˆè¾“å‡º$z$å¯ä»¥æ˜¯**èšç±»**æˆ–è€…**é™ç»´**ï¼‰
$$z =\displaystyle\argmax_{z}  P(z|x)$$
ä»¥åŠ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x|z)$ ï¼ˆç”¨æ¥åš**æ¦‚ç‡å¯†åº¦ä¼°è®¡**ï¼Œæ¯”å¦‚ GMM ä¸­$P(x|z)$å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå¦‚æœå‡è®¾çŸ¥é“æ•°æ®æ¥è‡ªå“ªä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå³çŸ¥é“$z$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼°è®¡ç›¸å…³å‚æ•°ï¼‰ã€‚

**æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰ä¸éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰æˆ–è€…ç¡®å®šæ€§æ¨¡å‹ï¼ˆdeterministic modelï¼‰**ï¼š

æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰- æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ P(y|x)å’Œ éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰ - å‡½æ•° y=f(x)å¯ä»¥**ç›¸äº’è½¬åŒ–**ï¼Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæœ€å¤§åŒ–åå¾—åˆ°å‡½æ•°ï¼Œå‡½æ•°å½’ä¸€åŒ–åå¾—åˆ°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚æ‰€ä»¥æ¦‚ç‡æ¨¡å‹ä¸éæ¦‚ç‡æ¨¡å‹çš„åŒºåˆ«ä¸åœ¨äºè¾“å…¥è¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œè€Œåœ¨äºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼šæ¦‚ç‡æ¨¡å‹ä¸€å®šå¯ä»¥è¡¨ç¤ºä¸ºè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œè€Œéæ¦‚ç‡æ¨¡å‹åˆ™ä¸ä¸€å®šå­˜åœ¨è¿™æ ·çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚

æ¦‚ç‡æ¨¡å‹çš„ä»£è¡¨æ˜¯**æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆprobabilistic graphical modelï¼‰**$^{å‚è€ƒæ–‡çŒ®[3]}$ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒå¯ä»¥æ ¹æ®å›¾çš„ç»“æ„åˆ†è§£ä¸ºå› å­ä¹˜ç§¯çš„å½¢å¼ï¼Œå¯ä»¥ç”¨æœ€åŸºæœ¬çš„åŠ æ³•è§„åˆ™å’Œä¹˜æ³•è§„åˆ™è¿›è¡Œæ¦‚ç‡æ¨ç†ï¼š
$$P(x) = \sum_yP(x,y) \\ P(x,y) = P(x)P(y|x)$$

**å‚æ•°åŒ–æ¨¡å‹ï¼ˆparametric modelï¼‰å’Œéå‚æ•°åŒ–æ¨¡å‹ï¼ˆnon-parametric modelï¼‰**ï¼š

å‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„ç»´åº¦å›ºå®šï¼Œæ¨¡å‹å¯ä»¥ç”±æœ‰é™ç»´å‚æ•°å®Œå…¨åˆ»ç”»ã€‚(å¦‚ï¼šæ„ŸçŸ¥æœºã€GMM)
éå‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„å”¯ç‹¬ä¸å›ºå®šæˆ–è€…è¯´æ— ç©·å¤§ï¼Œéšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ è€Œä¸æ–­å¢å¤§ã€‚(å¦‚ï¼šå†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœº)

**åœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰å’Œæ‰¹é‡å­¦ä¹ ï¼ˆbatch learningï¼‰**ï¼š

åœ¨çº¿å­¦ä¹ æ¯æ¬¡æ¥å—ä¸€ä¸ªæ ·æœ¬ï¼Œé¢„æµ‹åå­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸æ–­é‡å¤è¯¥æ“ä½œã€‚
æ‰¹é‡å­¦ä¹ ä¸€æ¬¡æ¥å—æ‰€æœ‰æ•°æ®ï¼Œå­¦ä¹ æ¨¡å‹ä¹‹åè¿›è¡Œé¢„æµ‹ã€‚

åœ¨çº¿å­¦ä¹ æ¯”æ‰¹é‡å­¦ä¹ æ›´éš¾ï¼Œå› ä¸ºæ¯æ¬¡æ¨¡å‹æ›´æ–°ä¸­å¯åˆ©ç”¨çš„æ•°æ®æœ‰é™ã€‚

**è´å¶æ–¯å­¦ä¹ ï¼ˆBayesian learningï¼‰/ è´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰**ï¼š
$$\mathrm{Bayes \; Rule:} \\ \underbrace{P(X|Y)}_{\mathrm{posterior}} = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{P(Y)}_{\mathrm{evidence}}}   = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{\sum_{x}P(Y|X)P(X)}_{\mathrm{evidence}}}$$

**æ ¸æŠ€å·§ï¼ˆkernel trickï¼‰/ æ ¸æ–¹æ³•ï¼ˆkernel methodï¼‰**ï¼š

**æ ¸æ–¹æ³•**æ˜¯ä¸€ç±»æŠŠä½ç»´ç©ºé—´çš„éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œè½¬åŒ–ä¸ºé«˜ç»´ç©ºé—´çš„çº¿æ€§å¯åˆ†é—®é¢˜çš„æ–¹æ³•ã€‚
**æ ¸æŠ€å·§**æ˜¯ä¸€ç§åˆ©ç”¨æ ¸å‡½æ•°ç›´æ¥è®¡ç®— $\lang \phi(x),\phi(z) \rang$ ï¼Œä»¥é¿å¼€åˆ†åˆ«è®¡ç®— $\phi(x)$ å’Œ $\phi(z)$ ï¼Œä»è€ŒåŠ é€Ÿæ ¸æ–¹æ³•è®¡ç®—çš„æŠ€å·§ã€‚

**æ ¸å‡½æ•°**ï¼šè®¾ $\mathcal X$ æ˜¯è¾“å…¥ç©ºé—´ï¼ˆå³ $x_i \in \mathcal X $ ï¼Œ $\mathcal X$ æ˜¯ $\mathbb R^n$ çš„å­é›†æˆ–ç¦»æ•£é›†åˆ ï¼‰ï¼Œåˆè®¾ $\mathcal H$ ä¸ºç‰¹å¾ç©ºé—´ï¼ˆâ€‹ $\mathcal H$ æ˜¯å¸Œå°”ä¼¯ç‰¹ç©ºé—´[^2]ï¼‰ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªä» $\mathcal X$ åˆ° $\mathcal H$ çš„æ˜ å°„

$$\phi(x) : \mathcal X \to \mathcal H$$

ä½¿å¾—å¯¹æ‰€æœ‰ $x,z \in \mathcal X$ ï¼Œå‡½æ•° $K(x,z)$ æ»¡è¶³æ¡ä»¶

$$K(x,z) = \phi(x).\phi(z) = \lang \phi(x),\phi(z) \rang$$

åˆ™ç§° $K(x,z)$ ä¸ºæ ¸å‡½æ•°ã€‚å…¶ä¸­ $\phi(x) $ ä¸ºæ˜ å°„å‡½æ•°ï¼Œ $\lang \phi(x),\phi(z) \rang$ ä¸ºå†…ç§¯ã€‚

### å‚è€ƒæ–‡çŒ®

[1] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[2] Bishop M. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). Springer,2006

[3] [Probabilistic Graphical Models: Principles and Techniques](https://djsaunde.github.io/read/books/pdfs/probabilistic%20graphical%20models.pdf) by Daphne Koller, Nir Friedman from The MIT Press

[4] [Deep Learning](https://raw.fastgit.org/Zhenye-Na/machine-learning-uiuc/master/docs/Deep%20Learning.pdf) (Ian Goodfellow, Yoshua Bengio, Aaron Courville)

[5] Tom M Michelle. [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html). McGraw-Hill Companies,Inc. 1997ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬ï¼šæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003ï¼‰

[6] [Bayesian Reasoning and Machine Learning by David Barber 2007â€“2020](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf) ,[other version](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/)

[7] [Reinforcement Learning:An Introduction (second edition 2020) by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020trimmed.pdf) ,[other version](http://incompleteideas.net/book/)

[8] å‘¨å¿—åï¼Œ[æœºå™¨å­¦ä¹ ](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)ï¼Œæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ ([æ‰‹æ¨ç¬”è®°](https://github.com/Sophia-11/Machine-Learning-Notes) ä»¥åŠ [å…¬å¼æ¨å¯¼è§£æ](https://github.com/datawhalechina/pumpkin-book))

[9] [Lecture Notes in MACHINE LEARNING](https://news.vidyaacademy.ac.in/wp-content/uploads/2018/10/NotesOnMachineLearningForBTech-1.pdf) Dr V N Krishnachandran

## ç¬¬ 2 ç«  æ„ŸçŸ¥æœº
