---
title: ç»Ÿè®¡å­¦ä¹ æ–¹æ³•
render_with_liquid: false
---
<!-- {% raw %} -->
[TOC]

# ç»Ÿè®¡å­¦ä¹ æ–¹æ³•

[ç¬¬ä¸€ç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-first_edition)

[ç¬¬äºŒç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-second_edition)

## ç¬¬ 1 ç«  ç»Ÿè®¡å­¦ä¹ åŠç›‘ç£å­¦ä¹ æ¦‚è®º

**ç»Ÿè®¡å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹æ˜¯**ï¼š

1. ç»Ÿè®¡å­¦ä¹ ä»¥è®¡ç®—æœºåŠç½‘ç»œä¸ºå¹³å°ï¼Œæ˜¯å»ºç«‹åœ¨è®¡ç®—æœºåŠç½‘ç»œä¹‹ä¸Šçš„ï¼›
2. ç»Ÿè®¡å­¦ä¹ ä»¥æ•°æ®ä¸ºç ”ç©¶å¯¹è±¡ï¼Œæ˜¯æ•°æ®é©±åŠ¨çš„å­¦ç§‘ï¼›
3. ç»Ÿè®¡å­¦ä¹ çš„ç›®çš„æ˜¯å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
4. ç»Ÿè®¡å­¦ä¹ ä»¥æ–¹æ³•ä¸ºä¸­å¿ƒï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•æ„å»ºæ¨¡å‹å¹¶åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
5. ç»Ÿè®¡å­¦ä¹ æ˜¯æ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€è®¡ç®—ç†è®ºã€æœ€ä¼˜åŒ–ç†è®ºåŠè®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªé¢†åŸŸçš„äº¤å‰å­¦ç§‘ï¼Œå¹¶ä¸”åœ¨å‘å±•ä¸­é€æ­¥å½¢æˆç‹¬è‡ªçš„ç†è®ºä½“ç³»ä¸æ–¹æ³•è®ºã€‚

**å‡è®¾ç©ºé—´(hypothesis space)**ï¼š
$$\mathcal H = \{ f(x;\theta) | \theta \in \mathbb{R}^D\} \\ or \quad \mathcal F = \{P|P(Y|X;\theta),\theta \in \mathbb{R}^D\}$$
å…¶ä¸­$f(x; \theta)$æ˜¯å‚æ•°ä¸º$\theta$ çš„å‡½æ•°ï¼ˆ**å†³ç­–å‡½æ•°**ï¼‰ï¼Œä¹Ÿç§°ä¸ºæ¨¡å‹ï¼ˆModelï¼‰ï¼Œå‚æ•°å‘é‡$\theta$å–å€¼ä¸$D$ç»´æ¬§å¼ç©ºé—´$\mathbb{R}^D$,ä¹Ÿç§°ä¸ºå‚æ•°ç©ºé—´(parameter space)ï¼Œ$D$ ä¸ºå‚æ•°çš„æ•°é‡(ç»´åº¦)

æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæˆ–å†³ç­–å‡½æ•°

**ç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰**ï¼š
æ¯ä¸ªå…·ä½“çš„è¾“å…¥æ˜¯ä¸€ä¸ªå®ä¾‹ï¼ˆinstanceï¼‰ï¼Œé€šå¸¸ç”±ç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰è¡¨ç¤ºã€‚è¿™
æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾å‘é‡å­˜åœ¨çš„ç©ºé—´ç§°ä¸ºç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰ã€‚ç‰¹å¾ç©ºé—´çš„æ¯ä¸€ç»´å¯¹åº”äº
ä¸€ä¸ªç‰¹å¾ã€‚

> è¾“å…¥ç©ºé—´ä¸­çš„ä¸€ä¸ªè¾“å…¥å‘é‡$x = (x_1,x_2)$ï¼Œåœ¨å¤šé¡¹å¼æ¨¡å‹ä¸­ç‰¹å¾å‘é‡æ˜¯($x_1^2,x_1x_2,x_2^2,...$)
> ä¸€èˆ¬è¯´çš„çº¿æ€§æ¨¡å‹ï¼ŒæŒ‡çš„æ˜¯ç‰¹å¾å‘é‡çš„çº¿æ€§ç»„åˆï¼Œè€Œä¸æ˜¯æŒ‡è¾“å…¥å‘é‡ï¼Œæ‰€ä»¥è¯´æ¨¡å‹éƒ½æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„

**ç»Ÿè®¡å­¦ä¹ çš„ä¸‰è¦ç´ **ï¼š

1. æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)ï¼Œç®€ç§°ï¼šæ¨¡å‹(model)ã€‚å‡è®¾ç©ºé—´å³æˆ‘ä»¬å¯¹æ¨¡å‹å½¢å¼çš„å…ˆéªŒå‡è®¾ï¼Œæœ€ç»ˆæˆ‘ä»¬æ±‚å¾—çš„æ¨¡å‹å¿…å®šç¬¦åˆæˆ‘ä»¬å¯¹æ¨¡å‹å½¢å¼çš„å…ˆéªŒå‡è®¾ã€‚
2. æ¨¡å‹é€‰æ‹©çš„å‡†åˆ™(evaluation criterion)ï¼Œç®€ç§°ï¼šç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™ã€‚å³æˆ‘ä»¬ç”¨ä»€ä¹ˆæ ‡å‡†æ¥è¯„ä»·ä¸€ä¸ªæ¨¡å‹çš„å¥½åã€‚ç­–ç•¥å†³å®šäº†æˆ‘ä»¬ä»å‡è®¾ç©ºé—´ä¸­é€‰æ‹©æ¨¡å‹çš„åå¥½ã€‚
3. æ¨¡å‹å­¦ä¹ çš„ç®—æ³•(algorithm)ï¼Œç®€ç§°ï¼šç®—æ³•(algorithm)ã€‚ä¼˜åŒ–ç®—æ³•æŒ‡çš„æ˜¯é€šè¿‡ä»€ä¹ˆæ ·çš„æ–¹å¼è°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ç»“æ„æˆ–æ¨¡å‹è¶…å‚æ•°å–å€¼ï¼Œä½¿å¾—æ¨¡å‹çš„ç›®æ ‡å‡½æ•°å–å€¼ä¸æ–­é™ä½ã€‚ä¼˜åŒ–ç®—æ³•å†³å®šäº†æˆ‘ä»¬ç”¨ä»€ä¹ˆæ ·çš„æ­¥éª¤åœ¨å‡è®¾ç©ºé—´ä¸­å¯»æ‰¾åˆé€‚çš„æ¨¡å‹ã€‚

> ä»¥çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ä¸ºä¾‹ï¼š
> æ¨¡å‹ï¼š $f(x;w,b) = w^Tx +b$
> ç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™: å¹³æ–¹æŸå¤±å‡½æ•° $\mathcal L(y,\hat{y}) = (y-f(x,\theta))^2$
> ç®—æ³•ï¼šè§£æè§£ analytical solution(é—­å¼è§£ closed-form solution)å’Œæ•°å€¼è§£ numerical solutionï¼Œå¦‚ï¼šclosed-form çš„æœ€å°äºŒä¹˜çš„è§£ä»¥åŠæ¢¯åº¦ä¸‹é™æ³•

**æœºå™¨å­¦ä¹ çš„å®šä¹‰**ï¼š

```mermaid
graph LR;
    F(["æœªçŸ¥çš„ç›®æ ‡å‡½æ•°(ç†æƒ³ä¸­å®Œç¾çš„å‡½æ•°)ï¼šğ‘“: ğ’™âŸ¶ğ‘¦"])-->D["è®­ç»ƒæ ·æœ¬D:{(ğ’™Â¹,ğ‘¦Â¹),...,(ğ’™â¿,ğ‘¦â¿)}"];
    D-->A{{"ç®—æ³•"}}
    H{{"å‡è®¾ç©ºé—´"}}-->A
    A-->G["æ¨¡å‹ gâ‰ˆf"]
```

ä½¿ç”¨è®­ç»ƒæ•°æ®æ¥è®¡ç®—æ¥è¿‘ç›®æ ‡ ğ‘“ çš„å‡è®¾ï¼ˆhypothesis ï¼‰g ï¼ˆæ¥è‡ªï¼š[Machine Learning Foundationsï¼ˆæœºå™¨å­¦ä¹ åŸºçŸ³ï¼‰- the learning problem,25 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/01_handout.pdf)ï¼‰

**ç›‘ç£å­¦ä¹ **ï¼š
ç›‘ç£å­¦ä¹ (supervised learning)æ˜¯æŒ‡ä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„çš„ç»Ÿè®¡è§„å¾‹**ã€‚

è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºè¿ç»­å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**å›å½’é—®é¢˜**ï¼›
è¾“å‡ºå˜é‡ä¸ºæœ‰é™ä¸ªç¦»æ•£å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**åˆ†ç±»é—®é¢˜**ï¼›
è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºå˜é‡åºåˆ—çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**æ ‡æ³¨é—®é¢˜**(åˆ†ç±»é—®é¢˜çš„æ¨å¹¿ï¼Œå¦‚ï¼šéšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼Œæ¡ä»¶éšæœºåœº CRF)ã€‚

ç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥æ˜¯æ¦‚ç‡æ¨¡å‹æˆ–éæ¦‚ç‡æ¨¡å‹ï¼Œç”±**æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ**$P(Y|X)$æˆ–**å†³ç­–å‡½æ•°ï¼ˆdecision functionï¼‰**$Y=f(X)$è¡¨ç¤ºï¼Œéšå…·ä½“å­¦ä¹ æ–¹æ³•è€Œå®šã€‚å¯¹å…·ä½“çš„è¾“å…¥è¿›è¡Œç›¸åº”çš„è¾“å‡ºé¢„æµ‹æ—¶ï¼Œå†™ä½œ$P(y|x)$æˆ–$Y=f(x)$ã€‚
$$y =\displaystyle\argmax_{y}  P(y|x)$$

**è”åˆæ¦‚ç‡åˆ†å¸ƒ**ï¼š
ç›‘ç£å­¦ä¹ å‡è®¾è¾“å…¥ä¸è¾“å‡ºçš„éšæœºå˜é‡ X å’Œ Y éµå¾ªè”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ã€‚$P(X,Y)$è¡¨ç¤ºåˆ†å¸ƒå‡½æ•°ï¼Œæˆ–åˆ†å¸ƒå¯†åº¦å‡½æ•°ã€‚æ³¨æ„ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå‡å®šè¿™ä¸€è”åˆæ¦‚ç‡åˆ†å¸ƒå­˜åœ¨ï¼Œä½†å¯¹å­¦ä¹ ç³»ç»Ÿæ¥è¯´ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å…·ä½“å®šä¹‰æ˜¯æœªçŸ¥çš„ã€‚**è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®è¢«çœ‹ä½œæ˜¯ä¾è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ç‹¬ç«‹åŒåˆ†å¸ƒäº§ç”Ÿçš„**ã€‚
ç»Ÿè®¡å­¦ä¹ å‡è®¾æ•°æ®å­˜åœ¨ä¸€å®šçš„ç»Ÿè®¡è§„å¾‹ï¼Œ$X$å’Œ$Y$å…·æœ‰è”åˆæ¦‚ç‡åˆ†å¸ƒçš„å‡è®¾å°±æ˜¯ç›‘ç£å­¦ä¹ å…³äºæ•°æ®çš„åŸºæœ¬å‡è®¾ã€‚

**éç›‘ç£å­¦ä¹ **ï¼š
éç›‘ç£å­¦ä¹ (unsupervised learning)æ˜¯æŒ‡ä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ æ•°æ®ä¸­çš„ç»Ÿè®¡è§„å¾‹æˆ–æ½œåœ¨ç»“æ„**ã€‚

éç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•°$z = g(x)$æˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(z|x)$ ï¼ˆè¾“å‡º$z$å¯ä»¥æ˜¯**èšç±»**æˆ–è€…**é™ç»´**ï¼‰
$$z =\displaystyle\argmax_{z}  P(z|x)$$
ä»¥åŠ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x|z)$ ï¼ˆç”¨æ¥åš**æ¦‚ç‡å¯†åº¦ä¼°è®¡**ï¼Œæ¯”å¦‚ GMM ä¸­$P(x|z)$å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå¦‚æœå‡è®¾çŸ¥é“æ•°æ®æ¥è‡ªå“ªä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå³çŸ¥é“$z$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼°è®¡ç›¸å…³å‚æ•°ï¼‰ã€‚

[æ ¸å¯†åº¦ä¼°è®¡ Kernel Density Estimation.](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) - åº”ç”¨å¯†åº¦ä¼°è®¡æ£€æµ‹ç¦»ç¾¤å€¼ï¼ˆoutlierï¼‰çš„[LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)

**æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰ä¸éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰æˆ–è€…ç¡®å®šæ€§æ¨¡å‹ï¼ˆdeterministic modelï¼‰**ï¼š

æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰- æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ P(y|x)å’Œ éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰ - å‡½æ•° y=f(x)å¯ä»¥**ç›¸äº’è½¬åŒ–**ï¼Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæœ€å¤§åŒ–åå¾—åˆ°å‡½æ•°ï¼Œå‡½æ•°å½’ä¸€åŒ–åå¾—åˆ°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚æ‰€ä»¥æ¦‚ç‡æ¨¡å‹ä¸éæ¦‚ç‡æ¨¡å‹çš„åŒºåˆ«ä¸åœ¨äºè¾“å…¥è¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œè€Œåœ¨äºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼šæ¦‚ç‡æ¨¡å‹ä¸€å®šå¯ä»¥è¡¨ç¤ºä¸ºè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œè€Œéæ¦‚ç‡æ¨¡å‹åˆ™ä¸ä¸€å®šå­˜åœ¨è¿™æ ·çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚

æ¦‚ç‡æ¨¡å‹çš„ä»£è¡¨æ˜¯**æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆprobabilistic graphical modelï¼‰**$^{å‚è€ƒæ–‡çŒ®[1-3]}$ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒå¯ä»¥æ ¹æ®å›¾çš„ç»“æ„åˆ†è§£ä¸ºå› å­ä¹˜ç§¯çš„å½¢å¼ï¼Œå¯ä»¥ç”¨æœ€åŸºæœ¬çš„åŠ æ³•è§„åˆ™å’Œä¹˜æ³•è§„åˆ™è¿›è¡Œæ¦‚ç‡æ¨ç†ï¼š
$$P(x) = \sum_yP(x,y) \\ P(x,y) = P(x)P(y|x)$$

**å‚æ•°åŒ–æ¨¡å‹ï¼ˆparametric modelï¼‰å’Œéå‚æ•°åŒ–æ¨¡å‹ï¼ˆnon-parametric modelï¼‰**ï¼š

å‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„ç»´åº¦å›ºå®šï¼Œæ¨¡å‹å¯ä»¥ç”±æœ‰é™ç»´å‚æ•°å®Œå…¨åˆ»ç”»ï¼Œä¸éšæ•°æ®ç‚¹çš„å˜åŒ–è€Œå˜åŒ–ã€‚(å¦‚ï¼šæ„ŸçŸ¥æœºã€GMMã€logistic regressionã€æœ´ç´ è´å¶æ–¯ã€k å‡å€¼èšç±»ã€æ½œåœ¨è¯­ä¹‰åˆ†æã€æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æã€æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…)
éå‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„å”¯ç‹¬ä¸å›ºå®šæˆ–è€…è¯´æ— ç©·å¤§ï¼Œéšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ è€Œä¸æ–­å¢å¤§ã€‚(å¦‚ï¼šå†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€AdaBoostã€k è¿‘é‚»)

> éå‚æ•°åŒ–æ¨¡å‹æ„å‘³ç€å†³ç­–æ ‘æ²¡æœ‰å‡è®¾ç©ºé—´åˆ†å¸ƒå’Œåˆ†ç±»å™¨ç»“æ„?

**åœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰å’Œæ‰¹é‡å­¦ä¹ ï¼ˆbatch learningï¼‰**ï¼š

åœ¨çº¿å­¦ä¹ æ¯æ¬¡æ¥å—ä¸€ä¸ªæ ·æœ¬ï¼Œé¢„æµ‹åå­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸æ–­é‡å¤è¯¥æ“ä½œã€‚
æ‰¹é‡å­¦ä¹ ä¸€æ¬¡æ¥å—æ‰€æœ‰æ•°æ®ï¼Œå­¦ä¹ æ¨¡å‹ä¹‹åè¿›è¡Œé¢„æµ‹ã€‚

åœ¨çº¿å­¦ä¹ æ¯”æ‰¹é‡å­¦ä¹ æ›´éš¾ï¼Œå› ä¸ºæ¯æ¬¡æ¨¡å‹æ›´æ–°ä¸­å¯åˆ©ç”¨çš„æ•°æ®æœ‰é™ã€‚

**è´å¶æ–¯å­¦ä¹ ï¼ˆBayesian learningï¼‰/ è´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰**ï¼š
$$\mathrm{Bayes \; Rule:} \\ \underbrace{P(X|Y)}_{\mathrm{posterior}} = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{P(Y)}_{\mathrm{evidence}}}   = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{\sum_{x}P(Y|X)P(X)}_{\mathrm{evidence}}}$$

**æ ¸æŠ€å·§ï¼ˆkernel trickï¼‰/ æ ¸æ–¹æ³•ï¼ˆkernel methodï¼‰**ï¼š

**æ ¸æ–¹æ³•**æ˜¯ä¸€ç±»æŠŠä½ç»´ç©ºé—´çš„éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œè½¬åŒ–ä¸ºé«˜ç»´ç©ºé—´çš„çº¿æ€§å¯åˆ†é—®é¢˜çš„æ–¹æ³•ã€‚
**æ ¸æŠ€å·§**æ˜¯ä¸€ç§åˆ©ç”¨æ ¸å‡½æ•°ç›´æ¥è®¡ç®— $\lang \phi(x),\phi(z) \rang$ ï¼Œä»¥é¿å¼€åˆ†åˆ«è®¡ç®— $\phi(x)$ å’Œ $\phi(z)$ ï¼Œä»è€ŒåŠ é€Ÿæ ¸æ–¹æ³•è®¡ç®—çš„æŠ€å·§ã€‚

**æ ¸å‡½æ•°**ï¼š[Kernel function](https://en.jinzhao.wiki/wiki/Positive-definite_kernel)
è®¾ $\mathcal X$ æ˜¯è¾“å…¥ç©ºé—´ï¼ˆå³ $x_i \in \mathcal X $ ï¼Œ $\mathcal X$ æ˜¯ $\mathbb R^n$ çš„å­é›†æˆ–ç¦»æ•£é›†åˆ ï¼‰ï¼Œåˆè®¾ $\mathcal H$ ä¸ºç‰¹å¾ç©ºé—´ï¼ˆâ€‹ å¸Œå°”ä¼¯ç‰¹ç©ºé—´$^{é™„åŠ çŸ¥è¯†:å„ç§ç©ºé—´ä»‹ç»}$ï¼‰ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªä» $\mathcal X$ åˆ° $\mathcal H$ çš„æ˜ å°„

$$\phi(x) : \mathcal X \to \mathcal H$$

ä½¿å¾—å¯¹æ‰€æœ‰ $x,z \in \mathcal X$ ï¼Œå‡½æ•° $K(x,z)$ æ»¡è¶³æ¡ä»¶

$$K(x,z) = \phi(x).\phi(z) = \lang \phi(x),\phi(z) \rang$$

åˆ™ç§° $K(x,z)$ ä¸ºæ ¸å‡½æ•°ã€‚å…¶ä¸­ $\phi(x) $ ä¸ºæ˜ å°„å‡½æ•°ï¼Œ $\lang \phi(x),\phi(z) \rang$ ä¸ºå†…ç§¯ã€‚

[æ ¸æŠ€å·§](https://en.jinzhao.wiki/wiki/Kernel_method)çš„æƒ³æ³•æ˜¯ï¼Œåœ¨å­¦ä¹ å’Œé¢„æµ‹ä¸­åªå®šä¹‰æ ¸å‡½æ•° $K(x,z)$ ï¼Œè€Œä¸æ˜¾å¼åœ°å®šä¹‰æ˜ å°„å‡½æ•° $\phi $ã€‚é€šå¸¸ç›´æ¥è®¡ç®—$K(x,z)$æ¯”è¾ƒå®¹æ˜“ï¼Œè€Œé€šè¿‡$\phi(x) $å’Œ$\phi(z) $è®¡ç®—$K(x,z)$å¹¶ä¸å®¹æ˜“ã€‚

> æ³¨æ„ï¼š$\phi $æ˜¯è¾“å…¥ç©ºé—´$\mathbb{R}^n$åˆ°ç‰¹å¾ç©ºé—´$\mathcal H$çš„æ˜ å°„ï¼Œç‰¹å¾ç©ºé—´$\mathcal H$ä¸€èˆ¬æ˜¯é«˜ç»´çš„ï¼Œç”šè‡³æ˜¯æ— ç©·ç»´çš„ã€‚æ‰€ä»¥$\phi$ä¸å¥½è®¡ç®—ï¼Œç”šè‡³ä¼šå¸¦æ¥**ç»´åº¦ç¾éš¾**åˆç§°**ç»´åº¦è¯…å’’ï¼ˆCurse of Dimensionalityï¼‰**$^{é™„åŠ çŸ¥è¯†:ç»´åº¦è¯…å’’}$ã€‚

### é™„åŠ çŸ¥è¯†

#### æ­£åˆ™åŒ–

æ­£åˆ™åŒ–ç¬¦åˆå¥¥å¡å§†å‰ƒåˆ€ï¼ˆOccam's razorï¼‰åŸç†ã€‚

å‚è€ƒï¼š[L1L2 æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)

#### æ¨¡å‹é€‰æ‹©

å‚è€ƒï¼š[æ¨¡å‹é€‰æ‹©](../Model-Selection.md)

#### ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹

å‚è€ƒï¼š[ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹](../ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹.md)

#### å„ç§ç©ºé—´ä»‹ç»

**çº¿æ€§ç©ºé—´**å°±æ˜¯å®šä¹‰äº†**åŠ æ³•å’Œæ•°ä¹˜**çš„ç©ºé—´(ç©ºé—´é‡Œçš„ä¸€ä¸ªå…ƒç´ å°±å¯ä»¥ç”±å…¶ä»–å…ƒç´ çº¿æ€§è¡¨ç¤º)ã€‚

---

**åº¦é‡ç©ºé—´**å°±æ˜¯å®šä¹‰äº†**è·ç¦»**çš„ç©ºé—´ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼Œæ¬§æ°è·ç¦»ï¼Œé—µå¯å¤«æ–¯åŸºè·ç¦»ï¼Œé©¬æ°è·ç¦»ï¼Œåˆ‡æ¯”é›ªå¤«è·ç¦»ï¼‰ã€‚
å®šä¹‰è·ç¦»æ—¶ï¼Œæœ‰ä¸‰æ¡å…¬ç†å¿…é¡»éµå®ˆï¼š

1. éè´Ÿæ€§ã€åŒä¸€æ€§ï¼š$dist(x_i,x_j) \geq 0$(éè´Ÿæ€§)ï¼Œ$dist(x_i,x_j) = 0$å½“ä¸”ä»…å½“$x_i=x_j$(åŒä¸€æ€§)
2. å¯¹ç§°æ€§ï¼š$dist(x_i,x_j) = dist(x_j,x_i)$
3. ä¸‰è§’ä¸ç­‰å¼(ä¹Ÿå«ç›´é€’æ€§)ï¼š$dist(x_i,x_j) \leq dist(x_i,x_k) + dist(x_k,x_j)$
   å¸Œå°”ä¼¯ç‰¹ç©ºé—´(Hilbert)
   > æ–‡å­—è§£é‡Šï¼šã€ä¸¤ç‚¹ä¹‹é—´è·ç¦»ä¸ä¸ºè´Ÿï¼›ä¸¤ä¸ªç‚¹åªæœ‰åœ¨ ç©ºé—´ ä¸Šé‡åˆæ‰å¯èƒ½è·ç¦»ä¸ºé›¶ï¼›a åˆ° b çš„è·ç¦»ç­‰äº b åˆ° a çš„è·ç¦»;a åˆ° c çš„è·ç¦»åŠ ä¸Š c åˆ° b çš„è·ç¦»å¤§äºç­‰äº a ç›´æ¥åˆ° b çš„è·ç¦»;ã€‘

---

**èµ‹èŒƒç©ºé—´**å°±æ˜¯å®šä¹‰äº†**èŒƒæ•°**çš„ç©ºé—´ã€‚
x çš„èŒƒæ•°||x||å°±æ˜¯ x çš„**é•¿åº¦**ã€‚é‚£ä¹ˆè¿™é‡Œçš„é•¿åº¦å’Œä¸Šä¸€èŠ‚ä¸­è¯´çš„è·ç¦»åˆ°åº•æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ã€‚**è·ç¦»çš„æ¦‚å¿µæ˜¯é’ˆå¯¹ä¸¤ä¸ªå…ƒç´ æ¥è¯´çš„**ï¼Œä¾‹å¦‚ d(x,y)æŒ‡çš„æ˜¯ x ä¸ y ä¸¤ä¸ªå…ƒç´ ä¹‹é—´çš„è·ç¦»ï¼Œè€Œ**èŒƒæ•°æ˜¯é’ˆå¯¹ä¸€ä¸ªå…ƒç´ æ¥è¯´çš„**ï¼Œæ¯ä¸€ä¸ªå…ƒç´ éƒ½å¯¹åº”ä¸€ä¸ªèŒƒæ•°ï¼Œå¯ä»¥å°†èŒƒæ•°ç†è§£ä¸ºä¸€ä¸ªå…ƒç´ åˆ°é›¶ç‚¹çš„è·ç¦»ï¼ˆè¿™åªæ˜¯ä¸€ç§ç†è§£ï¼Œå¹¶ä¸æ˜¯å®šä¹‰ï¼‰ï¼Œä¹Ÿå°±æ˜¯å®ƒè‡ªå·±çš„é•¿åº¦ã€‚
å®šä¹‰ï¼š
ç§° æ˜ å°„$||.|| : \mathbb{R}^n \to \mathbb{R}$ä¸º $\mathbb{R}^n$ ä¸Šçš„èŒƒæ•°ï¼Œå½“ä¸”ä»…å½“ï¼š

1. éè´Ÿæ€§ï¼š $\forall x \in \mathbb{R}^n ,||x|| \geq 0$ ,$||x|| = 0$å½“ä¸”ä»…å½“$x=0$
2. æ•°ä¹˜ï¼š$\forall x \in \mathbb{R}^n ,a \in \mathbb{R}^n, ||ax|| = |a|.||x||$
3. ä¸‰è§’ä¸ç­‰å¼: $\forall x,y \in \mathbb{R}^n ,||x+y|| \leq ||x|| + ||y||$

å¦‚æœæˆ‘ä»¬å®šä¹‰äº†èŒƒæ•°ï¼Œå¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šå®šä¹‰è·ç¦»ï¼šdist(x,y)=||x-y||ã€‚æ ¹æ®èŒƒæ•°çš„ä¸‰æ¡æ€§è´¨ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜æˆ‘ä»¬è¿™æ ·å®šä¹‰çš„è·ç¦»ä¹Ÿæ»¡è¶³è·ç¦»çš„å®šä¹‰ï¼Œèªæ˜çš„ä½ å¯ä»¥è‡ªå·±è¯æ˜ä¸€ä¸‹ï¼ˆå¯¹ç§°æ€§çš„è¯æ˜ï¼Œæä¸€ä¸ª-1 å‡ºæ¥ï¼Œä¸€åŠ ç»å¯¹å€¼å°±æ˜¯ 1 äº†ï¼‰ã€‚

ä¹Ÿå°±æ˜¯è¯´èŒƒæ•°å…¶å®æ˜¯ä¸€ä¸ªæ›´åŠ å…·ä½“çš„æ¦‚å¿µï¼Œ**æœ‰äº†èŒƒæ•°ä¸€å®šèƒ½åˆ©ç”¨èŒƒæ•°å®šä¹‰è·ç¦»ï¼Œä½†æ˜¯æœ‰è·ç¦»ä¸èƒ½å®šä¹‰èŒƒæ•°**ã€‚

ä¹Ÿè®¸ä½ ä¼šé—®ï¼Œä½ ä¸æ˜¯è¯´ç†è§£èŒƒæ•°å°±æ˜¯ä¸€ä¸ªå…ƒç´ åˆ°é›¶ç‚¹çš„è·ç¦»å—ï¼Œé‚£å®šä¹‰èŒƒæ•°ä¸º||x||=dist(x,0) ä¸å°±è¡Œäº†å—ã€‚è¿™æ ·çš„è¯ï¼Œå¯¹äºèŒƒæ•°çš„ç¬¬äºŒæ¡æ€§è´¨å°±ä¸ä¸€å®šä¼šæ»¡è¶³ï¼Œ||ax||=dist(ax,0)ï¼Œè€Œ dist(ax,0)ä¸ä¸€å®šç­‰äº|a|dist(x,0)ï¼Œå…·ä½“ç­‰ä¸ç­‰äºè¿˜è¦çœ‹ä½ çš„è·ç¦»æ˜¯æ€ä¹ˆå®šä¹‰çš„ã€‚

ä¾‹å¦‚ï¼šL<sub>p</sub>èŒƒæ•°
æ¬§å¼è·ç¦»å¯¹åº” L2 èŒƒæ•°
æ›¼å“ˆé¡¿è·ç¦»å¯¹åº” L1 èŒƒæ•°
åˆ‡æ¯”é›ªå¤«è·ç¦»å¯¹åº” Lâˆ èŒƒæ•°
L<sub>p</sub>èŒƒæ•°ï¼šå½“ p>=1 æ—¶ï¼Œå‘é‡çš„ L<sub>p</sub>èŒƒæ•°æ˜¯å‡¸çš„ã€‚(è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä¸€èˆ¬ä¸ç”¨ L0 èŒƒæ•°çš„åŸå› ä¹‹ä¸€)

---

**çº¿æ€§èµ‹èŒƒç©ºé—´**å°±æ˜¯å®šä¹‰äº†åŠ æ³•ã€æ•°ä¹˜å’ŒèŒƒæ•°çš„ç©ºé—´ã€‚

---

**å·´æ‹¿èµ«ç©ºé—´**å°±æ˜¯**å®Œå¤‡çš„èµ‹èŒƒçº¿æ€§ç©ºé—´**ã€‚(Banach space)
**å®Œå¤‡çš„ç©ºé—´**çš„å®šä¹‰ï¼šå¦‚æœä¸€ä¸ªç©ºé—´æ˜¯å®Œå¤‡çš„ï¼Œé‚£ä¹ˆè¯¥ç©ºé—´ä¸­çš„ä»»ä½•ä¸€ä¸ªæŸ¯è¥¿åºåˆ—éƒ½æ”¶æ•›åœ¨è¯¥ç©ºé—´ä¹‹å†…ã€‚

é¦–å…ˆæ¥è¯´ä¸€ä¸‹æŸ¯è¥¿åºåˆ—æ˜¯ä»€ä¹ˆï¼ŒæŸ¯è¥¿åºåˆ—å°±æ˜¯éšç€åºæ•°å¢åŠ ï¼Œå€¼ä¹‹é—´çš„è·ç¦»è¶Šæ¥è¶Šå°çš„åºåˆ—ã€‚æ¢ä¸€ç§è¯´æ³•æ˜¯ï¼ŒæŸ¯è¥¿åºåˆ—å¯ä»¥åœ¨å»æ‰æœ‰é™ä¸ªå€¼ä¹‹åï¼Œä½¿ä»»æ„ä¸¤ä¸ªå€¼ä¹‹é—´çš„$\underline{\mathrm{è·ç¦»}}$éƒ½å°äºä»»æ„ç»™å®šæ­£å¸¸æ•°ï¼ˆå…¶å®è¿™å°±æ˜¯å®šä¹‰äº†ä¸€ä¸ªæé™è€Œå·²ï¼‰ã€‚

é‚£ä¹ˆä»»æ„ä¸€ä¸ªæŸ¯è¥¿åºåˆ—éƒ½æ”¶æ•›åœ¨è¯¥ç©ºé—´å†…æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Œä¸¾ä¸ªä¾‹å­ä½ å°±æ˜ç™½äº†ã€‚

è®¾å®šä¹‰åœ¨æœ‰ç†æ•°ç©ºé—´ Q ä¸Šçš„åºåˆ—ï¼š$x_n = \frac{[\sqrt{2}n]}{n}$ï¼Œå…¶ä¸­[x]è¡¨ç¤º x å–æ•´æ•°éƒ¨åˆ†ã€‚
å¯¹äºè¿™ä¸ªæ•°åˆ—æ¥è¯´ï¼Œæ¯ä¸€ä¸ªå…ƒç´ çš„åˆ†å­åˆ†æ¯éƒ½æ˜¯æ•´æ•°ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ª$x_n$éƒ½åœ¨æœ‰ç†æ•°ç©ºé—´ Q ä¸Šï¼Œé‚£è¿™ä¸ªåºåˆ—çš„æé™å‘¢ï¼Œç¨æœ‰å¸¸è¯†çš„äººéƒ½èƒ½çœ‹å‡ºï¼Œè¿™ä¸ªåºåˆ—çš„æé™æ˜¯$\sqrt{2}$ï¼Œè€Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæœ‰ç†æ•°ï¼Œæ‰€ä»¥è¿™ä¸ªæŸ¯è¥¿åºåˆ—çš„æé™ä¸åœ¨è¯¥ç©ºé—´é‡Œé¢ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰ç†æ•°ç©ºé—´ Q æ˜¯ä¸å®Œå¤‡çš„ã€‚

æ‰€ä»¥å®Œå¤‡çš„æ„ä¹‰æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ï¼Œé‚£å°±æ˜¯**åœ¨ä¸€ä¸ªç©ºé—´ä¸Šæˆ‘ä»¬å®šä¹‰äº†æé™ï¼Œä½†æ˜¯ä¸è®ºä½ æ€ä¹ˆå–æé™ï¼Œå®ƒçš„æé™çš„å€¼éƒ½ä¸ä¼šè·‘å‡ºè¿™ä¸ªç©ºé—´ï¼Œé‚£ä¹ˆè¿™ä¸ªç©ºé—´å°±æ˜¯å®Œå¤‡ç©ºé—´**ã€‚

å¦å¤–ï¼Œä¸çŸ¥é“ä½ æœ‰æ²¡æœ‰å‘ç°ï¼Œä¸Šé¢åœ¨è§£é‡Šä»€ä¹ˆæ˜¯æŸ¯è¥¿åºåˆ—çš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªè¯æˆ‘åŠ äº†ä¸‹åˆ’çº¿ï¼Œé‚£å°±æ˜¯è·ç¦»ï¼Œä¹Ÿå°±è¯´è¯´åœ¨å®šä¹‰å®Œå¤‡ç©ºé—´ä¹‹å‰ï¼Œè¦å…ˆæœ‰è·ç¦»çš„æ¦‚å¿µã€‚æ‰€ä»¥**å®Œå¤‡ç©ºé—´ï¼Œå…¶å®ä¹Ÿæ˜¯å®Œå¤‡åº¦é‡ç©ºé—´**ã€‚

æ‰€ä»¥ï¼Œå·´æ‹¿èµ«ç©ºé—´æ»¡è¶³å‡ æ¡ç‰¹æ€§å‘¢ï¼šè·ç¦»ã€èŒƒæ•°ã€å®Œå¤‡ã€‚

---

**å†…ç§¯ç©ºé—´**å°±æ˜¯å®šä¹‰äº†å†…ç§¯çš„ç©ºé—´ã€‚[Inner product space](https://en.jinzhao.wiki/wiki/Inner_product_space)
æœ‰æ—¶ä¹Ÿç§°å‡†å¸Œå°”ä¼¯ç‰¹ç©ºé—´ã€‚
å†…ç§¯å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„ç‚¹ä¹˜ã€æ ‡ç§¯ï¼Œå®ƒçš„å®šä¹‰æ–¹å¼ä¹Ÿä¸æ˜¯å”¯ä¸€çš„ï¼Œä½†å¦‚åŒè·ç¦»èŒƒæ•°çš„å®šä¹‰ä¸€æ ·ï¼Œå†…ç§¯çš„å®šä¹‰ä¹Ÿè¦æ»¡è¶³æŸäº›æ¡ä»¶ï¼Œä¸èƒ½éšä¾¿å®šä¹‰ã€‚

å®šä¹‰æ˜ å°„$\lang .,. \rang : V \times V \to \mathbb{F}$, å…¶ä¸­$V$æ˜¯å‘é‡ï¼Œ$\mathbb{F}$æ˜¯æ ‡é‡
æœ‰$x,y,z \in V ,s \in \mathbb{F}$ï¼Œé‚£ä¹ˆå†…ç§¯æ»¡è¶³

1. ç¬¬ä¸€ä¸ªå‚æ•°ä¸­çš„çº¿æ€§:
   $$\lang sx,y \rang = s\lang x,y \rang \\ \lang x+y,z \rang = \lang x,z \rang + \lang y,z \rang \\ \lang 0,x \rang = 0$$

2. å…±è½­å¯¹ç§°:$\lang x,y \rang = \overline{\lang y,x \rang }$

3. æ­£å®šæ€§:$\lang x,x \rang > 0 \quad\mathrm{if}\; x \neq 0$

4. æ­£åŠå®šæ€§æˆ–éè´Ÿå®šæ€§:$\forall{x}, \lang x,x \rang \geq 0 $

5. ç¡®å®šæ€§ï¼š$\lang x,x \rang = 0 å¿…ç„¶æœ‰ x=0$

3ï¼Œ4ï¼Œ5 å¯ä»¥è·Ÿä¸Šé¢å®šä¹‰èŒƒæ•°å’Œè·ç¦»ä¸€æ ·å†™æˆä¸€ä¸ª

ä¾‹å­-æ¬§å‡ é‡Œå¾—å‘é‡ç©ºé—´:
$ x,y \in \mathbb{R}^n , \lang x,y \rang = x^Ty=\sum\_{i=1}^n{x_iy_i}$

**åªæœ‰å®šä¹‰äº†å†…ç§¯ï¼Œæ‰ä¼šæœ‰å¤¹è§’çš„æ¦‚å¿µï¼Œæ‰ä¼šæœ‰æ­£äº¤çš„æ¦‚å¿µï¼Œå¦å¤–å†…ç§¯ä¹Ÿå¯ä»¥å®šä¹‰èŒƒæ•°ï¼Œä¹Ÿå°±æ˜¯è¯´å†…ç§¯æ˜¯æ¯”èŒƒæ•°æ›´å…·ä½“çš„ä¸€ä¸ªæ¦‚å¿µã€‚**

---

**æ¬§å¼ç©ºé—´**å°±æ˜¯å®šä¹‰äº†å†…ç§¯çš„æœ‰é™ç»´å®çº¿æ€§ç©ºé—´ã€‚

---

**å¸Œå°”ä¼¯ç‰¹ç©ºé—´**å°±æ˜¯å®Œå¤‡çš„å†…ç§¯ç©ºé—´ã€‚(Hilbert space)
å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„å…ƒç´ ä¸€èˆ¬æ˜¯å‡½æ•°ï¼Œå› ä¸ºä¸€ä¸ªå‡½æ•°å¯ä»¥è§†ä¸ºä¸€ä¸ªæ— ç©·ç»´çš„å‘é‡ã€‚

```mermaid
graph LR;
    LS(("Linear Space"))-->NLS(("Normed Linear Space"));
    NLS-->BS(("Banach Space"))
    NLS-->IPS(("Inner Product Space"))
    IPS-->HS(("Hilbert Space"))
    IPS-->ES(("Euclid Space"))
```

![](https://pic2.zhimg.com/80/v2-be26b2ba1df2edc9636647a28b22238d_720w.jpg?source=1940ef5c)

å‚è€ƒï¼š[ä¸€ç‰‡æ–‡ç« å¸¦ä½ ç†è§£å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰ä»¥åŠå„ç§ç©ºé—´](https://blog.csdn.net/ChangHengyi/article/details/80577318)

#### ç»´åº¦è¯…å’’

ç»´åº¦è¯…å’’é€šå¸¸æ˜¯æŒ‡åœ¨æ¶‰åŠåˆ°å‘é‡çš„è®¡ç®—çš„é—®é¢˜ä¸­ï¼Œéšç€ç»´æ•°çš„å¢åŠ ï¼Œè®¡ç®—é‡å‘ˆæŒ‡æ•°å€å¢é•¿çš„ä¸€ç§ç°è±¡ã€‚é«˜ç»´åº¦æœ‰æ›´å¤§çš„ç‰¹å¾ç©ºé—´ï¼Œéœ€è¦æ›´å¤šçš„æ•°æ®æ‰å¯ä»¥è¿›è¡Œè¾ƒå‡†ç¡®çš„ä¼°è®¡ã€‚

> è‹¥ç‰¹å¾æ˜¯äºŒå€¼çš„ï¼Œåˆ™æ¯å¢åŠ ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€éœ€æ•°æ®é‡éƒ½åœ¨ä»¥ 2 çš„æŒ‡æ•°çº§è¿›è¡Œå¢é•¿ï¼Œæ›´ä½•å†µå¾ˆå¤šç‰¹å¾ä¸åªæ˜¯äºŒå€¼çš„ã€‚

å‡ ä½•è§’åº¦ 1ï¼š

<svg width="52" height="52" xmlns="http://www.w3.org/2000/svg">
 <!-- Created with Method Draw - http://github.com/duopixel/Method-Draw/ -->
 <g>
  <title>background</title>
  <rect fill="#fff" id="canvas_background" height="54" width="54" y="-1" x="-1"/>
  <g display="none" overflow="visible" y="0" x="0" height="100%" width="100%" id="canvasGrid">
   <rect fill="url(#gridpattern)" stroke-width="0" y="0" x="0" height="100%" width="100%"/>
  </g>
 </g>
 <g>
  <title>Layer 1</title>
  <rect stroke="#000" id="svg_1" height="50" width="50" y="1.134891" x="1.227186" stroke-width="1.5" fill="#fff"/>
  <ellipse stroke="#000" ry="25" rx="25" id="svg_2" cy="26.316708" cx="25.727185" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" fill="#fff"/>
  <line stroke-linecap="null" stroke-linejoin="null" id="svg_3" y2="26.363651" x2="49.090879" y1="26.363651" x1="23.636325" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="none"/>
  <text stroke="#000" transform="matrix(0.8454890517551235,0,0,0.38060957631270753,66.36433546231878,120.48066499237646) " xml:space="preserve" text-anchor="start" font-family="Helvetica, Arial, sans-serif" font-size="24" id="svg_4" y="-262.016546" x="-56.089448" fill-opacity="null" stroke-opacity="null" stroke-width="0" fill="#000000">0.5</text>
 </g>
</svg>

ä¸Šå›¾è¡¨ç¤ºä¸€ä¸ªå¤šç»´ç©ºé—´ï¼ˆä»¥äºŒç»´ä¸ºä¾‹ï¼‰ï¼Œè®¾æ­£æ–¹å½¢è¾¹é•¿ä¸º 1ï¼Œåˆ™å…¶å†…åˆ‡åœ†åŠå¾„ä¸º$r=0.5$ï¼Œåˆ™æ­£æ–¹å½¢é¢ç§¯ä¸º 1ï¼Œå†…åˆ‡åœ†é¢ç§¯ä¸º$\pi(0.5)^2$ ã€‚è‹¥å°†æ­¤å˜ä¸ºä¸‰ç»´æƒ…å†µä¸‹ï¼Œæ­£æ–¹ä½“ä½“ç§¯ä¸º 1ï¼Œå†…åˆ‡çƒä½“ç§¯ä¸º$\frac{4}{3}\pi(0.5)^3$ã€‚

å› æ­¤çƒä½“çš„ä½“ç§¯å¯ä»¥è¡¨ç¤ºä¸º$V(d) = \frac{\pi^{d/2}}{\varGamma(\frac{d}{2}+1)}0.5^d = k(0.5)^d$(d ä¸ºç»´åº¦),åˆ™ $\lim_{d \to \infty}k(0.5)^d = 0$ï¼Œå…¶å†…åˆ‡è¶…çƒä½“çš„ä½“ç§¯ä¸º 0ã€‚ç”±æ­¤å¯çŸ¥ï¼Œ**é«˜ç»´æƒ…å†µä¸‹ï¼Œæ•°æ®å¤§éƒ½åˆ†å¸ƒåœ¨å››è§’ï¼ˆæ­£æ–¹å½¢å†…ï¼Œå†…åˆ‡åœ†å¤–ï¼‰**ï¼Œç¨€ç–æ€§å¤ªå¤§ï¼Œä¸å¥½åˆ†ç±»ã€‚

> ç»´åº¦è¶Šå¤§ï¼Œè¶…çƒä½“ä½“ç§¯è¶Šå°ã€‚è¯´æ˜è½åœ¨è¶…çƒä½“å†…çš„æ ·æœ¬è¶Šå°‘ï¼Œå› ä¸ºè¶…çƒä½“æ˜¯è¶…ç«‹æ–¹ä½“çš„å†…åˆ‡çƒã€‚ä¸åœ¨çƒå†…,é‚£åªèƒ½åœ¨è§’è½ï¼

å‡ ä½•è§’åº¦ 2ï¼š

<svg width="52" height="52" xmlns="http://www.w3.org/2000/svg">
 <!-- Created with Method Draw - http://github.com/duopixel/Method-Draw/ -->
 <g>
  <title>background</title>
  <rect fill="#fff" id="canvas_background" height="54" width="54" y="-1" x="-1"/>
  <g display="none" overflow="visible" y="0" x="0" height="100%" width="100%" id="canvasGrid">
   <rect fill="url(#gridpattern)" stroke-width="0" y="0" x="0" height="100%" width="100%"/>
  </g>
 </g>
 <g>
  <title>Layer 1</title>
  <ellipse stroke="#000" ry="25" rx="25" id="svg_5" cy="25" cx="25" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" fill="#fff"/>
  <ellipse id="svg_6" cy="24.593763" cx="34.636353" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="#fff"/>
  <ellipse ry="20" rx="20" id="svg_7" cy="25" cx="25" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="#fff"/>
 </g>
</svg>

ä¸Šå›¾ä¹Ÿè¡¨ç¤ºä¸€ä¸ªå¤šç»´ç©ºé—´ï¼ˆä»¥äºŒç»´ä¸ºä¾‹ï¼‰ï¼Œåˆ™å…¶ä¸­å›¾å½¢çš„ä½“ç§¯æœ‰å¦‚ä¸‹å…³ç³»ï¼šå¤–åœ†åŠå¾„$r=1$ï¼Œå†…åœ†åŠå¾„ä¸º$râˆ’\varepsilon$ ã€‚åŒæ ·åœ¨é«˜ç»´æƒ…å†µä¸‹ï¼Œå¤–åœ†ä½“ç§¯ä¸º$V_{å¤–åœ†} = k.1^d = k$ï¼Œä¸­é—´çš„åœ†ç¯ä½“ç§¯ä¸º$V_{åœ†ç¯} = k - k(1-\varepsilon)^d$ï¼Œåˆ™ï¼š
$$\lim_{d \to \infty}\frac{V_{åœ†ç¯}}{V_{å¤–åœ†}} = \lim_{d \to \infty}\frac{ k - k(1-\varepsilon)^d}{k} = \lim_{d \to \infty}(1-(1-\varepsilon)^d) = 1$$

> é«˜ç»´æƒ…å†µä¸‹ï¼Œæ— è®º$\varepsilon$å¤šå°ï¼Œåªè¦ d è¶³å¤Ÿå¤§ï¼Œåœ†ç¯å‡ ä¹å æ®äº†æ•´ä¸ªå¤–åœ†ï¼Œå†…åœ†ä½“ç§¯è¶‹å‘äº 0ï¼Œå¯¼è‡´æ•°æ®**ç¨€ç–**ã€‚

å‚è€ƒï¼š
[The Curse of Dimensionality in classification](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(äº”)-é™ç»´ï¼ˆDimensionality Reductionï¼‰](https://www.bilibili.com/video/BV1vW411S7tH)

#### ä¸ç­‰å¼(Inequality)

[æ‰€æœ‰ä¸ç­‰å¼](https://en.jinzhao.wiki/wiki/Category:Inequalities) ä»¥åŠ[æ‰€æœ‰æ¦‚ç‡ï¼ˆProbabilisticï¼‰ä¸ç­‰å¼](https://en.jinzhao.wiki/wiki/Category:Probabilistic_inequalities)

- **[ç»å¯¹å€¼ä¸ç­‰å¼](https://chi.jinzhao.wiki/wiki/%E7%BB%9D%E5%AF%B9%E5%80%BC%E4%B8%8D%E7%AD%89%E5%BC%8F) - Absolute value inequality**

- **å¹‚å¹³å‡å€¼ä¸ç­‰å¼- [Power-Mean Inequality](https://artofproblemsolving.com/wiki/index.php/Power_Mean_Inequality)**

- **[ä¸‰è§’å½¢å†…è§’çš„åµŒå…¥ä¸ç­‰å¼](https://chi.jinzhao.wiki/wiki/%E4%B8%89%E8%A7%92%E5%BD%A2%E5%86%85%E8%A7%92%E7%9A%84%E5%B5%8C%E5%85%A5%E4%B8%8D%E7%AD%89%E5%BC%8F) - æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º Wolstenholme ä¸ç­‰å¼**

- **ä¼¯åŠªåˆ©ä¸ç­‰å¼ - [Bernoulli's inequality](https://en.jinzhao.wiki/wiki/Bernoulli%27s_inequality)**
- **æ’åºä¸ç­‰å¼ - [Rearrangement inequality](https://en.jinzhao.wiki/wiki/Rearrangement_inequality)**
- **å‡å€¼ä¸ç­‰å¼ - [Inequality of arithmetic and geometric means](https://en.jinzhao.wiki/wiki/Inequality_of_arithmetic_and_geometric_means)**

- **èˆ’å°”ä¸ç­‰å¼ - [Schur's inequality](https://en.jinzhao.wiki/wiki/Schur%27s_inequality)**

- **é—µå¯å¤«æ–¯åŸº (Minkowski) ä¸ç­‰å¼ - [Minkowski inequality](https://en.jinzhao.wiki/wiki/Minkowski_inequality)**

- **å‰å¸ƒæ–¯ (Gibbs) ä¸ç­‰å¼ - [Gibbs' inequality](https://en.jinzhao.wiki/wiki/Gibbs%27_inequality)**
  $${\displaystyle -\sum _{i=1}^{n}p_{i}\log p_{i}\leq -\sum _{i=1}^{n}p_{i}\log q_{i}}$$

ç”± KL divergence å°±èƒ½è¯æ˜
$${\displaystyle D_{\mathrm {KL} }(P\|Q)\equiv \sum _{i=1}^{n}p_{i}\log {\frac {p_{i}}{q_{i}}}\geq 0.}$$

##### æ¦‚ç‡ä¸ç­‰å¼ Probabilistic inequalities

- **æŸ¯è¥¿-æ–½ç“¦èŒ¨ (Cauchyâ€“Schwarz) ä¸ç­‰å¼ - [Cauchyâ€“Schwarz inequality](https://en.jinzhao.wiki/wiki/Cauchy%E2%80%93Schwarz_inequality)**
  $$[\sum_{i=1}^{n}{a_ib_i}]^2  \leq [\sum_{i=1}^{n}a_i^2].[\sum_{i=1}^{n}b_i^2] ç­‰å¼æˆç«‹ï¼šb_i=ka_i \\ å‘é‡å½¢å¼ï¼š|\braket{u,v}| \leq ||u||.||v|| \\ æ¦‚ç‡ä¸­ï¼š|E(XY)|^2 \leq E(X^2)E(Y^2)$$
  è¯æ˜ï¼š
  $$\vec{A} = (a_1,...,a_n),  \vec{B} = (b_1,...,b_n) \\ \vec{A}.\vec{B} = (a_1b_1,...,a_nb_n) = ||\vec{A}||.||\vec{B}||\cos\theta \leq ||\vec{A}||.||\vec{B}|| = \sqrt{a_1^2+...+a_n^2}.\sqrt{b_1^2+...+b_n^2}$$
  åº”ç”¨:

  1. è¯æ˜ covariance inequalityï¼š$Var(Y) \geq \frac{Cov(Y,X)^2}{Var(X)}$,æœ‰$\braket{X,Y} := E(XY)$
     $$|Cov(Y,X)|^2 = |E((X-\mu)(Y-v))|^2 = |\braket{X-\mu,Y-v}|^2 \\ \leq \braket{X-\mu,X-\mu}\braket{Y-v,Y-v} = E((X-\mu)^2)E((Y-v)^2) = Var(X)Var(Y)$$

- **èµ«å°”å¾· (Holder) ä¸ç­‰å¼ - [HÃ¶lder's inequality](https://en.jinzhao.wiki/wiki/H%C3%B6lder%27s_inequality)**

- **ç´ç”Ÿ (Jensen) ä¸ç­‰å¼ - [Jensen's inequality](https://en.jinzhao.wiki/wiki/Jensen%27s_inequality)**
  $$f(tx_1 +(1-t)x_2) \leq tf(x_1) + (1-t)f(x_2), \text{f is convex function} \\ æ¨å¹¿ï¼šf(a_1x_1 +...+ a_nx_n) \leq a_1f(x_1) +...+ a_nf(x_n), a_1+...+a_n = 1 , a_i \geq 0 \\ or: f(\sum_{i=1}^n{a_ix_i}) \leq \sum_{i=1}^n{a_if(x_i)} , \sum_{i=1}^n{a_i} = 1, a_i \geq 0$$

  æ¦‚ç‡ä¸­ï¼šå¦‚æœ$X$æ˜¯éšæœºå˜é‡ï¼Œè€Œ$\varphi$æ˜¯å‡¸å‡½æ•°ï¼Œåˆ™:$\varphi(E[X]) \leq E[\varphi(X)]$,ä¸ç­‰å¼ä¸¤è¾¹çš„å·®ï¼Œ$ E[\varphi(X)] - \varphi(E[X]) $ç§°ä¸º Jensen gap(é—´éš™)ï¼›
  åº”ç”¨ï¼š

  1. EM ç®—æ³•ä¸­æœ‰ç”¨åˆ°(log å‡½æ•°æ˜¯å‡¹å‡½æ•°æ­£å¥½ä¸å‡¸å‡½æ•°ç›¸å);
  2. è¯æ˜ KL æ•£åº¦>=0;

- **é©¬å°”å¯å¤«ä¸ç­‰å¼ - [Markov's inequality](https://en.jinzhao.wiki/wiki/Markov%27s_inequality)**
  $$P(X \geq a) \leq \frac{E(X)}{a}$$
  å…¶ä¸­$X$ä¸ºéè´Ÿéšæœºå˜é‡ï¼Œ$\forall a>0$
  åº”ç”¨ï¼š

  1. ç”¨äºä¼°è®¡ä¸€ä¸ªæ¦‚ç‡çš„ä¸Šç•Œï¼Œæ¯”å¦‚å‡è®¾ä½ æ‰€åœ¨å…¬å¸çš„äººå‡å·¥èµ„æ˜¯ 1 ä¸‡ï¼Œé‚£ä¹ˆéšæœºé€‰ä¸€ä¸ªä½ å¸å‘˜å·¥ï¼Œå…¶å·¥èµ„è¶…è¿‡ 10 ä¸‡çš„æ¦‚ç‡ï¼Œä¸ä¼šè¶…è¿‡ 1/10ï¼›
  2. ç”¨äºå…¶ä»–æ¦‚ç‡ä¸ç­‰å¼çš„è¯æ˜ï¼Œæ¯”å¦‚éœå¤«ä¸ä¸ç­‰å¼ï¼›

- **åˆ‡æ¯”é›ªå¤« (Chebyshev) ä¸ç­‰å¼ - [Chebyshev's inequality](https://en.jinzhao.wiki/wiki/Chebyshev%27s_inequality)**
  $$P\{|X-\mu| \geq k\} \leq \frac{\sigma^2}{k^2}$$
  å…¶ä¸­$X$ä¸ºéšæœºå˜é‡ï¼Œ$\forall k>0$, $\mu$ä¸ºå‡å€¼ï¼Œ$\sigma^2$ä¸ºæ–¹å·®
  ï¼ˆè¯æ˜å¯ä»¥åˆ©ç”¨é©¬å°”å¯å¤«ä¸ç­‰å¼ï¼Œè§æ¦‚ç‡è®ºåŸºç¡€æ•™ç¨‹ 313 é¡µï¼‰

- **éœå¤«ä¸ä¸ç­‰å¼ - [Hoeffding's inequality](https://en.jinzhao.wiki/wiki/Hoeffding%27s_inequality)**
  åº”ç”¨ï¼š
  1. [Machine Learning Foundationsï¼ˆæœºå™¨å­¦ä¹ åŸºçŸ³ï¼‰- feasibility of learning,12,13,18 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/04_handout.pdf)
  2. ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼Œ26 é¡µï¼Œè¯æ˜æ³›åŒ–è¯¯å·®ä¸Šç•Œï¼ˆåœ¨[æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„å‡ ä¸ªæ¦‚ç‡ä¸ç­‰å¼åŠè¯æ˜](https://zhuanlan.zhihu.com/p/392348396)ä¸­ä¹Ÿæœ‰æåˆ°ï¼‰

å‚è€ƒï¼š[åˆç­‰æ•°å­¦å­¦ä¹ ç¬”è®°](https://github.com/zhcosin/elementary-math/blob/master/elementary-math-note.pdf)

### å‚è€ƒæ–‡çŒ®

[1-1] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[1-2] Bishop M. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). Springer,2006

[1-3] [Probabilistic Graphical Models: Principles and Techniques](https://djsaunde.github.io/read/books/pdfs/probabilistic%20graphical%20models.pdf) by Daphne Koller, Nir Friedman from The MIT Press

[1-4] [Deep Learning](https://raw.fastgit.org/Zhenye-Na/machine-learning-uiuc/master/docs/Deep%20Learning.pdf) (Ian Goodfellow, Yoshua Bengio, Aaron Courville)

[1-5] Tom M Michelle. [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html). McGraw-Hill Companies,Inc. 1997ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬ï¼šæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003ï¼‰

[1-6] [Bayesian Reasoning and Machine Learning by David Barber 2007â€“2020](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf) ,[other version](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/)

[1-7] [Reinforcement Learning:An Introduction (second edition 2020) by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020trimmed.pdf) ,[other version](http://incompleteideas.net/book/)

[1-8] å‘¨å¿—åï¼Œ[æœºå™¨å­¦ä¹ ](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)ï¼Œæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ ([æ‰‹æ¨ç¬”è®°](https://github.com/Sophia-11/Machine-Learning-Notes) ä»¥åŠ [å…¬å¼æ¨å¯¼è§£æ](https://github.com/datawhalechina/pumpkin-book))

[1-9] [Lecture Notes in MACHINE LEARNING](https://news.vidyaacademy.ac.in/wp-content/uploads/2018/10/NotesOnMachineLearningForBTech-1.pdf) Dr V N Krishnachandran

## ç¬¬ 2 ç«  æ„ŸçŸ¥æœº

åˆ¤åˆ«æ¨¡å‹

æ„ŸçŸ¥æœº[Perceptron](https://en.jinzhao.wiki/wiki/Perceptron)æ˜¯**ç¥ç»ç½‘ç»œ**å’Œ**æ”¯æŒå‘é‡æœº**çš„åŸºç¡€ã€‚æœ€æ—©åœ¨ 1957 å¹´ç”± Rosenblatt æå‡º$^{å‚è€ƒæ–‡çŒ®[2-1]}$ã€‚Novikoff$^{å‚è€ƒæ–‡çŒ®[2-2]}$ï¼ŒMinsky ä¸ Papert$^{å‚è€ƒæ–‡çŒ®[2-3]}$ç­‰äººå¯¹æ„ŸçŸ¥æœºè¿›è¡Œäº†ä¸€ç³»åˆ—ç†è®ºç ”ç©¶ã€‚æ„ŸçŸ¥æœºçš„æ‰©å±•å­¦ä¹ æ–¹æ³•åŒ…æ‹¬å£è¢‹ç®—æ³•(pocket algorithm)$^{å‚è€ƒæ–‡çŒ®[2-4]}$ã€è¡¨å†³æ„ŸçŸ¥æœº(voted perceptron)$^{å‚è€ƒæ–‡çŒ®[2-5]}$ã€å¸¦è¾¹ç¼˜æ„ŸçŸ¥æœº(perceptron with margin)$^{å‚è€ƒæ–‡çŒ®[2-6]}$ç­‰ã€‚
[Brief History of Machine Learning](https://erogol.com/brief-history-machine-learning/)

è¦æ±‚ï¼šæ•°æ®é›†çº¿æ€§å¯åˆ†(linearly separable data set)

- **æ¨¡å‹**ï¼š
  $$f(x) = sign(w.x + b)$$
  å…¶ä¸­$x,w \in \mathbb{R}^n ,b \in \mathbb{R}$,$w$å«ä½œæƒå€¼ï¼ˆweightï¼‰æˆ–æƒå€¼å‘é‡ï¼ˆweight vectorï¼‰ï¼Œ$b$å«ä½œåç½®ï¼ˆbiasï¼‰ï¼Œsign æ˜¯ç¬¦å·å‡½æ•°
  $$
  sign(x) = \begin{cases}
     +1 & x \geq 0 \\
     -1 & x<0
  \end{cases}
  $$

æ„ŸçŸ¥æœºæ˜¯ä¸€ç§çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼Œå±äºåˆ¤åˆ«æ¨¡å‹ã€‚æ„ŸçŸ¥æœºæ¨¡å‹çš„å‡è®¾ç©ºé—´æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©º é—´ä¸­çš„æ‰€æœ‰çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼ˆlinear classification modelï¼‰æˆ–çº¿æ€§åˆ†ç±»å™¨(linear classifier)ï¼Œå³ å‡½æ•°é›†åˆ$\{f|f(x)ï¼wÂ·x+b\}$

è¶…å¹³é¢ Sï¼š$w.x+b = 0$,å…¶ä¸­$w$æ˜¯ S çš„æ³•å‘é‡ï¼Œ$b$æ˜¯ S çš„æˆªè·ï¼Œè¶…å¹³é¢ S ç§°ä¸ºåˆ†ç¦»è¶…å¹³é¢ï¼ˆseparating hyperplaneï¼‰

- **ç­–ç•¥**ï¼š
  $$L(w,b) = -\sum_{x_i \in M}{y_i(w.x_i + b)}$$
  å…¶ä¸­$M$ä¸ºè¯¯åˆ†ç±»ç‚¹çš„é›†åˆã€‚è¯¯åˆ†ç±»æ•°æ®$M = \{ (x_i,y_i)|-y_i(w.x_i +b) > 0\}$

å‡½æ•°é—´éš”ï¼š$y(w.x + b)$
å‡ ä½•é—´éš”ï¼š$\frac{1}{||w||}|w.x + b|$ (åœ¨ä¸Šé¢çš„ loss function ä¸­æ²¡æœ‰è€ƒè™‘$\frac{1}{||w||}$)

- **ç®—æ³•**ï¼š
  $$\min_{w,b} L(w,b) = -\sum_{x_i \in M}{y_i(w.x_i + b)}$$
  ä½¿ç”¨**éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆstochastic gradientï¼‰**:

1. åˆå§‹åŒ–å‚æ•°(éšæœºæ³•)ï¼š$w_0,b_0$
2. é€‰å–æ•°æ®$(x_i,y_i)$
3. å¦‚æœ$(x_i,y_i)$æ˜¯è¯¯åˆ†ç±»ç‚¹ï¼Œä¹Ÿå°±æ˜¯$y_i(w.x_i + b) \leq 0$ï¼Œåˆ™å¯¹$w,b$è¿›è¡Œæ›´æ–°
   $$åœ¨(x_i,y_i)ç‚¹å¤„æ¢¯åº¦ä¸ºï¼š\\ \nabla_wL(w,b) = -y_ix_i \\ \nabla_bL(w,b) = -y_i\\ æ›´æ–°wï¼šw_{k+1} \gets w_{k}+\eta y_ix_i \\ æ›´æ–°bï¼šb_{k+1} \gets b_{k}+\eta y_i \\å…¶ä¸­å­¦ä¹ ç‡\eta \in (0,1]$$
4. å¾ªç¯ 2-3ï¼Œç›´åˆ°è®­ç»ƒé›†ä¸­æ²¡æœ‰è¯¯åˆ†ç±»ç‚¹ã€‚

- ä¸Šè¿°**ç®—æ³•çš„æ”¶æ•›æ€§**ï¼š

Novikoff å®šç†ï¼š
è®¾è®­ç»ƒé›†$T = \{(x_1,y_1),...,(x_N,y_N)\}$æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œ

1. è®¾å®Œç¾è¶…å¹³é¢$\hat{w}_{opt}.\hat{x} = 0 , ||\hat{w}_{opt}||=1$ å°†è®­ç»ƒé›†å®Œå…¨æ­£ç¡®åˆ†å¼€ï¼ˆç®€åŒ–èµ·è§ $\hat{w}_{opt}.\hat{x} = w_{opt}.x +b$ï¼‰ï¼Œå­˜åœ¨$\gamma >0$ ,å¯¹æ‰€æœ‰ç‚¹æœ‰$y_i(\hat{w}_{opt}.\hat{x_i}) \geq \gamma$ï¼›

2. ä»¤$R = \max_{1\leq i\leq N}||\hat{x_i}||$,åˆ™ç®—æ³•ä¼šåœ¨æœ‰é™æ­¥ k æ»¡è¶³ä¸ç­‰å¼$k \leq (\frac{R}{\gamma})^2$

è¯æ˜(æ³¨æ„ï¼šå¸¦ hat çš„è¡¨ç¤ºæ‰©å……å‘é‡)ï¼š

1. å› ä¸ºæ•°æ®çº¿æ€§å¯åˆ†ï¼Œå¯¹äºæ‰€æœ‰ç‚¹$y_i(\hat{w}_{opt}.\hat{x_i}) > 0$,æ‰€ä»¥å­˜åœ¨
   $$\gamma = \min_i{y_i(\hat{w}_{opt}.\hat{x_i})} \leq {y_i(\hat{w}_{opt}.\hat{x_i})} \label{2-1}\tag{2-1}$$
   æ‰€ä»¥è¿™é‡Œçš„$\gamma$ä»£è¡¨äº†æ‰€æœ‰ç‚¹ç¦»å®Œç¾è¶…å¹³é¢çš„æœ€å°è·ç¦»ï¼›

2. ä¸ºäº†æ–¹ä¾¿è®¡ç®— è®¾ æ‰©å……å‘é‡$\hat{w} = (w^T,b)^T$ï¼Œ æœ‰
   $$\hat{w}_{k} = \hat{w}_{k-1}+\eta y_i\hat{x_i} \label{2-2}\tag{2-2}$$

3. æ¨å¯¼ä¸ç­‰å¼
   $$\hat{w}_{k}.\hat{w}_{opt} \geq k\eta\gamma \label{2-3}\tag{2-3}$$

ç”±$\eqref{2-1}$å’Œ$\eqref{2-2}$
$$\hat{w}_{k}.\hat{w}_{opt} = \hat{w}_{k-1}.\hat{w}_{opt} + \eta{y_i}\hat{w}_{opt}.\hat{x_i} \\ \geq \hat{w}_{k-1}.\hat{w}_{opt} + \eta\gamma \\ \geq \hat{w}_{k-2}.\hat{w}_{opt} + 2\eta\gamma \\ \geq k\eta\gamma$$

4. æ¨å¯¼ä¸ç­‰å¼
   $$||\hat{w}_{k}||^2 \leq k\eta^2R^2 \label{2-4}\tag{2-4}$$
   ç”±$\eqref{2-2}$
   $$||\hat{w}_{k}||^2=||\hat{w}_{k-1}+\eta y_i\hat{x_i}||^2 = ||\hat{w}_{k-1}||^2 + 2\eta{y_i}\hat{w}_{k-1}.\hat{x}_{i} + \eta^2||\hat{x}_{i}||^2$$
   å‡è®¾ k æ¬¡å®Œå…¨åˆ†å¯¹ï¼Œé‚£ä¹ˆ k-1 æ¬¡æœ‰è¯¯åˆ†ç±»ç‚¹ï¼Œåˆ™${y_i}\hat{w}_{k-1}.\hat{x}_{i} \leq 0$
   æ‰€ä»¥
   $$||\hat{w}_{k}||^2 =||\hat{w}_{k-1}||^2 + 2\eta{y_i}\hat{w}_{k-1}.\hat{x}_{i} + \eta^2||\hat{x}_{i}||^2 \\ \leq ||\hat{w}_{k-1}||^2 +  \eta^2||\hat{x}_{i}||^2 \\ \leq ||\hat{w}_{k-1}||^2 +  \eta^2R^2  \\ \leq ||\hat{w}_{k-2}||^2 +  2\eta^2R^2 \leq ... \\ \leq k\eta^2R^2$$

5. ç”±$\eqref{2-3}$å’Œ$\eqref{2-4}$

$$k\eta\gamma \leq \underbrace{\hat{w}_{k}.\hat{w}_{opt} \leq ||\hat{w}_{k}||.\underbrace{||\hat{w}_{opt}||}_{=1} }_{\text{æŸ¯è¥¿-æ–½ç“¦èŒ¨ (Cauchyâ€“Schwarz) ä¸ç­‰å¼}} \leq \sqrt{k} \eta R \\ \; \\ \Rightarrow k^2\gamma^2 \leq kR^2 \\ \Rightarrow k \leq (\frac{R}{\gamma})^2$$

ä¹Ÿå°±æ˜¯è¯´ k æ˜¯æœ‰ä¸Šç•Œçš„ã€‚

> ä¹¦ä¸­è¿˜ä»‹ç»äº†åŸå½¢å¼çš„**å¯¹å¶å½¢å¼**,ä¹Ÿå°±æ˜¯ç­‰ä»·å½¢å¼ï¼ˆSVM ä¸­ 7.2.2 èŠ‚ 127 é¡µä¹Ÿæ˜¯ç­‰ä»·çš„æ„æ€ï¼ŒåŒºåˆ«äºæ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼‰ï¼Œè¿™ä¸¤ä¸ªåœ°æ–¹çš„ç­‰ä»·éƒ½æ˜¯ç»è¿‡åŸºæœ¬æ¨å¯¼ï¼Œæ±‚å‡º w å‚æ•°ï¼Œç„¶åå¯¹åŸé—®é¢˜è¿›è¡Œäº†æ›¿æ¢ã€‚

### å‚è€ƒæ–‡çŒ®

[2-1] Rosenblatt, F. (1958). [The perceptron: A probabilistic model for information storage and organization in the brain](http://homepages.math.uic.edu/~lreyzin/papers/rosenblatt58.pdf). Psychological Review, 65(6), 386â€“408.

[2-2] Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.

[2-3] Minsky M L and Papert S A 1969 Perceptrons (Cambridge, MA: MIT Press)

[2-4] Gallant, S. I. (1990). Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179-191.

[2-5] Freund, Y. and Schapire, R. E. 1998. Large margin classification using the perceptron algorithm. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT' 98). ACM Press.

[2-6] Li YY,Zaragoza H,Herbrich R,Shawe-Taylor J,Kandola J. The Perceptron algorithmwith uneven margins. In: Proceedings of the 19th International Conference on MachineLearning. 2002,379â€“386

[2-7] [Widrow, B.](https://en.jinzhao.wiki/wiki/Bernard_Widrow), Lehr, M.A., "[30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation,](http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/widrow.pdf)" Proc. IEEE, vol 78, no 9, pp. 1415-1442, (1990)ã€‚

[2-8] Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines and OtherKernelbased Learning Methods. Cambridge University Press,2000

## ç¬¬ 3 ç«  k è¿‘é‚»æ³•

åˆ¤åˆ«æ¨¡å‹

k è¿‘é‚»æ³•ï¼ˆ[k-nearest neighborï¼Œk-NN](https://en.jinzhao.wiki/wiki/K-nearest_neighbors_algorithm)ï¼‰1968 å¹´ç”± Cover å’Œ Hart æå‡ºï¼Œæ˜¯ä¸€ç§åŸºæœ¬åˆ†ç±»ä¸å›å½’æ–¹æ³•ã€‚æœ¬ä¹¦åªè®¨è®ºåˆ†ç±»é—®é¢˜ä¸­çš„ k è¿‘é‚»æ³•ã€‚
k å€¼çš„é€‰æ‹©ã€è·ç¦»åº¦é‡åŠåˆ†ç±»å†³ç­–è§„åˆ™æ˜¯ k è¿‘é‚»æ³•çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ ã€‚
æœ€åè®²è¿° k è¿‘é‚»æ³•çš„ä¸€ä¸ªå®ç°æ–¹æ³•â€”â€”kd æ ‘ï¼Œä»‹ç»æ„é€  kd æ ‘å’Œæœç´¢ kd æ ‘çš„ç®—æ³•

**k è¿‘é‚»æ³•çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ **ï¼š
k å€¼çš„é€‰æ‹©ï¼šè¶…å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯æ³•æ¥é€‰å–æœ€ä¼˜ k å€¼
è·ç¦»åº¦é‡ï¼š$L_2$è·ç¦»/æ¬§æ°è·ç¦»ï¼Œ$L_p$è·ç¦»/Minkowski è·ç¦»
åˆ†ç±»å†³ç­–è§„åˆ™ï¼šå¤šæ•°è¡¨å†³ï¼ˆ0-1 æŸå¤±ä¹Ÿå°±æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼‰

- **æ¨¡å‹**ï¼š
  k è¿‘é‚»æ³•æ²¡æœ‰æ˜¾å¼çš„å­¦ä¹ è¿‡ç¨‹ï¼ˆä¸å­¦ä¹ ä¹Ÿèƒ½é¢„æµ‹ï¼‰ï¼Œå®ƒæœ¬èº«å¹¶æ²¡æœ‰å¯¹æ•°æ®è¿›è¡Œç†è®ºå»ºæ¨¡çš„è¿‡ç¨‹ï¼Œè€Œæ˜¯åˆ©ç”¨è®­ç»ƒæ•°æ®å¯¹ç‰¹å¾å‘é‡ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶å°†å…¶åˆ’åˆ†çš„ç»“æœä½œä¸ºå…¶æœ€ç»ˆçš„ç®—æ³•æ¨¡å‹ã€‚è¿™å°±å¥½æ¯”ï¼Œåœ¨ç°å®ä¸–ç•Œçš„ç»´åº¦ä¸­ï¼Œç»å¸¸æ¸¸èµ°äºç”·å•æ‰€çš„æˆ‘ä»¬å½’ä¸ºç”·æ€§ï¼Œè€Œç»å¸¸åœ¨å¥³å•æ‰€å‡ºæ²¡çš„äººæˆ‘ä»¬å½’ä¸ºå¥³æ€§æˆ–è€…æ˜¯å˜æ€ã€‚

- **ç­–ç•¥**ï¼š
  $$y = \argmin_{c_j} \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1- \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j) $$
  æœ€å¤§åŒ–ç±»åˆ«å±äº$c_j$ç±»çš„æ¦‚ç‡$\frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j)$
  æœ€å°åŒ–è¯¯åˆ†ç±»ç‡$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j)$
  $N_k(x)$è¡¨ç¤ºæ¶µç›– k ä¸ªç‚¹çš„ x çš„é‚»åŸŸ
- **ç®—æ³•**ï¼š
  ç›´æ¥è®¡ç®—ï¼ˆçº¿æ€§æ‰«æ linear scanï¼‰,å½“è®­ç»ƒé›†å¾ˆå¤§æ—¶ï¼Œè®¡ç®—å¾ˆè€—æ—¶ï¼ˆæ¯æ¬¡éƒ½è¦è®¡ç®—æ‰€æœ‰è·ç¦»ï¼Œç„¶åæ‰¾åˆ° k ä¸ªæœ€è¿‘è·ç¦»çš„ç‚¹ï¼‰ï¼Œå› ä¸ºæ²¡æœ‰å­¦ä¹ ã€‚
  ä¸ºäº†æé«˜ k è¿‘é‚»æœç´¢çš„æ•ˆç‡ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç‰¹æ®Šçš„ç»“æ„å­˜å‚¨è®­ç»ƒæ•°æ®ï¼Œä»¥å‡å°‘è®¡ç®—è·ç¦»çš„æ¬¡æ•°ã€‚
  å…·ä½“æ–¹æ³•å¾ˆå¤šï¼Œå¦‚ï¼š[kd_tree](https://en.jinzhao.wiki/wiki/K-d_tree)ï¼Œ[ball_tree](https://arxiv.org/pdf/1511.00628.pdf)ï¼Œbrute(è›®åŠ›å®ç°,ä¸ç®—ä¼˜åŒ–ï¼Œåªæ˜¯æŠŠ sklearn ä¸­çš„å‚æ•°æ‹¿è¿‡æ¥)ï¼Œä»¥åŠå…¶å®ƒ[æ ‘ç»“æ„](<https://en.jinzhao.wiki/wiki/Category:Trees_(data_structures)>)
  ä¸ºäº†æ”¹è¿› KDtree çš„äºŒå‰æ ‘æ ‘å½¢ç»“æ„ï¼Œå¹¶ä¸”æ²¿ç€ç¬›å¡å°”åæ ‡è¿›è¡Œåˆ’åˆ†çš„ä½æ•ˆç‡ï¼Œball tree å°†åœ¨ä¸€ç³»åˆ—åµŒå¥—çš„è¶…çƒä½“ä¸Šåˆ†å‰²æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼šä½¿ç”¨è¶…çƒé¢è€Œä¸æ˜¯è¶…çŸ©å½¢åˆ’åˆ†åŒºåŸŸã€‚è™½ç„¶åœ¨æ„å»ºæ•°æ®ç»“æ„çš„èŠ±è´¹ä¸Šå¤§è¿‡äº KDtreeï¼Œä½†æ˜¯åœ¨é«˜ç»´ç”šè‡³å¾ˆé«˜ç»´çš„æ•°æ®ä¸Šéƒ½è¡¨ç°çš„å¾ˆé«˜æ•ˆã€‚

  ä¸‹é¢ä»‹ç»å…¶ä¸­çš„ kd æ ‘ï¼ˆkd tree æ˜¯ä¸€ä¸ªäºŒå‰æ ‘ï¼‰æ–¹æ³•ï¼ˆkd æ ‘æ˜¯å­˜å‚¨ k ç»´ç©ºé—´æ•°æ®çš„æ ‘ç»“æ„ï¼Œè¿™é‡Œçš„ k ä¸ k è¿‘é‚»æ³•çš„ k æ„ä¹‰ä¸åŒï¼‰ã€‚
  æ•°æ®é›†$T = \{x_1,...,x_N\}$ï¼Œå…¶ä¸­$x_i$æ˜¯ k ç»´å‘é‡$x_i = (x_i^{(1)},...,x_i^{(k)})^T$

  - **æ„é€  kd æ ‘**ï¼š

  ```
  function kdtree (list of points pointList, int depth)
  {
      // Select axis based on depth so that axis cycles through all valid values
      var int axis := depth mod k;

      // Sort point list and choose median as pivot element
      select median by axis from pointList;

      // Create node and construct subtree
      node.location := median;
      node.leftChild := kdtree(points in pointList before median, depth+1);
      node.rightChild := kdtree(points in pointList after median, depth+1);
      return node;
  }
  ```

  1. æ ¹æ®ç¬¬(depth mod k)ç»´æŸ¥æ‰¾ä¸­ä½æ•°ï¼ˆä¸­ä½æ•°æ‰€åœ¨çš„ç‚¹ä½œä¸ºèŠ‚ç‚¹ï¼Œç¬¬ä¸€æ¬¡å°±æ˜¯ root èŠ‚ç‚¹ï¼‰ï¼Œå°†æ•°æ®åˆ’åˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸï¼Œå°äºä¸­ä½æ•°çš„åˆ’åˆ†åœ¨å·¦è¾¹ï¼Œå¤§äºä¸­ä½æ•°çš„åˆ’åˆ†åœ¨å³è¾¹
  2. é‡å¤ 1ï¼Œdepth++

  - **æœç´¢ kd æ ‘**ï¼š

  1. åœ¨ kd æ ‘ä¸­æ‰¾å‡ºåŒ…å«ç›®æ ‡ç‚¹ x çš„å¶ç»“ç‚¹ï¼šä»æ ¹ç»“ç‚¹å‡ºå‘ï¼Œé€’å½’åœ°å‘ä¸‹è®¿é—® kd æ ‘ã€‚è‹¥ç›®æ ‡ç‚¹ x å½“å‰ç»´çš„åæ ‡å°äºåˆ‡åˆ†ç‚¹çš„åæ ‡ï¼Œåˆ™ç§»åŠ¨åˆ°å·¦å­ç»“ç‚¹ï¼Œå¦åˆ™ç§»åŠ¨åˆ°å³å­ç»“ç‚¹ã€‚ç›´åˆ°å­ç»“ç‚¹ä¸ºå¶ç»“ç‚¹ä¸ºæ­¢ã€‚
  2. ä»¥æ­¤å¶ç»“ç‚¹ä¸ºâ€œå½“å‰æœ€è¿‘ç‚¹â€ã€‚
  3. é€’å½’åœ°å‘ä¸Šå›é€€ï¼Œåœ¨æ¯ä¸ªç»“ç‚¹è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
     a. å¦‚æœè¯¥ç»“ç‚¹ä¿å­˜çš„å®ä¾‹ç‚¹æ¯”å½“å‰æœ€è¿‘ç‚¹è·ç¦»ç›®æ ‡ç‚¹æ›´è¿‘ï¼Œåˆ™ä»¥è¯¥å®ä¾‹ç‚¹ä¸ºâ€œå½“å‰æœ€è¿‘ç‚¹â€ã€‚
     b. å½“å‰æœ€è¿‘ç‚¹ä¸€å®šå­˜åœ¨äºè¯¥ç»“ç‚¹ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸã€‚æ£€æŸ¥è¯¥å­ç»“ç‚¹çš„çˆ¶ç»“ç‚¹çš„å¦ä¸€å­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸæ˜¯å¦æœ‰æ›´è¿‘çš„ç‚¹ã€‚å…·ä½“åœ°ï¼Œæ£€æŸ¥å¦ä¸€å­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸæ˜¯å¦ä¸ä»¥ç›®æ ‡ç‚¹ä¸ºçƒå¿ƒã€ä»¥ç›®æ ‡ç‚¹ä¸â€œå½“å‰æœ€è¿‘ç‚¹â€é—´çš„è·ç¦»ä¸ºåŠå¾„çš„è¶…çƒä½“ç›¸äº¤ã€‚
     å¦‚æœç›¸äº¤ï¼Œå¯èƒ½åœ¨å¦ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸå†…å­˜åœ¨è·ç›®æ ‡ç‚¹æ›´è¿‘çš„ç‚¹ï¼Œç§»åŠ¨åˆ°å¦ä¸€ä¸ªå­ç»“ç‚¹ã€‚æ¥ç€ï¼Œé€’å½’åœ°è¿›è¡Œæœ€è¿‘é‚»æœç´¢ï¼›
     å¦‚æœä¸ç›¸äº¤ï¼Œå‘ä¸Šå›é€€ã€‚
  4. å½“å›é€€åˆ°æ ¹ç»“ç‚¹æ—¶ï¼Œæœç´¢ç»“æŸã€‚æœ€åçš„â€œå½“å‰æœ€è¿‘ç‚¹â€å³ä¸º x çš„æœ€è¿‘é‚»ç‚¹ã€‚
     å¦‚æœå®ä¾‹ç‚¹æ˜¯éšæœºåˆ†å¸ƒçš„ï¼Œkd æ ‘æœç´¢çš„å¹³å‡è®¡ç®—å¤æ‚åº¦æ˜¯ O(logN)ï¼Œè¿™é‡Œ N æ˜¯è®­ç»ƒå®ä¾‹æ•°ã€‚kd æ ‘æ›´é€‚ç”¨äºè®­ç»ƒå®ä¾‹æ•°è¿œå¤§äºç©ºé—´ç»´æ•°æ—¶çš„ k è¿‘é‚»æœç´¢ã€‚å½“ç©ºé—´ç»´æ•°æ¥è¿‘è®­ç»ƒå®ä¾‹æ•°æ—¶ï¼Œå®ƒçš„æ•ˆç‡ä¼šè¿…é€Ÿä¸‹é™ï¼Œå‡ ä¹æ¥è¿‘çº¿æ€§æ‰«æã€‚

  | ç®—æ³• | å¹³å‡        | æœ€å·®çš„æƒ…å†µ |
  | ---- | ----------- | ---------- |
  | ç©ºé—´ | $O(n)$      | $O(n)$     |
  | æœç´¢ | $O(\log n)$ | $O(n)$     |
  | æ’å…¥ | $O(\log n)$ | $O(n)$     |
  | åˆ é™¤ | $O(\log n)$ | $O(n)$     |

### é™„åŠ çŸ¥è¯†

#### è·ç¦»åº¦é‡

[Distance](https://en.jinzhao.wiki/wiki/Category:Distance)

[sklearn.neighbors.DistanceMetric](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)

[Distance computations(scipy.spatial.distance)](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)

[24 ç§è·ç¦»åº¦é‡å°ç»“](https://blog.csdn.net/weixin_43840403/article/details/89075759)

> å…ˆäº†è§£åº¦é‡ç©ºé—´å’Œèµ‹èŒƒç©ºé—´

å®å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Euclidean(æ¬§å‡ é‡Œå¾—è·ç¦»ä¹Ÿç§°æ¬§å¼è·ç¦») ${||u-v||}_2$ or $\sqrt{\sum_i{(u_i - v_i)^2}}$
- SEuclidean(æ ‡å‡†åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»)
- SqEuclidean(å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»)
- Mahalanobis(é©¬æ°è·ç¦») $\sqrt{ (u-v) \Sigma^{-1} (u-v)^T }$
- Manhattan/cityblock(åŸå¸‚è¡—åŒºï¼ˆæ›¼å“ˆé¡¿ï¼‰è·ç¦») $\sum_i{|u_i-v_i|}$
- Chebyshev(åˆ‡æ¯”é›ªå¤«è·ç¦») $L_\infty$åº¦é‡ $\max_i{|u_i-v_i|}$
- Minkowski(é—µå¯å¤«æ–¯åŸºè·ç¦») æ¬§å¼è·ç¦»çš„æ¨å¹¿ï¼Œp=1 æ—¶ç­‰ä»·äºæ›¼å“ˆé¡¿è·ç¦»ï¼Œp=2 æ—¶ç­‰ä»·äºæ¬§æ°è·ç¦»ï¼Œp=âˆ æ—¶ç­‰ä»·äºåˆ‡æ¯”é›ªå¤«è·ç¦»;$\sqrt[p]{\sum_i{(u_i - v_i)^p}}$
- WMinkowski(åŠ æƒ Minkowski)

å®å€¼å‘é‡ç©ºé—´çš„åº¦é‡(scipy)ï¼š

- Correlation(çš®å°”é€Šç›¸å…³ç³»æ•°(Pearson Correlation))
- Cosine(ä½™å¼¦è·ç¦»)
- JensenShannon(JS æ•£åº¦ä¹Ÿç§° JS è·ç¦»ï¼Œæ˜¯ KL æ•£åº¦çš„ä¸€ç§å˜å½¢)

æ•´æ•°å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Hamming(æ±‰æ˜è·ç¦»)
- Canberra(å ªåŸ¹æ‹‰è·ç¦»)
- BrayCurtis(å¸ƒé›·æŸ¯è’‚æ–¯è·ç¦»)

å¸ƒå°”å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Jaccard(Jaccard-Needham ä¸ç›¸ä¼¼åº¦)
- Matching(Hamming åŒä¹‰è¯)
- Dice(Dice ç³»æ•°)
- Kulsinski(Kulsinski ç›¸å¼‚åº¦)
- RogersTanimoto(Rogers-Tanimoto ç›¸å¼‚åº¦)
- RussellRao(Russell-Rao ç›¸å¼‚æ€§)
- SokalMichener(Sokal-Michener ç›¸å¼‚æ€§)
- SokalSneath(Sokal-Sneath ç›¸å¼‚æ€§)
- Yuleï¼ˆscipy ä¸­çš„ Yule ç›¸å¼‚åº¦ï¼‰

ç»çº¬åº¦è·ç¦»ï¼š

- Haversine(sklearn ä¸­çš„åŠæ­£çŸ¢è·ç¦»)

å…¶å®ƒï¼š

- ç›¸å¯¹ç†µåˆç§° KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰[scipy.special.kl_div](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.kl_div.html)
- äº¤å‰ç†µï¼ˆCross Entropyï¼‰ [scipy.stats.entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)

### å‚è€ƒæ–‡çŒ®

[3-1] Cover T,Hart P. Nearest neighbor pattern classification. IEEE Transactions onInformation Theory,1967

[3-2] Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[3-3] Friedman J. Flexible metric nearest neighbor classification. Technical Report,1994

[3-4] Weinberger KQ,Blitzer J,Saul LK. Distance metric learning for large margin nearestneighbor classification. In: Proceedings of the NIPS. 2005

[3-5] Samet H. The Design and Analysis of Spatial Data Structures. Reading,MA: Addison-Wesley,1990

## ç¬¬ 4 ç«  æœ´ç´ è´å¶æ–¯æ³•

æœ´ç´ è´å¶æ–¯ï¼ˆ[NaÃ¯ve Bayes](https://en.jinzhao.wiki/wiki/Naive_Bayes_classifier)ï¼‰æ³•æ˜¯åŸºäº**è´å¶æ–¯å®šç†**ä¸**ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼ˆNaive å¤©çœŸçš„ï¼‰çš„åˆ†ç±»æ–¹æ³•ã€‚
å¯¹äºç»™å®šçš„è®­ç»ƒæ•°æ®é›†ï¼Œé¦–å…ˆåŸºäºç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å­¦ä¹ è¾“å…¥/è¾“å‡ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼›ç„¶ååŸºäºæ­¤æ¨¡å‹ï¼Œå¯¹ç»™å®šçš„è¾“å…¥ xï¼Œåˆ©ç”¨è´å¶æ–¯å®šç†æ±‚å‡ºåéªŒæ¦‚ç‡æœ€å¤§çš„è¾“å‡º yã€‚
æœ´ç´ è´å¶æ–¯æ³•å®ç°ç®€å•ï¼Œå­¦ä¹ ä¸é¢„æµ‹çš„æ•ˆç‡éƒ½å¾ˆé«˜ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•ã€‚å¹¶ä¸”æ”¯æŒ online learningï¼ˆæœ‰ partial_fit æ–¹æ³•ï¼‰ã€‚

æœ´ç´ è´å¶æ–¯æ³•æ˜¯å…¸å‹çš„**ç”Ÿæˆå­¦ä¹ æ–¹æ³•**ã€‚ç”Ÿæˆæ–¹æ³•ç”±è®­ç»ƒæ•°æ®å­¦ä¹ è”åˆæ¦‚ç‡åˆ†å¸ƒ P(X,Y)ï¼Œç„¶åæ±‚å¾—åéªŒæ¦‚ç‡åˆ†å¸ƒ P(Y|X)ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®å­¦ä¹  P(X|Y)å’Œ P(Y)çš„ä¼°è®¡ï¼Œå¾—åˆ°è”åˆæ¦‚ç‡åˆ†å¸ƒï¼šP(X,Y)ï¼ P(Y)P(X|Y) ï¼›æ¦‚ç‡ä¼°è®¡æ–¹æ³•å¯ä»¥æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–è´å¶æ–¯ä¼°è®¡ã€‚

**[è´å¶æ–¯å®šç†(Bayes' theorem)](https://en.jinzhao.wiki/wiki/Bayes%27_theorem)**ï¼š
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

- $P(A|B)$ æ˜¯æ¡ä»¶æ¦‚ç‡[conditional probability](https://en.jinzhao.wiki/wiki/Conditional_probability)ï¼šæ˜¯å·²çŸ¥ B å‘ç”Ÿåï¼ŒA çš„æ¦‚ç‡ï¼Œä¹Ÿè¢«ç§°ä¸º å·²çŸ¥ B çš„æƒ…å†µä¸‹ A çš„åéªŒæ¦‚ç‡[posterior probability](https://en.jinzhao.wiki/wiki/Posterior_probability)

- $P(B|A)$ ä¹Ÿæ˜¯ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ï¼šå·²çŸ¥ A æ—¶ï¼ŒB çš„ä¼¼ç„¶æ€§/å¯èƒ½æ€§([likelihood](https://en.jinzhao.wiki/wiki/Likelihood_function)), ä¸ºä»€ä¹ˆå« likelihoodï¼Ÿå› ä¸º$P(B|A) = L(A|B) ^{å‚è§ï¼šé™„åŠ çŸ¥è¯†-å‚æ•°ä¼°è®¡-æå¤§ä¼¼ç„¶ä¼°è®¡}$

- $P(A)$ å« A çš„è¾¹é™…æ¦‚ç‡([marginal probability](https://en.jinzhao.wiki/wiki/Marginal_probability))æˆ–å…ˆéªŒæ¦‚ç‡([prior probability](https://en.jinzhao.wiki/wiki/Prior_probability))

- $P(B)$ å« B çš„è¾¹é™…æ¦‚ç‡æˆ–å…ˆéªŒæ¦‚ç‡ï¼Œä¹Ÿç§°ä¸º evidence è¯æ®

**[ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾](https://en.jinzhao.wiki/wiki/Conditional_independence)**ï¼š
**æ¡ä»¶ç‹¬ç«‹**
$$(A\perp B|C) \iff P(A|B,C) = P(A|C) \\ (A\perp B|C) \iff P(A,B|C) = P(A|C)P(B|C)$$

ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å°±æ˜¯å·²çŸ¥ y çš„æƒ…å†µä¸‹ï¼Œx ä¸­æ¯ä¸ªç‰¹å¾ç›¸äº’ç‹¬ç«‹ã€‚

æ•°æ®é›†$T = \{(x_1,y_1),...,(x_N,y_N)\}$ï¼Œ$K$ä¸ºç±»åˆ«ä¸ªæ•°,å…¶ä¸­$x_i$æ˜¯ n ç»´å‘é‡$x_i = (x_i^{(1)},...,x_i^{(n)})^T$

- **æ¨¡å‹**ï¼š
  $$\underbrace{P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}}_{\text{è´å¶æ–¯å®šç†}} \varpropto P(Y=c_k) \underbrace{\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}}_{\text{ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾}}$$

  å…¶ä¸­
  $$P(X=x) = \sum_k{P(X=x|Y=c_k)P(Y=c_k)} = \sum_k{P(Y=c_k) \prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}}$$
  P(X)æ˜¯ evidence(å·²çŸ¥çš„,è§‚å¯Ÿå˜é‡)ï¼Œå¯çœ‹åšå¸¸æ•°(ä¹Ÿå¯ä»¥è¯´å¯¹$c_k$æ¥è¯´åˆ†æ¯ P(X)æ˜¯ç›¸åŒçš„ï¼Œæ±‚æœ€å¤§æœ€å°æ—¶å¯ä»¥å»æ‰)ï¼Œåˆ™ï¼š
  $$P(Y=c_k|X=x) \varpropto {P(X=x|Y=c_k)P(Y=c_k)}$$

- **ç­–ç•¥**ï¼š
  åéªŒæœ€å¤§åŒ–ï¼ˆç­‰ä»· 0-1 æŸå¤±ï¼‰ï¼š
  $$y = \argmax_{c_k} P(Y=c_k|X=x)= \argmax_{c_k}P(Y=c_k)\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}$$
  åŸç†ï¼ˆè¯æ˜ï¼‰ï¼šä½¿ç”¨ 0-1 æŸå¤±
  $$\argmin_{y}\sum_{k=1}^K P(y \neq c_k|X=x) \\= \argmin_{y}(1- P(y = c_k|X=x)) \\= \argmax_y P(y = c_k|X=x)$$

- **ç®—æ³•**ï¼šå‚æ•°ä¼°è®¡
  æˆ‘ä»¬éœ€è¦çŸ¥é“$P(Y=c_k)$ä»¥åŠ$\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}$
  **æå¤§ä¼¼ç„¶ä¼°è®¡**ï¼š

  1. å…ˆéªŒ$P(Y=c_k)$çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ï¼š
     $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k)}{N}$$
  2. ç¬¬$j$ä¸ªç‰¹å¾$x^{(j)}$çš„å–å€¼é›†åˆæ˜¯$\{a_{j1},...,a_{jS_j}\}$,([æ³¨æ„è¿™é‡Œç”¨çš„éƒ½æ˜¯é¢‘ç‡è®¡æ•°ï¼Œä¹Ÿå°±æ˜¯ç¦»æ•£ç‰¹å¾ï¼Œå¦‚æœæ˜¯è¿ç»­ç‰¹å¾å˜é‡ï¼Œåˆ™ä½¿ç”¨é«˜æ–¯æœ´ç´ è´å¶æ–¯](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes))
     æ¡ä»¶æ¦‚ç‡(likelihood)$P(X^{(j)}=x^{(j)}|Y=c_k)$çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ï¼š
     $$P(X^{(j)} = a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} =a_{jl} , y_i = c_k)}{\sum_{i=1}^N I(y_i=c_k)}$$
     å…¶ä¸­$j = 1,2,...N; \quad l=1,2...S_j ;\quad k=1,2,...K$ï¼Œ$x_i^{(j)}$æ˜¯ç¬¬$i$ä¸ªæ ·æœ¬çš„ç¬¬$j$ä¸ªç‰¹å¾ï¼›$a_{jl}$æ˜¯ç¬¬$j$ä¸ªç‰¹å¾å¯èƒ½å–å€¼çš„ç¬¬$l$ä¸ªå€¼ã€‚

  **è´å¶æ–¯ä¼°è®¡**ï¼ˆsmoothed version of maximum likelihoodï¼‰ï¼š
  æå¤§ä¼¼ç„¶ä¼°è®¡æœ‰ä¸€ä¸ªé—®é¢˜å°±æ˜¯æ¡ä»¶æ¦‚ç‡$P(X^{(j)}=x^{(j)}|Y=c_k)$æœ‰ä¸€ä¸ªä¸º 0ï¼Œå°±ä¼šå‡ºç°æ— æ³•ä¼°è®¡çš„æƒ…å†µ(å°±æ˜¯æ¦‚ç‡ä¸º 0)ï¼Œä¹Ÿå°±æ˜¯ç»™å®šè¦é¢„æµ‹çš„ç‰¹å¾å‘é‡çš„ä¸€ä¸ªç‰¹å¾å‡ºç°äº†æ–°çš„ç±»åˆ«ï¼ˆå¦‚ï¼šç¬¬$j$ä¸ªç‰¹å¾$x^{(j)} = a_{jS_j+1}$ï¼‰ï¼Œé‚£ä¹ˆå°±ä¼šå¯¼è‡´æ¦‚ç‡ä¸º 0ï¼Œè¿™æ˜¯è¦ä¹ˆå¢åŠ æ ·æœ¬æ•°é‡ï¼Œè¦ä¹ˆä½¿ç”¨è´å¶æ–¯ä¼°è®¡

  > æ³¨æ„ï¼šæœ´ç´ è´å¶æ–¯æ³•ä¸è´å¶æ–¯ä¼°è®¡ï¼ˆBayesian estimationï¼‰æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚

  1. å…ˆéªŒ$P(Y=c_k)$çš„è´å¶æ–¯ä¼°è®¡æ˜¯ï¼š
     $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$$
  2. æ¡ä»¶æ¦‚ç‡(likelihood)çš„è´å¶æ–¯ä¼°è®¡æ˜¯ï¼š
     $$P(X^{(j)} = a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl} , y_i = c_k) + \lambda}{\sum_{i=1}^N I(y_i=c_k) + S_j\lambda}$$

  å…¶ä¸­$\lambda \geq 0$,å½“$\lambda = 0$æ—¶å°±ç­‰ä»·äºæå¤§ä¼¼ç„¶ä¼°è®¡ï¼›å½“$\lambda = 1$æ—¶ï¼Œç§°ä¸ºæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆ[Laplacian smoothing](https://en.jinzhao.wiki/wiki/Laplacian_smoothing)ï¼‰ï¼›å½“$\lambda < 1$æ—¶ä¸º Lidstone å¹³æ»‘

  > é«˜æ–¯æœ´ç´ è´å¶æ–¯:æ¡ä»¶æ¦‚ç‡(likelihood)
  > $$P(X^{(j)} = x^{(j)}|Y=c_k) = \frac{1}{\sqrt{2\pi\sigma_{j,k}^2}} exp\bigg(-\frac{(x^{(j)}-\mu_{j,k})^2}{2\sigma_{j,k}^2}\bigg) $$
  > å…¶ä¸­$\mu_{j,k}$ä¸ºæ ·æœ¬ä¸­ç±»åˆ«ä¸º$c_k$çš„ æ‰€æœ‰$x^{(j)}$çš„å‡å€¼ï¼›$\sigma_{j,k}^2$ä¸ºæ ·æœ¬ä¸­ç±»åˆ«ä¸º$c_k$çš„ æ‰€æœ‰$x^{(j)}$çš„æ–¹å·®ï¼ˆå…¶å®å°±æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡å‡å€¼å’Œæ–¹å·®ï¼‰ã€‚
  > sklearn ä¸­ GaussianNB ç±»çš„ä¸»è¦å‚æ•°ä»…æœ‰ä¸€ä¸ªï¼Œå³å…ˆéªŒæ¦‚ç‡ priors ï¼Œå¯¹åº” Y çš„å„ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡$P(Y=c_k)$ã€‚è¿™ä¸ªå€¼é»˜è®¤ä¸ç»™å‡ºï¼Œå¦‚æœä¸ç»™å‡ºæ­¤æ—¶$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$ã€‚å¦‚æœç»™å‡ºçš„è¯å°±ä»¥ priors ä¸ºå‡†ã€‚

### é™„åŠ çŸ¥è¯†

#### å‚æ•°ä¼°è®¡

å‚æ•°ä¼°è®¡([Parameter Estimation](https://en.jinzhao.wiki/wiki/Estimation_theory)) æœ‰ç‚¹ä¼°è®¡ï¼ˆ[point estimation](https://en.jinzhao.wiki/wiki/Point_estimation)ï¼‰å’ŒåŒºé—´ä¼°è®¡ï¼ˆ[interval estimation](https://en.jinzhao.wiki/wiki/Interval_estimation)ï¼‰ä¸¤ç§

**ç‚¹ä¼°è®¡æ³•ï¼š**

- **æå¤§ä¼¼ç„¶ä¼°è®¡([Maximum likelihood estimation, MLE](https://en.jinzhao.wiki/wiki/Maximum_likelihood_estimation))**
  æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯å…¸å‹çš„**é¢‘ç‡å­¦æ´¾**è§‚ç‚¹ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå¾…ä¼°è®¡å‚æ•°$\theta$ æ˜¯å®¢è§‚å­˜åœ¨çš„ï¼Œåªæ˜¯æœªçŸ¥è€Œå·²
  $$L(\theta|x) = f(x|\theta) = P(X|\theta) \\ \hat{\theta}_{MLE} = \argmax_{\theta} L(\theta|x)$$
  è¿™é‡Œç”¨ | å’Œ ; æ˜¯ç­‰ä»·çš„; è¦æœ€å¤§åŒ– Lï¼Œå¯¹ L æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º 0 å³å¯æ±‚è§£ã€‚
  $P(X|\theta)$å°±æ˜¯è´å¶æ–¯å…¬å¼ä¸­çš„ likelihoodï¼Œ$\theta$å°±æ˜¯$c_k$
  log-likelihood:$\ell(\theta|x) = \log L(\theta|x)$ï¼ˆlog å‡½æ•°å¹¶ä¸å½±å“å‡½æ•°çš„å‡¹å‡¸æ€§ï¼‰

- **æœ€å¤§åéªŒä¼°è®¡([maximum a posteriori estimation, MAP](https://en.jinzhao.wiki/wiki/Maximum_a_posteriori_estimation))**
  è´å¶æ–¯å®šç†ï¼š
  $$f(\theta|x) = \frac{f(x|\theta)g(\theta)}{\int_\vartheta f(x|\vartheta)g(\vartheta)d\vartheta}$$
  $g$ æ˜¯$\theta $çš„å¯†åº¦å‡½æ•°ï¼ˆdensity functionï¼‰
  $$\hat{\theta}_{MAP} = \argmax_{\theta} f(\theta|x) \\= \argmax_{\theta} \frac{f(x|\theta)g(\theta)}{\int_\vartheta f(x|\vartheta)g(\vartheta)d\vartheta} \\= \argmax_{\theta}f(x|\theta)g(\theta)$$
  è¿™é‡Œåˆ†æ¯ä¸$\theta$æ— å…³ï¼Œå¯ä»¥çœç•¥
  æˆ‘ä»¬å°†likelihoodå˜æˆlog-likelihoodï¼š
  $$\hat{\theta}_{MAP} =  \argmax_{\theta}\log{f(x|\theta)g(\theta)} =  \argmax_{\theta} (\log{f(x|\theta)} + \log{g(\theta)})$$
  è¿™æ ·æˆ‘ä»¬å¯ä»¥å°†$\log{g(\theta)}$çœ‹ä½œæœºå™¨å­¦ä¹ ç»“æ„é£é™©ä¸­çš„**æ­£åˆ™åŒ–é¡¹**ï¼Œé‚£ä¹ˆå¸¦æœ‰æ­£åˆ™åŒ–é¡¹çš„æœ€å¤§ä¼¼ç„¶å­¦ä¹ å°±å¯ä»¥è¢«è§£é‡Šä¸º MAPï¼ˆå¦‚ï¼š[Ridge å›å½’å’Œ Lasso å›å½’](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)ï¼‰ã€‚
  å½“ç„¶ï¼Œè¿™å¹¶ä¸æ˜¯æ€»æ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚ï¼Œæœ‰äº›æ­£åˆ™åŒ–é¡¹å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å¯¹æ•°ï¼Œè¿˜æœ‰äº›æ­£åˆ™åŒ–é¡¹ä¾èµ–äºæ•°æ®ï¼Œå½“ç„¶ä¹Ÿä¸ä¼šæ˜¯ä¸€ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒã€‚ä¸è¿‡ï¼ŒMAP æä¾›äº†ä¸€ä¸ªç›´è§‚çš„æ–¹æ³•æ¥è®¾è®¡å¤æ‚ä½†å¯è§£é‡Šçš„æ­£åˆ™åŒ–é¡¹ï¼Œä¾‹å¦‚ï¼Œæ›´å¤æ‚çš„æƒ©ç½šé¡¹å¯ä»¥é€šè¿‡æ··åˆé«˜æ–¯åˆ†å¸ƒä½œä¸ºå…ˆéªŒå¾—åˆ°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„é«˜æ–¯åˆ†å¸ƒã€‚

  > æœ€å¤§åéªŒä¼°è®¡å°±æ˜¯**è€ƒè™‘åéªŒåˆ†å¸ƒæå¤§åŒ–è€Œæ±‚è§£å‚æ•°**çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼›MAP = æœ€å¤§ä¼¼ç„¶ä¼°è®¡ + æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ­£åˆ™åŒ–ã€‚
  > è¦æœ€å¤§åŒ– Lï¼Œå¯¹ L æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º 0 å³å¯æ±‚è§£ã€‚

- **è´å¶æ–¯ä¼°è®¡([Bayes estimation](https://en.jinzhao.wiki/wiki/Bayes_estimator))**
  è´å¶æ–¯ä¼°è®¡æ˜¯å…¸å‹çš„**è´å¶æ–¯å­¦æ´¾**è§‚ç‚¹ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå¾…ä¼°è®¡å‚æ•° $\theta$ ä¹Ÿæ˜¯éšæœºå˜é‡ï¼Œå› æ­¤éœ€è¦æ ¹æ®è§‚æµ‹æ ·æœ¬ä¼°è®¡å‚æ•° $\theta$ çš„åˆ†å¸ƒã€‚**è´å¶æ–¯ä¼°è®¡éœ€è¦è¦è®¡ç®—æ•´ä¸ªåéªŒæ¦‚ç‡çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆè€Œ MAP å€¼éœ€è¦æ±‚è§£åéªŒåˆ†å¸ƒæå¤§åŒ–æ—¶çš„å‚æ•°$\theta$ï¼‰**ã€‚

  è´å¶æ–¯ä¼°è®¡å’Œ MAP æŒºåƒçš„ï¼Œéƒ½æ˜¯ä»¥æœ€å¤§åŒ–åéªŒæ¦‚ç‡ä¸ºç›®çš„ã€‚åŒºåˆ«åœ¨äºï¼š

  1.  MLE å’Œ MAP éƒ½æ˜¯åªè¿”å›äº†çš„é¢„ä¼°å€¼
  2.  MAP åœ¨è®¡ç®—åéªŒæ¦‚ç‡çš„æ—¶å€™ï¼ŒæŠŠåˆ†æ¯ p(X)ç»™å¿½ç•¥äº†ï¼Œåœ¨è¿›è¡Œè´å¶æ–¯ä¼°è®¡çš„æ—¶å€™åˆ™ä¸èƒ½å¿½ç•¥
  3.  è´å¶æ–¯ä¼°è®¡è¦è®¡ç®—æ•´ä¸ªåéªŒæ¦‚ç‡çš„æ¦‚ç‡åˆ†å¸ƒ

> **å…±è½­å…ˆéªŒï¼ˆ[Conjugate prior](https://en.jinzhao.wiki/wiki/Conjugate_prior)ï¼‰**ï¼šå¦‚æœå…ˆéªŒåˆ†å¸ƒ prior å’ŒåéªŒåˆ†å¸ƒ posterior å±äºåŒä¸€åˆ†å¸ƒç°‡ï¼Œåˆ™ prior ç§°ä¸º likehood çš„å…±è½­å…ˆéªŒ
> likehood ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œprior ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸ºé«˜æ–¯åˆ†å¸ƒã€‚
> likehood ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹å¼åˆ†å¸ƒï¼‰ï¼Œprior ä¸º beta åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸º beta åˆ†å¸ƒã€‚
> likehood ä¸ºå¤šé¡¹å¼åˆ†å¸ƒï¼Œprior ä¸º Dirichlet åˆ†å¸ƒï¼ˆbeta åˆ†å¸ƒçš„ä¸€ä¸ªæ‰©å±•ï¼‰ï¼Œåˆ™ posterior ä¹Ÿä¸º Dirichletï¼ˆç‹„åˆ©å…‹é›·ï¼‰åˆ†å¸ƒã€‚beta åˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯ dirichlet åˆ†å¸ƒçš„ç‰¹æ®Šæƒ…å†µã€‚

æœ€å°äºŒä¹˜ä¼°è®¡([Least squares estimation, LSE](https://en.jinzhao.wiki/wiki/Least_squares))

çŸ©ä¼°è®¡(Method of moments estimators)

**åŒºé—´ä¼°è®¡æ³•ï¼š**
åŒºé—´ä¼°è®¡æœ€æµè¡Œçš„å½¢å¼æ˜¯ç½®ä¿¡åŒºé—´ [confidence intervals](https://en.jinzhao.wiki/wiki/Confidence_interval) ï¼ˆä¸€ç§[é¢‘ç‡è®ºæ–¹æ³•](https://en.jinzhao.wiki/wiki/Frequentism)ï¼‰å’Œ å¯ä¿¡åŒºé—´ [credible intervals](https://en.jinzhao.wiki/wiki/Credible_interval)ï¼ˆä¸€ç§[è´å¶æ–¯æ–¹æ³•](https://en.jinzhao.wiki/wiki/Bayesian_method)ï¼‰ï¼Œæ­¤å¤–è¿˜æœ‰é¢„æµ‹åŒºé—´ï¼ˆ[Prediction interval](https://en.jinzhao.wiki/wiki/Prediction_interval)ï¼‰ç­‰

**é‡‡æ ·æ³•ï¼š** è´å¶æ–¯ä¼°è®¡ï¼Œè¿‘ä¼¼æ¨æ–­
é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³• [Markov chain Monte Carlo, MCMC](https://en.jinzhao.wiki/wiki/Markov_chain_Monte_Carlo)

### å‚è€ƒæ–‡çŒ®

[4-1] Mitchell TM. Chapter 1: [Generative and discriminative classifiers: NaÃ¯ve Bayes andlogistic regression. In: Machine Learning.](http://www.cs.cmu.edu/~tom/mlbook/NBayeslogReg.pdf) Draft,2005.

[4-2] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning. DataMining,Inference,and Prediction. ](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf) Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[4-3] Bishop C. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf),Springer,2006

## ç¬¬ 5 ç«  å†³ç­–æ ‘

åˆ¤åˆ«æ¨¡å‹

å†³ç­–æ ‘ï¼ˆ[decision tree](https://en.jinzhao.wiki/wiki/Decision_tree_learning)ï¼‰æ˜¯ä¸€ç§åŸºæœ¬çš„åˆ†ç±»ä¸å›å½’æ–¹æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§(å¯è§†åŒ–)ï¼Œé€šå¸¸åŒ…æ‹¬ 3 ä¸ªæ­¥éª¤ï¼šç‰¹å¾é€‰æ‹©ã€å†³ç­–æ ‘çš„ç”Ÿæˆå’Œå†³ç­–æ ‘çš„ä¿®å‰ª
![](https://scikit-learn.org/stable/_images/iris.png)
**ç‰¹å¾é€‰æ‹©**ï¼š
ç‰¹å¾é€‰æ‹©åœ¨äºé€‰å–å¯¹è®­ç»ƒæ•°æ®å…·æœ‰åˆ†ç±»èƒ½åŠ›çš„ç‰¹å¾ã€‚ï¼ˆsklearn ä¸­å¯ä»¥è¿”å› `feature_importances_`ç‰¹å¾é‡è¦æ€§ï¼Œå±æ€§è¶Šé‡è¦ï¼Œç‰¹å¾ç©ºé—´åˆ’åˆ†çš„é¢ç§¯è¶Šå¤§ï¼‰

ä¹Ÿå°±æ˜¯è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ï¼ˆä¿¡æ¯å¢ç›Šï¼ŒåŸºå°¼æŒ‡æ•°ï¼‰æ¥é€‰æ‹©ç‰¹å¾ï¼ˆä½œä¸ºæ ¹èŠ‚ç‚¹ï¼‰è¿›è¡Œç‰¹å¾ç©ºé—´åˆ’åˆ†ï¼Œæ³¨æ„ï¼šåˆ’åˆ†åå†æ¬¡è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ï¼ˆä¿¡æ¯å¢ç›Šï¼ŒåŸºå°¼æŒ‡æ•°ï¼‰ï¼Œé™¤éè¯¥ç‰¹å¾æ‰€åœ¨çš„ç©ºé—´å°±åªæœ‰ä¸€ç±»äº†ï¼ˆä¹Ÿå°±æ˜¯è¯¥ç‰¹å¾ä¸å¯åˆ†äº†ï¼Œé‚£ä¹ˆå°±ç›´æ¥ç”Ÿæˆå¶å­èŠ‚ç‚¹ï¼‰ï¼›

ç‰¹å¾é€‰æ‹©çš„å‡†åˆ™ï¼š

- ä¿¡æ¯å¢ç›Š([information gain](https://en.jinzhao.wiki/wiki/Mutual_information))ï¼ˆID3ï¼‰ï¼Œè¶Šå¤§è¶Šå¥½
- ä¿¡æ¯å¢ç›Šæ¯”([information gain ratio](https://en.jinzhao.wiki/wiki/Information_gain_ratio)) ï¼ˆC4.5ï¼‰ï¼Œè¶Šå¤§è¶Šå¥½
- åŸºå°¼æŒ‡æ•°([Gini coefficient](https://en.jinzhao.wiki/wiki/Gini_coefficient) or Gini index or Gini ratio)ï¼ˆCARTï¼‰ï¼Œè¶Šå°è¶Šå¥½

**å†³ç­–æ ‘çš„ç”Ÿæˆ**ï¼š
å¸¸è§ç®—æ³•ï¼ˆå‚è§ï¼š[Decision tree learning](https://en.jinzhao.wiki/wiki/Decision_tree_learning)ä»¥åŠ[Tree algorithms](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)ï¼‰ï¼š

- [ID3](https://en.jinzhao.wiki/wiki/ID3_algorithm) (Iterative Dichotomiser 3)
- [C4.5](https://en.jinzhao.wiki/wiki/C4.5_algorithm) (successor of ID3)
- [CART](https://en.jinzhao.wiki/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) (Classification And Regression Tree)

**[å†³ç­–æ ‘çš„ä¿®å‰ª Decision tree pruning](https://en.jinzhao.wiki/wiki/Decision_tree_pruning)**ï¼š
ä¿®å‰ªæ˜¯æœºå™¨å­¦ä¹ å’Œæœç´¢ç®—æ³•ä¸­çš„ä¸€ç§æ•°æ®å‹ç¼©æŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ é™¤æ ‘ä¸­å¯¹åˆ†ç±»å®ä¾‹ä¸é‡è¦å’Œå†—ä½™çš„éƒ¨åˆ†æ¥å‡å°å†³ç­–æ ‘çš„å¤§å°ã€‚å‰ªæé™ä½äº†æœ€ç»ˆåˆ†ç±»å™¨çš„å¤æ‚åº¦ï¼Œä»è€Œé€šè¿‡å‡å°‘è¿‡æ‹Ÿåˆæ¥æé«˜é¢„æµ‹ç²¾åº¦ã€‚

- é¢„å‰ªæï¼ˆPre-pruningï¼ŒTop-down pruningï¼‰ï¼š

  - max_depth
    é™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œè¶…è¿‡è®¾å®šæ·±åº¦çš„æ ‘æå…¨éƒ¨å‰ªæ‰

  - min_samples_leaf
    min_samples_leaf é™å®šï¼Œä¸€ä¸ªèŠ‚ç‚¹åœ¨åˆ†æåçš„æ¯ä¸ªå­èŠ‚ç‚¹éƒ½å¿…é¡»åŒ…å«è‡³å°‘ min_samples_leaf ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¦åˆ™åˆ†æå°±ä¸ä¼šå‘ç”Ÿï¼Œæˆ–è€…ï¼Œåˆ†æä¼šæœç€æ»¡è¶³æ¯ä¸ªå­èŠ‚ç‚¹éƒ½åŒ…å« min_samples_leaf ä¸ªæ ·æœ¬çš„æ–¹å‘å»å‘ç”Ÿ

  - min_samples_split
    min_samples_split é™å®šï¼Œä¸€ä¸ªèŠ‚ç‚¹å¿…é¡»è¦åŒ…å«è‡³å°‘ min_samples_split ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿™ä¸ªèŠ‚ç‚¹æ‰å…è®¸è¢«åˆ†æï¼Œå¦åˆ™åˆ†æå°±ä¸ä¼šå‘ç”Ÿã€‚

  - max_features
    ä¸€èˆ¬ max_depth ä½¿ç”¨ï¼Œç”¨ä½œæ ‘çš„â€ç²¾ä¿®â€œ
    max_features é™åˆ¶åˆ†ææ—¶è€ƒè™‘çš„ç‰¹å¾ä¸ªæ•°ï¼Œè¶…è¿‡é™åˆ¶ä¸ªæ•°çš„ç‰¹å¾éƒ½ä¼šè¢«èˆå¼ƒã€‚å’Œ max_depth å¼‚æ›²åŒå·¥ï¼Œmax_features æ˜¯ç”¨æ¥é™åˆ¶é«˜ç»´åº¦æ•°æ®çš„è¿‡æ‹Ÿåˆçš„å‰ªæå‚æ•°ï¼Œä½†å…¶æ–¹æ³•æ¯”è¾ƒæš´åŠ›ï¼Œæ˜¯ç›´æ¥é™åˆ¶å¯ä»¥ä½¿ç”¨çš„ç‰¹å¾æ•°é‡è€Œå¼ºè¡Œä½¿å†³ç­–æ ‘åœä¸‹çš„å‚æ•°ï¼Œåœ¨ä¸çŸ¥é“å†³ç­–æ ‘ä¸­çš„å„ä¸ªç‰¹å¾çš„é‡è¦æ€§çš„æƒ…å†µä¸‹ï¼Œå¼ºè¡Œè®¾å®šè¿™ä¸ªå‚æ•°å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ ä¸è¶³ã€‚å¦‚æœå¸Œæœ›é€šè¿‡é™ç»´çš„æ–¹å¼é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå»ºè®®ä½¿ç”¨ PCAï¼ŒICA æˆ–è€…ç‰¹å¾é€‰æ‹©æ¨¡å—ä¸­çš„é™ç»´ç®—æ³•ã€‚

  - min_impurity_decrease
    min_impurity_decrease é™åˆ¶ä¿¡æ¯å¢ç›Šçš„å¤§å°ï¼Œä¿¡æ¯å¢ç›Šå°äºè®¾å®šæ•°å€¼çš„åˆ†æä¸ä¼šå‘ç”Ÿã€‚è¿™æ˜¯åœ¨ 0.19 ç‰ˆæœ¬ç§æ›´æ–°çš„åŠŸèƒ½ï¼Œåœ¨ 0.19 ç‰ˆæœ¬ä¹‹å‰æ—¶ä½¿ç”¨ min_impurity_splitã€‚
  - min_weight_fraction_leaf
    åŸºäºæƒé‡çš„å‰ªæå‚æ•°

- åå‰ªæï¼ˆPost-pruningï¼ŒBottom-up pruningï¼‰ï¼š

  - ccp_alphaï¼šCCP(Cost Complexity Pruning)-[ccp_alpha å‚æ•°å¦‚ä½•è°ƒä¼˜](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py)
    [7 å¤§åå‰ªæç®—æ³•](https://blog.csdn.net/appleyuchi/article/details/83692381)
    [7 å¤§åå‰ªæç®—æ³• - æºç ](https://github.com/appleyuchi/Decision_Tree_Prune)

ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸‰è¦ç´ ï¼š

- **æ¨¡å‹**ï¼š
  å†³ç­–æ ‘æ¨¡å‹çš„å…³é”®æ˜¯é€šè¿‡ä¸€ç³»åˆ— if then å†³ç­–è§„åˆ™çš„é›†åˆï¼Œå°†ç‰¹å¾ç©ºé—´åˆ’åˆ†æˆä¸ç›¸äº¤çš„å­åŒºåŸŸï¼Œè½åœ¨ç›¸åŒå­åŒºåŸŸçš„æ ·æœ¬å…·æœ‰ç›¸åŒçš„é¢„æµ‹å€¼ã€‚
  ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80V2dJTEhCd1ZIOTR6dFI3QURGUUp3a2N2b2Z3MFNpY21CUTdzb2FScHplekRCMzZCUHB2WUx2Y054aWFGdGlhZm5nOTk5b0taUkpaU3loMnlINlZ2cGpMdy82NDA_d3hfZm10PXBuZw)
- **ç­–ç•¥**ï¼š
  ç­–ç•¥ä¸€èˆ¬åŒ…æ‹¬ä¸¤ä¸ªæ–¹é¢ï¼šç¬¬ä¸€ä¸ªæ˜¯ååº”å†³ç­–æ ‘å¯¹æ ·æœ¬æ•°æ®ç‚¹æ‹Ÿåˆå‡†ç¡®åº¦çš„æŸå¤±é¡¹ï¼Œç¬¬äºŒä¸ªæ˜¯ååº”å†³ç­–æ ‘æ¨¡å‹å¤æ‚ç¨‹åº¦çš„æ­£åˆ™åŒ–é¡¹ã€‚
  å¯¹äºæŸå¤±é¡¹ï¼Œå¦‚æœæ˜¯å›å½’é—®é¢˜ï¼ŒæŸå¤±é¡¹å¯ä»¥å–å¹³æ–¹æŸå¤±ï¼Œå¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸çº¯åº¦æ¥ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼ˆä¿¡æ¯ç†µï¼ŒåŸºå°¼ä¸çº¯åº¦ï¼Œä»¥åŠåˆ†ç±»è¯¯å·®ç‡ï¼‰ã€‚
  æ­£åˆ™åŒ–é¡¹å¯ä»¥å–æ¨¡å‹çš„å¶å­èŠ‚ç‚¹çš„æ•°é‡ã€‚å³å†³ç­–æ ‘æ¨¡å‹åˆ’åˆ†å¾—åˆ°çš„ä¸ç›¸äº¤å­åŒºåŸŸè¶Šå¤šï¼Œæˆ‘ä»¬è®¤ä¸ºæ¨¡å‹è¶Šå¤æ‚ã€‚

- **ç®—æ³•**ï¼š
  ä¼˜åŒ–ç®—æ³•ï¼ˆå¯å‘å¼ç®—æ³•ï¼‰åŒ…æ‹¬æ ‘çš„ç”Ÿæˆç­–ç•¥å’Œæ ‘çš„å‰ªæç­–ç•¥ã€‚
  æ ‘çš„ç”Ÿæˆç­–ç•¥ä¸€èˆ¬é‡‡ç”¨è´ªå¿ƒçš„æ€æƒ³ä¸æ–­é€‰æ‹©ç‰¹å¾å¯¹ç‰¹å¾ç©ºé—´è¿›è¡Œåˆ‡åˆ†ã€‚
  æ ‘çš„å‰ªæç­–ç•¥ä¸€èˆ¬åˆ†ä¸ºé¢„å‰ªæå’Œåå‰ªæç­–ç•¥ã€‚ä¸€èˆ¬æ¥è¯´åå‰ªæç­–ç•¥ç”Ÿæˆçš„å†³ç­–æ ‘æ•ˆæœè¾ƒå¥½ï¼Œä½†å…¶è®¡ç®—æˆæœ¬ä¹Ÿæ›´é«˜ã€‚

[Overview of Decision Trees](http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/dtrees/4_dtrees1.html)

[Decision Tree](https://webdocs.cs.ualberta.ca/~rgreiner/C-466/SLIDES/14.4-DecisionTree.pdf)

[Decision Trees](https://courses.cs.washington.edu/courses/csep546/16sp/slides/dtrees.pdf)

### é™„åŠ çŸ¥è¯†

#### ä¿¡æ¯è®ºï¼ˆ[Information Theory](https://en.jinzhao.wiki/wiki/Information_theory)ï¼‰

[Entropy, Relative Entropy, Cross Entropy](https://iitg.ac.in/cseweb/osint/slides/Anasua_Entropy.pdf)

##### ç†µï¼ˆ[Entropy](<https://en.jinzhao.wiki/wiki/Entropy_(information_theory)>)ï¼‰

åœ¨ä¿¡æ¯è®ºä¸­ï¼Œç†µç”¨æ¥è¡¡é‡ä¸€ä¸ªéšæœºäº‹ä»¶çš„**ä¸ç¡®å®šæ€§**ã€‚ä¹Ÿå«é¦™å†œç†µ Shannon'sï¼ˆäººåï¼‰ entropyã€‚
$$H(X) = E_{p(x)}[I(X)] = E_{p(x)}[-\log {p(x)}] \\= -\sum_{i=1}^n {p(x_i)} \log {p(x_i)} \\= -\int_{X} {p(x)} \log {p(x)} dx$$
å…¶ä¸­$I(X) = -\log {p(x)}$ ç§°ä¸º**è‡ªä¿¡æ¯**ï¼ˆ[Self Information](https://en.jinzhao.wiki/wiki/Information_content)ï¼‰ï¼Œæ˜¯ä¸€ä¸ªéšæœºäº‹ä»¶æ‰€åŒ…å«çš„ä¿¡æ¯é‡ã€‚ä¸€ä¸ªéšæœºäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šé«˜ï¼Œå…¶è‡ªä¿¡æ¯è¶Šä½ã€‚å¦‚æœä¸€ä¸ªäº‹ä»¶å¿…ç„¶å‘ç”Ÿï¼Œå…¶è‡ªä¿¡æ¯ä¸º 0ã€‚
åœ¨è‡ªä¿¡æ¯çš„å®šä¹‰ä¸­ï¼Œå¯¹æ•°çš„åº•å¯ä»¥ä½¿ç”¨ 2ã€è‡ªç„¶å¸¸æ•° ğ‘’ æˆ–æ˜¯ 10ã€‚å½“åº•ä¸º 2 æ—¶ï¼Œè‡ªä¿¡æ¯çš„å•ä½ä¸º bitï¼›å½“åº•ä¸º ğ‘’ æ—¶ï¼Œè‡ªä¿¡æ¯çš„å•ä½ä¸º natã€‚

ç†µè¶Šé«˜ï¼Œåˆ™éšæœºå˜é‡çš„ä¿¡æ¯è¶Šå¤šï¼ˆä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œç³»ç»Ÿè¶Šå¤æ‚ï¼‰ï¼›ç†µè¶Šä½ï¼Œåˆ™éšæœºå˜é‡çš„ä¿¡æ¯è¶Šå°‘ã€‚

---
æ±‚æœ€å¤§ç†µï¼šå‡è®¾æ¦‚ç‡åˆ†å¸ƒ

| X    | 1   | 2   | ... | n   |
| ---- | --- | --- | --- | --- |
| p(x) | pâ‚  | pâ‚‚  | ... | pâ¿  |

$$\max H(p) = \max -\sum_{i=1}^n p_i \log p_i \\ s.t. \sum_{i=1}^n p_i = 1$$

ç”±æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•(Lagrange Multiplier)ï¼Œæœ€å¤§å˜æœ€å°æ—¶å»æ‰è´Ÿå·
$$\mathcal L(p,\lambda) = \sum_{i=1}^n p_i \log p_i + \lambda(1-\sum_{i=1}^n p_i) \\åå¯¼ï¼š\frac{\partial\mathcal L}{\partial p_i} = \log p_i + p_i.\frac{1}{p_i} - \lambda \\ ä»¤åå¯¼ä¸º0å¾—ï¼šp_i^*=exp(\lambda-1)$$

å› ä¸º$\lambda$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼ˆå¸¸æ•°ï¼‰ï¼Œæ‰€ä»¥$p_i^*$æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæ‰€ä»¥ $p_1^*=p_2^*=...=p_n^*=\frac{1}{n}$

æ‰€ä»¥**æ¦‚ç‡åˆ†å¸ƒä¸ºä¸€ä¸ªå‡åŒ€åˆ†å¸ƒï¼Œåˆ™ç†µæœ€å¤§**ï¼Œç”±æ­¤æ€§è´¨æˆ‘ä»¬æ¥è¯æ˜ç†µçš„å–å€¼èŒƒå›´ï¼šè®¾ p æ˜¯ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ$p = \frac{1}{n}$
$$H(p) = -\sum_{i=1}^n \frac{1}{n} \log \frac{1}{n} \\= -\sum_{i=1}^n \frac{1}{n} \log n^{-1} \\= \sum_{i=1}^n \frac{1}{n} \log n \\= \log n$$
æ‰€ä»¥ï¼š$$0 \leq H(p) \leq \log n$$

---
å·²çŸ¥è¿ç»­éšæœºå˜é‡çš„å‡å€¼ä¸º$\mu$ï¼Œæ–¹å·®ä¸º$\sigma^2$ï¼Œæ±‚ç†µæœ€å¤§å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒï¼š
$$\argmax_{p(x)} -\int p(x)\log p(x)dx \\ s.t. \int p(x)dx =1 \\ \int xp(x)dx = \mu \\ \int (x-\mu)^2p(x)dx=\sigma^2$$
æ‹‰æ ¼æœ—æ—¥å‡½æ•°
$$L(p(x),\lambda_1,\lambda_2,\lambda_3) = -\int p(x)\log p(x)dx +\lambda_1(\int p(x)dx - 1)+\lambda_2(\int xp(x)dx - \mu) +\lambda_3(\int (x-\mu)^2p(x)dx - \sigma^2)$$
ä»¤$F(p)=(-\log p(x) + \lambda_{1} +\lambda_{2}x+ \lambda_{3}(x-\mu)^{2})p(x)$
æ±‚åå¯¼å¹¶ä»¤å…¶ä¸º0ï¼ˆå¯ä»¥æŠŠæ±‚ç§¯åˆ†å½“åšæ±‚å’Œï¼Œè¿™æ ·æ±‚åå¯¼å°±å®¹æ˜“æƒ³è±¡äº†ï¼‰
$$\frac{\partial L}{\partial p(x)} = -[\log p(x)+1]+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2$$
å¾—
$$p(x) = \exp\{\lambda_1-1+\lambda_2x+\lambda_3(x-\mu)^2\}$$
æŠŠè·Ÿxæœ‰å…³çš„ä¿ç•™ï¼Œå…¶å®ƒçš„è®¾ä¸ºå¸¸æ•°ï¼Œæœ‰
$$p(x) = \exp\{\lambda_1-1+\lambda_2x+\lambda_3(x-\mu)^2\}\\ =e^{-1+\lambda_{1}}\cdot e^{\lambda_{2}x+ \lambda_{3}(x-\mu)^{2}}=C e^{\lambda_{2}x+ \lambda_{3}(x-\mu)^{2}} \\ = Ce^{\lambda_{3}(x^{2} -2(\mu-\frac{\lambda_{2}}{2\lambda_{3}})x+ u^{2})} = C e^{\lambda_{3}(x -\mu+ \frac{\lambda_{2}}{2\lambda_{3}})^{2}} \\= C.\exp\{\lambda_3(x-\mu+\frac{\lambda_2}{2\lambda_3})^2\}$$

æ ¹æ®$(x-\mu+\frac{\lambda_2}{2\lambda_3})^2$å¾—åˆ°$p(x)$å…³äº$\mu - \frac{\lambda_{2}}{2\lambda_{3}}$å¯¹ç§°(å¶å‡½æ•°å…³äºx=0å¯¹ç§°$p(x) = p(-x)$)ï¼Œæ‰€ä»¥$E[p(x)] = \mu - \frac{\lambda_{2}}{2\lambda_{3}} = \mu$ï¼Œå¾—$\lambda_{2} = 0$

é‚£ä¹ˆ
$$p(x)= C e^{\lambda_{3}(x -\mu)^{2}} $$
å› ä¸º $p(x)>0$ ï¼Œæ‰€ä»¥ $C>0,\lambda_{3}<0$
ä»¤$\lambda = -\lambda_3$
æ ¹æ®ç§¯åˆ†ä¸º1çš„çº¦æŸ ä»¥åŠ$\int e^{-\frac{x^2}{2}}dx = \sqrt{2\pi}$ï¼Œå¾—ï¼š
$$\int p(x)dx = 1 = C\int e^{-\lambda(x -\mu)^{2}} dx = C\sqrt{\frac{\pi}{\lambda}}$$
å¾—åˆ°$C = \sqrt{\frac{\lambda}{\pi}}$
æ ¹æ®æ–¹å·®çš„çº¦æŸï¼Œå¾—ï¼š
$$\int (x-\mu)^2p(x)dx=\sigma^2 = \int (x-\mu)^2e^{-\lambda(x -\mu)^{2}}dx = C\sqrt{\frac{\pi}{\lambda}}.\frac{1}{2\lambda} = \frac{1}{2\lambda}$$
å¾—åˆ°$\lambda_3 = -\frac{1}{2\sigma^2}$ä»¥åŠ$C = \sqrt{\frac{\lambda}{\pi}} = \sqrt{\frac{1}{2\pi\sigma^2}}$
æ‰€ä»¥
$$p(x) = \sqrt{\frac{1}{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$


---

$X$å’Œ$Y$çš„**è”åˆç†µ**ï¼ˆ[Joint Entropy](https://en.jinzhao.wiki/wiki/Joint_entropy)ï¼‰ä¸ºï¼š
$${\displaystyle \mathrm {H} (X,Y)=-\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}P(x,y)\log _{2}[P(x,y)]} \\=\mathbb {E} _{X,Y}[-\log p(x,y)]=-\sum _{x,y}p(x,y)\log p(x ,y)\,$$

ç§¯åˆ†å½¢å¼ï¼ˆè¿ç»­ï¼‰ï¼š
$${\displaystyle h(X,Y)=-\int _{{\mathcal {X}},{\mathcal {Y}}}f(x,y)\log f(x,y)\,dxdy}$$

å¤šä¸ªéšæœºå˜é‡ï¼š
$${\displaystyle \mathrm {H} (X_{1},...,X_{n})=-\sum _{x_{1}\in {\mathcal {X}}_{1}}...\sum _{x_{n}\in {\mathcal {X}}_{n}}P(x_{1},...,x_{n})\log _{2}[P(x_{1},...,x_{n})]}$$
å¤šä¸ªéšæœºå˜é‡çš„ç§¯åˆ†å½¢å¼ï¼ˆè¿ç»­ï¼‰ï¼š
$${\displaystyle h(X_{1},\ldots ,X_{n})=-\int f(x_{1},\ldots ,x_{n})\log f(x_{1},\ldots ,x_{n})\,dx_{1}\ldots dx_{n}}$$

$X$å’Œ$Y$çš„**æ¡ä»¶ç†µ**ï¼ˆ[Conditional Entropy](https://en.jinzhao.wiki/wiki/Conditional_entropy)ï¼‰ä¸ºï¼š

$${\displaystyle \mathrm {H} (Y|X)\ =-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)}}}$$

è¯æ˜ï¼š
$${\displaystyle {\begin{aligned}\mathrm {H} (Y|X)\ &\equiv \sum _{x\in {\mathcal {X}}}\,p(x)\,\mathrm {H} (Y|X=x)\\&=-\sum _{x\in {\mathcal {X}}}p(x)\sum _{y\in {\mathcal {Y}}}\,p(y|x)\,\log \,p(y|x)\\&=-\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}\,p(x,y)\,\log \,p(y|x)\\&=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log \,p(y|x)\\&=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)}}.\\&=\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x)}{p(x,y)}}.\\\end{aligned}}}$$

ç§¯åˆ†å½¢å¼ï¼ˆè¿ç»­ï¼‰ï¼š
$${\displaystyle h(X|Y)=-\int _{{\mathcal {X}},{\mathcal {Y}}}f(x,y)\log f(x|y)\,dxdy}$$

æ ¹æ®å®šä¹‰å†™ä½œï¼š
$${\displaystyle \mathrm {H} (Y|X)\,=\,\mathrm {H} (X,Y)-\mathrm {H} (X)}$$
ä¸€èˆ¬å½¢å¼ï¼š
$${\displaystyle \mathrm {H} (X_{1},X_{2},\ldots ,X_{n})=\sum _{i=1}^{n}\mathrm {H} (X_{i}|X_{1},\ldots ,X_{i-1})}$$

è¯æ˜ï¼š
$${\displaystyle {\begin{aligned}\mathrm {H} (Y|X)&=\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log \left({\frac {p(x)}{p(x,y)}}\right)\\[4pt]&=\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)(\log(p(x))-\log(p(x,y)))\\[4pt]&=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log(p(x,y))+\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}{p(x,y)\log(p(x))}\\[4pt]&=\mathrm {H} (X,Y)+\sum _{x\in {\mathcal {X}}}p(x)\log(p(x))\\[4pt]&=\mathrm {H} (X,Y)-\mathrm {H} (X).\end{aligned}}}$$

##### äº’ä¿¡æ¯ï¼ˆ[Mutual information](https://en.jinzhao.wiki/wiki/Mutual_information)ï¼‰

å¦‚æœå˜é‡ ğ‘‹ å’Œ ğ‘Œ äº’ç›¸ç‹¬ç«‹ï¼Œå®ƒä»¬çš„äº’ä¿¡æ¯ä¸ºé›¶ï¼

$$I(X;Y)=\mathbb {E} _{X,Y}[SI(x,y)]=\sum _{x,y}p(x,y)\log {\frac {p(x,y)}{p(x)\,p(y)}}$$
å…¶ä¸­ SIï¼ˆSpecific mutual Informationï¼‰æ˜¯[pointwise mutual information](https://en.jinzhao.wiki/wiki/Pointwise_mutual_information)

åŸºæœ¬æ€§è´¨ï¼š
$$I(X;Y)=H(X)-H(X|Y) =H(Y)- H(Y|X).\,$$
å¯¹ç§°æ€§ï¼š
$$I(X;Y)=I(Y;X)=H(X)+H(Y)-H(X,Y).\,$$

äº’ä¿¡æ¯å¯ä»¥è¡¨ç¤ºä¸ºç»™å®š Y å€¼çš„ X çš„åéªŒæ¦‚ç‡åˆ†å¸ƒ ä¸ X çš„å…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„å¹³å‡ Kullback-Leibler æ•£åº¦:
$$I(X;Y)=\mathbb {E} _{p(y)}[D_{\mathrm {KL} }(p(X|Y=y)\|p(X))].$$
or
$$I(X;Y)=D_{\mathrm {KL} }(p(X,Y)\|p(X)p(Y)).$$

> ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­è®²åˆ° ä¿¡æ¯å¢ç›Šç­‰ä»·äº’ä¿¡æ¯ï¼ˆ74 é¡µï¼‰ï¼Œè€Œç»´åŸºç™¾ç§‘ä¸­ä¿¡æ¯å¢ç›Šæ˜¯[Kullback-Leibler æ•£åº¦](https://en.jinzhao.wiki/wiki/Information_gain)

##### äº¤å‰ç†µï¼ˆ[Cross Entropy](https://en.jinzhao.wiki/wiki/Cross_entropy)ï¼‰

åœ¨ç»™å®š åˆ†å¸ƒ ğ‘ çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ åˆ†å¸ƒ ğ‘ å’Œ åˆ†å¸ƒ ğ‘ è¶Šæ¥è¿‘ï¼Œäº¤å‰ç†µè¶Šå°ï¼›å¦‚æœ åˆ†å¸ƒ ğ‘ å’Œ åˆ†å¸ƒ ğ‘ è¶Šè¿œï¼Œäº¤å‰
ç†µå°±è¶Šå¤§ï¼
$${\displaystyle H(p,q)=-\operatorname {E} _{p}[\log q]} =-\sum _{x\in {\mathcal {X}}}p(x)\,\log q(x) = H(p)+D_{\mathrm {KL} }(p\|q)$$

> æ³¨æ„ä¸è”åˆç†µ${H} (X,Y)$çš„åŒºåˆ«ï¼Œè”åˆç†µæè¿°ä¸€å¯¹éšæœºå˜é‡å¹³å‡æ‰€éœ€çš„ä¿¡æ¯é‡ï¼Œäº¤å‰ç†µæè¿°ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚
> ä¹Ÿæœ‰è¯´äº¤å‰ç†µ$H(p,q)$æ˜¯ä¸æ ‡å‡†çš„å†™æ³•ï¼Œåº”è¯¥æ˜¯$H_q(p)$ (äº¤å‰ç†µä¸æ˜¯å¯¹ç§°çš„ï¼Œè€Œè”åˆç†µæ˜¯å¯¹ç§°çš„)ï¼Œå‚è§ [Difference of notation between cross entropy and joint entropy](https://stats.stackexchange.com/questions/373098/difference-of-notation-between-cross-entropy-and-joint-entropy) ä»¥åŠ[Relation between cross entropy and joint entropy](https://math.stackexchange.com/questions/2505015/relation-between-cross-entropy-and-joint-entropy)
> åº”ç”¨ï¼šä¸€èˆ¬åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¦‚ï¼šç¥ç»ç½‘ç»œï¼Œé€»è¾‘å›å½’

##### KL æ•£åº¦ï¼ˆ[Kullbackâ€“Leibler divergence](https://en.jinzhao.wiki/wiki/Kullback%E2%80%93Leibler_divergence)ï¼‰

KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰ï¼Œä¹Ÿå« KL è·ç¦»æˆ–ç›¸å¯¹ç†µ(Relative Entropy)ï¼Œæ˜¯ç”¨æ¦‚ç‡åˆ†å¸ƒ ğ‘ æ¥è¿‘ä¼¼ ğ‘ æ—¶æ‰€é€ æˆçš„ä¿¡æ¯æŸå¤±é‡ï¼ŒKL æ•£åº¦æ€»æ˜¯å¤§äºç­‰äº 0 çš„ã€‚**å¯ä»¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»**ï¼KL æ•£åº¦åªæœ‰å½“ ğ‘ = ğ‘ æ—¶ï¼ŒKL(ğ‘, ğ‘) = 0ï¼å¦‚æœä¸¤ä¸ªåˆ†å¸ƒè¶Šæ¥è¿‘ï¼ŒKL æ•£åº¦è¶Šå°ï¼›å¦‚æœä¸¤ä¸ªåˆ†å¸ƒè¶Šè¿œï¼ŒKL æ•£åº¦å°±è¶Šå¤§ï¼ä½† KL æ•£åº¦å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸæ­£çš„åº¦é‡æˆ–è·ç¦»ï¼Œä¸€æ˜¯ KL æ•£åº¦ä¸æ»¡è¶³è·ç¦»çš„å¯¹ç§°æ€§ï¼ŒäºŒæ˜¯ KL æ•£åº¦ä¸æ»¡è¶³è·ç¦»çš„ä¸‰è§’ä¸ç­‰å¼æ€§è´¨ï¼

$$D_{\mathrm {KL} }(p(X)\|q(X))=\sum _{x\in X}-p(x)\log {q(x)}\,-\,\sum _{x\in X}-p(x)\log {p(x)} \\=\sum _{x\in X}p(x)\log {\frac {p(x)}{q(x)}} = -\sum _{x\in X}p(x)\log {\frac {q(x)}{p(x)}}.$$
ä¹Ÿæœ‰å†™ä½œï¼š
$$KL(p,q)  , KL(p|q) , KL(p\|q) , D_{KL}(p,q)$$

> åº”ç”¨ï¼šå¦‚ï¼šå˜åˆ†æ¨æ–­

##### JS æ•£åº¦ï¼ˆ[Jensen-Shannon divergence](https://en.jinzhao.wiki/wiki/Jensen%E2%80%93Shannon_divergence)ï¼‰

JS æ•£åº¦ï¼ˆJensen-Shannon Divergenceï¼‰æ˜¯ä¸€ç§å¯¹ç§°çš„è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒç›¸ä¼¼åº¦çš„åº¦é‡æ–¹å¼ï¼Œæ˜¯ KL æ•£åº¦ä¸€ç§æ”¹è¿›ï¼ä½†ä¸¤ç§æ•£åº¦éƒ½å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå³å¦‚æœä¸¤ä¸ªåˆ†å¸ƒ ğ‘, ğ‘ æ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘æ—¶ï¼ŒKL æ•£åº¦å’Œ JS æ•£åº¦éƒ½å¾ˆéš¾è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»ï¼

$${\rm {D}_{JS}}(P\parallel Q)={\frac  {1}{2}}D_{KL}(P\parallel M)+{\frac  {1}{2}}D_{KL}(Q\parallel M)$$
å…¶ä¸­$M={\frac  {1}{2}}(P+Q)$, JS æ•£åº¦ä¹Ÿæœ‰å†™ä½œ$JSD(P\|Q), JS(P\|Q) ,JS(P,Q)$ç­‰ã€‚

å±äºä¸€ç§ç»Ÿè®¡è·ç¦»ï¼ˆ[Statistical distance](https://en.jinzhao.wiki/wiki/Category:Statistical_distance)ï¼‰

ç»Ÿè®¡è·ç¦»è¿˜æœ‰**Wasserstein è·ç¦»**[Wasserstein distance](https://en.jinzhao.wiki/wiki/Wasserstein_metric)ï¼Œä¹Ÿç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·
ç¦»ï¼å¯¹äºä¸¤ä¸ªåˆ†å¸ƒ$\mu ,\nuï¼Œp^{th}-Wasserstein$è·ç¦»å®šä¹‰ä¸º

$${\displaystyle W_{p}(\mu ,\nu ):=\left(\inf _{\gamma \in \Gamma (\mu ,\nu )}\int _{M\times M}d(x,y)^{p}\,\mathrm {d} \gamma (x,y)\right)^{1/p},}$$

Wasserstein è·ç¦»ç›¸æ¯” KL æ•£åº¦å’Œ JS æ•£åº¦çš„ä¼˜åŠ¿åœ¨äºï¼šå³ä½¿ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘ï¼ŒWasserstein è·ç¦»ä»ç„¶èƒ½åæ˜ ä¸¤ä¸ªåˆ†å¸ƒçš„è¿œè¿‘ï¼å‚è§[pdf443 é¡µé™„å½•](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)

### å‚è€ƒæ–‡çŒ®

[5-1] Olshen R A, Quinlan J R. [Induction of decision trees](https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf). Machine Learning,1986,1(1): 81â€“106

[5-2] Olshen R A, Quinlan J R. [C4. 5: Programs for Machine Learning](https://link.springer.com/content/pdf/10.1007/BF00993309.pdf). Morgan Kaufmann,1992

[5-3] Olshen R A, Breiman L,Friedman J,Stone C. Classification and Regression Trees. Wadsworth,1984

[5-4] Ripley B. Pattern Recognition and Neural Networks. Cambridge UniversityPress,1996

[5-5] Liu B. Web Data Mining: Exploring Hyperlinks,Contents and Usage Data. Springer-Verlag,2006

[5-6] Hyafil L,Rivest R L. Constructing Optimal Binary Decision Trees is NP-complete.Information Processing Letters,1976,5(1): 15â€“17

[5-7] Hastie T,Tibshirani R,Friedman JH. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). New York: Springer-Verlag,2001

[5-8] Yamanishi K. A learning criterion for stochastic rules. Machine Learning,1992

[5-9] Li H,Yamanishi K. Text classification using ESC-based stochastic decision lists.Information Processing & Management,2002,38(3): 343â€“361

## ç¬¬ 6 ç«  é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹

**é€»è¾‘æ–¯è°›å›å½’**ï¼ˆ[logistic regression](https://en.jinzhao.wiki/wiki/Logistic_regression)ï¼‰ï¼ˆä¹Ÿæœ‰ç§° å¯¹æ•°å‡ ç‡å›å½’ï¼‰æ˜¯ç»Ÿè®¡å­¦ä¹ ä¸­çš„ç»å…¸åˆ†ç±»æ–¹æ³•ã€‚æœ€å¤§ç†µæ˜¯æ¦‚ç‡æ¨¡å‹å­¦ä¹ çš„ä¸€ä¸ªå‡†åˆ™ï¼Œå°†å…¶æ¨å¹¿åˆ°åˆ†ç±»é—®é¢˜å¾—åˆ°**æœ€å¤§ç†µæ¨¡å‹**ï¼ˆ[maximum entropy model](https://en.jinzhao.wiki/wiki/Principle_of_maximum_entropy)ï¼‰ã€‚é€»è¾‘æ–¯è°›å›å½’æ¨¡å‹ä¸æœ€å¤§ç†µæ¨¡å‹éƒ½å±äº**å¯¹æ•°çº¿æ€§æ¨¡å‹**ï¼ˆä¹Ÿæœ‰ç§°æœ€å¤§ç†µåˆ†ç±»æˆ–å¯¹æ•°çº¿æ€§åˆ†ç±»ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ¨¡å‹éƒ½æ˜¯åˆ†ç±»æ¨¡å‹ï¼‰ã€‚

### é€»è¾‘æ–¯è°›å›å½’

ä¸€ä¸ªäº‹ä»¶çš„å‡ ç‡ï¼ˆoddsï¼‰æ˜¯æŒ‡è¯¥äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ä¸è¯¥äº‹ä»¶ä¸å‘ç”Ÿçš„æ¦‚ç‡çš„æ¯”å€¼ã€‚å¦‚æœäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡æ˜¯ pï¼Œé‚£ä¹ˆè¯¥äº‹ä»¶çš„å‡ ç‡æ˜¯$\frac{p}{1-p}$ï¼Œè¯¥äº‹ä»¶çš„**å¯¹æ•°å‡ ç‡**ï¼ˆlog oddsï¼‰æˆ– logit å‡½æ•°æ˜¯ï¼š
$$logit(p) = \log\frac{p}{1-p} \label{6-1}\tag{6-1}$$

- **æ¨¡å‹**ï¼š
  äºŒé¡¹é€»è¾‘æ–¯è°›å›å½’çš„æ¨¡å‹å¦‚ä¸‹(w å’Œ x æ˜¯å¢å¹¿å‘é‡ï¼Œw.x ä½œä¸º Sigmoid çš„è¾“å…¥,yâˆˆ{0,1})ï¼š
  $$P(Y=1|x) = \frac{\exp{(w.x)}}{1+\exp{(w.x)}} = \sigma{(w.x)} \\ P(Y=0|x) = \frac{1}{1+\exp{(w.x)}} = 1 - \sigma{(w.x)} \label{6-2}\tag{6-2}$$

  è¯¥äº‹ä»¶çš„å¯¹æ•°å‡ ç‡ï¼š
  $$\log\frac{P(Y=1|x)}{1-P(Y=1|x)} = w.x$$
  æ‰€ä»¥åˆå«å¯¹æ•°å‡ ç‡å›å½’ã€‚

- **ç­–ç•¥**ï¼š
  æŸå¤±å‡½æ•°:è´Ÿå¯¹æ•°ä¼¼ç„¶,negative log likelihood(NLL), è´Ÿçš„ log ä¼¼ç„¶
  æ•°æ®é›†$T=\{(x_1,y_1),...,(x_N,y_N)\}  , x_i \in \mathbb{R}^n , y_i \in \{0,1\}$
  likelihood(6-2 çš„ä¸¤ä¸ªå¼å­åˆèµ·æ¥å°±æ˜¯$[\sigma{(w.x_i)}]^{y_i}[1-\sigma{(w.x_i)}]^{1-y_i}$)ï¼š
  $$L(w|y;x) = P(Y|X;w) = \prod_{i=1}^N P(y_i|x_i;w)= \prod_{i=1}^N [\sigma{(w.x_i)}]^{y_i}[1-\sigma{(w.x_i)}]^{1-y_i}$$

  log likelihoodï¼ˆmaximizedï¼‰ï¼š
  $$\log {L(w|y;x)} = \sum_{i=1}^N[y_i\log\sigma{(w.x_i)} + (1-y_i)\log(1-\sigma{(w.x_i)})]$$

  negative log likelihoodï¼ˆminimizeï¼‰ï¼š
  $$-\log {L(w|y;x)}$$
  è¿™ä¸å°±æ˜¯äº¤å‰ç†µçš„å®šä¹‰çš„å—ã€‚

- **ç®—æ³•**ï¼š
  1. æå¤§ä¼¼ç„¶ä¼°è®¡ MLE(Maximum Likelihood Estimation)
     $$w^* = \argmin_w -\log {L(w|y;x)}$$
  2. ç„¶åä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆStochastic Gradient Descentï¼‰æ±‚æœ€ä¼˜å€¼å¤„çš„å‚æ•°
     -log æ˜¯ä¸€ä¸ªè¿ç»­çš„å‡¸å‡½æ•°

**[sklearn ä¸­ä»£ä»·å‡½æ•°](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)**ï¼š$y \in \{-1,+1\}$
$$P(Y=+1|x) = \frac{\exp{(w.x)}}{1+\exp{(w.x)}} = \sigma{(w.x)} \\ P(Y=-1|x) = \frac{1}{1+\exp{(w.x)}} = 1 - \sigma{(w.x)} = \sigma{(-w.x)}$$
ä¸¤ä¸ªå¼å­åˆèµ·æ¥å°±æ˜¯ï¼š$\sigma{(y_i.w.x_i)}$
negative log likelihoodï¼š

$$
-\log\prod_{i=1}^N \sigma{(y_i.w.x_i)} = \sum_{i=1}^N-\log\sigma{(y_i.w.x_i)}= \sum_{i=1}^N \log\frac{1}{\sigma{(y_i.w.x_i)}} \\=
\sum_{i=1}^N \log\frac{1}{\frac{\exp{(y_i.w.x_i)}}{1+\exp{(y_i.w.x_i)}}}\\= \sum_{i=1}^N \log(1+\frac{1}{\exp{(y_i.w.x_i)}})\\= \sum_{i=1}^N \log(1+\exp{(-y_i.w.x_i)})
$$

å½“ç„¶ sklearn ä¸­åŠ å…¥çš„æ­£åˆ™é¡¹ã€‚

> Softmax å›å½’æ˜¯ Logistic å›å½’çš„å¤šåˆ†ç±»æƒ…å†µã€‚
> LogisticRegression å°±æ˜¯ä¸€ä¸ªè¢« logistic æ–¹ç¨‹å½’ä¸€åŒ–åçš„çº¿æ€§å›å½’ã€‚å°†é¢„æµ‹çš„è¾“å‡ºæ˜ å°„åˆ° 0,1 ä¹‹é—´ã€‚

> é€»è¾‘æ–¯è’‚å›å½’æ¨¡å‹çš„æ€æƒ³è·Ÿçº¿æ€§å›å½’æ¨¡å‹æ€æƒ³ä¸ä¸€æ ·ï¼Œçº¿æ€§å›å½’æ¨¡å‹æ€æƒ³æ˜¯æœ€å°åŒ–çœŸå®å€¼ä¸æ¨¡å‹é¢„æµ‹å€¼çš„è¯¯å·®ï¼Œè€Œé€»è¾‘æ–¯è’‚å›å½’æ¨¡å‹æ€æƒ³å°±æ¯”è¾ƒç‹ äº†ï¼Œé¢„æµ‹å€¼é¢„æµ‹å¯¹äº†æŸå¤±å‡½æ•°å°±æ˜¯ 0ï¼Œé”™äº†æŸå¤±å°±æ˜¯æ— ç©·å¤§ï¼Œæˆ‘ä¸ªäººçš„ç†è§£(ä¸€èˆ¬é‡‡ç”¨çš„æ˜¯-log(h(x)) è¿™æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°,åˆšå¥½æ»¡è¶³è¦æ±‚)

### æœ€å¤§ç†µæ¨¡å‹

ç†µçš„æ¦‚å¿µåœ¨ç»Ÿè®¡å­¦ä¹ ä¸æœºå™¨å­¦ä¹ ä¸­çœŸæ˜¯å¾ˆé‡è¦ï¼Œæœ€å¤§ç†µæ¨¡å‹ï¼ˆ[maximum entropy model](https://en.jinzhao.wiki/wiki/Principle_of_maximum_entropy)ï¼‰æ˜¯æ¦‚ç‡æ¨¡å‹å­¦ä¹ ä¸­ä¸€ä¸ªå‡†åˆ™ï¼Œå…¶æ€æƒ³ä¸ºï¼šåœ¨å­¦ä¹ æ¦‚ç‡æ¨¡å‹æ—¶ï¼Œæ‰€æœ‰å¯èƒ½çš„æ¨¡å‹ä¸­ç†µæœ€å¤§çš„æ¨¡å‹æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼›è‹¥æ¦‚ç‡æ¨¡å‹éœ€è¦æ»¡è¶³ä¸€äº›çº¦æŸï¼Œåˆ™æœ€å¤§ç†µåŸç†ï¼ˆPrinciple of maximum entropyï¼‰å°±æ˜¯åœ¨æ»¡è¶³å·²çŸ¥çº¦æŸçš„æ¡ä»¶é›†åˆä¸­é€‰æ‹©ç†µæœ€å¤§æ¨¡å‹ã€‚
æœ€å¤§ç†µåŸç†æŒ‡å‡ºï¼Œå¯¹ä¸€ä¸ªéšæœºäº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé¢„æµ‹æ—¶ï¼Œé¢„æµ‹åº”å½“æ»¡è¶³å…¨éƒ¨å·²çŸ¥çš„çº¦æŸï¼Œè€Œå¯¹æœªçŸ¥çš„æƒ…å†µä¸è¦åšä»»ä½•ä¸»è§‚å‡è®¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¦‚ç‡åˆ†å¸ƒæœ€å‡åŒ€ï¼Œé¢„æµ‹çš„é£é™©æœ€å°ï¼Œå› æ­¤å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒçš„ç†µæ˜¯æœ€å¤§ã€‚

> å‡å€¼å’Œæ–¹å·®ä¹Ÿè¢«ç§°ä¸ºä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©ï¼ˆäºŒè‡³å››é˜¶ä¸­å¿ƒçŸ©è¢«å®šä¹‰ä¸ºæ–¹å·®ï¼ˆvarianceï¼‰ã€ååº¦ï¼ˆskewnessï¼‰å’Œå³°åº¦ï¼ˆkurtosisï¼‰ï¼‰
> å¯¹äºè¿ç»­åˆ†å¸ƒï¼šç»™å®šå‡å€¼å’Œæ–¹å·®ï¼Œé«˜æ–¯åˆ†å¸ƒçš„ç†µæœ€å¤§ï¼ˆä¹Ÿå¯ä»¥è¯´å·²çŸ¥å‡å€¼å’Œæ–¹å·®æ—¶ï¼Œé«˜æ–¯åˆ†å¸ƒéšæœºæ€§æœ€å¤§ [è¯æ˜](#ç†µentropyhttpsenjinzhaowikiwikientropy_information_theory)ï¼‰
> å¯¹äºè¿ç»­åˆ†å¸ƒï¼šå·²çŸ¥åŒºé—´ï¼Œè¿ç»­å‡åŒ€åˆ†å¸ƒçš„ç†µæœ€å¤§
> å¯¹äºè¿ç»­åˆ†å¸ƒï¼šå·²çŸ¥å‡å€¼ï¼ˆä¸€é˜¶çŸ©ï¼‰ï¼ŒæŒ‡æ•°åˆ†å¸ƒçš„ç†µæœ€å¤§
> å¯¹äºç¦»æ•£åˆ†å¸ƒï¼šç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„ç†µæœ€å¤§ï¼ˆè¿™é‡Œåœ¨å°†ç†µæ—¶æœ‰[è¯æ˜](#ç†µentropyhttpsenjinzhaowikiwikientropy_information_theory)è¿‡ï¼‰


- **æ¨¡å‹**ï¼š
è®¾ Xâˆ¼p(x) æ˜¯ä¸€ä¸ªè¿ç»­å‹éšæœºå˜é‡ï¼Œå…¶å¾®åˆ†ç†µå®šä¹‰ä¸º
$$h(X) = - \int p(x)\log p(x) dx$$
å…¶ä¸­ï¼Œlog ä¸€èˆ¬å–è‡ªç„¶å¯¹æ•° ln, å•ä½ä¸º å¥ˆç‰¹ï¼ˆnatsï¼‰

- **ç­–ç•¥**ï¼š
è€ƒè™‘å¦‚ä¸‹ä¼˜åŒ–é—®é¢˜ï¼š
$$\begin{array}{ll}
&\underset{p}{\text{Maximize}} & \displaystyle h(p) = - \int_S p(x)\log p(x) dx \\
&\text{Subject to} &\displaystyle \int_S p(x) dx = 1 \\[2pt]
&~ & p(x) \ge 0 \\[2pt]
&~ & \displaystyle \int_S p(x) f_i(x) dx = \alpha_i, ~i=1,2,3,\dots,n
\end{array}$$
å…¶ä¸­ï¼Œé›†åˆ S æ˜¯éšæœºå˜é‡çš„supportï¼Œå³å…¶æ‰€æœ‰å¯èƒ½çš„å–å€¼ã€‚æˆ‘ä»¬æ„å›¾æ‰¾åˆ°è¿™æ ·çš„æ¦‚ç‡åˆ†å¸ƒ p, ä»–æ»¡è¶³æ‰€æœ‰çš„çº¦æŸï¼ˆå‰ä¸¤æ¡æ˜¯æ¦‚ç‡å…¬ç†çš„çº¦æŸï¼Œæœ€åä¸€æ¡å«åš**çŸ©çº¦æŸ**ï¼ˆå‡å€¼å’Œæ–¹å·®ä¹Ÿè¢«ç§°ä¸ºä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©ï¼‰ï¼Œåœ¨æ¨¡å‹ä¸­æœ‰æ—¶ä¼šå‡è®¾éšæœºå˜é‡çš„çŸ©ä¸ºå¸¸æ•°ï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿å¾—ç†µæœ€å¤§ã€‚å°†ä¸Šè¿°ä¼˜åŒ–é—®é¢˜å†™æˆæ ‡å‡†å½¢å¼ï¼š
$$\begin{array}{ll}
&\underset{p}{\text{Minimize}} & \displaystyle  \int_S p(x)\log p(x) dx \\
&\text{Subject to} &-p(x) \le 0 \\[2pt]
&~ &\displaystyle \int_S p(x) dx = 1 \\ 
&~ & \displaystyle \int_S p(x) f_i(x) dx = \alpha_i, ~i=1,2,3,\dots,n
\end{array}$$

- **ç®—æ³•**ï¼š
ä½¿ç”¨Lagrangeä¹˜æ•°æ³•å¾—åˆ°å…¶Lagrangianå‡½æ•°
$$L(p,\boldsymbol{\lambda}) = \int_S p\log p ~dx - \mu_{-1}p + \mu_0 \left(\int_S p ~dx - 1\right) + \sum_{j=1}^n \lambda_j \left(\int_S pf_j~dx - \alpha_j\right)$$
æ ¹æ®KKTæ¡ä»¶å¯¹Lagrangianæ±‚å¯¼ä»¤ä¸º0ï¼Œå¯å¾—æœ€ä¼˜è§£ã€‚
$$\begin{gathered}
\frac{\partial L}{\partial p} = \ln p + 1 - \mu_{-1} + \mu_0 + \sum_{j=1}^n \lambda_jf_j := 0 \\
\implies p = \exp\left(-1 + \mu_{-1} - \mu_0 - \sum_{j=1}^n \lambda_j f_j \right) =\displaystyle c^* e^{-\sum_{j=1}^n\lambda_j^* f_j(x)} := p^*
\end{gathered}$$
å…¶ä¸­ï¼Œæˆ‘ä»¬è¦é€‰æ‹© câˆ—,Î»âˆ— ä½¿å¾— p(x) æ»¡è¶³çº¦æŸã€‚åˆ°è¿™é‡Œæˆ‘ä»¬çŸ¥é“ï¼Œåœ¨æ‰€æœ‰æ»¡è¶³çº¦æŸçš„æ¦‚ç‡åˆ†å¸ƒå½“ä¸­ï¼Œpâˆ— æ˜¯ä½¿å¾—ç†µè¾¾åˆ°æœ€å¤§çš„é‚£ä¸€ä¸ªï¼

### å‚è€ƒèµ„æ–™

[é€»è¾‘å›å½’ï¼ˆéå¸¸è¯¦ç»†ï¼‰](https://zhuanlan.zhihu.com/p/74874291)
[æœºå™¨å­¦ä¹ å®ç°ä¸åˆ†æä¹‹å››ï¼ˆå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼‰](http://blog.sina.com.cn/s/blog_13ec1876a0102xb47.html)

[é€»è¾‘å›å½’â€”â€”Logistic çš„èµ·æº](https://www.bilibili.com/video/BV1W3411z71D)
[Logistic å›å½’çš„èµ·æºï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/146206709)
[Logistic å›å½’çš„èµ·æºï¼ˆä¸­ï¼‰](https://zhuanlan.zhihu.com/p/147708076)
[Logistic å›å½’çš„èµ·æºï¼ˆä¸‹ï¼‰](https://zhuanlan.zhihu.com/p/155027693)

[6.2 Logistic Regression and the Cross Entropy Cost - Logistic regression - y å±äº 0 æˆ– 1](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_2_Cross_entropy.html)

[6.3 Logistic Regression and the Softmax Cost-Logistic regression ](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_3_Softmax.html) sklearn ä¸­çš„ä»£ä»·å‡½æ•°ï¼Œè¿™é‡Œçš„ [y å±äº-1 æˆ– 1](https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/notes/6_Linear_twoclass_classification/6_3_Softmax.ipynb)

### é™„åŠ çŸ¥è¯†

#### Generalized Linear Models å¹¿ä¹‰çº¿æ€§æ¨¡å‹

[Generalized Linear Models (GLM)](https://en.jinzhao.wiki/wiki/Generalized_linear_model)

[Generalized Linear Models](https://www.statsmodels.org/devel/glm.html)

[Generalized Linear Models Explained with Examples](https://vitalflux.com/generalized-linear-models-explained-with-examples/)

[Generalized Linear Model Theory - æ¨è](https://data.princeton.edu/wws509/notes/a2.pdf)

[Generalized Linear Models](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf)

åœ¨çº¿æ€§å›å½’æ¨¡å‹ä¸­çš„å‡è®¾ä¸­ï¼Œæœ‰ä¸¤ç‚¹éœ€è¦æå‡ºï¼š

1. å‡è®¾å› å˜é‡æœä»é«˜æ–¯åˆ†å¸ƒï¼š$Y={\theta }^{T}x+\xi$ï¼Œå…¶ä¸­è¯¯å·®é¡¹ $\xi \sim N(0,{ {\sigma }^{2}})$ï¼Œé‚£ä¹ˆå› å˜é‡ $Y\sim N({\theta }^{T}x,{{\sigma }^{2}})$
2. æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºä¸º$E[Y]$ï¼Œæ ¹æ® $Y={\theta }^{T}x+\xi$ï¼Œ$E[Y]=E[{\theta }^{T}x+\xi ]={\theta }^{T}x$,è®° $\eta ={\theta }^{T}x$ï¼Œåˆ™$\eta =E[Y]$

å¹¿ä¹‰çº¿æ€§æ¨¡å‹å¯ä»¥è®¤ä¸ºåœ¨ä»¥ä¸Šä¸¤ç‚¹å‡è®¾åšäº†æ‰©å±•ï¼š

1. å› å˜é‡åˆ†å¸ƒä¸ä¸€å®šæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œæœä»ä¸€ä¸ªæŒ‡æ•°åˆ†å¸ƒæ—ï¼ˆ[Exponential family](https://en.jinzhao.wiki/wiki/Exponential_family)ï¼‰å³å¯ã€‚
2. æ¨¡å‹é¢„æµ‹è¾“å‡ºä»ç„¶å¯ä»¥è®¤ä¸ºæ˜¯$E[Y]$ï¼ˆå®é™…ä¸Šæ˜¯$E[T(Y)]$ï¼Œè®¸å¤šæƒ…å†µä¸‹$T(Y)=Y$ï¼‰ï¼Œä½†æ˜¯$Y$çš„åˆ†å¸ƒä¸ä¸€å®šæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œ$E[Y]$å’Œ$\eta ={{\theta }^{T}}x$ä¹Ÿä¸ä¸€å®šæ˜¯ç®€å•çš„ç›¸ç­‰å…³ç³»ï¼Œå®ƒä»¬çš„å…³ç³»ç”¨$\eta =g(E[Y])$æè¿°ï¼Œç§°ä¸ºè¿æ¥å‡½æ•°(link function)ï¼Œå…¶ä¸­$\eta$ç§°ä¸ºè‡ªç„¶å‚æ•°ã€‚

ç”±äºä»¥ä¸Šä¸¤ç‚¹çš„æ‰©å±•ï¼Œå¹¿ä¹‰çº¿æ€§æ¨¡å‹çš„åº”ç”¨æ¯”åŸºæœ¬çº¿æ€§æ¨¡å‹å¹¿æ³›è®¸å¤šã€‚å¯¹äºå¹¿ä¹‰çº¿æ€§è¿™ä¸ªæœ¯è¯­ï¼Œå¯ä»¥ç†è§£ä¸ºå¹¿ä¹‰ä½“ç°åœ¨å› å˜é‡çš„åˆ†å¸ƒå½¢å¼æ¯”è¾ƒå¹¿ï¼Œåªè¦æ˜¯ä¸€æŒ‡æ•°åˆ†å¸ƒæ—å³å¯ï¼Œè€Œçº¿æ€§åˆ™ä½“ç°åœ¨è‡ªç„¶å‚æ•°$\eta ={{\theta }^{T}}x$æ˜¯$\theta$çš„çº¿æ€§å‡½æ•°ã€‚

> è¿™ä¸€å®¶æ—ä¸­çš„æ¨¡å‹å½¢å¼åŸºæœ¬ä¸Šéƒ½å·®ä¸å¤šï¼Œä¸åŒçš„å°±æ˜¯å› å˜é‡(Y)ä¸åŒï¼Œå¦‚æœæ˜¯è¿ç»­çš„ï¼Œå°±æ˜¯å¤šé‡çº¿æ€§å›å½’ï¼Œå¦‚æœæ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå°±æ˜¯ logistic å›å½’ï¼Œå¦‚æœæ˜¯ poisson åˆ†å¸ƒï¼Œå°±æ˜¯ poisson å›å½’ï¼Œå¦‚æœæ˜¯è´ŸäºŒé¡¹åˆ†å¸ƒï¼Œå°±æ˜¯è´ŸäºŒé¡¹å›å½’ï¼Œç­‰ç­‰ã€‚åªè¦æ³¨æ„åŒºåˆ†å®ƒä»¬çš„å› å˜é‡å°±å¯ä»¥äº†ã€‚logistic å›å½’çš„å› å˜é‡å¯ä»¥æ˜¯äºŒåˆ†ç±»çš„(äºŒé¡¹é€»è¾‘å›å½’)ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤šåˆ†ç±»çš„ï¼ˆå¤šé¡¹é€»è¾‘å›å½’æˆ–è€… softmax å›å½’ï¼‰ï¼Œä½†æ˜¯äºŒåˆ†ç±»çš„æ›´ä¸ºå¸¸ç”¨ï¼Œä¹Ÿæ›´åŠ å®¹æ˜“è§£é‡Šã€‚æ‰€ä»¥å®é™…ä¸­æœ€ä¸ºå¸¸ç”¨çš„å°±æ˜¯äºŒåˆ†ç±»çš„ logistic å›å½’ã€‚

æ ¹æ®[sklearn ä¸­çš„å¹¿ä¹‰çº¿æ€§å›å½’ Generalized Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression)çš„ç¬¬äºŒç§æ–¹å¼[exponential dispersion model (EDM)](https://en.jinzhao.wiki/wiki/Exponential_dispersion_model)ï¼š

å…¶å®å°±æ˜¯è¦è®©çœŸå® y ä¸é¢„æµ‹ y ä¹‹é—´çš„å·®å¼‚è¶Šå°è¶Šå¥½ï¼š
$$\min_{w} \frac{1}{2 n_{\text{samples}}} \sum_i d(y_i, \hat{y}_i) + \frac{\alpha}{2} \|w\|_2$$

å‡è®¾ y åˆ†åˆ«ç¬¦åˆä¸‹åˆ—åˆ†å¸ƒï¼Œæ±‚çœŸå® y ä¸é¢„æµ‹ y ä¹‹é—´çš„å·®å¼‚ï¼ˆDevianceï¼‰ï¼ˆlog ç›¸å‡ä¸å°±æ˜¯ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„æ¯”å—ï¼Ÿä¸å°±æ˜¯å¯¹æ•°å‡ ç‡ï¼ˆlog oddsï¼‰å—ï¼Ÿå¯¹æ•°å‡ ç‡ä¸º 0 æ—¶ä¸å°±æ˜¯æ¦‚ç‡æ¯”ä¸º 1 å—ï¼Ÿä¸å°±æ˜¯å·®å¼‚æœ€å°ä¹ˆï¼ï¼‰ï¼š

- **Normalï¼ˆGaussianï¼‰**ï¼š
  å°±ç›¸å½“äºæ™®é€šçš„çº¿æ€§å›å½’ï¼ˆåŠ ä¸Šæ­£åˆ™å°±æ˜¯ Ridge, ElasticNet ç­‰ï¼‰
  $$f(y;\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-\mu)^2}{2\sigma^2})$$
  $$\log f(y;\mu,\sigma) = -\log\sqrt{2\pi\sigma^2} - \frac{y^2-2y\mu+\mu^2}{2\sigma^2}= -\log\sqrt{2\pi\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{-2y\mu+\mu^2}{2\sigma^2}$$
  Deviance(log-likelihood ratio)(é¢„æµ‹$\hat{y}$å°±æ˜¯é¢„æµ‹çš„å‡å€¼(å³æœŸæœ›$\mu$))ï¼š
  $$\log f(y;y,\sigma) - \log f(y;\hat{y},\sigma) = - \frac{-2y.y+y^2}{2\sigma^2} - (- \frac{-2y.\hat{y}+\hat{y}^2}{2\sigma^2}) \\= \frac{y^2 -2y.\hat{y}+\hat{y}^2}{2\sigma^2} \\= \frac{(y-\hat{y})^2}{2\sigma^2} \\= \frac{D(y,\hat{y})}{2\sigma^2}$$

- **Poisson**ï¼š
  å°±ç›¸å½“äº PoissonRegressor
  $$f(y;\mu) = \frac{\mu^y e^{-\mu}}{y!}$$
  $$\log f(y;\mu) = y\log\mu -\mu -\log(y!)$$
  Deviance(log-likelihood ratio)(é¢„æµ‹$\hat{y}$å°±æ˜¯é¢„æµ‹çš„å‡å€¼(å³æœŸæœ›$\mu$))ï¼š
  $$\log f(y;y) -\log f(y;\hat{y}) = y\log\frac{y}{\hat{y}} - y + \hat{y}$$

- **Binomial(sklearn ä¸­æ²¡æœ‰)**ï¼š
  å°±ç›¸å½“äº Logistic Regression
  $$f(y;n,p) = \binom{n}{y} p^y (1-p)^{n-y}$$
  $$\log f(y;n,p) =y\log p+(n-y)\log(1-p) + \log(\binom{n}{y})$$
  Deviance(log-likelihood ratio)(é¢„æµ‹$\hat{y}$å°±æ˜¯é¢„æµ‹çš„å‡å€¼(å³æœŸæœ›$\hat{y} = \mu = np$))ï¼š
  $$\log f(y;n,\frac{y}{n}) - \log f(y;n,\frac{\hat{y}}{n})= y\log\frac{y}{\hat{y}} + (n-y)\log\frac{1-\frac{y}{n}}{1-\frac{\hat{y}}{n}} = y\log\frac{y}{\hat{y}} + (n-y)\log\frac{n-y}{n-\hat{y}}$$
  Binomial distribution(B(n,p))ï¼Œè€Œ Bernoulli distribution ä¸­ n=1

> ä¸Šè¿°è®¡ç®—éƒ½æœ‰ä¸€ä¸ª 2 å€ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ„æ€æ‰€ä»¥æ²¡æœ‰å†™å‡ºæ¥ã€‚
> è¿˜æœ‰ç”¨ link function è§£é‡Šçš„ï¼Œç›®å‰ä¸æ˜¯å¾ˆæ˜ç™½-å‚è€ƒ[å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰](https://www.cnblogs.com/dreamvibe/p/4259460.html)ã€‚

#### S å‹å‡½æ•°ï¼ˆLogistic & Sigmoid å‡½æ•°ï¼‰

**Logistic å‡½æ•°**ï¼ˆ[Logistic function](https://en.jinzhao.wiki/wiki/Logistic_function)ï¼‰çš„å…¬å¼å®šä¹‰ï¼š

$${\displaystyle f(x)={\frac {L}{1+e^{-k(x-x_{0})}}}}$$
å…¶ä¸­$L$æ˜¯æœ€å¤§å€¼ï¼Œ$x_0$æ˜¯ä¸­å¿ƒç‚¹(ä½ç½®å‚æ•°)ï¼Œ$K$æ˜¯æ›²çº¿çš„å€¾æ–œåº¦ï¼ˆå½¢çŠ¶å‚æ•°ï¼Œ|K|>0 è¶Šå¤§ï¼Œæ›²çº¿åœ¨ä¸­å¿ƒç‚¹é™„è¿‘å¢é•¿è¶Šå¿«ï¼‰ã€‚

**é€»è¾‘æ–¯è°›åˆ†å¸ƒ**[Logistic distribution](https://en.jinzhao.wiki/wiki/Logistic_distribution)ï¼š

- åˆ†å¸ƒå‡½æ•° CDF(Cumulative distribution function)ï¼š
  $${\displaystyle F(x;\mu ,s)={\frac {1}{1+e^{-(x-\mu )/s}}}={\frac {1}{2}}+{\frac {1}{2}}\operatorname {tanh} \left({\frac {x-\mu }{2s}}\right).}$$
- å¯†åº¦å‡½æ•° PDF(Probability density function)ï¼š
  $${\displaystyle {\begin{aligned}f(x;\mu ,s)&={\frac {e^{-(x-\mu )/s}}{s\left(1+e^{-(x-\mu )/s}\right)^{2}}}\\[4pt]&={\frac {1}{s\left(e^{(x-\mu )/(2s)}+e^{-(x-\mu )/(2s)}\right)^{2}}}\\[4pt]&={\frac {1}{4s}}\operatorname {sech} ^{2}\left({\frac {x-\mu }{2s}}\right).\end{aligned}}}$$

> $\mu$ä¸ºå‡å€¼ï¼Œs æ˜¯ä¸€ä¸ªä¸æ ‡å‡†å·®ï¼ˆ[standard deviation](https://en.jinzhao.wiki/wiki/Standard_deviation)ï¼‰æˆæ¯”ä¾‹çš„å‚æ•°

**Sigmoid å‡½æ•°**ï¼ˆ[Sigmoid function](https://en.jinzhao.wiki/wiki/Sigmoid_function)ï¼‰çš„å…¬å¼å®šä¹‰ï¼š
$${\displaystyle S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}=1-S(-x).}$$

Sigmoid å‡½æ•°æ˜¯ä¸€ä¸ªæœ‰ç•Œã€å¯å¾®çš„å®å‡½æ•°ï¼Œå¯ä»¥å°†è¾“å…¥å‹ç¼©åˆ°(0,1)åŒºé—´ï¼Œç»å¸¸ç”¨ä½œæ¿€æ´»å‡½æ•°å’Œæ¦‚ç‡è¾“å‡ºã€‚
Sigmoid å‡½æ•°å¯¹äºå°äº 0 çš„å€¼æ˜¯å‡¸çš„ï¼Œå¯¹äºå¤§äº 0 çš„å€¼æ˜¯å‡¹çš„ã€‚
Sigmoid å‡½æ•°æ˜¯ä¸€ä¸ªæ ‡å‡† Logistic å‡½æ•°ï¼ˆstandard logistic function(ä¸€èˆ¬ç”¨$\sigma(x)$)ï¼š$K=1,x_0=0,L=1$ï¼‰

å¯¼æ•°ï¼š
$$ S'(x)= S(x)(1-S(x))$$
æ¨å¯¼ï¼š
$$ S(x) = {\frac {1}{1+e^{-x}}} = (1+e^{-x})^{-1} \\ S'(x)=(-1)*(1+e^{-x})^{-2}*e^{-x}*(-1) \\= (1+e^{-x})^{-2}*e^{-x} \\= \frac{e^{-x}}{(1+e^{-x})^{2}} \\= \frac{1+e^{-x}-1}{(1+e^{-x})^{2}} \\= \frac{1+e^{-x}}{(1+e^{-x})^{2}} - \frac{1}{(1+e^{-x})^{2}} \\=\frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^{2}} \\= \frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})$$

ä¸**åŒæ›²æ­£åˆ‡å‡½æ•°**ï¼ˆ [hyperbolic tangent function](https://en.jinzhao.wiki/wiki/Hyperbolic_tangent)ï¼‰ï¼š
$${\displaystyle f(x)={\frac {1}{2}}+{\frac {1}{2}}\tanh \left({\frac {x}{2}}\right),} \\ {\displaystyle \tanh(x)=2f(2x)-1.}$$

$${\displaystyle {\begin{aligned}\tanh(x)&={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}={\frac {e^{x}\cdot \left(1-e^{-2x}\right)}{e^{x}\cdot \left(1+e^{-2x}\right)}}\\&=f(2x)-{\frac {e^{-2x}}{1+e^{-2x}}}=f(2x)-{\frac {e^{-2x}+1-1}{1+e^{-2x}}}=2f(2x)-1.\end{aligned}}}$$

#### å½’ä¸€åŒ–æŒ‡æ•°å‡½æ•°ï¼ˆSoftmax å‡½æ•°ï¼‰

**Softmax å‡½æ•°**ï¼ˆ[Softmax function](https://en.jinzhao.wiki/wiki/Softmax_function)ï¼Œä¹Ÿç§°ä¸ºå½’ä¸€åŒ–æŒ‡æ•°å‡½æ•° normalized exponential functionï¼‰çš„å…¬å¼å®šä¹‰ï¼š
standard (unit) softmax function${\displaystyle \sigma :\mathbb {R} ^{K}\to [0,1]^{K}}$

$${\displaystyle \sigma (\mathbf {z} )_{i}={\frac {e^{z_{i}}}{\sum _{j=1}^{K}e^{z_{j}}}}\ \ \ \ {\text{ for }}i=1,\dotsc ,K{\text{ and }}\mathbf {z} =(z_{1},\dotsc ,z_{K})\in \mathbb {R} ^{K}.}$$

ä¸€èˆ¬å½¢å¼ï¼š
$${\displaystyle \sigma (\mathbf {z} )_{i}={\frac {e^{\beta z_{i}}}{\sum _{j=1}^{K}e^{\beta z_{j}}}}{\text{ or }}\sigma (\mathbf {z} )_{i}={\frac {e^{-\beta z_{i}}}{\sum _{j=1}^{K}e^{-\beta z_{j}}}}{\text{ for }}i=1,\dotsc ,K.}$$

æ€§è´¨ï¼š
$${\displaystyle \sigma (\mathbf {z} +\mathbf {c} )_{j}={\frac {e^{z_{j}+c}}{\sum _{k=1}^{K}e^{z_{k}+c}}}={\frac {e^{z_{j}}\cdot e^{c}}{\sum _{k=1}^{K}e^{z_{k}}\cdot e^{c}}}=\sigma (\mathbf {z} )_{j}.}$$

ç­‰å¼å·¦è¾¹çš„${\displaystyle \mathbf {c} =(c,\dots ,c)}$

> å¦‚æœ$z_i$éƒ½ç­‰äºä¸€ä¸ªå‚æ•° C æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿä»ç†è®ºä¸Šè¾“å‡ºä¸º$(\frac{1}{K},...,\frac{1}{K}) \in \mathbb{R}^K$ï¼Œä½†æ˜¯ä»æ•°å€¼è®¡ç®—ä¸Šè¯´ï¼Œå½“ C å¾ˆå¤§æ—¶$e^C$ä¼šå‘ç”Ÿä¸Šæº¢ï¼Œå½“ C å¾ˆå°æ—¶$\sum _{k=1}^{K}e^C$ä¼šå‘ç”Ÿä¸‹æº¢ï¼Œè¿™æ—¶æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ä¸Šè¿°æ€§è´¨ï¼Œå°†$\mathbf {z}$å‡å»$\max_i {z_i}$,é‚£ä¹ˆæœ€å¤§å€¼å°±æ˜¯ 0ï¼Œæ’é™¤äº†ä¸Šæº¢çš„å¯èƒ½ï¼ŒåŒæ ·çš„åˆ†æ¯è‡³å°‘æœ‰ä¸€ä¸ªä¸º 1 çš„é¡¹ï¼Œæ’é™¤äº†å› ä¸ºåˆ†æ¯ä¸‹æº¢è€Œå¯¼è‡´è¢« 0 é™¤çš„å¯èƒ½æ€§ã€‚
> $$\sigma (\mathbf {z} )_{i}={\frac {e^{z_{i}}}{\sum _{j=1}^{K}e^{z_{j}}}} = {\frac {e^{(z_{i}-z_{max})}}{\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}}  = {\frac {e^{(z_{i}-z_{max})}}{1+\sum _{j=1,j\neq max}^{K}e^{(z_{j}-z_{max})}}}$$

**log softmax**å‡½æ•°åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¹Ÿç»å¸¸é‡è§ï¼Œå…¶å®å°±æ˜¯æ±‚å®Œ softmaxï¼Œå†å¯¹å…¶æ±‚ logï¼Œå¦‚æœç›´æ¥è®¡ç®—å¯èƒ½ä¼šå‡ºç°é—®é¢˜ï¼ˆå½“ softmax å¾ˆå°æ—¶ï¼Œlog ä¼šå¾—åˆ°$-\infty$ï¼‰,è¿™æ—¶æˆ‘å°±è¦æ¨å¯¼å‡º log softmax çš„è¡¨è¾¾å¼ï¼š
$$\log(\sigma (\mathbf {z} ))_{i}=\log{\frac {e^{(z_{i}-z_{max})}}{\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}} \\= \log e^{(z_{i}-z_{max})} - \log {\sum _{j=1}^{K}e^{(z_{j}-z_{max})}} \\= (z_{i}-z_{max})- \log {\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}$$

è€Œ${\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}$æ˜¯å¤§äºç­‰äº 1 çš„ï¼Œå¹¶ä¸”ä¸ä¼šå¤§çš„ç¦»è°±ï¼Œæ‰€ä»¥ä¸ä¼šå‡ºé—®é¢˜ã€‚

**negative log-likelihood**ï¼ˆNLLï¼‰ï¼Œlikelihood æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼ˆsoftmax ä¹Ÿæ˜¯æ¦‚ç‡ï¼‰ï¼Œæ‰€ä»¥ log-likelihood å°äº 0ï¼Œnegative log-likelihood åˆ™å¤§äº 0ï¼Œè¿™æ ·å°±å¯ä»¥æœ€å°åŒ– negative log-likelihood äº†

### å‚è€ƒæ–‡çŒ®

[6-1] Berger A,Della Pietra SD,Pietra VD. A maximum entropy approach to naturallanguage processing. Computational Linguistics,1996,22(1),39â€“71

[6-2] Berger A. The Improved Iterative Scaling Algorithm: A Gentle Introduction.http://www.cs.cmu.edu/ afs/cs/user/aberger/www/ps/scaling.ps

[6-3] Hastie T,Tibshirani Rï¼ŒFriedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction. Springer-Verlag. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬:ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[6-4] Mitchell TM. Machine Learning. McGraw-Hill Companies,Inc. 1997ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬:æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003ï¼‰

[6-5] Collins M,Schapire RE,Singer Y. Logistic Regression,AdaBoost and BregmanDistances. Machine Learning Journal,2004

[6-6] Canu S,Smola AJ. Kernel method and exponential family. Neurocomputing,2005,69:714â€“720

## ç¬¬ 7 ç«  æ”¯æŒå‘é‡æœº

**æ”¯æŒå‘é‡æœº**ï¼ˆ[support vector machineï¼ŒSVM](https://en.jinzhao.wiki/wiki/Support-vector_machine)ï¼‰æ˜¯ä¸€ç§äºŒç±»åˆ†ç±»æ¨¡å‹ã€‚å®ƒçš„åŸºæœ¬æ¨¡å‹æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„é—´éš”æœ€å¤§çš„çº¿æ€§åˆ†ç±»å™¨ï¼Œé—´éš”æœ€å¤§ä½¿å®ƒæœ‰åˆ«äºæ„ŸçŸ¥æœºï¼›æ”¯æŒå‘é‡æœºè¿˜åŒ…æ‹¬**æ ¸æŠ€å·§**ï¼Œè¿™ä½¿å®ƒæˆä¸ºå®è´¨ä¸Šçš„éçº¿æ€§åˆ†ç±»å™¨ã€‚æ”¯æŒå‘é‡æœºçš„å­¦ä¹ ç­–ç•¥å°±æ˜¯é—´éš”æœ€å¤§åŒ–ï¼Œå¯å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ±‚è§£**å‡¸äºŒæ¬¡è§„åˆ’**ï¼ˆconvex quadratic programmingï¼‰çš„é—®é¢˜ï¼Œä¹Ÿç­‰ä»·äºæ­£åˆ™åŒ–çš„**åˆé¡µæŸå¤±å‡½æ•°**çš„æœ€å°åŒ–é—®é¢˜ã€‚æ”¯æŒå‘é‡æœºçš„å­¦ä¹ ç®—æ³•æ˜¯æ±‚è§£å‡¸äºŒæ¬¡è§„åˆ’çš„æœ€ä¼˜åŒ–ç®—æ³•ã€‚

æ”¯æŒå‘é‡æœºå­¦ä¹ æ–¹æ³•åŒ…å«æ„å»ºç”±ç®€è‡³ç¹çš„æ¨¡å‹ï¼š**çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœº**ï¼ˆlinear supportvector machine in linearly separable caseï¼‰ã€**çº¿æ€§æ”¯æŒå‘é‡æœº**ï¼ˆlinear support vectormachineï¼‰åŠ**éçº¿æ€§æ”¯æŒå‘é‡æœº**ï¼ˆnon-linear support vector machineï¼‰ã€‚ç®€å•æ¨¡å‹æ˜¯å¤æ‚æ¨¡å‹çš„åŸºç¡€ï¼Œä¹Ÿæ˜¯å¤æ‚æ¨¡å‹çš„ç‰¹æ®Šæƒ…å†µã€‚å½“è®­ç»ƒæ•°æ®çº¿æ€§å¯åˆ†æ—¶ï¼Œé€šè¿‡**ç¡¬é—´éš”æœ€å¤§åŒ–**ï¼ˆhardmargin maximizationï¼‰ï¼Œå­¦ä¹ ä¸€ä¸ªçº¿æ€§çš„åˆ†ç±»å™¨ï¼Œå³çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœºï¼Œåˆç§°ä¸º**ç¡¬é—´éš”æ”¯æŒå‘é‡æœº**ï¼›å½“è®­ç»ƒæ•°æ®è¿‘ä¼¼çº¿æ€§å¯åˆ†æ—¶ï¼Œé€šè¿‡**è½¯é—´éš”æœ€å¤§åŒ–**ï¼ˆsoft marginmaximizationï¼‰ï¼Œä¹Ÿå­¦ä¹ ä¸€ä¸ªçº¿æ€§çš„åˆ†ç±»å™¨ï¼Œå³çº¿æ€§æ”¯æŒå‘é‡æœºï¼Œåˆç§°ä¸º**è½¯é—´éš”æ”¯æŒå‘é‡æœº**ï¼›å½“è®­ç»ƒæ•°æ®çº¿æ€§ä¸å¯åˆ†æ—¶ï¼Œé€šè¿‡ä½¿ç”¨**æ ¸æŠ€å·§ï¼ˆkernel trickï¼‰åŠè½¯é—´éš”æœ€å¤§åŒ–ï¼Œå­¦ä¹ éçº¿æ€§æ”¯æŒå‘é‡æœº**ã€‚

å½“è¾“å…¥ç©ºé—´ä¸ºæ¬§æ°ç©ºé—´æˆ–ç¦»æ•£é›†åˆã€ç‰¹å¾ç©ºé—´ä¸ºå¸Œå°”ä¼¯ç‰¹ç©ºé—´æ—¶ï¼Œæ ¸å‡½æ•°ï¼ˆkernelfunctionï¼‰è¡¨ç¤ºå°†è¾“å…¥ä»è¾“å…¥ç©ºé—´æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´å¾—åˆ°çš„ç‰¹å¾å‘é‡ä¹‹é—´çš„å†…ç§¯ã€‚é€šè¿‡ä½¿ç”¨æ ¸å‡½æ•°å¯ä»¥å­¦ä¹ éçº¿æ€§æ”¯æŒå‘é‡æœºï¼Œç­‰ä»·äºéšå¼åœ°åœ¨é«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­å­¦ä¹ çº¿æ€§æ”¯æŒå‘é‡æœºã€‚è¿™æ ·çš„æ–¹æ³•ç§°ä¸ºæ ¸æŠ€å·§ã€‚æ ¸æ–¹æ³•ï¼ˆkernel methodï¼‰æ˜¯æ¯”æ”¯æŒå‘é‡æœºæ›´ä¸ºä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

> SVM æœ‰ä¸‰å®ï¼šé—´éš”ã€å¯¹å¶ã€æ ¸æŠ€å·§

å‡½æ•°é—´éš”ï¼š$\hat\gamma = y(w.x + b), y\in \{-1,+1\}$
å‡ ä½•é—´éš”ï¼š$\gamma = \frac{1}{\|w\|}|w.x + b| = \frac{\hat\gamma}{\|w\|}$

- **æ¨¡å‹**ï¼š

$$w.x + b=0$$

- **ç­–ç•¥**ï¼š

1. æœ€å¤§é—´éš”(ç¡¬é—´éš”åŸå§‹é—®é¢˜-å‡¸äºŒæ¬¡è§„åˆ’)
   $$\begin{aligned} &\max_{w,b} &\frac{\hat\gamma}{\|w\|} \\ &\text{s.t.} &y_i(w.x_i+b) \geq \hat\gamma\end{aligned}$$
   å‡½æ•°é—´éš”$\hat\gamma$çš„å¤§å°æ˜¯å¯ä»¥å˜çš„ï¼Œæˆ‘ä»¬è®©å…¶ç­‰äº 1ï¼Œé‚£ä¹ˆå°†ä¸Šè¿°é—®é¢˜æ”¹å†™ä¸‹ï¼š
   $$\begin{aligned} &\min_{w,b} &\frac{1}{2}\|w\|^2 \\ &\text{s.t.} &y_i(w.x_i+b) \geq 1\end{aligned}$$
   è¿™ä¸å°±æ˜¯ä¸€ä¸ªæ ‡å‡†çš„å‡¸äºŒæ¬¡è§„åˆ’é—®é¢˜ä¹ˆï¼ï¼ˆ$\frac{1}{\|w\|}$åœ¨$\|w\| = 0$å¤„ä¸å¯å¾®ï¼Œ$\argmax \frac{1}{\|w\|} ä¸ \argmin \frac{1}{2}\|w\|^2$ç­‰ä»·ï¼‰
   å¦‚æœæ•°æ®é›†çº¿æ€§å¯åˆ†ï¼Œé‚£ä¹ˆæœ€å¤§é—´éš”åˆ†ç¦»è¶…å¹³é¢**å­˜åœ¨ä¸”å”¯ä¸€**ï¼Œå…·ä½“è¯æ˜å°±ä¸è¯äº†ï¼ˆè§ç»Ÿè®¡å­¦ä¹ æ–¹æ³• 117 é¡µï¼‰ã€‚
   åœ¨çº¿æ€§å¯åˆ†çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒé›†ä¸­çš„æ ·æœ¬ç‚¹ä¸åˆ†ç¦»è¶…å¹³é¢è·ç¦»æœ€è¿‘çš„ç‚¹ç§°ä¸º**æ”¯æŒå‘é‡**ï¼Œä¹Ÿå°±æ˜¯æ»¡è¶³$y_i(w.x_i +b ) =1$çš„ç‚¹ã€‚
   è€Œè¶…å¹³é¢$w.x_i +b = +1,w.x_i +b = -1$ç§°ä¸º**é—´éš”è¾¹ç•Œ**ï¼Œä¸¤ä¸ªé—´éš”è¾¹ç•Œä¹‹é—´çš„è·ç¦»ç§°ä¸º**é—´éš”**ï¼ˆmarginï¼‰ï¼Œé—´éš”å¤§å°ä¸º$\frac{2}{\|w\|}$ã€‚
   åœ¨å†³å®šåˆ†ç¦»è¶…å¹³é¢æ—¶**åªæœ‰æ”¯æŒå‘é‡èµ·ä½œç”¨**ï¼Œè€Œå…¶å®ƒæ ·æœ¬ç‚¹å¹¶ä¸èµ·ä½œç”¨ï¼Œæ‰€ä»¥è¯¥æ¨¡å‹å«åšæ”¯æŒå‘é‡æœºã€‚

   - æ‹‰æ ¼æœ—æ—¥å‡½æ•°(æ±‚æœ€å°)
     $$L(w,b,\alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^N \alpha_i(1-y_i(w.x_i+b))$$
     $$\min_{w,b} \max_{\alpha} L(w,b,\alpha)$$

   - æ‹‰æ ¼æœ—æ—¥å¯¹å¶å‡½æ•°ï¼ˆæ±‚æœ€å¤§ï¼‰
     $$\max_{\alpha} g(\alpha) = \max_{\alpha} \inf_{w,b} L(w,b,\alpha)$$

2. å¸¦æ­£åˆ™é¡¹çš„åˆé¡µæŸå¤±å‡½æ•°(è½¯é—´éš”åŸå§‹é—®é¢˜-å‡¸äºŒæ¬¡è§„åˆ’)
   $$\min_{w,b} \underbrace{\sum_{i=1}^N\max(0,1-y_i(w.x_i+b))}_{\text{hinge loss function}} + \underbrace{\lambda\|w\|^2}_{\text{æ­£åˆ™åŒ–é¡¹}}$$
   ç­‰ä»·**è½¯é—´éš”**æœ€å¤§åŒ–çš„ä¼˜åŒ–é—®é¢˜ï¼š
   $$\begin{aligned} &\min_{w,b,\xi} & \sum _{i=1}^{n}\xi _{i}+\lambda \|\mathbf {w} \|^{2} \\ &\displaystyle {\text{subject to }} & y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}+b)\geq 1-\xi _{i}\\ &&\xi _{i}\geq 0,\,{\text{for all }}i.\end{aligned}$$
   å…¶ä¸­${\displaystyle \xi _{i}=\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}+b)\right)}$æ˜¯æ¾å¼›å˜é‡ï¼ˆæ¾å¼›æ–¹æ³•[Relaxation](<https://en.jinzhao.wiki/wiki/Category:Relaxation_(approximation)>)æœ‰å¾ˆå¤šï¼Œä¸‹é¢åªæ˜¯ä¸€ç§è€Œå·²ï¼‰ï¼ˆè·Ÿä¸‹é¢æ¾å¼›å˜é‡å®šä¹‰ä¸åŒï¼Œåªæœ‰ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­æœ‰è¯´åˆ°ï¼Œå…¶å®ƒåœ°æ–¹æ²¡æœ‰æ‰¾åˆ°ï¼‰ã€‚
   $y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}+b)\geq 1-\xi _{i}$ç›¸å½“äºåˆ†ç±»ç‚¹å¯ä»¥å¤„äºé—´éš”ä¸­(ä¸å¤ªå‡†ç¡®ï¼Œå¤„äºé—´éš”è¾¹ç•Œçš„ä¸€ä¾§)ï¼Œå¯¹äºè½¯é—´éš”æ”¯æŒå‘é‡æœºä¸­çš„æ”¯æŒå‘é‡åŒ…å«äº†é—´éš”ä¸­çš„å‘é‡ã€‚

   - æ‹‰æ ¼æœ—æ—¥å‡½æ•°(æ±‚æœ€å°)
     $$L(w,b,\xi,\alpha,\mu) = \lambda\|w\|^2 + \sum_{i=1}^N \xi_{i} + \sum_{i=1}^N \alpha_i(1-\xi_{i}-y_i(w.x_i+b)) + \sum_{i=1}^N(\mu_i*(-\xi_{i}))$$
     $$\min_{w,b,\xi} \max_{\alpha,\mu} L(w,b,\xi,\alpha,\mu)$$

   - æ‹‰æ ¼æœ—æ—¥å¯¹å¶å‡½æ•°ï¼ˆæ±‚æœ€å¤§ï¼‰
     $$\max_{\alpha,\mu} g(\alpha,\mu) = \max_{\alpha,\mu} \inf_{w,b,\xi} L(w,b,\xi,\alpha,\mu)$$

- **ç®—æ³•**ï¼š
  ç›´æ¥æ±‚å¯¼ï¼Œä»¤å…¶ç­‰äº 0ï¼ˆå…¶å®å°±æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰

**æ ¸æŠ€å·§**ï¼š
é¦–å…ˆå†™å‡ºäº†åŸå½¢å¼çš„å¯¹å¶å½¢å¼ï¼Œç„¶åæŠŠ$xç”¨\phi (x)$ä»£æ›¿ï¼Œæœ€ç»ˆå‘ç°æ ¹æœ¬ä¸éœ€è¦çŸ¥é“$\phi (x)$ï¼Œåªéœ€è¦æ ¸å‡½æ•°å°±è¡Œäº†ï¼Œå…·ä½“è¯æ˜å°±ä¸è¯äº†ï¼Œå¾ˆç®€å•ï¼Œä¸Šé¢ä¹Ÿæœ‰ä»‹ç»äº†æ ¸æŠ€å·§çŸ¥è¯†ã€‚

> ä¹¦ä¸­è¿˜ä»‹ç»äº†åŸå½¢å¼çš„**å¯¹å¶å½¢å¼**ï¼ˆåŒºåˆ«äºæ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼‰,ä¹Ÿå°±æ˜¯ç­‰ä»·å½¢å¼ï¼ˆæ„ŸçŸ¥æœºä¸­ 2.3.3 èŠ‚ 44 é¡µ ä¹Ÿæ˜¯ç­‰ä»·çš„æ„æ€ï¼‰ï¼Œè¿™ä¸¤ä¸ªåœ°æ–¹çš„ç­‰ä»·éƒ½æ˜¯ç»è¿‡åŸºæœ¬æ¨å¯¼ï¼Œæ±‚å‡º w å‚æ•°ï¼Œç„¶åå¯¹åŸé—®é¢˜è¿›è¡Œäº†æ›¿æ¢ã€‚

### é™„åŠ çŸ¥è¯†

#### æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•

[æœ€ä¼˜åŒ–ï¼šå»ºæ¨¡ã€ç®—æ³•ä¸ç†è®º/æœ€ä¼˜åŒ–è®¡ç®—æ–¹æ³•](http://bicmr.pku.edu.cn/~wenzw/optbook.html)
[Stephen Boyd çš„ Convex Optimization - å‡¸ä¼˜åŒ–](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
[Nonlinear Programming by Dimitri P. Bertsekas - éçº¿æ€§è§„åˆ’](http://www.athenasc.com/nonlinbook.html)

**å‡¸ä¼˜åŒ–**ï¼ˆ[Convex optimization](https://en.jinzhao.wiki/wiki/Convex_optimization)ï¼‰ï¼š
å‡¸ä¼˜åŒ–é—®é¢˜æ˜¯ç›®æ ‡å‡½æ•°ä¸ºå‡¸å‡½æ•°ï¼Œå¯è¡Œé›†ä¸ºå‡¸é›†çš„ä¼˜åŒ–é—®é¢˜ã€‚
æ ‡å‡†å½¢å¼ï¼š
$${\displaystyle {\begin{aligned}&{\underset {\mathbf {x} }{\operatorname {minimize} }}&&f(\mathbf {x} )\\&\operatorname {subject\ to} &&g_{i}(\mathbf {x} )\leq 0,\quad i=1,\dots ,m\\&&&h_{i}(\mathbf {x} )=0,\quad i=1,\dots ,p,\end{aligned}}}$$
å…¶ä¸­$\mathbf {x} \in \mathbb {R} ^{n}$ä¸ºä¼˜åŒ–å˜é‡ï¼Œ**ç›®æ ‡å‡½æ•°**ï¼ˆobjective functionï¼‰${\displaystyle f:{\mathcal {D}}\subseteq \mathbb {R} ^{n}\to \mathbb {R} }$æ˜¯å‡¸çš„ï¼Œ**ä¸ç­‰å¼çº¦æŸ**${\displaystyle g_{i}:\mathbb {R} ^{n}\to \mathbb {R} }$ä¹Ÿæ˜¯å‡¸çš„ï¼Œ**ç­‰å¼çº¦æŸ**${\displaystyle h_{i}:\mathbb {R} ^{n}\to \mathbb {R} }$æ˜¯**ä»¿å°„**ï¼ˆ[affine](https://en.jinzhao.wiki/wiki/Affine_transformation)ï¼‰çš„

**äºŒæ¬¡çº¦æŸäºŒæ¬¡è§„åˆ’**ï¼ˆ[Quadratically constrained quadratic program](https://en.jinzhao.wiki/wiki/Quadratically_constrained_quadratic_program)ï¼‰ï¼š

$${\begin{aligned}&{\text{minimize}}&&{\tfrac  12}x^{{\mathrm  {T}}}P_{0}x+q_{0}^{{\mathrm  {T}}}x\\&{\text{subject to}}&&{\tfrac  12}x^{{\mathrm  {T}}}P_{i}x+q_{i}^{{\mathrm  {T}}}x+r_{i}\leq 0\quad {\text{for }}i=1,\dots ,m,\\&&&Ax=b,\end{aligned}}$$

å…¶ä¸­$P_0ä»¥åŠP_1,..,P_m \in \mathbb{R}^{n \times n}$,$\mathbf {x} \in \mathbb {R} ^{n}$ä¸ºä¼˜åŒ–å˜é‡
å¦‚æœ$P_0ä»¥åŠP_1,..,P_m \in \mathbb{R}^{n \times n}$æ˜¯åŠæ­£å®šçŸ©é˜µï¼Œé‚£ä¹ˆé—®é¢˜æ˜¯å‡¸çš„ï¼Œå¦‚æœ$P_1,..,P_m$ä¸º 0ï¼Œé‚£ä¹ˆçº¦æŸæ˜¯çº¿æ€§çš„ï¼Œå°±æ˜¯**äºŒæ¬¡è§„åˆ’**ï¼ˆ[Quadratic programming](https://en.jinzhao.wiki/wiki/Quadratic_programming)ï¼‰,å³ç›®æ ‡å‡½æ•°æ˜¯äºŒæ¬¡çš„ï¼Œä¸ç­‰å¼ä»¥åŠç­‰å¼çº¦æŸä¹Ÿæ˜¯çº¿æ€§çš„ï¼›äºŒæ¬¡è§„åˆ’çš„å‰æä¸‹ï¼Œå¦‚æœ$P_0$æ˜¯åŠæ­£å®šçŸ©é˜µé‚£ä¹ˆå°±æ˜¯**å‡¸äºŒæ¬¡è§„åˆ’**ï¼›å¦‚æœ$P_0$ä¸º 0ï¼Œå°±æ˜¯**ä¸æ ‡å‡†çš„çº¿æ€§è§„åˆ’**ï¼ˆ[Linear programming](https://en.jinzhao.wiki/wiki/Linear_programming)ï¼‰ï¼Œå³ç›®æ ‡å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œä¸ç­‰å¼ä»¥åŠç­‰å¼çº¦æŸä¹Ÿæ˜¯çº¿æ€§çš„ã€‚
**æ ‡å‡†çš„çº¿æ€§è§„åˆ’**ï¼šå³ç›®æ ‡å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œéè´Ÿçº¦æŸï¼ˆä¼˜åŒ–å˜é‡æ˜¯éè´Ÿçš„ï¼‰ï¼Œç­‰å¼çº¦æŸä¹Ÿæ˜¯çº¿æ€§çš„ã€‚

çº¿æ€§è§„åˆ’è§£æ³•æœ‰[å•çº¯å½¢æ³•](https://en.jinzhao.wiki/wiki/Simplex_algorithm)ç­‰ã€‚å…¶å®ƒè§„åˆ’çš„[ä¼˜åŒ–ç®—æ³•çœ‹è¿™é‡Œ](https://en.jinzhao.wiki/wiki/Category:Optimization_algorithms_and_methods):å†…ç‚¹æ³•ï¼Œå•çº¯å½¢æ³•ç­‰ï¼›æœ‰çº¿æ€§è§„åˆ’è‡ªç„¶ä¹Ÿæœ‰**åŠ¨æ€è§„åˆ’**ï¼ˆ[Dynamic programming](https://en.jinzhao.wiki/wiki/Dynamic_programming)ï¼‰

**æœ€å°äºŒä¹˜ä¸å°±æ˜¯å‡¸äºŒæ¬¡è§„åˆ’ä¹ˆ**ï¼ˆ$\|y-f(x)\|^2,f(x) = Ax+c$ï¼‰ï¼š
$$\text{minimize} f(x) =\frac{1}{2} \|Ax-b\|^2 = \frac{1}{2}(Ax-b)^T(Ax-b) \\= \frac{1}{2}(x^TA^TAx - x^TA^Tb -b^TAx + b^tb) \\= \frac{1}{2}(x^TA^TAx - 2x^TA^Tb + b^tb)$$
å…¶ä¸­$A \in \mathbb{R}^{m \times n}ï¼Œx \in \mathbb{R}^{n \times 1}ï¼Œb \in \mathbb{R}^{m \times 1} $ï¼Œæ‰€ä»¥$x^TA^Tb å’Œ b^TAx$éƒ½æ˜¯ç›¸ç­‰çš„å®æ•°ï¼Œ$b^tb$ä¹Ÿæ˜¯å®æ•°$A^TA$æ˜¯åŠæ­£å®šçŸ©é˜µ

**L1 å’Œ L2 å›å½’ä¹Ÿèƒ½è½¬åŒ–ç§°ç›¸åº”çš„ä¼˜åŒ–é—®é¢˜**ï¼Œå‚è€ƒï¼š[L1L2 æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)

**å…±è½­å‡½æ•°**ï¼ˆ[conjugate function](https://en.jinzhao.wiki/wiki/Convex_conjugate)ï¼‰ï¼š
è®¾å‡½æ•°$f:\mathbb{R}^n \to \mathbb{R}$ï¼Œå®šä¹‰ f çš„å…±è½­å‡½æ•°$f^*:\mathbb{R}^n \to \mathbb{R}$ä¸ºï¼š
$$f^*(y) = \sup_{x \in \mathrm{dom} f} (y^Tx - f(x))$$
å…±è½­å‡½æ•°ä¸€å®šæ˜¯å‡¸çš„ï¼Œsupremum ä¸ºä¸Šç•Œï¼Œ infimum ä¸ºä¸‹ç•Œã€‚

**æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•**ï¼ˆ[Lagrange multiplier](https://en.jinzhao.wiki/wiki/Lagrange_multiplier)ï¼‰ï¼š
æ ¹æ®ä¸Šé¢æ ‡å‡†å½¢å¼çš„ä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æ¥æ„é€ ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š
$$L(x,\lambda,\nu) = f(x) +\sum _{i=1}^{m}\lambda _{i}g_{i}(x)+\sum _{ i=1}^{p}\nu _{i}h_{i}(x)$$
å…¶ä¸­$\lambda _{i},\nu _{i}$åˆ†åˆ«æ˜¯ä¸ç­‰å¼å’Œç­‰å¼å¯¹åº”çš„ Lagrange ä¹˜å­ï¼Œå½“ç„¶å¦‚æœç”¨å‘é‡$\lambda,\nu$è¡¨ç¤ºï¼Œç§°ä¸ºåŸé—®é¢˜çš„ **Lagrange ä¹˜å­å‘é‡**æˆ–**å¯¹å¶å˜é‡**ã€‚

**å¯¹å¶å‡½æ•°**ï¼ˆ[Dual function](<https://en.jinzhao.wiki/wiki/Duality_(optimization)>)ï¼‰ï¼š
$$g(\lambda,\nu) = \inf_{x} L(x,\lambda,\nu)$$
å¯¹å¶å‡½æ•°ä¸€å®šæ˜¯å‡¹çš„ï¼Œåˆç§°**Lagrange å¯¹å¶å‡½æ•°**ã€‚å¯¹å¶å‡½æ•°$g(\lambda,\nu) \leq p^{\star}$, $p^{\star}$æ˜¯åŸé—®é¢˜çš„æœ€ä¼˜å€¼ã€‚Lagrange å¯¹å¶é—®é¢˜çš„æœ€ä¼˜å€¼ç”¨$d^{\star}$è¡¨ç¤ºï¼Œåˆ™$d^{\star}\leq p^{\star}$ï¼Œè¿™ä¸ªæ€§è´¨ç§°ä¸º**å¼±å¯¹å¶æ€§**ï¼ˆ[Weak Duality](https://en.jinzhao.wiki/wiki/Weak_duality)ï¼‰ï¼Œ$p^{\star}- d^{\star}$ç§°ä¸ºåŸé—®é¢˜çš„**æœ€ä¼˜å¯¹å¶é—´éš™**ï¼ˆ[Duality gap](https://en.jinzhao.wiki/wiki/Duality_gap)ï¼‰ï¼Œå½“$d^{\star}= p^{\star}$æ—¶ç§°ä¸º**å¼ºå¯¹å¶æ€§**ï¼ˆ[Strong Duality](https://en.jinzhao.wiki/wiki/Strong_duality)ï¼‰ã€‚

æ‰€ä»¥å½“å¼ºå¯¹å¶æ€§æˆç«‹æ—¶ï¼Œæ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„æœ€å°ç­‰ä»·äºå¯¹å¶å‡½æ•°çš„æœ€å¤§å€¼$\min_{x} \max_{\lambda,\nu}L(x,\lambda,\nu) \iff \max_{\lambda,\nu} g(\lambda,\nu)$ï¼Œ**æˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥å‡½æ•°æ±‚æœ€å°åŒ–æ—¶çš„å‚æ•°$x$ï¼Œç„¶åå¸¦å…¥å…¶ä¸­ï¼Œåˆ©ç”¨å¯¹å¶å‡½æ•°æ±‚å…¶æœ€å¤§æ—¶çš„ Lagrange ä¹˜å­ï¼Œå³$\lambda,\nu$ã€‚**
è¿™é‡Œçš„$\max_{\lambda,\nu}L(x,\lambda,\nu)$æ˜¯ä¸ºäº†è®©çº¦æŸèµ·ä½œç”¨(æè¿°ä¸æ˜¯å¾ˆå‡†ç¡®ï¼Œå…¶å®å°±æ˜¯ç›®æ ‡å‡½æ•°å’Œçº¦æŸçš„äº¤ç‚¹ï¼Œå‚è§[é™„å½• C.3.2](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf))ã€‚

å½“å¼ºå¯¹å¶æ€§æˆç«‹æ—¶ï¼Œé‚£ä¹ˆ$x^{\star},\lambda^{\star},\nu^{\star}$åˆ†åˆ«æ˜¯åŸé—®é¢˜å’Œå¯¹å¶é—®é¢˜çš„æœ€ä¼˜è§£çš„å……åˆ†å¿…è¦æ¡ä»¶æ˜¯æ»¡è¶³ä¸‹é¢çš„**KKT æ¡ä»¶**ï¼ˆ[Karushâ€“Kuhnâ€“Tucker conditions](https://en.jinzhao.wiki/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)ï¼‰ï¼š

$$\nabla_x L(x^{\star},\lambda^{\star},\nu^{\star}) = 0 \\ \lambda_i^{\star}g_i(x^{\star}) = 0,i=1,2,...,m \quad (\text{Complementary slackness})\\ \lambda_i^{\star} \geq 0,i=1,2,...,m \quad (\text{Dual feasibility})\\ g_i(x^{\star}) \leq 0,i=1,2,...,m \quad (\text{Primal feasibility})\\ h_i(x^{\star}) = 0,i=1,2,...,p \quad (\text{Primal feasibility})$$

> å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åªå¯¹ x å‚æ•°æ±‚å¯¼ï¼Œæ²¡æœ‰å¯¹ Lagrange ä¹˜å­æ±‚å¯¼ï¼›

å…¶ä¸­**äº’è¡¥æ¾å¼›**ï¼ˆComplementary slacknessï¼‰æ¡ä»¶ç”¨$\sum_{i=0}^m\lambda_i^{\star}g_i(x^{\star})=0$å¯èƒ½å¾ˆåˆç†ï¼Œå¦‚æœæœ€ä¼˜è§£$x^{\star}$å‡ºç°åœ¨ä¸ç­‰å¼çº¦æŸçš„è¾¹ç•Œä¸Š$g_i(x) = 0$ï¼Œåˆ™$\lambda_i^{\star} > 0$ï¼›å¦‚æœæœ€ä¼˜è§£$x^{\star}$å‡ºç°åœ¨ä¸ç­‰å¼çº¦æŸçš„å†…éƒ¨$g_i(x) < 0$ï¼Œåˆ™$\lambda_i^{\star} = 0$ï¼›**äº’è¡¥æ¾å¼›æ¡ä»¶è¯´æ˜å½“æœ€ä¼˜è§£å‡ºç°åœ¨ä¸ç­‰å¼çº¦æŸçš„å†…éƒ¨ï¼Œåˆ™çº¦æŸå¤±æ•ˆ**ï¼Œæ‰€ä»¥$\lambda_i^{\star} \geq 0,i=1,2,...,m$è¡¨ç¤ºå¯¹å¶å¯è¡Œæ€§ï¼ˆDual feasibilityï¼‰ã€‚

å¦‚ä½•å°†ä¸æ ‡å‡†çš„ä¼˜åŒ–é—®é¢˜è½¬æ¢ç§°æ ‡å‡†çš„ä¼˜åŒ–é—®é¢˜ï¼ˆçº¿æ€§è§„åˆ’ï¼‰ï¼šå‚è€ƒ[çº¿æ€§è§„åˆ’é—®é¢˜](https://www.bilibili.com/video/BV1TK4y1t74p)
A. å¦‚ä½•å°†ä¸ç­‰å¼çº¦æŸå˜æˆç­‰å¼çº¦æŸï¼š

1. $a^Tx \leq b$
   åªéœ€è¦åŠ ä¸Šæ¾å¼›å˜é‡ï¼ˆ[Slack variable](https://en.jinzhao.wiki/wiki/Slack_variable)ï¼‰ï¼Œæ¾å¼›å˜é‡æ˜¯æ·»åŠ åˆ°ä¸ç­‰å¼çº¦æŸä»¥å°†å…¶è½¬æ¢ä¸ºç­‰å¼çš„å˜é‡ï¼Œæ¾å¼›å˜é‡ç‰¹åˆ«ç”¨äºçº¿æ€§è§„åˆ’ã€‚æ¾å¼›å˜é‡ä¸èƒ½å–è´Ÿå€¼ï¼Œå› ä¸ºå•çº¯å½¢ç®—æ³•è¦æ±‚å®ƒä»¬ä¸ºæ­£å€¼æˆ–é›¶ã€‚
   $$a^Tx +s = b \\  s \geq 0$$

2. $a^Tx \geq b$
   åªéœ€è¦å‡å»å‰©ä½™å˜é‡ï¼ˆsurplus variableï¼‰ï¼Œå‰©ä½™å˜é‡ä¸èƒ½å–è´Ÿå€¼ã€‚
   $$a^Tx - e = b \\  e \geq 0$$

B. æ— çº¦æŸå˜é‡å˜æˆæœ‰éè´Ÿçº¦æŸå˜é‡ï¼š
$${\begin{aligned}&z_{1}=z_{1}^{+}-z_{1}^{-} \\& |z_{1}| = z_{1}^{+}+\,z_{1}^{-} \\&\braket{z_{1}^{+},z_{1}^{-}} = 0 \\&z_{1}^{+},\,z_{1}^{-} \geq 0\end{aligned}}$$

å¦‚ï¼š
a. åˆ©ç”¨ä¸Šè¿°ç¬¬ä¸€æ¡æ€§è´¨
$$5 = 5-0 \\ -5 = 0-5$$
or

$$
\begin{pmatrix}
   1 \\
   2 \\
   -3 \\
   -4
\end{pmatrix} =
\begin{pmatrix}
   1 \\
   2 \\
   0 \\
   0
\end{pmatrix} -
\begin{pmatrix}
   0 \\
   0 \\
   3 \\
   4
\end{pmatrix}
$$

b. ç›®æ ‡å‡½æ•°æœ‰å¸¦ç»å¯¹å€¼çš„(ç¬¬äºŒæ¡æ€§è´¨)
$$\begin{aligned} \max |x| \\ ç­‰ä»·ï¼š\max x^+ + x^- \\ s.t. \quad x^+ \geq 0 \\ x^- \geq 0\end{aligned}$$

### å‚è€ƒæ–‡çŒ®

[7-1] Cortes C,Vapnik V. Support-vector networks. Machine Learning,1995,20

[7-2] Boser BE,Guyon IM,Vapnik VN. A training algorithm for optimal margin classifiers.In: Haussler D,ed. Proc of the 5th Annual ACM Workshop on COLT. Pittsburgh,PA,1992,144â€“152

[7-3] Drucker H,Burges CJC,Kaufman L,Smola A,Vapnik V. Support vector regressionmachines. In: Advances in Neural Information Processing Systems 9,NIPS 1996. MITPress,155â€“161

[7-4] Vapnik Vladimir N. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag,1995ï¼ˆä¸­è¯‘æœ¬ï¼šå¼ å­¦å·¥ï¼Œè¯‘ã€‚ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„æœ¬è´¨ã€‚åŒ—äº¬ï¼šæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ2000ï¼‰

[7-5] Platt JC. Fast training of support vector machines using sequential minimaloptimization. [Microsoft Research](https://www.microsoft.com/en-us/research/project/support-vector-machines/)

[7-6] Weston JAE,Watkins C. Support vector machines for multi-class pattern recognition. In: Proceedings of the 7th European Symposium on Articial Neural Networks. 1999

[7-7] Crammer K,Singer Y. On the algorithmic implementation of multiclass kernel-basedmachines. Journal of Machine Learning Research,2001,2(Dec): 265â€“292

[7-8] Tsochantaridis I,Joachims T,Hofmann T,Altun Y. Large margin methods forstructured and interdependent output variables. JMLR,2005,6: 1453â€“1484

[7-9] Burges JC. A tutorial on support vector machines for pattern recognition. BellLaboratories,Lucent Technologies. 1997

[7-10] Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines andOthre KernerBased Learning Methods. Cambridge University Pressï¼Œ2000ï¼ˆä¸­è¯‘æœ¬ï¼šæå›½æ­£ï¼Œç­‰è¯‘ã€‚æ”¯æŒå‘é‡æœºå¯¼è®ºã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[7-11] é‚“ä¹ƒæ‰¬ï¼Œç”°è‹±æ°ã€‚æ•°æ®æŒ–æ˜ä¸­çš„æ–°æ–¹æ³•â€”â€”æ”¯æŒå‘é‡æœºã€‚åŒ—äº¬ï¼šç§‘å­¦å‡ºç‰ˆç¤¾ï¼Œ2004

[7-12] é‚“ä¹ƒæ‰¬ï¼Œç”°è‹±æ°ã€‚æ”¯æŒå‘é‡æœºâ€”â€”ç†è®ºï¼Œç®—æ³•ä¸æ‹“å±•ã€‚åŒ—äº¬ï¼šç§‘å­¦å‡ºç‰ˆç¤¾ï¼Œ2009

[7-13] Scholkpf B,Smola AJ. Learning with Kernels: Support VectorMachines,Regularization,Optimization,and Beyond. MIT Press,2002

[7-14] Herbrich R. Learning Kernel Classifiers,Theory and Algorithms. The MITPress,2002

[7-15] Hofmann T,Scholkopf B,Smola AJ. Kernel methods in machine learning. The Annalsof Statistics,2008,36(3): 1171â€“1220

## ç¬¬ 8 ç«  æå‡æ–¹æ³•

**é›†æˆå­¦ä¹ **ï¼ˆ[Ensemble Learning](https://en.jinzhao.wiki/wiki/Ensemble_learning)ï¼‰ä¹Ÿå«é›†æˆæ–¹æ³•ï¼ˆEnsemble methodsï¼‰æ˜¯ä¸€ç§å°†å¤šç§å­¦ä¹ ç®—æ³•ç»„åˆåœ¨ä¸€èµ·ä»¥å–å¾—æ›´å¥½è¡¨ç°çš„ä¸€ç§æ–¹æ³•ã€‚

åˆ©ç”¨æˆå‘˜æ¨¡å‹çš„å¤šæ ·æ€§æ¥çº æ­£æŸäº›æˆå‘˜æ¨¡å‹é”™è¯¯çš„èƒ½åŠ›ï¼Œå¸¸ç”¨çš„**å¤šæ ·æ€§æŠ€æœ¯**æœ‰ï¼š

- æœ€æµè¡Œçš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†æ¥è®­ç»ƒæ¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¿™äº›æ•°æ®é›†é€šè¿‡ä»æ€»ä½“æ•°æ®é›†ä¸­æœ‰æ”¾å›çš„éšæœºé‡‡æ ·è·å¾—ï¼Œä¾‹å¦‚ bootstrapping æˆ– bagging æŠ€æœ¯
- åœ¨åˆ†ç±»çš„åœºæ™¯ä¸­ï¼Œå¯ä½¿ç”¨**å¼±åˆ†ç±»å™¨**æˆ–è€…ä¸ç¨³å®šæ¨¡å‹ï¼ˆunstable modelï¼‰ä½œä¸ºæˆå‘˜æ¨¡å‹æ¥æé«˜å¤šæ ·æ€§ï¼Œå› ä¸ºå³ä½¿å¯¹è®­ç»ƒå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿä¼šå¾—åˆ°å®Œå…¨ä¸åŒçš„å†³ç­–è¾¹ç•Œï¼›
- ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒç±»å‹çš„åˆ†ç±»å™¨ï¼Œå¦‚å†³ç­–æ ‘ï¼Œæœ€è¿‘é‚»ï¼Œæ”¯æŒå‘é‡æœºç­‰æ··åˆåˆ°ä¸€èµ·æ¥å¢åŠ å¤šæ ·æ€§ï¼›

> å¼±åˆ†ç±»å™¨:æ¯”éšæœºçŒœæµ‹ç•¥å¥½ï¼Œå¦‚äºŒåˆ†ç±»ä¸­ï¼Œå‡†ç¡®ç‡å¤§äº 0.5 å°±å¯ä»¥äº†ï¼ˆå¦‚ 0.51ï¼‰ã€‚

å‚è§çš„é›†æˆå­¦ä¹ ç±»å‹ï¼š

- Bayes optimal classifier
- [Bootstrap aggregating](https://en.jinzhao.wiki/wiki/Bootstrap_aggregating) (ä¹Ÿç§°ä¸º bagging æ¥è‡ª **b**ootstrap **agg**regat**ing**)
  Bagging å°±æ˜¯é‡‡ç”¨æœ‰æ”¾å›çš„æ–¹å¼è¿›è¡ŒæŠ½æ ·ï¼Œç”¨æŠ½æ ·çš„æ ·æœ¬å»ºç«‹å­æ¨¡å‹,å¯¹å­æ¨¡å‹ï¼ˆå¹¶è¡Œè®­ç»ƒï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸ªè¿‡ç¨‹é‡å¤å¤šæ¬¡ï¼Œæœ€åè¿›è¡Œèåˆã€‚
- [Boosting](<https://en.jinzhao.wiki/wiki/Boosting_(meta-algorithm)>)
  Boosting çš„æ€æƒ³æ˜¯ä¸€ç§è¿­ä»£çš„æ–¹æ³•(ä¸²è¡Œ)ï¼Œæ¯ä¸€æ¬¡è®­ç»ƒçš„æ—¶å€™éƒ½æ›´åŠ å…³å¿ƒåˆ†ç±»é”™è¯¯çš„æ ·ä¾‹ï¼Œç»™è¿™äº›åˆ†ç±»é”™è¯¯çš„æ ·ä¾‹å¢åŠ æ›´å¤§çš„æƒé‡ï¼Œä¸‹ä¸€æ¬¡è¿­ä»£çš„ç›®æ ‡å°±æ˜¯èƒ½å¤Ÿæ›´å®¹æ˜“è¾¨åˆ«å‡ºä¸Šä¸€è½®åˆ†ç±»é”™è¯¯çš„æ ·ä¾‹ã€‚æœ€ç»ˆå°†è¿™äº›å¼±åˆ†ç±»å™¨è¿›è¡ŒåŠ æƒç›¸åŠ ã€‚
- Bayesian model averaging
- Bayesian model combination
- Bucket of models
- Stacking

æ¨¡å‹çš„ç»„åˆ(blending)æ–¹æ³•ï¼š

- çº¿æ€§ç»„åˆ(å¹³å‡æ³•-åŠ æƒå¹³å‡æ³•) - ç”¨äºå›å½’å’Œåˆ†ç±»é—®é¢˜
  å…¶ä¸­ x è¾“å…¥å‘é‡ï¼Œä¼°è®¡ç±»åˆ« y çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆæ¨¡å‹é›†åˆæ•´ä½“å¯¹ç±»åˆ« y çš„æ¦‚ç‡ä¼°è®¡ä¸ºï¼š
  $$\overline{f}(y|x) = \sum_{t=1}^T w_t f_t(y|x)$$
  æƒé‡ç¡®å®šæ¯”è¾ƒå›°éš¾ï¼Œè¿˜å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå› æ­¤å¹³å‡æƒé‡æ˜¯æœ€å¸¸ç”¨çš„ï¼ˆ$w_t = \frac{1}{T}$å³ç®€å•å¹³å‡æ³•ï¼‰ã€‚

- æŠ•ç¥¨ç»„åˆ - ç”¨äºåˆ†ç±»é—®é¢˜
  $$H(\textbf{x}) = sign( \sum_{t=1}^{T}w_t h_t(y|\textbf{x} ) )$$
  å¦‚äºŒåˆ†ç±»é—®é¢˜ï¼š$h_t(y|\textbf{x} ) \in \{-1,+1\}$,åŒæ ·ï¼Œæƒé‡å¯ä»¥æ˜¯å‡åŒ€çš„(ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•)ï¼Œä¹Ÿå¯ä»¥ä¸å‡åŒ€(åŠ æƒæŠ•ç¥¨æ³•)ã€‚

- ä¹˜ç§¯ç»„åˆ
  $$\overline f(y|\textbf{x}) = \frac{1}{Z}\prod_{t=1}^{T} f_t(y | \textbf{x})^{w_t}$$
  åœ¨å„æ¨¡å‹çš„ç±»åˆ«æ¡ä»¶æ¦‚ç‡ä¼°è®¡ç›¸äº’ç‹¬ç«‹çš„å‡è®¾ä¸‹ï¼Œä¹˜ç§¯ç»„åˆåœ¨ç†è®ºä¸Šæ˜¯æœ€å¥½çš„ç»„åˆç­–ç•¥ï¼Œä½†æ˜¯åœ¨å®é™…ä¸­ï¼Œè¿™ç§å‡è®¾å¾ˆéš¾æˆç«‹ï¼ŒåŒæ—¶æƒé‡ä¸çº¿æ€§ç»„åˆä¸€æ ·ä¸å¥½ç¡®å®šã€‚

- å­¦ä¹ ç»„åˆ
  å½“è®­ç»ƒæ•°æ®å¾ˆå¤šæ—¶ï¼Œä¸€ç§æ›´ä¸ºå¼ºå¤§çš„ç»„åˆç­–ç•¥å«â€œå­¦ä¹ æ³•â€ï¼Œå³é€šè¿‡ä¸€ä¸ªå­¦ä¹ å™¨æ¥è¿›è¡Œç»„åˆï¼Œè¿™ç§æ–¹æ³•å« Stackingã€‚
  è¿™é‡ŒæŠŠåŸºå­¦ä¹ å™¨ç§°ä¸ºåˆçº§å­¦ä¹ å™¨ï¼ŒæŠŠç”¨æ¥ç»„åˆçš„å­¦ä¹ å™¨ç§°ä¸ºæ¬¡çº§å­¦ä¹ å™¨ã€‚
  Stacking å…ˆä»åˆå§‹æ•°æ®é›†è®­ç»ƒå‡º**åˆçº§å­¦ä¹ å™¨**ï¼Œå†æŠŠ**åˆçº§å­¦ä¹ å™¨çš„è¾“å‡º**ç»„åˆæˆæ–°çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒ**æ¬¡çº§å­¦ä¹ å™¨**ã€‚å¯¹äºæµ‹è¯•é›†ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨åˆçº§å­¦ä¹ å™¨é¢„æµ‹ä¸€æ¬¡ï¼Œå¾—åˆ°æ¬¡çº§å­¦ä¹ å™¨çš„è¾“å…¥æ ·æœ¬ï¼Œå†ç”¨æ¬¡çº§å­¦ä¹ å™¨é¢„æµ‹ä¸€æ¬¡ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

| aggregation type | blending         | learning      |
| ---------------- | ---------------- | ------------- |
| uniform          | voting/averaging | Bagging       |
| non-uniform      | linear           | AdaBoost      |
| conditional      | stacking         | Decision Tree |

> [å‚è€ƒ](https://github.com/openjw/penter/blob/master/scikit-learn/api/ensemble.ipynb)

[AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) ï¼š[AdaBoost](https://en.jinzhao.wiki/wiki/AdaBoost)æ˜¯ Adaptive Boosting çš„ç¼©å†™

è®­ç»ƒé›†$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}, x_i \in \mathbb{R}^n,y_i\in \{-1,+1\}$

- **æ¨¡å‹**ï¼š

  1. é¦–å…ˆåˆå§‹åŒ–æ•°æ®é›†çš„æƒå€¼åˆ†å¸ƒï¼š$D_1 = (w_{11},...w_{1i},...,w_{iN}), w_{1i} = \frac{1}{N}$
  2. å¯¹$m=1,2...M$ä½¿ç”¨å…·æœ‰æƒå€¼åˆ†å¸ƒ$D_m$çš„è®­ç»ƒæ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œå¾—åˆ°åŸºæœ¬åˆ†ç±»å™¨$G_m(x):\mathbb{R}^n \to \{-1,+1\}$  
     a. è®¡ç®—$\alpha_m = \frac{1}{2}\log\frac{1-e_m}{e_m}$æ˜¯$G_m(x)$çš„ç³»æ•°ï¼ˆåŸºæœ¬\ä¸ªä½“åˆ†ç±»å™¨çš„æƒå€¼ï¼ŒåŒºåˆ«äº w æ˜¯è®­ç»ƒæ•°æ®çš„æƒé‡ï¼‰,$e$è¶Šå¤§é”™çš„è¶Šå¤š,é‚£ä¹ˆ$\alpha$å°±è¶Šå°ï¼ˆå°äº 0ï¼Œæ¥è¿‘è´Ÿæ— ç©·ï¼‰ï¼›
     b. è®¡ç®—$e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i)= \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$æ˜¯$G_m(x)$åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç‡ï¼ˆè¶Šå¤§é”™çš„è¶Šå¤šï¼‰ï¼›
     c. æ›´æ–°æƒå€¼åˆ†å¸ƒ$D_{m+1} = (w_{m+1,1},...w_{m+1,i},...,w_{m+1,N})$,å…¶ä¸­
     $$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i))$$
     $\alpha$è¶Šå°ï¼ˆå°äº 0ï¼Œæ¥è¿‘è´Ÿæ— ç©·ï¼‰$w$å°±è¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯è¢«åˆ†é”™çš„æ ·æœ¬æƒé‡å¤§ã€‚
     $Z_m = \sum_{i=1}^N w_{mi}\exp(-\alpha_m y_i G_m(x_i))$æ˜¯å½’ä¸€åŒ–å› å­ï¼ˆå› ä¸º D æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼‰
  3. æ„å»ºåŸºæœ¬åˆ†ç±»å™¨çš„çº¿æ€§ç»„åˆï¼Œå¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨
     $$G(x) =\mathrm{sign} ( \sum_{m=1}^M \alpha_m G_m(x))$$

- **ç­–ç•¥**ï¼š
  ä½¿æ¯æ¬¡è¿­ä»£çš„$e$ï¼ˆåˆ†ç±»è¯¯å·®ç‡ï¼‰è¶Šå°è¶Šå¥½
  $$e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i)= \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i) \\ æ¨å¯¼å‡º e_m = \sum_{G_m(x_i) \neq y_i} w_{mi}$$
- **ç®—æ³•**ï¼š

[Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)ï¼šæ¢¯åº¦æå‡ï¼ˆ[Gradient boosting](https://en.jinzhao.wiki/wiki/Gradient_boosting)ï¼‰æ˜¯ä¸€ç§ç”¨äºå›å½’ã€åˆ†ç±»å’Œå…¶ä»–ä»»åŠ¡çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒä»¥å¼±é¢„æµ‹æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰çš„é›†åˆå½¢å¼ç”Ÿæˆé¢„æµ‹æ¨¡å‹ã€‚å½“å†³ç­–æ ‘æ˜¯å¼±å­¦ä¹ å™¨æ—¶ï¼Œäº§ç”Ÿçš„ç®—æ³•ç§°ä¸º**æ¢¯åº¦æå‡æ ‘**(Gradient Tree Boosting or Gradient boosted trees or Gradient Boosted Decision Trees(GBDT))ï¼Œé€šå¸¸ä¼˜äºéšæœºæ£®æ—ï¼ˆ[Random forest](https://en.jinzhao.wiki/wiki/Random_forest)å®ƒæ˜¯ Bagging ç®—æ³•çš„è¿›åŒ–ç‰ˆï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒçš„æ€æƒ³ä»ç„¶æ˜¯ bagging,ä½†æ˜¯è¿›è¡Œäº†ç‹¬æœ‰çš„æ”¹è¿›ã€‚ï¼‰

- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š

éšæœºæ£®æ—ä¸ä¸€èˆ¬çš„ bagging ç›¸æ¯”ï¼šï¼ˆå‚è€ƒï¼š[Bagging ä¸éšæœºæ£®æ—ç®—æ³•åŸç†å°ç»“](https://www.cnblogs.com/pinard/p/6156009.html)å’Œ[æœºå™¨å­¦ä¹ ç®—æ³•ç³»åˆ—ï¼ˆäº”ï¼‰ï¼šbagging ä¸éšæœºæ£®æ—å¯¹æ¯”åŠéšæœºæ£®æ—æ¨¡å‹å‚æ•°ä»‹ç»](https://blog.csdn.net/qq_20106375/article/details/94383076)ï¼‰

- bagging æ–¹æ³•çš„çš„éšæœºæ€§ä»…ä»…æ¥è‡ªæ ·æœ¬æ‰°åŠ¨ï¼Œéšæœºæ—æ¨¡å‹ä¸­å¼•å…¥äº†å±æ€§æ‰°åŠ¨ï¼Œè¿™æ ·ä½¿å¾—æœ€ç»ˆæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å¯ä»¥é€šè¿‡ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´çš„å·®å¼‚åº¦çš„å¢åŠ è€Œè¿›ä¸€æ­¥æå‡ã€‚

- å’Œ bagging ç›¸æ¯”ï¼Œéšæœºæ£®æ—çš„èµ·å§‹æ€§èƒ½å¾€å¾€æ¯”è¾ƒå·®ï¼Œç„¶è€Œéšç€ä¸ªä½“å­¦ä¹ å™¨æ•°ç›®çš„å¢åŠ ï¼Œéšæœºæ£®æ—ä¼šæ”¶æ•›åˆ°æ›´å°çš„è¯¯å·®ã€‚

- éšæœºæ£®æ—çš„è®­ç»ƒæ•ˆç‡ä¼˜äº baggingï¼Œå› ä¸º bagging ä¸­çš„æ¯æ£µæ ‘æ˜¯å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œè€ƒå¯Ÿï¼Œè€Œéšæœºæ£®æ—ä»…ä»…è€ƒè™‘ä¸€ä¸ªç‰¹å¾å­é›†ï¼ˆmax_featuresï¼šéšæœºæ£®æ—å…è®¸å•ä¸ªå†³ç­–æ ‘ä½¿ç”¨ç‰¹å¾çš„æœ€å¤§æ•°é‡ã€‚ï¼‰ã€‚

å› ä¸º Bagging ä½¿ç”¨çš„æœ‰æ”¾å›é‡‡æ ·ï¼Œæ‰€ä»¥ BaggingClassifier or RandomForestClassifier éƒ½å…·æœ‰ oob*score*å±æ€§ï¼šçº¦æœ‰ 37%ï¼ˆ$\lim_{n \to -\infty}(1-1/n)^n=1/e$ï¼‰çš„æ ·æœ¬æ²¡æœ‰ç”¨æ¥è®­ç»ƒ,è¿™ä¸€éƒ¨åˆ†ç§°ä¸º out-of-bag(oob),å› ä¸ºæ¨¡å‹æ²¡æœ‰è§è¿‡è¿™éƒ¨åˆ†æ ·æœ¬ï¼Œæ‰€ä»¥å¯ä»¥æ‹¿æ¥å½“éªŒè¯é›†åˆï¼Œè€Œä¸éœ€è¦å†åˆ’åˆ†éªŒè¯é›†æˆ–è€…äº¤å‰éªŒè¯äº†ã€‚ æ¯”å¦‚æˆ‘ä»¬è®¡ç®— accuracy*score æ—¶ï¼Œä¹Ÿå¯ä»¥çœ‹ä¸‹ oob_score*çš„æƒ…å†µ

### å‚è€ƒæ–‡çŒ®

[8-1] Freund Yï¼ŒSchapire RE. [A short introduction to boosting](http://www.cs.columbia.edu/~jebara/6772/papers/IntroToBoosting.pdf). Journal of Japanese Societyfor Artificial Intelligence,1999,14(5): 771â€“780

[8-2] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ï¼Œç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[8-3] Valiant LG. [A theory of the learnable](http://web.mit.edu/6.435/www/Valiant84.pdf). Communications of the ACM,1984,27(11):1134â€“1142

[8-4] Schapire R. [The strength of weak learnability](https://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf). Machine Learning,1990,5(2): 197â€“227

[8-5] Freund Y,Schapire RE. [A decision-theoretic generalization of on-line learning and anapplication to boosting](https://www.ee.columbia.edu/~sfchang/course/spr/papers/freund95decisiontheoretic-adaboost.pdf). Computational Learning Theory. Lecture Notes in ComputerScience,Vol. 904,1995,23â€“37 ï¼ˆ[55, 119-139 (1997)](http://www.cim.mcgill.ca/~dmeger/mrlRead/papers/Boosting/AdaOrig.pdf)ï¼‰

[8-6] Friedman J,Hastie T,Tibshirani R. [Additive logistic regression: a statistical view ofboosting(with discussions)](https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf). Annals of Statistics,2000,28: 337â€“407

[8-7] Friedman J. [Greedy function approximation: a gradient boosting machine](http://biostat.jhsph.edu/~mmccall/articles/friedman_1999.pdf). Annals ofStatistics,2001,29(5)

[8-8] Schapire RE,Singer Y. [Improved boosting algorithms using confidence-ratedpredictions](<https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1999-ML-Improved%20boosting%20algorithms%20using%20confidence-rated%20predictions%20(Schapire%20y%20Singer).pdf>). Machine Learning,1999,37(3): 297â€“336

[8-9] Collins M,Schapire R E,Singer Y. [Logistic regression,AdaBoost and Bregmandistances](https://link.springer.com/content/pdf/10.1023%2FA%3A1013912006537.pdf). Machine Learning Journal,2004

## ç¬¬ 9 ç«  EM ç®—æ³•åŠå…¶æ¨å¹¿

EM ç®—æ³•ï¼ˆ[Expectationâ€“maximization algorithm](https://en.jinzhao.wiki/wiki/Expectation%E2%80%93maximization_algorithm)ï¼‰æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œç”¨äºå«æœ‰**éšå˜é‡**ï¼ˆhiddenï¼ˆunseen or unmeasurableï¼‰ variable or Latent variableï¼‰çš„æ¦‚ç‡æ¨¡å‹å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæˆ–æå¤§åéªŒæ¦‚ç‡ä¼°è®¡ã€‚EM ç®—æ³•çš„æ¯æ¬¡è¿­ä»£ç”±ä¸¤æ­¥ç»„æˆï¼šE æ­¥ï¼Œæ±‚æœŸæœ›ï¼ˆexpectationï¼‰ï¼›M æ­¥ï¼Œæ±‚æå¤§ï¼ˆmaximizationï¼‰ã€‚æ‰€ä»¥è¿™ä¸€ç®—æ³•ç§°ä¸ºæœŸæœ›æå¤§ç®—æ³•ï¼ˆexpectation maximization algorithmï¼‰ï¼Œç®€ç§° EM ç®—æ³•ã€‚

æ¦‚ç‡æ¨¡å‹æœ‰æ—¶æ—¢å«æœ‰è§‚æµ‹å˜é‡ï¼ˆobservable variableï¼‰ï¼Œåˆå«æœ‰éšå˜é‡æˆ–æ½œåœ¨å˜é‡ï¼ˆlatent variableï¼‰ã€‚å¦‚æœæ¦‚ç‡æ¨¡å‹çš„å˜é‡éƒ½æ˜¯è§‚æµ‹å˜é‡ï¼Œé‚£ä¹ˆç»™å®šæ•°æ®ï¼Œå¯ä»¥ç›´æ¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ³•ï¼Œæˆ–è´å¶æ–¯ä¼°è®¡æ³•ä¼°è®¡æ¨¡å‹å‚æ•°ã€‚ä½†æ˜¯ï¼Œå½“æ¨¡å‹å«æœ‰éšå˜é‡æ—¶ï¼Œå°±ä¸èƒ½ç®€å•åœ°ä½¿ç”¨è¿™äº›ä¼°è®¡æ–¹æ³•ã€‚EM ç®—æ³•å°±æ˜¯å«æœ‰éšå˜é‡çš„æ¦‚ç‡æ¨¡å‹å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ³•ï¼Œæˆ–æå¤§åéªŒæ¦‚ç‡ä¼°è®¡æ³•ã€‚

æœ¬ç« é¦–å…ˆå™è¿° EM ç®—æ³•ï¼Œç„¶åè®¨è®º EM ç®—æ³•çš„æ”¶æ•›æ€§ï¼›ä½œä¸º EM ç®—æ³•çš„åº”ç”¨ï¼Œä»‹ç»é«˜æ–¯æ··åˆæ¨¡å‹çš„å­¦ä¹ ï¼›æœ€åå™è¿° EM ç®—æ³•çš„æ¨å¹¿â€”â€”GEM ç®—æ³•ï¼ˆgeneralized expectation maximization (GEM) algorithmï¼Œå¹¿ä¹‰ EM ç®—æ³•ï¼‰ã€‚

> EM ç®—æ³•æ˜¯ä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œä¸æ˜¯ä¸€ä¸ªç»Ÿè®¡å­¦ä¹ æ¨¡å‹ã€‚
> EM ç®—æ³•ä¼˜ç‚¹:ä¸éœ€è¦è°ƒå‚æ•°ï¼Œæ²¡æœ‰è¶…å‚æ•°ï¼›ç¼–ç¨‹ç®€å•ï¼Œåªéœ€è¦è¿­ä»£ï¼›ç†è®ºä¼˜ç¾ï¼Œæ”¶æ•›æ€§ã€‚
> EM ç®—æ³•çš„æ¨å¹¿:F å‡½æ•°ï¼ˆF functionï¼‰ çš„æå¤§-æå¤§ç®—æ³• F-MM or MMï¼ˆmaximization maximization algorithmï¼‰ï¼ŒMCEMï¼ŒVBEM or VEMï¼ŒGEM

è§‚æµ‹æ•°æ®$X=\{x_i\}_{i=1}^N$ ï¼Œå¯¹åº”çš„éšå«(éšè—)æ•°æ®$Z=\{z_i\}_{i=1}^N$ï¼Œæ¨¡å‹å‚æ•°$\theta$ï¼Œå®Œå…¨æ•°æ®$T=\{(x_1,z_1),...,(x_N,z_N)\}$

> æ³¨æ„ä¸‹åˆ—å…¬å¼ä¸­$|$ä¹Ÿæœ‰ç”¨$;$è¡¨ç¤ºçš„ï¼Œæ˜¯ä¸€ä¸ªæ„æ€ï¼Œå› ä¸º$\theta$æ˜¯ä¸€ä¸ªæœªçŸ¥çš„å‚æ•°ï¼Œæœ€å¼€å§‹æ—¶ä¼šåˆå§‹åŒ–$\theta^{(0)}$
> å…¬å¼ä¸­çš„å¤§ P å’Œå° p ä»¥åŠå¤§ X å’Œå° x æ²¡æœ‰ç»Ÿä¸€ï¼Œä¸æ˜¯å¾ˆä¸¥è°¨

å¦‚æœä¸è€ƒè™‘éšè—æ•°æ®ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ä¼°è®¡å‡ºå‚æ•° $\theta$ :
$$\theta_{MLE} = \arg\max\limits_\theta\log p(x|\theta) = \arg\max\limits_\theta\sum_i^N\log p(x_i|\theta) $$

ä½†ç”±äºéšè—æ•°æ®çš„å­˜åœ¨ï¼Œ æˆ‘ä»¬æœ‰ x çš„è¾¹é™…ä¼¼ç„¶å‡½æ•°ï¼ˆ[Marginal Likelihood](https://en.jinzhao.wiki/wiki/Marginal_likelihood)ï¼‰,åœ¨è´å¶æ–¯ç»Ÿè®¡çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¾¹é™…ä¼¼ç„¶ä¹Ÿç§°ä¸ºè¯æ®ï¼ˆEvidenceï¼‰
$$p(x|\theta) = \sum_{z} p(x, z|\theta) = \int_Z p(x, z|\theta)dz$$
å°† x çš„è¾¹é™…ä¼¼ç„¶å‡½æ•°å¸¦å…¥æå¤§ä¼¼ç„¶ä¼°è®¡ä¸­ï¼Œåœ¨ log é‡Œé¢ä¼šå‡ºç°ç§¯åˆ†(æ±‚å’Œ)ç¬¦å·ï¼Œå¯¼è‡´å¯¹ä¼¼ç„¶å‡½æ•°çš„æ±‚å¯¼å˜å¾—å›°éš¾ï¼Œæ— æ³•æ±‚è§£ã€‚å¯¹äºè¿™ç§æ— æ³•ç›´æ¥æ±‚è§£çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé‡‡ç”¨è¿­ä»£æ±‚è§£çš„ç­–ç•¥ï¼Œä¸€æ­¥ä¸€æ­¥é€¼è¿‘æœ€ç»ˆçš„ç»“æœï¼Œåœ¨ EM ç®—æ³•ä¸­å°±æ˜¯ E æ­¥å’Œ M æ­¥çš„äº¤æ›¿è¿›è¡Œï¼Œç›´è‡³æ”¶æ•›ã€‚

ä¸‹é¢æ¥ä»‹ç»**EM ç®—æ³•**ï¼š

1. éšæœºåŒ–å‚æ•°$\theta^{(0)}$çš„åˆå§‹å€¼ï¼›
2. å‡è®¾åœ¨ç¬¬ $t$ æ¬¡è¿­ä»£åï¼Œå‚æ•°çš„ä¼°è®¡å€¼ä¸º $\theta^{(t)}$ ï¼Œå¯¹äºç¬¬ $t+1$ æ¬¡è¿­ä»£ï¼Œå…·ä½“åˆ†ä¸ºä¸¤æ­¥ï¼š
   a. E-stepï¼šæ±‚æœŸæœ›
   Q å‡½æ•°çš„å®šä¹‰ï¼š
   $$Q(\theta,\theta^{(t)}) = \int_Z P(Z|X,\theta^{(t)}) \log P(X,Z|\theta) dZ \\ =\mathbb{E}_{Z|X,\theta^{(t)}}[\log P(X,Z|\theta)]$$
   **å…¨æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°$\log p(x, z|\theta)$å…³äºåœ¨ç»™å®šè§‚æµ‹æ•°æ®$X$å’Œå½“å‰å‚æ•°$\theta^{(t)}$ä¸‹å¯¹æœªè§‚æµ‹æ•°æ®$Z$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$p(z|x,\theta^{(t)})$çš„æœŸæœ›ç§°ä¸º Q å‡½æ•°**

   > æ³¨æ„ Q å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è‡ªå˜é‡ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯å·²çŸ¥çš„ï¼ˆä¸Šä¸€æ­¥æ±‚å¾—çš„ï¼‰

   b. M-step: æœ€å¤§åŒ–$Q(\theta,\theta^{(t)})$ å¹¶æ±‚è§£$\theta^{(t+1)}$
   $$\theta^{(t+1)} = \arg\max\limits_\theta Q(\theta, \theta^{(t)}) $$

3. é‡å¤ 2ï¼Œç›´åˆ°æ”¶æ•›ã€‚
   ä¸€èˆ¬åˆ¤æ–­æ”¶æ•›æœ‰ä¸¤ç§æ–¹æ³•ï¼š1. åˆ¤æ–­å‚æ•°æ˜¯å¦æ”¶æ•›$\theta^{(t+1)}-\theta^{(t)} \leq \varepsilon$ï¼›2. åˆ¤æ–­å‡½æ•°å€¼æ˜¯å¦æ”¶æ•›$Q(\theta^{(t+1)},\theta^{(t)})-Q(\theta^{(t)},\theta^{(t)}) \leq \varepsilon$ã€‚

**EM ç®—æ³•çš„å¯¼å‡º**ï¼š
ä¸ºä»€ä¹ˆ EM ç®—æ³•èƒ½è¿‘ä¼¼å®ç°å¯¹è§‚æµ‹æ•°æ®çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Ÿ
ä¹¦ä¸­çš„æ¨å¯¼æˆ‘è¿™é‡Œå°±ä¸é‡å¤äº†(ä¹¦ä¸­ç”¨çš„ Jensen ä¸ç­‰å¼æ¨å¯¼)ï¼Œè¿™é‡Œä»‹ç»å˜åˆ†æ³•/ELBO+KL
$$P(X|\theta) = \frac{P(X,Z|\theta)}{P(Z|X, \theta)} \implies \log P(X|\theta) = \log P(X,Z|\theta) - \log P(Z|X,\theta) $$
æ ¹æ®å˜åˆ†æ¨æ–­çš„æ€æƒ³ï¼šå¯»æ‰¾ä¸€ä¸ªç®€å•åˆ†å¸ƒ$q(z)$æ¥è¿‘ä¼¼æ¡ä»¶æ¦‚ç‡å¯†åº¦$p(z|x)$
$$\log P(X|\theta) = \log \frac{P(X,Z|\theta)}{q(Z)} - \log \frac{P(Z|X,\theta)}{q(Z)} $$
ç„¶åä¸¤è¾¹åŒæ—¶æ±‚å…³äºå˜é‡ $Z$ çš„æœŸæœ›
$$\mathbb{E}_Z[\log P(X|\theta)] = \mathbb{E}\_Z[\log \frac{P(X,Z|\theta)}{q(Z)}] - \mathbb{E}\_Z[\log \frac{P(Z|X,\theta)}{q(Z)}]$$
å°†æœŸæœ›å†™æˆç§¯åˆ†çš„å½¢å¼
$$\int_Z q(Z)\log P(X|\theta)dZ = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ - \int_Zq(Z)\log \frac{P(Z|X,\theta)}{q(Z)}dZ$$
ç­‰å¼å·¦è¾¹å’Œ$Z$æ— å…³ï¼ˆ$\int_Zq(Z)dZ = 1$ï¼‰ï¼Œæ‰€ä»¥
$$\log P(X|\theta) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ - \int_Zq(Z)\log \frac{P(Z|X,\theta)}{q(Z)}dZ \\ = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ + D_{KL}(q(Z)||P(Z|X,\theta)) \\= ELBO + D*{KL}(q(Z)||P(Z|X,\theta))$$
æˆ‘ä»¬ç›´æ¥ä»¤$D*{KL} = 0, å³ q(Z)=P(Z|X,\theta^{(t)})$ï¼Œç„¶åæœ€å¤§åŒ–ELBO
$$\hat{\theta} = \argmax_{\theta}\int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ \\ = \argmax_{\theta}\int_Z P(Z|X,\theta^{(t)})\log \frac{P(X,Z|\theta)}{P(Z|X,\theta^{(t)})}dZ$$
$\theta^{(t)}$æ˜¯ä¸Šä¸€æ­¥æ±‚å‡ºçš„ï¼Œå¯ä»¥çœ‹ä½œå·²çŸ¥çš„å‚æ•°
$$\hat{\theta} = \argmax_{\theta} \int_Z P(Z|X,\theta^{(t)})\log P(X,Z|\theta)dZ - \int_Z P(Z|X,\theta^{(t)})\log {P(Z|X,\theta^{(t)})}dZ \\  = \argmax_{\theta} \int_Z P(Z|X,\theta^{(t)})\log P(X,Z|\theta)dZ - C \\  = \argmax_{\theta} \int_Z P(Z|X,\theta^{(t)})\log P(X,Z|\theta)dZ \\= \argmax_{\theta} Q(\theta,\theta^{(t)})$$

> å‚è€ƒ[EM ç®—æ³• 11.2.2.1 èŠ‚](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)
> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•ï¼ˆELBO+KL å½¢å¼ï¼‰](https://zhuanlan.zhihu.com/p/365641813)
> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•-Jensen ä¸ç­‰å¼](https://zhuanlan.zhihu.com/p/366365408)

**EM ç®—æ³•çš„æ”¶æ•›æ€§**ï¼š
è¯æ˜$p(\mathbf {X} \mid {\boldsymbol {\theta }})$æ˜¯å•è°ƒé€’å¢çš„ï¼ˆæ¦‚ç‡å¤§äºç­‰äº 0ï¼Œå°äºç­‰äº 1ï¼Œå•è°ƒå¢ä¸€å®šèƒ½æ”¶æ•›ï¼‰ï¼Œå°±æ˜¯è¯æ˜$\log p(\mathbf {X} \mid {\boldsymbol {\theta }})$æ˜¯å•è°ƒé€’å¢çš„ï¼Œå³ï¼š
$$\log p(\mathbf {X} \mid {\boldsymbol {\theta^{(t+1)} }}) \geq \log p(\mathbf {X} \mid {\boldsymbol {\theta^{(t)} }})$$

- æ–¹æ³•ä¸€ï¼šæ ¹æ®ç»´åŸºç™¾ç§‘
  $${\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})=\log p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})-\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}).}$$
  $${\displaystyle {\begin{aligned}\log p(\mathbf {X} \mid {\boldsymbol {\theta }})&=\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})-\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})\\&=Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}),\end{aligned}}}$$
  $${\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})-\log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)}) =Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-H({\boldsymbol {\theta } }^{(t)}\mid {\boldsymbol {\theta }}^{(t)}),}$$
  ç”±**å‰å¸ƒæ–¯ä¸ç­‰å¼(Gibbs' inequality)**${\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})\geq H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})}$å¾—ï¼š
  $${\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})-\log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)})\geq Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)}).}$$
  å› ä¸º M-step: æœ€å¤§åŒ–$Q(\theta,\theta^{(t)})$ï¼Œæ‰€ä»¥$Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}) \geq Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})$
  æ‰€ä»¥$\log p(\mathbf {X} \mid {\boldsymbol {\theta }})$æ˜¯å•è°ƒå¢å‡½æ•°

- æ–¹æ³•äºŒï¼šæ ¹æ®ç»Ÿè®¡å­¦ä¹ æ–¹æ³•
  ä¸åŒçš„æ˜¯ç”¨çš„**Jensen ä¸ç­‰å¼**
  $$H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}) -H({\boldsymbol {\theta } }^{(t)}\mid {\boldsymbol {\theta }}^{(t)}) = -\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}) + \sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}}) \\ = -\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log \frac{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}})} \\ \geq -\log \bigg( \sum _{\mathbf {Z} } \frac{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}})} p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)}) \bigg) \\= -\log \bigg( \sum _{\mathbf {Z} } p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}) \bigg) = -\log(1) = 0$$
  å› ä¸º$\sum _{\mathbf {Z} } p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}}) = 1$,log æ˜¯å•è°ƒå¢å‡½æ•°ï¼Œæ‰€ä»¥ Jensen ä¸ç­‰å¼æˆç«‹ã€‚

- æ–¹æ³•ä¸‰ï¼šæ ¹æ® **KL divergence çš„å®šä¹‰**ï¼Œå¹¶ä¸”å¤§äºç­‰äº 0
  $$H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}) -H({\boldsymbol {\theta } }^{(t)}\mid {\boldsymbol {\theta }}^{(t)}) = -\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log \frac{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}})} \\ = D_{KL}(p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)}) || p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})) \geq 0$$

> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•-æ”¶æ•›æ€§è¯æ˜](https://zhuanlan.zhihu.com/p/367072875)

**å¹¿ä¹‰ EM**ï¼ˆgeneralized expectation maximization (GEM) algorithmï¼‰
æˆ‘ä»¬ä»¥ EM ç®—æ³•çš„ ELBO+KL å½¢å¼ä¸ºä¾‹(ä¹¦ä¸­çš„å½¢å¼è‡ªå·±äº†è§£ï¼Œè¿™é‡Œä¸åšä»‹ç»)
$$\log P(X|\theta) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ + D_{KL}(q(Z)||P(Z|X,\theta))$$
ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä»¤
$$\mathcal{L}(q, \theta) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ$$
å…¶å®å°±æ˜¯å˜åˆ†å‡½æ•°ï¼ˆæ³›å‡½çš„æå€¼é—®é¢˜ï¼Œè¾“å…¥æ˜¯å‡½æ•° qï¼‰
åœ¨å‰é¢ï¼Œæˆ‘ä»¬ä¸€ç›´æ˜¯è®©$D_{KL} = 0$ï¼Œç„¶åæœ€å¤§åŒ– $\mathcal{L}(q,\theta)$ ä»¥å¢å¤§ $\log P(X|\theta)$ ã€‚ä½†æ˜¯æˆ‘ä»¬ç°åœ¨æ— æ³•ç›´æ¥è®©$q(Z) = P(Z|X,\theta)$ ï¼Œå› ä¸ºåéªŒæœ¬èº«ä¹Ÿæ¯”è¾ƒéš¾æ±‚ï¼Œæ‰€ä»¥æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ±‚å‡ºæŸä¸ª $q(Z)$ ï¼Œèƒ½å¤Ÿä½¿å¾—åœ¨å›ºå®š $\theta$ æ—¶ $D_{KL}$ å°½å¯èƒ½å°ï¼Œå°½å¯èƒ½ç­‰äº 0ï¼š
$$q = \arg\min\limits_q D_{KL}(q(Z)||P(Z|X,\theta))= \arg\max\limits_q \mathcal{L}(q, \theta)$$

- E-stepï¼šå›ºå®š$\theta = \theta^{(t)}$
  $$q^{(t)} = \arg\max\limits_q \mathcal{L}(q, \theta^{(t)})$$
- M-stepï¼šå›ºå®š$q = q^{(t)}$
  $$\theta^{(t+1)} = \arg\max\limits_\theta \mathcal{L}(q^{(t)}, \theta)$$

> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•-å¹¿ä¹‰ EM](https://zhuanlan.zhihu.com/p/367076459)

**é«˜æ–¯æ··åˆæ¨¡å‹**ï¼ˆ[Gaussian mixture model](https://en.jinzhao.wiki/wiki/Mixture_model#Gaussian_mixture_model)ï¼‰ï¼š

æœ‰æ ·æœ¬(è§‚æµ‹æ•°æ®)$Data = \{x_1,...,x_N\}$ï¼Œç”Ÿæˆæ¨¡å‹ï¼Œå¯¹å®Œå…¨æ•°æ®$T = \{(x_1,z_1),...,(x_N,z_N)\}$å»ºæ¨¡ï¼Œç»™å®šè¾“å…¥$x_i$é¢„æµ‹ï¼š$\text{arg max}_{k \in \{1, \dots, K \}} P(Z_i = k \mid \boldsymbol{x}_i ; \hat{\Theta})$ï¼Œæƒ³è¦é¢„æµ‹æ¨¡å‹ï¼Œå°±éœ€è¦æ±‚å‡ºæ¨¡å‹çš„å‚æ•°$\hat{\Theta}$ï¼Œå…¶ä¸­
$$\begin{align*} P(Z_i = k \mid \boldsymbol{x}_i ; \hat{\Theta}) &= \frac{p(\boldsymbol{x}_i \mid Z_i = k ; \hat{\Theta})P(Z_i = k; \hat{\Theta})}{\sum_{h=1}^K p(\boldsymbol{x}_i \mid Z_i = h ; \hat{\Theta})P(Z_i = h; \hat{\Theta})} \\ &= \frac{\phi(\boldsymbol{x}_i \mid \hat{\boldsymbol{\mu}}_k, \hat{\boldsymbol{\Sigma}}_k) \hat{\alpha}_k}{\sum_{h=1}^K \phi(\boldsymbol{x}_i \mid \hat{\boldsymbol{\mu}_h}, \hat{\boldsymbol{\Sigma}}_h) \hat{\alpha}_h} \end{align*}$$

- **æ¨¡å‹**ï¼š
  $$P(x;\theta) = \sum_{k=1}^K P(x,z=k;\theta) = \sum_{k=1}^K \underbrace{P(x|z=k;\theta)}_{\text{æœä»é«˜æ–¯åˆ†å¸ƒ}} \underbrace{P(z=k;\theta)}_{\text{å±äºkç±»çš„æ¦‚ç‡}} \\ = \sum_{k=1}^K \alpha_k \phi(x;\theta_k)$$
  å…¶ä¸­$\alpha_k \geq 0, \sum_{k=1}^K \alpha_k = 1$æ˜¯ç³»æ•°,$\theta_k = (\mu_k,\sigma_k^2)$,
  $$\phi(x;\theta_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}\exp(-\frac{(x-\mu_k)^2}{2\sigma_k^2})$$

- **ç­–ç•¥**ï¼š
  æ±‚å–å‚æ•°$\hat{\Theta} $ï¼Œä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡
  $$\hat{\Theta} := \argmax_{\Theta} \prod_{i=1}^N p(\boldsymbol{x}_i ; \Theta) \\ = \argmax_{\Theta} \prod_{i=1}^N \sum_{k=1}^K \alpha_k \phi(x_i;\theta_k)$$
  log æ±‚è§£ï¼Œä¼šå‘ç° log ä¸­æœ‰æ±‚å’Œï¼Œæ±‚å¯¼å¾ˆéš¾è§£å‡ºæ¥

- **ç®—æ³•**ï¼š
  æ ¹æ® EM ç®—æ³•ç¡®å®š Q å‡½æ•°
  $$Q(\theta,\theta^{(t)}) = \sum_{Z \in \{1,2...K\}} P(Z|X,\theta^{(t)}) \log P(X,Z|\theta) \\ = \sum_{Z_1,Z_2,...,Z_N} \bigg(\prod_{i=1}^N P(z_i|x_i,\theta^{(t)})\log \prod_{i=1}^N P(x_i,z_i|\theta) \bigg) \\ = \sum_{Z_1,Z_2,...,Z_N} \bigg(\prod_{i=1}^N P(z_i|x_i,\theta^{(t)}) \sum_{i=1}^N \log P(x_i,z_i|\theta) \bigg)$$
  å–ç¬¬ä¸€é¡¹åˆ†æï¼š
  $$\begin{equation*} \begin{split} &\sum_{z_1,z_2,\cdots,z_N}\log P(x_1,z_1|\theta)\cdot \prod_{i=1}^N P(z_i|x_i,\theta^{(t)})\\ =&\sum_{z_1,z_2,\cdots,z_N} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) \cdot \prod_{i=2}^N P(z_i|x_i,\theta^{(t)})\\ =& \sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) \sum_{z_2,\cdots,z_N}\prod_{i=2}^N P(z_i|x_i,\theta^{(t)})\\ =& \sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) \sum_{z_2} P(z_2|x_2,\theta^{(t)})\sum_{z_3} P(z_3|x_3,\theta^{(t)}) \cdots \sum_{z_N} P(z_N|x_N,\theta^{(t)}) \end{split}\end{equation*}$$
  ä¸Šå¼ä¸­ $\sum_{z_i} P(z_i|x_i,\theta^{(t)}) =1$ï¼Œå› æ­¤å¯ç®€åŒ–ä¸ºï¼š
  $$\sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)})$$
  å°†å…¶å¸¦å…¥åŸå¼ï¼š
  $$\begin{equation*}\begin{split} Q(\theta, \theta^{(t)})&= \sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) +\cdots + \sum_{z_N} \log P(x_N,z_N|\theta)\cdot P(z_N|x_N,\theta^{(t)}) \\ &=\sum_{i=1}^N \sum_{z_i} \log P(x_i,z_i|\theta)\cdot P(z_i|x_i,\theta^{(t)})  \end{split}\end{equation*}$$

å…·ä½“ç®—æ³•æ±‚è§£å‚è€ƒï¼š[Gaussian mixture models](https://mbernste.github.io/posts/gmm_em/) ä»¥åŠ[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åä¸€)-é«˜æ–¯æ··åˆæ¨¡å‹ GMMï¼ˆGaussian Mixture Modelï¼‰](https://www.bilibili.com/video/BV13b411w7Xj?p=3)
[æå¤§ä¼¼ç„¶ä¼°è®¡ã€EM ç®—æ³•åŠé«˜æ–¯æ··åˆæ¨¡å‹](https://blog.csdn.net/chris_xy/article/details/88970322)
[EM ç®—æ³•ä¸ GMMï¼ˆé«˜æ–¯æ··åˆèšç±»ï¼‰Jensen ä¸ç­‰å¼å’Œå˜åˆ†æ³•ä¸¤ç§æ¨å¯¼](https://zhuanlan.zhihu.com/p/50686800)
[Expectation-maximization algorithm EM ç®—æ³•](https://encyclopedia.thefreedictionary.com/Expectation-maximization+algorithm)
[Mixture model](https://encyclopedia.thefreedictionary.com/Mixture+model)
[Gaussian Mixture Model](https://brilliant.org/wiki/gaussian-mixture-model/)

### é™„åŠ çŸ¥è¯†

#### å˜åˆ†æ¨æ–­

**å˜åˆ†æ¨æ–­**ï¼ˆ[Variational Inference](https://en.jinzhao.wiki/wiki/Variational_Bayesian_methods)ï¼‰ä¹Ÿç§°ä¸ºå˜åˆ†è´å¶æ–¯ï¼ˆVariational Bayesianï¼‰ï¼Œè€Œå˜åˆ†æ³•ä¸»è¦æ˜¯ç ”ç©¶å˜åˆ†é—®é¢˜ï¼Œå³æ³›å‡½çš„æå€¼é—®é¢˜ï¼ˆå‡½æ•°çš„è¾“å…¥ä¹Ÿæ˜¯å‡½æ•°ï¼‰ï¼Œæ ¹æ®è´å¶æ–¯å…¬å¼ï¼ŒåéªŒæ¦‚ç‡
$${\displaystyle P(\mathbf {Z} \mid \mathbf {X} )={\frac {P(\mathbf {X} \mid \mathbf {Z} )P(\mathbf {Z} )}{P(\mathbf {X} )}}={\frac {P(\mathbf {X} \mid \mathbf {Z} )P(\mathbf {Z} )}{\int _{\mathbf {Z} }P(\mathbf {X} ,\mathbf {Z} ')\,d\mathbf {Z} '}}}$$

ä¸Šé¢å…¬å¼ä¸­çš„ç§¯åˆ†å¯¹äºå¾ˆå¤šæƒ…å†µä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼ˆæ‰€ä»¥æœ‰äº›æ¨¡å‹å¿½ç•¥äº† P(x)ï¼‰ï¼Œè¦ä¹ˆç§¯åˆ†æ²¡æœ‰é—­å¼è§£ï¼Œè¦ä¹ˆæ˜¯æŒ‡æ•°çº§åˆ«çš„è®¡ç®—å¤æ‚åº¦ï¼Œæ‰€ä»¥å¾ˆéš¾æ±‚å‡ºåéªŒæ¦‚ç‡ï¼Œè¿™æ—¶æˆ‘ä»¬éœ€è¦å¯»æ‰¾ä¸€ä¸ªç®€å•åˆ†å¸ƒ${\displaystyle q(\mathbf {Z} )\approx P(\mathbf {Z} \mid \mathbf {X} )}$ï¼Œè¿™æ ·æ¨æ–­é—®é¢˜è½¬åŒ–æˆä¸€ä¸ªæ³›å‡½ä¼˜åŒ–é—®é¢˜ï¼š
$$\hat{q(Z)} = \argmin_{q(Z) \in å€™é€‰çš„æ¦‚ç‡åˆ†å¸ƒæ—Q} KL(q(Z)|P(Z|X))$$

æˆ‘ä»¬æœ‰ä¸Šé¢**EM ç®—æ³•çš„å¯¼å‡º**å¾—åˆ°
$${\displaystyle \log P(\mathbf {X} )=D_{\mathrm {KL} }(q\parallel P)+{\mathcal {L}}(q)}$$
KL-divergence å¤§äºç­‰äº 0ï¼Œæ‰€ä»¥$\log P(\mathbf {X} ) \geq {\mathcal {L}}(q)$ï¼Œæ‰€ä»¥${\mathcal {L}}(q)$ç§°ä¸ºè¯æ®ä¸‹ç•Œï¼ˆ[Evidence Lower BOund,ELBO](https://en.jinzhao.wiki/wiki/Evidence_lower_bound)ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„å˜åˆ†å‡½æ•°ã€‚

KL-divergence ä¸­æœ‰åéªŒæ¦‚ç‡ P(Z|X)ï¼Œæœ¬èº«å°±æ˜¯éš¾ä»¥è®¡ç®—ï¼Œæ‰æƒ³æ‰¾ä¸ªç®€å•åˆ†å¸ƒ q(z)æ¥è¿‘ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¼˜åŒ– KL-divergenceï¼Œè¿›è€Œè½¬åŒ–æˆä¼˜åŒ– ELBO
$$\hat{q(Z)} = \argmax_{q(z) \in Q} ELBO(q,x)$$

åˆ†å¸ƒæ— Q ä¸€èˆ¬é€‰æ‹©æ˜¯**å¹³å‡åœº**ï¼ˆMean fieldï¼‰åˆ†å¸ƒæ—ï¼Œå³å¯ä»¥å°† Z æ‹†åˆ†ä¸ºå¤šç»„ç›¸äº’ç‹¬ç«‹çš„å˜é‡ï¼Œé‚£ä¹ˆ
$$q({\mathbf  {Z}})=\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m})$$

é‚£ä¹ˆ
$$ELBO(q,x) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ \\ = \int_Z (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \log P(X,Z|\theta)dZ - \int_Z (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \log \prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m})dZ$$

å‡è®¾æˆ‘ä»¬åªå…³å¿ƒå…¶ä¸­ä¸€ä¸ªå­é›†(åˆ†é‡)$Z_j$çš„è¿‘ä¼¼åˆ†å¸ƒ$q_j(Z_j)$ï¼ˆå…ˆæ±‚ä¸€ä¸ªå­é›†ï¼Œå…¶å®ƒå­é›†ä¹Ÿå°±æ±‚å‡ºæ¥äº†ï¼‰
å…ˆçœ‹å‡å·åé¢çš„é¡¹(è¿›è¡Œå±•å¼€)ï¼š
$$\int_Z (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \sum_{m=1}^M \log q_{m}({\mathbf  {Z}}_{m})dZ \\ = \int_Z q_1(Z_1).q_2(Z_2)...q_M(Z_M).[\log q_1(Z_1)+...+\log q_M(Z_M)]dZ$$

åªçœ‹å…¶ä¸­ä¸€é¡¹
$$\int_{Z_1Z_2...Z_M} q_1(Z_1).q_2(Z_2)...q_M(Z_M).\log q_1(Z_1){dZ_1dZ_2...dZ_M} = \int_{Z_1} q_1(Z_1).\log q_1(Z_1)dZ_1.\int_{Z_2}q_2(Z_2)dZ_2....\int_{Z_M}q_M(Z_M)dZ_M$$

é‚£ä¹ˆæœ€ç»ˆæˆ‘ä»¬å¾—åˆ°å‡å·åé¢çš„é¡¹
$$\sum_{m=1}^M \int_{Z_m} q_m(Z_m).\log q_m(Z_m)dZ_m$$

å‰é¢è¯´äº†ï¼Œæˆ‘ä»¬åªå…³æ³¨å…¶ä¸­ä¸€ä¸ªå­é›†$Z_j$ï¼Œå…¶å®ƒ$m \neq j$çš„å¯¹å…¶æ¥è¯´å¯ä»¥çœ‹ä½œå¸¸æ•°é¡¹ï¼Œå¾—åˆ°
$$\int_{Z_j} q_j(Z_j).\log q_j(Z_j)dZ_j + C$$

å†çœ‹å‡å·å‰é¢çš„é¡¹
$$\int_{Z_1Z_2...Z_M} (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \log P(X,Z|\theta){dZ_1dZ_2...dZ_M} \\= \int_{Z_j}q_j(Z_j) \bigg(\int_{Z_i} \prod_{i \neq j}^M q_i(Z_i) \log P(X,Z|\theta) dZ_i \bigg)dZ_j \\= \int_{Z_j}q_j(Z_j) \bigg(E_{\prod_{i \neq j}^M q_i(Z_i)}[\log P(X,Z|\theta)] \bigg)dZ_j \\= \int_{Z_j}q_j(Z_j) \bigg(\log \tilde{p}(X,Z_j) \bigg)dZ_j$$

æœ€ç»ˆï¼š
$$ELBO(q,x) =\int_{Z_j}q_j(Z_j) \bigg(\log \tilde{p}(X,Z_j) \bigg)dZ_j - \int_{Z_j} q_j(Z_j).\log q_j(Z_j)dZ_j - C \\= -KL(q_j(Z_j) \| \tilde{p}(X,Z_j)) -C$$
æ‰€ä»¥ï¼š
$$\argmax ELBO(q,x) = \argmin KL(q_j(Z_j) \| \tilde{p}(X,Z_j))$$
å½“$q_j(Z_j) = \tilde{p}(X,Z_j)$å– KL æœ€å°å€¼

å…·ä½“æ±‚æœ€ä¼˜ç®—æ³•è¿™é‡Œä¸åšä»‹ç»ï¼Œå¯ä»¥å‚è€ƒ[å˜åˆ†æ¨æ–­ï¼ˆäºŒï¼‰â€”â€” è¿›é˜¶](https://www.cnblogs.com/kai-nutshell/p/13156319.html) ä»¥åŠ [ã€ä¸€æ–‡å­¦ä¼šã€‘å˜åˆ†æ¨æ–­åŠå…¶æ±‚è§£æ–¹æ³•](https://blog.csdn.net/weixin_40255337/article/details/83088786)

> å‚è€ƒ[11.4 èŠ‚å˜åˆ†æ¨æ–­](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)

### å‚è€ƒæ–‡çŒ®

[9-1] Dempster AP,Laird NM,Rubin DB. Maximum-likelihood from incomplete data via theEM algorithm. J. Royal Statist. Soc. Ser. B.,1977ï¼Œ39

[9-2] Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction. Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[9-3] McLachlan G,Krishnan T. The EM Algorithm and Extensions. New York: John Wiley& Sons,1996

[9-4] èŒ†è¯—æ¾ï¼Œç‹é™é¾™ï¼Œæ¿®æ™“é¾™ã€‚é«˜ç­‰æ•°ç†ç»Ÿè®¡ã€‚åŒ—äº¬ï¼šé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼›æµ·ç™»å ¡ï¼šæ–¯æ™®æ—æ ¼å‡ºç‰ˆç¤¾ï¼Œ1998

[9-5] Wu CFJ. On the convergence properties of the EM algorithm. The Annals ofStatistics,1983,11: 95â€“103

[9-6] Radford N,Geoffrey H,Jordan MI. A view of the EM algorithm that justifiesincremental,sparse,and other variants. In: Learning in Graphical Models. Cambridge,MA: MITPress,1999,355â€“368

## ç¬¬ 10 ç«  éšé©¬å°”å¯å¤«æ¨¡å‹

**éšé©¬å°”å¯å¤«æ¨¡å‹**ï¼ˆ[Hidden Markov Model,HMM](https://en.jinzhao.wiki/wiki/Hidden_Markov_model)ï¼‰æ˜¯å¯ç”¨äº**æ ‡æ³¨é—®é¢˜**çš„ç»Ÿè®¡å­¦ä¹ æ¨¡å‹ï¼Œæè¿°ç”±éšè—çš„é©¬å°”å¯å¤«é“¾éšæœºç”Ÿæˆè§‚æµ‹åºåˆ—çš„è¿‡ç¨‹ï¼Œå±äºç”Ÿæˆæ¨¡å‹ã€‚

é©¬å°”å¯å¤«æ¨¡å‹æ˜¯å…³äºæ—¶åºçš„æ¦‚ç‡æ¨¡å‹ï¼Œæè¿°ç”±ä¸€ä¸ªéšè—çš„é©¬å°”å¯å¤«é“¾éšæœºç”Ÿæˆ**ä¸å¯è§‚æµ‹çš„çŠ¶æ€éšæœºåºåˆ—**ï¼Œå†ç”±å„ä¸ªçŠ¶æ€ç”Ÿæˆä¸€ä¸ªè§‚æµ‹è€Œäº§ç”Ÿè§‚æµ‹éšæœºåºåˆ—çš„è¿‡ç¨‹ã€‚
éšè—çš„é©¬å°”å¯å¤«é“¾éšæœºç”Ÿæˆçš„çŠ¶æ€çš„åºåˆ—ï¼Œç§°ä¸º**çŠ¶æ€åºåˆ—**ï¼ˆstate sequenceï¼‰ï¼›æ¯ä¸ªçŠ¶æ€ç”Ÿæˆä¸€ä¸ªè§‚æµ‹ï¼Œè€Œç”±æ­¤äº§ç”Ÿçš„è§‚æµ‹çš„éšæœºåºåˆ—ï¼Œç§°ä¸º**è§‚æµ‹åºåˆ—**ï¼ˆobservation sequenceï¼‰ã€‚åºåˆ—çš„æ¯ä¸€ä¸ªä½ç½®åˆå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ—¶åˆ»ã€‚

éšé©¬å°”å¯å¤«æ¨¡å‹ç”±åˆå§‹æ¦‚ç‡åˆ†å¸ƒã€çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒä»¥åŠè§‚æµ‹æ¦‚ç‡åˆ†å¸ƒç¡®å®šã€‚
éšé©¬å°”å¯å¤«æ¨¡å‹çš„å½¢å¼å®šä¹‰å¦‚ä¸‹ï¼š
è®¾ Q æ˜¯æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€çš„é›†åˆï¼ŒN æ˜¯å¯èƒ½çš„çŠ¶æ€æ•°ï¼›V æ˜¯æ‰€æœ‰å¯èƒ½çš„è§‚æµ‹çš„é›†åˆï¼ŒM æ˜¯å¯èƒ½çš„è§‚æµ‹æ•°ã€‚
$$Q = \{q_1,q_2,...,q_N\} , V= \{v_1,v_2,...,v_M\}$$
é•¿åº¦ä¸º T çš„çŠ¶æ€åºåˆ—$I = (i_1,i_2,...,i_T)$ä»¥åŠä¸çŠ¶æ€åºåˆ—å¯¹åº”çš„é•¿åº¦ä¸º T çš„è§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$

**çŠ¶æ€è½¬ç§»çŸ©é˜µ(çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒ)**ï¼šï¼ˆå°±æ˜¯åˆå§‹åŒ–å‚æ•°[transmat_prior](https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn-hmm)ï¼Œä¹Ÿå¯ä»¥ç”¨ params å’Œæ±‚å‡ºçš„å±æ€§ transmat_ï¼‰
$$A=[a_{ij}]_{N\times N}$$
å…¶ä¸­$a_{ij} = P(i\_{t+1} = q_j | i_t = q_i) ,ä¸‹æ ‡ i,j = 1,...,N$è¡¨ç¤ºåœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_i$çš„æ¡ä»¶ä¸‹ åœ¨æ—¶åˆ»$t+1$è½¬ç§»åˆ°çŠ¶æ€$q_j$çš„æ¦‚ç‡

**è§‚æµ‹çŸ©é˜µ(è§‚æµ‹æ¦‚ç‡åˆ†å¸ƒ)**ï¼šï¼ˆå¯¹äº MultinomialHMM ç”¨ params å’Œæ±‚å‡ºçš„å±æ€§ emissionprob_ï¼Œå«å‘ç”Ÿæ¦‚ç‡çŸ©é˜µï¼›å¯¹äº GMMHMM æœ‰ n_mix ã€means_priorã€covars_prior ï¼›å¯¹äº GaussianHMM æœ‰ means_priorã€covars_prior ï¼‰
$$B = [b_j(k)]_{N \times M}$$
å…¶ä¸­$b_j(k) = P(o_t = v_k | i_t = q_j) ,k = 1,...,M,j = 1,...,N$è¡¨ç¤ºåœ¨æ—¶åˆ»$t$å¤„äºçŠ¶æ€$q_j$çš„æ¡ä»¶ä¸‹ç”Ÿæˆè§‚æµ‹$v_k$çš„æ¦‚ç‡

**åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡ï¼ˆåˆå§‹æ¦‚ç‡åˆ†å¸ƒï¼‰**ï¼šï¼ˆå°±æ˜¯åˆå§‹åŒ–å‚æ•°[startprob_prior](https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn-hmm)å’Œæ±‚å‡ºçš„å±æ€§ startprob\_ ï¼‰
$$\pi = (\pi_i)$$
å…¶ä¸­$\pi_i = P(i_1 =q_i) ,ä¸‹æ ‡i = 1,...,N$è¡¨ç¤ºæ—¶åˆ»$t=1$æ—¶ å¤„äºçŠ¶æ€$q_i$çš„æ¦‚ç‡

éšé©¬å°”å¯å¤«æ¨¡å‹ç”±åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡$\pi$ã€çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ$A$å’Œè§‚æµ‹æ¦‚ç‡çŸ©é˜µ$B$å†³å®šã€‚
$\pi$å’Œ$A$å†³å®šçŠ¶æ€åºåˆ—ï¼Œ$B$å†³å®šè§‚æµ‹åºåˆ—ã€‚
å› æ­¤ï¼Œ**ä¸€ä¸ªéšé©¬å°”å¯å¤«æ¨¡å‹**å¯ä»¥ç”¨ä¸‰å…ƒç¬¦å·è¡¨ç¤ºï¼Œå³
$$\lambda = (A,B,\pi)$$
ç§°ä¸ºéšé©¬å°”å¯å¤«æ¨¡å‹çš„ä¸‰è¦ç´ ã€‚

çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ$A$ä¸åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡$\pi$ç¡®å®šäº†éšè—çš„é©¬å°”å¯å¤«é“¾ï¼Œç”Ÿæˆä¸å¯è§‚æµ‹çš„çŠ¶æ€åºåˆ—ã€‚è§‚æµ‹æ¦‚ç‡çŸ©é˜µ$B$ç¡®å®šäº†å¦‚ä½•ä»çŠ¶æ€ç”Ÿæˆè§‚æµ‹ï¼Œä¸çŠ¶æ€åºåˆ—ç»¼åˆç¡®å®šäº†å¦‚ä½•äº§ç”Ÿè§‚æµ‹åºåˆ—ã€‚

ä»å®šä¹‰å¯çŸ¥ï¼Œéšé©¬å°”å¯å¤«æ¨¡å‹ä½œäº†**ä¸¤ä¸ªåŸºæœ¬å‡è®¾**ï¼š

1. **é½æ¬¡é©¬å°”å¯å¤«æ€§å‡è®¾**ï¼Œå³å‡è®¾éšè—çš„é©¬å°”å¯å¤«é“¾åœ¨ä»»æ„æ—¶åˆ» t çš„çŠ¶æ€åªä¾èµ–äºå…¶å‰ä¸€æ—¶åˆ»çš„çŠ¶æ€ï¼Œä¸å…¶ä»–æ—¶åˆ»çš„çŠ¶æ€åŠè§‚æµ‹æ— å…³ï¼Œä¹Ÿä¸æ—¶åˆ» t æ— å…³ã€‚
   $$P(i_{t}|i_{t-1},o_{t-1},...,i_{1},o_{1}) = P(i_{t}|i_{t-1}), t=1,2,...,T$$
1. **è§‚æµ‹ç‹¬ç«‹æ€§å‡è®¾**ï¼Œå³å‡è®¾ä»»æ„æ—¶åˆ»çš„è§‚æµ‹åªä¾èµ–äºè¯¥æ—¶åˆ»çš„é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€ï¼Œä¸å…¶ä»–è§‚æµ‹åŠçŠ¶æ€æ— å…³ã€‚
   $$P(o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_{t},o_{t},i_{t-1},o_{t-1},...,i_{1},o_{1}) = P(o_{t}|i_{t})$$

éšé©¬å°”å¯å¤«æ¨¡å‹çš„**ä¸‰ä¸ªåŸºæœ¬é—®é¢˜**ï¼š

1. æ¦‚ç‡è®¡ç®—é—®é¢˜ã€‚ç»™å®šæ¨¡å‹$\lambda = (A,B,\pi)$å’Œè§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$ï¼Œè®¡ç®—åœ¨æ¨¡å‹$\lambda$ä¸‹è§‚æµ‹åºåˆ—$O$å‡ºç°çš„æ¦‚ç‡$P(O|\lambda)$ã€‚

1. å­¦ä¹ é—®é¢˜ã€‚å·²çŸ¥è§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$ï¼Œä¼°è®¡æ¨¡å‹$\lambda = (A,B,\pi)$å‚æ•°ï¼Œä½¿å¾—åœ¨è¯¥æ¨¡å‹ä¸‹è§‚æµ‹åºåˆ—æ¦‚ç‡$P(O|\lambda)$æœ€å¤§ã€‚å³ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ä¼°è®¡å‚æ•°ã€‚ï¼ˆ$\lambda_{MLE}=\argmax_{\lambda}P(O|\lambda)$ï¼Œä½¿ç”¨ EM ç®—æ³•æ±‚è§£ã€‚ï¼‰

1. é¢„æµ‹é—®é¢˜ï¼Œä¹Ÿç§°ä¸ºè§£ç ï¼ˆdecodingï¼‰é—®é¢˜ã€‚å·²çŸ¥æ¨¡å‹$\lambda = (A,B,\pi)$å’Œè§‚æµ‹åºåˆ—$O = (o_1,o_2,...,o_T)$ï¼Œæ±‚å¯¹ç»™å®šè§‚æµ‹åºåˆ—æ¡ä»¶æ¦‚ç‡$P(I|O)$æœ€å¤§çš„çŠ¶æ€åºåˆ—$I = (i_1,i_2,...,i_T)$ã€‚å³ç»™å®šè§‚æµ‹åºåˆ—ï¼Œæ±‚æœ€æœ‰å¯èƒ½çš„å¯¹åº”çš„çŠ¶æ€åºåˆ—ã€‚ï¼ˆViterbi ç®—æ³•æ±‚$\hat{I}=\argmax_{I}P(I|O,\lambda)$ï¼‰

**æ¦‚ç‡è®¡ç®—é—®é¢˜**ï¼š
å¼•å…¥éšå˜é‡ï¼Œå¯¹å®Œå…¨æ•°æ®å»ºæ¨¡(è¿™é‡Œè¿˜æ˜¯ä¸€æ ·$P(O|\lambda),P(O;\lambda)$æ˜¯ä¸€æ ·çš„ï¼Œ$\lambda$æ˜¯å‚æ•°)
$$P(O|\lambda) = \sum_{I}P(O,I|\lambda)= \sum_{I}P(O|I,\lambda)P(I|\lambda)$$
æ ¹æ®ä¹˜æ³•è§„åˆ™ï¼ˆæ¦‚ç‡è®ºåŸºç¡€æ•™ç¨‹ 51 é¡µï¼Œæ³¨æ„$P(i_1|\lambda) = P(i_1)$ï¼‰ä»¥åŠé©¬å°”å¯å¤«å‡è®¾æœ‰ï¼š
$$P(I|\lambda) = P(i_1,i_2,...,i_T|\lambda)=P(i_1).P(i_2|i_1,\lambda).P(i_3|i_1,i_2,\lambda)...P(i_T|i_1,i_2,...,i_{T-1},\lambda) \\= P(i_1)\prod_{t=2}^T P(i_t|i_1,i_2,...,i_{t-1},\lambda) \\= P(i_1)\prod_{t=2}^T P(i_t|i_{t-1},\lambda) \\= \pi_{i_1}\prod_{t=2}^T a_{i_{t-1}i_{t}}$$
æ ¹æ®ä¹˜æ³•è§„åˆ™ä»¥åŠè§‚æµ‹ç‹¬ç«‹æ€§å‡è®¾æœ‰ï¼š
$$P(O|I,\lambda) = P(o_1,o_2,...,o_T|i_1,i_2,...,i_{T},\lambda) \\= P(o_1|i_1,i_2,...,i_{T},\lambda).P(o_2|o_1,i_1,i_2,...,i_{T},\lambda).P(o_3|o_1,o_2,i_1,i_2,...,i_{T},\lambda)...P(o_T|o_1,o_2,...,o_{T-1},i_1,i_2,...,i_{T},\lambda) \\ = P(o_1|i_1,\lambda).P(o_2|i_2,\lambda)...P(o_T|i_T,\lambda) \\= \prod_{t=1}^Tb_{i_t}(o_t)$$
é‚£ä¹ˆ

$$
P(O,I|\lambda) = P(O|I,\lambda)P(I|\lambda) = \pi_{i_1}\prod_{t=2}^T a_{i_{t-1}i_{t}}\prod_{t=1}^Tb_{i_t}(o_t)
\\= \pi_{i_1}b_{i_1}(o_1) .a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T) = \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t)
$$

**æ¦‚ç‡è®¡ç®—é—®é¢˜- ç›´æ¥ç”±ä¸Šé¢è®¡ç®—æ¦‚ç‡**å¯å¾—
$$P(O|\lambda) = \sum_{I}P(O,I|\lambda)= \sum_{I}P(O|I,\lambda)P(I|\lambda) \\= \sum_{i_1,i_2,...,i_T} \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t) \\= \sum_{i_1 \in N}...\sum_{i_T\in N} \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t) $$
æ—¶é—´å¤æ‚åº¦$O(TN^{T})$ï¼Œæ‰€ä»¥ä¸å¯è¡Œã€‚

ä¸Šé¢è¯´è¿‡ç›´æ¥æ±‚ä¸å¥½æ±‚ï¼Œæœ‰ä»¥ä¸‹æ–¹æ³•å¯æ±‚å¾—ï¼š
**æ¦‚ç‡è®¡ç®—é—®é¢˜- å‰å‘è®¡ç®—**ï¼š
é¦–å…ˆæˆ‘ä»¬å®šä¹‰**å‰å‘æ¦‚ç‡**$\alpha_t(i) = P(o_1,o_2,...,o_t,i_t=q_i | \lambda)$ï¼Œè¡¨ç¤ºæ—¶åˆ»$t$éƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸º$o_1,o_2,...,o_t$ä¸”çŠ¶æ€ä¸º$q_i$çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆ
$$P(O|\lambda) = \sum_{i=1}^N P(O,i_T=q_i|\lambda) = \sum_{i=1}^N P(o_1,...,o_T,i_T=q_i|\lambda) = \sum_{i=1}^N \alpha_T(i)$$

> å…¶å®$P(O|\lambda) = \sum_{j=1}^N P(O,i_1=q_j|\lambda) =...= \sum_{j=1}^N P(O,i_t=q_j|\lambda) = \sum_{i=1}^N\sum_{j=1}^N P(O,i_1=q_i,i_2=q_j|\lambda)$ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å° tï¼Œåªä¸è¿‡æˆ‘ä»¬å®šä¹‰äº†å‰å‘æ¦‚ç‡ï¼Œå¹¶ä¸”$O=(o_1,...,o_T)$

æ‰€ä»¥æˆ‘ä»¬åªè¦æ±‚å‡º$\alpha_T(i)$ï¼Œå¦‚ä½•æ±‚ï¼Ÿä¾æ¬¡$\alpha_1(i) ... \alpha_{t+1}(i) ... \alpha_T(i)$

$$\alpha_1(i) = P(o_1,i_1=q_i | \lambda) =P(i_1=q_i | \lambda)P(o_1|i_1=q_i , \lambda) = \pi_ib_i(o_1) \\  \vdots\\ \alpha_{t+1}(i) = P(o_1,o_2,...,o_t,o_{t+1},i_{t+1}=q_i | \lambda)  \\=\sum_{j=1}^N P(o_1,o_2,...,o_t,o_{t+1},i_{t+1}=q_i,i_{t}=q_j | \lambda) \\ =\sum_{j=1}^NP(o_{t+1}|o_1,..,o_t,i_{t+1}=q_i,i_{t}=q_j,\lambda)P(o_1,o_2,...,o_t,i_{t+1}=q_i,i_{t}=q_j | \lambda) \\=\sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(o_1,o_2,...,o_t,i_{t+1}=q_i,i_{t}=q_j | \lambda)  \\= \sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(i_{t+1}=q_i | o_1,o_2,...,o_t,i_{t}=q_j,\lambda)P(o_1,o_2,...,o_t,i_{t}=q_j | \lambda)  \\=\sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(i_{t+1}=q_i | i_{t}=q_j,\lambda)P(o_1,o_2,...,o_t,i_{t}=q_j | \lambda) \\=\sum_{j=1}^NP(o_{t+1}|i_{t+1}=q_i)P(i_{t+1}=q_i | i_{t}=q_j,\lambda)\alpha_t(j) \\= P(o_{t+1}|i_{t+1}=q_i)\sum_{j=1}^NP(i_{t+1}=q_i | i_{t}=q_j,\lambda)\alpha_t(j) \\= \bigg[\sum_{j=1}^N\alpha_t(j)a_{ji} \bigg]  b_i(o_{t+1})$$

**æ¦‚ç‡è®¡ç®—é—®é¢˜- åå‘è®¡ç®—**ï¼š
é¦–å…ˆæˆ‘ä»¬å®šä¹‰**åå‘æ¦‚ç‡**$\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=q_i , \lambda)$ï¼Œè¡¨ç¤ºæ—¶åˆ»çŠ¶æ€ä¸º$q_i$çš„æ¡ä»¶ä¸‹ï¼Œä»$t+1$åˆ°$T$çš„éƒ¨åˆ†è§‚æµ‹åºåˆ—ä¸º$o_{t+1},o_{t+2},...,o_T$æ¦‚ç‡ï¼Œé‚£ä¹ˆ
$$P(O|\lambda) = \sum_{i=1}^N P(O,i_1=q_i|\lambda) = \sum_{i=1}^N P(o_1,...,o_T,i_1=q_i|\lambda) \\= \sum_{i=1}^N P(o_1,...,o_T|i_1=q_i,\lambda)P(i_1=q_i|\lambda) \\= \sum_{i=1}^N P(o_1|o_2,...,o_T,i_1=q_i,\lambda)P(o_2,...,o_T|i_1=q_i,\lambda)P(i_1=q_i|\lambda) \\ = \sum_{i=1}^N P(o_1|i_1=q_i,\lambda)P(o_2,...,o_T|i_1=q_i,\lambda)P(i_1=q_i|\lambda) \\= \sum_{i=1}^N b_i(o_1)\beta_1(i)\pi_i$$
æ‰€ä»¥æˆ‘ä»¬åªè¦æ±‚å‡º$\beta_1(i)$ï¼Œå¦‚ä½•æ±‚ï¼Ÿä¾æ¬¡$\beta_T(i) ... \beta_1{t-1}(i) ... \beta_1(i)$

$$\beta_T(i) = P(i_T = q_i,\lambda) = 1 \\ \vdots \\ \beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=q_i , \lambda) \\= \sum_{j=1}^N P(o_{t+1},o_{t+2},...,o_T,i_{t+1}=q_j|i_t=q_i , \lambda) \\= \sum_{j=1}^N P(o_{t+1},o_{t+2},...,o_T|i_{t+1}=q_j,i_t=q_i , \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda) \\ æ¡ä»¶å‰é¢æ²¡æœ‰o_t(æ ¹æ®æ¦‚ç‡å›¾ä¹Ÿèƒ½å¾—å‡ºç»™å®ši_{t+1}æ—¶ï¼Œi_tä¸o_{t+1},...,o_Tæ— å…³) \\= \sum_{j=1}^N P(o_{t+1},o_{t+2},...,o_T|i_{t+1}=q_j, \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda) \\= \sum_{j=1}^N P(o_{t+1}|o_{t+2},...,o_T,i_{t+1}=q_j, \lambda)P(o_{t+2},...,o_T|i_{t+1}=q_j, \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda)\\= \sum_{j=1}^N P(o_{t+1}|i_{t+1}=q_j, \lambda)P(o_{t+2},...,o_T|i_{t+1}=q_j, \lambda) P(i_{t+1}=q_j|i_t=q_i , \lambda) \\ =\sum_{j=1}^N b_j(o_{t+1}) \beta_{t+1}(j) a_{ij}$$

- **æ¨¡å‹**ï¼š
  $$P(O|\lambda) = \sum_{I}P(O,I|\lambda) = \sum_{I}P(O|I,\lambda)P(I|\lambda)$$

- **ç­–ç•¥**ï¼š
  $$\argmax_{\lambda} P(O|\lambda)$$

- **ç®—æ³•**ï¼š
  Baum-Welch ç®—æ³•ï¼Œå…¶å®å°±æ˜¯ EM ç®—æ³•çš„ä¸€ä¸ªå®ç°
  æ ¹æ® EM ç®—æ³•å¾— Q å‡½æ•°
  $$Q(\lambda,\={\lambda}) = \sum_{I} \log P(O,I|\lambda) P(I|O,\={\lambda}) = \sum_{I} \log P(O,I|\lambda) P(I,O|\={\lambda}).\frac{1}{P(O|\={\lambda})}$$
  å› ä¸ºæˆ‘ä»¬è¦æ±‚$\lambda$,è€Œ$1/{P(O|\={\lambda})}$å¯¹äº$\lambda$è€Œè¨€ï¼Œå¯ä»¥çœ‹ä½œå¸¸æ•°ï¼Œæ‰€ä»¥
  $$Q(\lambda,\={\lambda}) =\sum_{I} \log P(O,I|\lambda) P(I,O|\={\lambda})$$
  å› ä¸º
  $$P(O,I|\lambda) = \sum_{I} \pi_{i_1}b_{i_1}(o_1)\prod_{t=2}^T a_{i_{t-1}i_{t}}b_{i_t}(o_t)  = \sum_{I} \pi_{i_1}\prod_{t=2}^T a_{i_{t-1}i_{t}}\prod_{t=1}^T b_{i_t}(o_t) $$
  æ‰€ä»¥
  $$Q(\lambda,\={\lambda}) = \sum_{I}\bigg[ \log\pi_{i_1}+ \sum_{t=2}^T\log a_{i_{t-1}i_{t}} + \sum_{t=1}^T \log b_{i_t}(o_t) \bigg]P(I,O|\={\lambda})$$
  è¿™é‡Œæˆ‘ä»¬ä»¥æ±‚$\pi$ï¼ˆæ¦‚ç‡å‘é‡ï¼‰ä¸ºä¾‹å­ï¼ˆA,Bå°±ä¸æ¨å¯¼äº†,å‚è§[ä¸€ç«™å¼è§£å†³ï¼šéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å…¨è¿‡ç¨‹æ¨å¯¼åŠå®ç°](https://zhuanlan.zhihu.com/p/85454896)ï¼‰ï¼Œå‘ç°åªæœ‰ä¸€é¡¹ä¸$\pi$æœ‰å…³ç³»
  $$\pi^{(t+1)} = \argmax_{\pi} Q(\lambda,\lambda^{(t)}) \\= \argmax_{\pi} \sum\_{I}\bigg[ \log\pi_{i_1}P(I,O|\lambda^{(t)})\bigg] \\= \argmax_{\pi} \sum_{i_1}\sum_{i_2}...\sum_{i_T}\bigg[ \log\pi_{i_1}P(i_1,i_2,...,i_T,O|\lambda^{(t)})\bigg] \\ æˆ‘ä»¬è§‚å¯Ÿä¸€ä¸‹ï¼Œå‘ç°è¾¹ç¼˜åˆ†å¸ƒ å¯ä»¥åªä¿ç•™ä¸€é¡¹æ¥è®¡ç®—\\ =\argmax_{\pi} \sum_{i_1}\bigg[ \log\pi_{i_1}P(i_1,O|\lambda^{(t)})\bigg] \\ æˆ‘ä»¬æŠŠ i_1 æ›¿æ¢æ‰ \\ =\argmax_{\pi} \sum_{j=1}^N \bigg[ \log\pi_{j}P(i_1 = q_j,O|\lambda^{(t)})\bigg]$$
  æˆ‘ä»¬çŸ¥é“$\pi = (\pi_1,..,\pi_N)$æ˜¯æ¦‚ç‡å‘é‡ï¼Œ$ \sum_{j=1}^N \pi_{j} =1$ï¼Œåˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼Œå†™å‡ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š
  $$L(\pi,\gamma) = \sum_{j=1}^N \bigg[ \log\pi_{j}P(i_1 = q_j,O|\lambda^{(t)})\bigg] + \gamma\bigg(\sum_{j=1}^N \pi_{j} -1\bigg)$$
  æ±‚å…¶ä¸­ä¸€ä¸ªåˆ†é‡$\pi_j$ï¼Œåˆ™å¯¹å…¶æ±‚åå¯¼ï¼Œä»¤åå¯¼æ•°ä¸º0å¾—ï¼š
  $$\frac{\partial L}{\partial \pi_j} = \frac{1}{\pi_j}P(i_1 = q_j,O|\lambda^{(t)})+\gamma = 0$$
  å¾—
  $$P(i_1 = q_j,O|\lambda^{(t)}) + \gamma\pi_j=0$$
  é‚£ä¹ˆ
   $$\sum_{j=1}^N\bigg[P(i_1 = q_j,O|\lambda^{(t)}) + \gamma\pi_j \bigg]=0 \\ \Darr \\ P(O|\lambda^{(t)}) + \gamma = 0$$
   å¸¦å…¥ä¸Šé¢å¾—åˆ°çš„å¼å­ä¸­ï¼Œæœ‰ï¼š
   $$\pi_j^{(t+1)} = \frac{P(i_1 = q_j,O|\lambda^{(t)})}{P(O|\lambda^{(t)})}$$
  å¼å­ä¸­åˆ†æ¯$P\left(X|\lambda^{\left(t\right)}\right)$å¯ä»¥æ ¹æ®å‰å‘ç®—æ³•å’Œåå‘ç®—æ³•æ±‚è§£å‡ºæ¥ï¼Œå°±æ˜¯å½“å‰å‚æ•°ä¸‹è§‚æµ‹æ•°æ®çš„æ¦‚ç‡ã€‚
  å¦å¤–ï¼Œåˆ©ç”¨å®šä¹‰çš„å‰å‘æ¦‚ç‡å’Œåå‘æ¦‚ç‡ï¼Œæœ‰ï¼š
  $$\begin{align} &\alpha_t\left(i\right)\beta_t\left(i\right)\\ &=P\left(x_i,x_2,\ldots,x_t,z_t=q_i|\lambda\right)P\left(x_T,x_{T-1},\ldots,x_{t+1}|z_t=q_i,\lambda\right)\\ &=P\left(x_i,x_2,\ldots,x_t|z_t=q_i,\lambda\right)P\left(x_T,x_{T-1},\ldots,x_{t+1}|z_t=q_i,\lambda\right)P\left(z_t=q_i|\lambda\right)\\ &=P\left(x_i,x_2,\ldots,x_T|z_t=q_i,\lambda\right)P\left(z_t=q_i|\lambda\right)\\ &=P\left(X,z_t=q_i|\lambda\right) \end{align}$$
  é‚£ä¹ˆ
  $$\pi_i^{\left(t+1\right)}=\frac{\alpha_1\left(i\right)\beta_1\left(i\right)}{P\left(X|\lambda^{\left(t\right)}\right)}$$



**é¢„æµ‹é—®é¢˜ï¼Œä¹Ÿç§°ä¸ºè§£ç ï¼ˆdecodingï¼‰é—®é¢˜**ï¼š
ç»´ç‰¹æ¯”ç®—æ³•ï¼ˆ[Viterbi algorithm](https://en.jinzhao.wiki/wiki/Viterbi_algorithm)ï¼‰å®é™…æ˜¯ç”¨åŠ¨æ€è§„åˆ’è§£éšé©¬å°”å¯å¤«æ¨¡å‹é¢„æµ‹é—®é¢˜ï¼Œå³ç”¨åŠ¨æ€è§„åˆ’ï¼ˆ[dynamic programming](https://en.jinzhao.wiki/wiki/Dynamic_programming)ï¼‰æ±‚æ¦‚ç‡æœ€å¤§è·¯å¾„ï¼ˆæœ€ä¼˜è·¯å¾„ï¼‰ï¼Œè¿™é‡Œçš„æœ€ä¼˜è·¯å¾„å°±æ˜¯æœ€ä¼˜çŠ¶æ€åºåˆ—$I$ã€‚

> è¯·å‚è€ƒä¹¦ç±å’Œ[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åå››)-éšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼ˆHidden Markov Modelï¼‰](https://www.bilibili.com/video/BV1MW41167Rf?p=6)

è¿™ä¸€ç±»æ¨¡å‹éœ€è¦æ±‚è§£çš„é—®é¢˜çš„å¤§ä½“æ¡†æ¶ä¸ºï¼š
å…¶ä¸­$X$ä»£è¡¨è§‚æµ‹åºåˆ—ï¼Œ$Z$ä»£è¡¨éšå˜é‡åºåˆ—ï¼Œ$\lambda$ä»£è¡¨å‚æ•°ã€‚

$$
\begin{cases}
   Representation &  \text{Probabilistic graphical model} \\
   Learning & \lambda_{MLE}=arg \underset{\lambda}{\max} P(X|\lambda)  \boxed{\text{Baum Welch Algorithm(EM)}}\\
   Inference & \begin{cases} Decoding & Z=arg\underset{Z}{\max}P(Z|X,\lambda) or P(z_1,z_2,\cdots,z_t|x_1,x_2,\cdots,x_t,\lambda) \boxed{\text{Viterbi Algorithm}}\\ \text{Prob of evidence} & P(X|\lambda)  \boxed{\text{Forward Algorithm,Backward Algorithm}} \\ Filtering & P(z_t|x_1,x_2,\cdots,x_t,\lambda) \boxed{\text{(online)Forward Algorithm}}\\ Smothing & P(z_t|x_1,x_2,\cdots,x_T,\lambda) \boxed{\text{(offline)Forward-Backward Algorithm}}\\Prediction & \begin{Bmatrix} P(z_{t+1},z_{t+2},...|x_1,x_2,\cdots,x_t,\lambda) \\ P(x_{t+1},x_{t+2},...|x_1,x_2,\cdots,x_t,\lambda) \end{Bmatrix} \boxed{\text{Forward Algorithm}} \end{cases}\\
\end{cases}
$$

[Filtering problem (stochastic processes)](<https://en.jinzhao.wiki/wiki/Filtering_problem_(stochastic_processes)>)ï¼š
[Smoothing problem (stochastic processes)](<https://en.jinzhao.wiki/wiki/Smoothing_problem_(stochastic_processes)>)

### é™„åŠ çŸ¥è¯†

#### éšæœºè¿‡ç¨‹

**éšæœºè¿‡ç¨‹**ï¼ˆ[Stochastic process](https://en.jinzhao.wiki/wiki/Stochastic_process)ï¼‰ï¼š

è®¾$(\Omega ,{\mathcal {F}},P)$ä¸ºä¸€ä¸ª**æ¦‚ç‡ç©ºé—´**ï¼ˆ[Probability space](https://en.jinzhao.wiki/wiki/Probability_space)ï¼‰,$\Omega$ ä¸º**æ ·æœ¬ç©ºé—´**ï¼ˆ[sample space](https://en.jinzhao.wiki/wiki/Sample_space)ï¼‰ï¼Œ $\mathcal {F}$ æ˜¯ï¼ˆ[Sigma-algebra](https://en.jinzhao.wiki/wiki/%CE%A3-algebra)ï¼‰ï¼Œ$P$ æ˜¯ï¼ˆ[Probability measure](https://en.jinzhao.wiki/wiki/Probability_measure)ï¼‰ï¼›
è®¾$(S,\Sigma )$ä¸ºå¯æµ‹é‡çš„ç©ºé—´ï¼ˆmeasurable spaceï¼‰ï¼Œ$S$ä¸ºéšæœºå˜é‡çš„é›†åˆ
$${\displaystyle \{X(t):t\in T\}} or {\displaystyle \{X(t,\omega ):t\in T\}}$$

å…¶ä¸­$X(t)$æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œï¼ˆåœ¨è‡ªç„¶ç§‘å­¦çš„è®¸å¤šé—®é¢˜ä¸­$t$è¡¨ç¤ºæ—¶é—´ï¼Œé‚£ä¹ˆ$X(t)$è¡¨ç¤ºåœ¨æ—¶åˆ»$t$è§‚å¯Ÿåˆ°çš„å€¼ï¼‰ï¼›$\omega \in \Omega$ï¼›$T$æ˜¯æŒ‡æ ‡é›† or å‚æ•°é›†ï¼ˆindex set or parameter setï¼‰ï¼Œä¸€èˆ¬è¡¨ç¤ºæ—¶é—´æˆ–ç©ºé—´ï¼Œå¦‚ï¼šç¦»æ•£$T=\{0,1,2,...\}$ä¸€èˆ¬ç§°ä¸ºéšæœºåºåˆ—æˆ–æ—¶é—´åºåˆ—ï¼Œè¿ç»­$T=[a,b] ,aå¯ä»¥å–0æˆ–è€… -\infty,bå¯ä»¥å–+\infty$

æ˜ å°„$X(t,\omega):T \times \Omega \to R$ï¼Œå³$X(.,.)$æ˜¯å®šä¹‰åœ¨$T \times \Omega$ä¸Šçš„äºŒå…ƒå€¼å‡½æ•°;
$\forall t \in T$ï¼ˆå›ºå®š$t \in T$ï¼‰,$ X(t,.)$æ˜¯å®šä¹‰åœ¨æ ·æœ¬ç©ºé—´$\Omega $ä¸Šçš„å‡½æ•°ï¼Œç§°ä¸º**éšæœºå˜é‡**; 
$\forall \omega \in \Omega$,æ˜ å°„$X(.,\omega):T \to S$ï¼ˆå…¶å®å°±æ˜¯å›ºå®š$\omega \in \Omega $ï¼Œå˜æˆå…³äºTçš„å‡½æ•°ï¼‰,è¢«ç§°ä¸º**æ ·æœ¬å‡½æ•°**ï¼ˆsample functionï¼‰,ç‰¹åˆ«æ˜¯å½“$T$è¡¨ç¤ºæ—¶é—´æ—¶ï¼Œç§°ä¸ºéšæœºè¿‡ç¨‹${\displaystyle \{X(t,\omega ):t\in T\}}$çš„**æ ·æœ¬è·¯å¾„**ï¼ˆsample pathï¼‰ã€‚

#### å‚»å‚»åˆ†ä¸æ¸…æ¥šçš„é©¬å°”å¯å¤«

> å‚è§ä¹¦ä¸­ç¬¬19ç« 

**é©¬å°”å¯å¤«æ€§è´¨**ï¼ˆ[Markov property](https://en.jinzhao.wiki/wiki/Markov_property)ï¼‰ï¼š
å¦‚æœ**éšæœºè¿‡ç¨‹**ï¼ˆ[Stochastic process](https://en.jinzhao.wiki/wiki/Stochastic_process)ï¼‰çš„æœªæ¥çŠ¶æ€çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼ˆä»¥è¿‡å»å’Œç°åœ¨å€¼ä¸ºæ¡ä»¶ï¼‰ä»…å–å†³äºå½“å‰çŠ¶æ€ï¼Œåˆ™éšæœºè¿‡ç¨‹å…·æœ‰é©¬å°”å¯å¤«æ€§è´¨ï¼›ä¸æ­¤å±æ€§çš„è¿‡ç¨‹è¢«è®¤ä¸ºæ˜¯**é©¬æ°**æˆ–**é©¬å°”å¯å¤«è¿‡ç¨‹**ï¼ˆ[Markov process](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚æœ€è‘—åçš„é©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov chain](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚**å¸ƒæœ—è¿åŠ¨**ï¼ˆ[Brownian motion](https://en.jinzhao.wiki/wiki/Brownian_motion)ï¼‰æ˜¯å¦ä¸€ä¸ªè‘—åçš„é©¬å°”å¯å¤«è¿‡ç¨‹ã€‚é©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯ä¸å…·å¤‡è®°å¿†ç‰¹è´¨çš„ï¼ˆ[Memorylessness](https://en.jinzhao.wiki/wiki/Memorylessness)ï¼‰

**ä¸€é˜¶ ç¦»æ•£**
$${\displaystyle P(X_{n}=x_{n}\mid X_{n-1}=x_{n-1},\dots ,X_{0}=x_{0})=P(X_{n}=x_{n}\mid X_{n-1}=x_{n-1}).}$$

**m é˜¶ ç¦»æ•£**
$${\displaystyle {\begin{aligned}&\Pr(X_{n}=x_{n}\mid X_{n-1}=x_{n-1},X_{n-2}=x_{n-2},\dots ,X_{1}=x_{1})\\={}&\Pr(X_{n}=x_{n}\mid X_{n-1}=x_{n-1},X_{n-2}=x_{n-2},\dots ,X_{n-m}=x_{n-m}){\text{ for }}n>m\end{aligned}}}$$

**æ—¶é—´é½æ¬¡**ï¼ˆTime-homogeneousï¼‰
$${\displaystyle \Pr(X_{t+s}=x\mid X_{t+s-1}=y)=\Pr(X_{t}=x\mid X_{t-1}=y)}$$

**é©¬å°”å¯å¤«æ¨¡å‹**ï¼ˆ[Markov model](https://en.jinzhao.wiki/wiki/Markov_model)ï¼‰ï¼š
.|System state is fully observable |System state is partially observable
---|---|---
System is autonomous |[Markov chain](https://en.jinzhao.wiki/wiki/Markov_chain) | [Hidden Markov model](https://en.jinzhao.wiki/wiki/Hidden_Markov_model)
System is controlled |[Markov decision process](https://en.jinzhao.wiki/wiki/Markov_decision_process) |[Partially observable Markov decision process](https://en.jinzhao.wiki/wiki/Partially_observable_Markov_decision_process)

é©¬å°”å¯å¤«æ¨¡å‹æ˜¯å…·æœ‰é©¬å°”å¯å¤«æ€§å‡è®¾çš„éšæœºè¿‡ç¨‹ï¼Œæœ€ç®€å•çš„é©¬å°”å¯å¤«æ¨¡å‹æ˜¯**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov chain](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰

| .                                        | Countable state spaceï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„ç¦»æ•£$\Omega$ï¼‰               | Continuous or general state spaceï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„è¿ç»­$\Omega$ï¼‰                              |
| ---------------------------------------- | ----------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| Discrete-timeï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„ç¦»æ•£$T$ï¼‰   | (discrete-time) Markov chain on a countable or finite state space | Markov chain on a measurable state space (for example, Harris chain)                         |
| Continuous-timeï¼ˆå¯¹åº”éšæœºè¿‡ç¨‹çš„è¿ç»­$T$ï¼‰ | Continuous-time Markov process or Markov jump process             | Any continuous stochastic process with the Markov property (for example, the Wiener process) |

ä»¤$\{X_n|n=1,2,\cdots\}$æ˜¯æœ‰é™ä¸ªæˆ–å¯æ•°ä¸ªå¯èƒ½å€¼çš„éšæœºè¿‡ç¨‹ã€‚é™¤éç‰¹åˆ«æé†’ï¼Œè¿™ä¸ªéšæœºè¿‡ç¨‹çš„å¯èƒ½å€¼çš„é›†åˆéƒ½å°†è®°ä¸ºéè´Ÿæ•´æ•°é›†$\{0,1,2,\cdots\}$ ã€‚
å¦‚æœ $X_n=i$ï¼Œé‚£ä¹ˆç§°è¯¥è¿‡ç¨‹åœ¨æ—¶åˆ» $t$ åœ¨çŠ¶æ€ $i$ ï¼Œå¹¶å‡è®¾$P_{i,j}$ ç§°ä¸º(å•æ­¥(one-step))**è½¬ç§»æ¦‚ç‡**(transition probability)ï¼Œè¡¨ç¤ºå¤„åœ¨$i$çŠ¶æ€çš„éšæœºå˜é‡ä¸‹ä¸€æ—¶åˆ»å¤„åœ¨$j$ çŠ¶æ€çš„æ¦‚ç‡ï¼Œå¦‚æœå¯¹æ‰€æœ‰çš„çŠ¶æ€ $i_0,i_1,\cdots,i_{n-1},i,j$åŠä»»æ„$n\ge 0$ ï¼Œ$P\{X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},\cdots,X_1=i_1,X_0=i_0\}=P_{i,j}$ï¼Œè¿™æ ·çš„è¿‡ç¨‹ç§°ä¸º**é©¬å°”å¯å¤«é“¾**(Markov chain)ã€‚

> ä¸€ä¸ªéšæœºè¿‡ç¨‹$\{X(t):t \geq 0\}$å¦‚æœ$t \in \mathbb{R}_+$åˆ™ç§°ä¸ºè¿ç»­æ—¶é—´çš„é©¬å°”ç§‘å¤«é“¾ï¼Œå¦‚æœ$t \in \mathbb{N}_0$åˆ™ç§°ä¸ºç¦»æ•£æ—¶é—´çš„é©¬å°”ç§‘å¤«é“¾

æ ¹æ® $P_{i,j}$çš„å®šä¹‰æ˜¾ç„¶æœ‰$P_{i,j}\ge0,\;i,j\ge0;\;\;\sum_{j=0}^\infty P_{i,j}=1,\;i=0,1,\cdots$,
ç”¨ $P_{i,j}$ è®°å½•ä» $i$ åˆ° $j$ çš„(å•æ­¥)**è½¬ç§»ï¼ˆæ¦‚ç‡ï¼‰çŸ©é˜µ**ï¼ˆ[transition matrix](https://en.jinzhao.wiki/wiki/Stochastic_matrix)ï¼‰ä¹Ÿç§°ä¸º**éšæœºçŸ©é˜µã€æ¦‚ç‡çŸ©é˜µã€è½¬ç§»çŸ©é˜µã€æ›¿ä»£çŸ©é˜µæˆ–é©¬å°”å¯å¤«çŸ©é˜µ**ï¼š
$$\mathbf{P}_{i,j}=(P_{i_{n},i_{n+1}}) =\begin{bmatrix}P_{0,0}&P_{0,1}&P_{0,2}&\cdots\\P_{1,0}&P_{1,1}&P_{1,2}&\cdots\\\vdots&\vdots&\vdots\\P_{i,0}&P_{i,1}&P_{i,2}&\cdots\\\vdots&\vdots&\vdots\end{bmatrix}$$
ç°åœ¨å®šä¹‰ n æ­¥(n-step)**è½¬ç§»æ¦‚ç‡**$P_{i,j}^n$ ï¼š$P_{i,j}^n=P\{X_{n+k=j}|X_k=i\},\;n\ge 0,i,j\ge 0$

**å³éšæœºçŸ©é˜µ**æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°çš„æ–¹é˜µï¼Œæ¯ä¸ªè¡Œæ€»å’Œä¸º 1ã€‚
**å·¦éšæœºçŸ©é˜µ**æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°çš„æ–¹é˜µï¼Œæ¯ä¸ªåˆ—çš„æ€»å’Œä¸º 1ã€‚
**åŒéšæœºçŸ©é˜µ**æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°çš„æ–¹é˜µï¼Œæ¯ä¸ªè¡Œå’Œæ¯ä¸ªåˆ—çš„æ€»å’Œä¸º 1ã€‚

å‡è®¾$A$æ˜¯é©¬å°”å¯å¤«çŸ©é˜µï¼Œå…¶æ€§è´¨æœ‰ï¼š

1. çŸ©é˜µ$A$çš„ k æ¬¡å¹‚$A^k$ä¹Ÿæ˜¯é©¬å°”å¯å¤«çŸ©é˜µã€‚
1. è‡³å°‘æœ‰ä¸€ä¸ªç‰¹å¾å€¼ä¸º 1ï¼Œå…¶ç‰¹å¾å€¼åœ¨[-1,1]åŒºé—´ï¼Œç‰¹å¾å€¼ä¸º 1 å¯¹åº”çš„ç‰¹å¾å‘é‡$\pi$ç§°ä¸º**å¹³ç¨³æ¦‚ç‡å‘é‡**ï¼ˆstationary probability vectorï¼‰ã€‚
1. å¯¹äºä»»æ„**æ¦‚ç‡å‘é‡**ï¼ˆ[Probability vector](https://en.jinzhao.wiki/wiki/Probability_vector)ï¼‰æˆ–è€…**éšæœºå‘é‡**$\pi_0$æœ‰$\lim_{k \to \infty} A^k \pi_0 = \pi$ ï¼ˆè¿™é‡Œæ˜¯åœ¨æ²¡æœ‰-1 ç‰¹å¾å€¼çš„æƒ…å†µä¸‹ï¼‰ã€‚
1. å¯¹äºä»»æ„**æ¦‚ç‡å‘é‡**$\mu_0$æœ‰$\mu_1 = A \mu_0$ä¹Ÿæ˜¯æ¦‚ç‡å‘é‡ã€‚

ç‰¹å¾å€¼çš„æ±‚è§£ï¼š$\det(A-\lambda I)=0$

ç”±äº $A$ çš„æ¯ä¸€åˆ—ç›¸åŠ ç­‰äº 1ï¼Œæ‰€ä»¥ $Aâˆ’I$ çš„æ¯ä¸€åˆ—ç›¸åŠ ç­‰äº 0ï¼Œè¿™ä¹Ÿå°±æ˜¯è¯´ $Aâˆ’I$ çš„è¡Œæ˜¯ç›¸å…³çš„ï¼Œå…¶è¡Œåˆ—å¼$\det(A-I)=0$ä¸ºé›¶ï¼Œæ‰€ä»¥ $Aâˆ’I$å¥‡å¼‚çŸ©é˜µï¼Œæ‰€ä»¥ 1 æ˜¯ $A$ çš„ä¸€ä¸ªç‰¹å¾å€¼ã€‚

[å¯¹è§’åŒ–](https://en.jinzhao.wiki/wiki/Diagonalizable_matrix#Diagonalization) $A = P \Lambda P^{-1} $ ï¼ˆå‚è§çº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨279é¡µï¼Œç‰¹å¾å€¼ç›¸åŒç‰¹å¾å‘é‡ä¸ä¸€å®šç›¸åŒï¼‰,å…¶ä¸­$\Lambda$æ˜¯ç”±$A$çš„ç‰¹å¾å€¼ç»„æˆçš„å¯¹è§’çŸ©é˜µ
$$\mu_k = A^k \mu_0 = (P \Lambda P^{-1})^k \mu_0 = P \Lambda^k P^{-1} \mu_0$$
ä¸å¦¨è®¾ $A$çš„ç‰¹å¾å‘é‡å’Œç›¸åº”çš„ç‰¹å¾å€¼åˆ†åˆ«ä¸º ${x_1},...,{x_n}$å’Œ $\lambda_1,...,\lambda_n$ï¼Œå¯ä»¥ç”¨ç‰¹å¾å‘é‡æ¥åšä¸€ç»„åŸºï¼Œå¯ä»¥æŠŠç©ºé—´ä¸­ä»»ä½•å‘é‡å†™æˆå®ƒçš„çº¿æ€§ç»„åˆï¼š$\mu_0 = c_1{x_1} + \cdots + c_n{x_n}$
é‚£ä¹ˆï¼š
$$A^k\mathbf{\mu_0} = A^kc_1\mathbf{x_1} + \cdots + A^kc_n\mathbf{x_n}\\ = c_1A^k\mathbf{x_1} + \cdots + c_nA^k\mathbf{x_n} \\= c_1A^{k-1}A\mathbf{x_1} + \cdots + c_nA^{k-1}A\mathbf{x_n} \\= c_1A^{k-1}\lambda_1\mathbf{x_1} + \cdots + c_nA^{k-1}\lambda_n\mathbf{x_n}\\=c_1\lambda_1^k\mathbf{x_1} + \cdots + c_n\lambda_n^k\mathbf{x_n}\\=\sum_{i=1}^n{c_i\lambda_i^k\bm{x_i}}$$

ä¸å¦¨ä»¤$\lambda_1=1$, æœ‰$|\lambda_i|\leq 1$,é‚£ä¹ˆï¼š
$$\bm{u_\infty}=\lim_{k\to\infty}{A^k\bm{u_0}}=\lim_{k\to\infty}{\sum_{i=1}^k{c_i\lambda_i^k\bm{x_i}}}=c_1\bm{x_1}$$

å› ä¸º$u_\infty$æ˜¯æ¦‚ç‡å‘é‡ï¼Œè€Œç‰¹å¾å€¼ä¸º 1 å¯¹åº”çš„ç‰¹å¾å‘é‡ä¹Ÿæ˜¯æ¦‚ç‡å‘é‡ï¼Œæ‰€ä»¥$c_1=1$ï¼Œå¾—åˆ°$\bm{u_\infty}=\bm{x_1}$

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé™¤åŒ…å« $\lambda=1$ çš„æƒ…å½¢è¿˜åŒ…å« $\lambda=-1$ çš„æƒ…å½¢ã€‚
ä¸Šå¼å¦‚æœé™¤$\lambda_1=1$è¿˜æœ‰$\lambda_2=-1$ï¼Œé‚£ä¹ˆå°±æœ‰ï¼š
$$\bm{u_\infty}=\lim_{k\to\infty}{\sum_{i=1}^k{c_i\lambda_i^k\bm{x_i}}}=c_1\bm{x_1}+(-1)^k c_2\bm{x_2}$$

å¾—$\bm{u_\infty}=\bm{x_1}+(-1)^k\bm{x_2}$,æ­¤æ—¶$k$ä¸ºå¥‡æ•°å’Œå¶æ•°ç»“æœæ˜¯ä¸åŒçš„ï¼Œé€ æˆçš„ç»“æœå°±æ˜¯åœ¨ä¸¤ç§ç»“æœä¹‹é—´åå¤æ¨ªè·³ï¼Œæ°¸è¿œè¾¾ä¸åˆ°ç¨³æ€ã€‚

å¦‚ï¼š
$$A=\begin{bmatrix}0&1\\1&0\\\end{bmatrix}$$
å…¶ç‰¹å¾å€¼ä¸º$\lambda_1=1ï¼Œ\lambda_2=-1$

> ä¹Ÿå¯ä»¥å‚è€ƒç¬¬ 21 ç«  PageRank ç®—æ³•

#### è§„åˆ’è®º

è§„åˆ’è®ºåˆç§°æ•°å­¦è§„åˆ’,è¿ç­¹å­¦ï¼ˆ[Operations research](https://en.jinzhao.wiki/wiki/Category:Operations_research)ï¼‰çš„ä¸€ä¸ªåˆ†æ”¯ã€‚ è§„åˆ’è®ºæ˜¯æŒ‡åœ¨æ—¢å®šæ¡ä»¶ï¼ˆçº¦æŸæ¡ä»¶ï¼‰ä¸‹ï¼ŒæŒ‰ç…§æŸä¸€è¡¡é‡æŒ‡æ ‡ï¼ˆç›®æ ‡å‡½æ•°ï¼‰åœ¨å¤šç§ æ–¹æ¡ˆä¸­å¯»æ±‚æœ€ä¼˜æ–¹æ¡ˆï¼ˆå–æœ€å¤§æˆ–æœ€å°å€¼ï¼‰ã€‚è§„åˆ’è®ºåŒ…æ‹¬çº¿æ€§è§„åˆ’ã€éçº¿æ€§è§„åˆ’å’ŒåŠ¨æ€è§„åˆ’ç­‰ï¼Œæ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•æˆ–æ–¹æ³•ï¼ˆ[Optimization algorithms and methods](https://en.jinzhao.wiki/wiki/Category:Optimization_algorithms_and_methods)ï¼‰

æ•°å­¦ä¼˜åŒ–ï¼ˆ[Mathematical optimization](https://en.jinzhao.wiki/wiki/Category:Mathematical_optimization)ï¼‰

[ä¼˜åŒ–æŠ€æœ¯](https://en.jinzhao.wiki/wiki/Mathematical_optimization#Computational_optimization_techniques)ï¼š

- ä¼˜åŒ–ç®—æ³• Optimization algorithms
  [ä¼˜åŒ–ç®—æ³•åˆ—è¡¨](https://en.jinzhao.wiki/wiki/List_of_algorithms#Optimization_algorithms)

- è¿­ä»£æ–¹æ³• Iterative methods
  [Iterative method](https://en.jinzhao.wiki/wiki/Iterative_method)

- å…¨å±€æ”¶æ•› Global convergence
- å¯å‘å¼ Heuristics
  [Heuristic algorithm](<https://en.jinzhao.wiki/wiki/Heuristic_(computer_science)>)

**çº¿æ€§è§„åˆ’**ï¼š

å½“ç›®æ ‡å‡½æ•°ä¸çº¦æŸæ¡ä»¶éƒ½æ˜¯çº¿å½¢çš„ï¼Œåˆ™ç§°ä¸ºçº¿æ€§è§„åˆ’ï¼ˆ[Linear programming](https://en.jinzhao.wiki/wiki/Linear_programming)â€ï¼‰ã€‚

æ±‚è§£æ–¹æ³•ï¼šå›¾è§£æ³•(graphical method)ã€å•çº¯å½¢æ³•ï¼ˆ[simplex algorithm](https://en.jinzhao.wiki/wiki/Simplex_algorithm)ï¼‰ã€å¯¹å¶å•çº¯å½¢æ³•ç­‰

**éçº¿æ€§è§„åˆ’**ï¼š

é™¤å»çº¿æ€§è§„åˆ’ï¼Œåˆ™ä¸ºéçº¿æ€§è§„åˆ’ï¼ˆ[Nonlinear programming](https://en.jinzhao.wiki/wiki/Nonlinear_programming)ï¼‰ã€‚å…¶ä¸­ï¼Œå‡¸è§„åˆ’ï¼ˆå‰é¢çš„ç« èŠ‚æœ‰è®²åˆ°å‡¸ä¼˜åŒ–ï¼‰ã€äºŒæ¬¡è§„åˆ’ï¼ˆ[Quadratic programming](https://en.jinzhao.wiki/wiki/Quadratic_programming)ï¼‰ã€å‡ ä½•è§„åˆ’éƒ½æ˜¯ä¸€ç§ç‰¹æ®Šçš„éçº¿æ€§è§„åˆ’ã€‚

æ±‚è§£æ–¹æ³•ï¼šæ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ã€å¯è¡Œæ–¹å‘æ³•ã€åˆ¶çº¦å‡½æ•°æ³•(constrained function method )ç­‰ã€‚

å†…ç‚¹æ³•([Interior point methods](https://en.jinzhao.wiki/wiki/Interior-point_method))æ˜¯ä¸€ç§æ±‚è§£çº¿æ€§è§„åˆ’æˆ–éçº¿æ€§å‡¸ä¼˜åŒ–é—®é¢˜çš„ç®—æ³•ã€‚

**æ— çº¦æŸä¼˜åŒ–é—®é¢˜**ï¼š

å»é™¤å¸¦çº¦æŸçš„è§„åˆ’é—®é¢˜ï¼Œåˆ™ä¸ºæ— çº¦æŸä¼˜åŒ–é—®é¢˜ï¼ˆUnconstrained convex optimizationï¼Œå¯¹åº”çš„æœ‰çº¦æŸä¼˜åŒ–ï¼ˆ[Constrained optimization](https://en.jinzhao.wiki/wiki/Constrained_optimization)ï¼‰ï¼‰ã€‚

æ±‚è§£æ–¹æ³•ï¼š 1ã€ æœ€é€Ÿä¸‹é™æ³•(ä¹Ÿå«æ¢¯åº¦ä¸‹é™) 2ã€ å…±è½­æ¢¯åº¦ä¸‹é™ 3ã€ ç‰›é¡¿æ³• 4ã€ æ‹Ÿç‰›é¡¿æ³•

**åŠ¨æ€è§„åˆ’**ï¼š

è‹¥è§„åˆ’é—®é¢˜ä¸æ—¶é—´æœ‰å…³ï¼Œåˆ™ç§°ä¸ºåŠ¨æ€è§„åˆ’ï¼ˆ[Dynamic programmingâ€](https://en.jinzhao.wiki/wiki/Dynamic_programming)ï¼‰ï¼›

> æŠŠå¤šé˜¶æ®µè¿‡ç¨‹è½¬åŒ–ä¸ºä¸€ç³»åˆ—å•é˜¶æ®µé—®é¢˜ï¼Œé€ä¸ªæ±‚è§£ï¼Œè§£å†³è¿™ç±»é—®é¢˜çš„æ–¹æ³•ç§°ä¸ºåŠ¨æ€è§„åˆ’ï¼Œå®ƒæ˜¯ä¸€ç§æ–¹æ³•ã€è€ƒå¯Ÿé—®é¢˜çš„ä¸€ç§é€”å¾„ï¼Œä½†ä¸æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç®—æ³•ã€‚ æ²¡æœ‰ç»Ÿä¸€çš„æ ‡å‡†æ¨¡å‹ï¼Œä¹Ÿæ²¡æœ‰æ„é€ æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼Œç”šè‡³è¿˜æ²¡æœ‰åˆ¤æ–­ä¸€ä¸ªé—®é¢˜èƒ½å¦æ„é€ åŠ¨æ€è§„åˆ’æ¨¡å‹çš„å‡†åˆ™ã€‚è¿™æ ·å°±åªèƒ½å¯¹æ¯ç±»é—®é¢˜è¿›è¡Œå…·ä½“åˆ†æï¼Œæ„é€ å…·ä½“çš„æ¨¡å‹ã€‚å¯¹äºè¾ƒå¤æ‚çš„é—®é¢˜åœ¨é€‰æ‹©çŠ¶æ€ã€å†³ç­–ã€ç¡®å®šçŠ¶æ€è½¬ç§»è§„å¾‹ç­‰æ–¹é¢éœ€è¦ä¸°å¯Œçš„æƒ³è±¡åŠ›å’Œçµæ´»çš„æŠ€å·§æ€§ï¼Œè¿™å°±å¸¦æ¥äº†åº”ç”¨ä¸Šçš„å±€é™æ€§ã€‚

åŠ¨æ€è§„åˆ’ä¸€èˆ¬å¯åˆ†ä¸ºçº¿æ€§åŠ¨è§„ï¼ŒåŒºåŸŸåŠ¨è§„ï¼Œæ ‘å½¢åŠ¨è§„ï¼ŒèƒŒåŒ…åŠ¨è§„ï¼ˆ[Knapsack problem](https://en.jinzhao.wiki/wiki/Knapsack_problem)ï¼‰å››ç±»ã€‚
çº¿æ€§åŠ¨è§„ï¼šæ‹¦æˆªå¯¼å¼¹ï¼Œåˆå”±é˜Ÿå½¢ï¼ŒæŒ–åœ°é›·ï¼Œå»ºå­¦æ ¡ï¼Œå‰‘å®¢å†³æ–—ç­‰ï¼›
åŒºåŸŸåŠ¨è§„ï¼šçŸ³å­åˆå¹¶ï¼Œ åŠ åˆ†äºŒå‰æ ‘ï¼Œç»Ÿè®¡å•è¯ä¸ªæ•°ï¼Œç‚®å…µå¸ƒé˜µç­‰ï¼›
æ ‘å½¢åŠ¨è§„ï¼šè´ªåƒçš„ä¹å¤´é¾™ï¼ŒäºŒåˆ†æŸ¥æ‰¾æ ‘ï¼Œèšä¼šçš„æ¬¢ä¹ï¼Œæ•°å­—ä¸‰è§’å½¢ç­‰ï¼›
èƒŒåŒ…é—®é¢˜ï¼šèƒŒåŒ…é—®é¢˜ï¼Œå®Œå…¨èƒŒåŒ…é—®é¢˜ï¼Œåˆ†ç»„èƒŒåŒ…é—®é¢˜ï¼ŒäºŒç»´èƒŒåŒ…ï¼Œè£…ç®±é—®é¢˜ï¼ŒæŒ¤ç‰›å¥¶

**éšæœºè§„åˆ’**ï¼š

è‹¥è§„åˆ’é—®é¢˜ä¸éšæœºå˜é‡æœ‰å…³ï¼Œåˆ™ç§°ä¸ºéšæœºè§„åˆ’ï¼ˆ[Stochastic programming](https://en.jinzhao.wiki/wiki/Stochastic_programming)ï¼‰ã€‚

**éšæœºåŠ¨æ€è§„åˆ’**ï¼š

[Stochastic dynamic programming](https://en.jinzhao.wiki/wiki/Stochastic_dynamic_programming)

**ç»„åˆè§„åˆ’**ï¼š

è‹¥è§„åˆ’é—®é¢˜ä¸æœ‰é™ä¸ªäº‹ç‰©çš„æ’åˆ—ç»„åˆæœ‰å…³ï¼Œåˆ™ç§°ä¸ºç»„åˆè§„åˆ’([combinatorial optimization](https://en.jinzhao.wiki/wiki/Combinatorial_optimization))

### å‚è€ƒæ–‡çŒ®

[10-1] Rabiner L,Juang B. [An introduction to hidden markov Models](http://ai.stanford.edu/~pabbeel/depth_qual/Rabiner_Juang_hmms.pdf). IEEE ASSPMagazine,January 1986

[10-2] Rabiner L. [A tutorial on hidden Markov models and selected applications in speechrecognition](https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf). Proceedings of IEEE,1989

[10-3] Baum L,et al. [A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177697196). Annals of Mathematical Statistics,1970,41: 164â€“171

[10-4] Bilmes JA. [A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models](https://people.ece.uw.edu/bilmes/p/mypubs/bilmes1997-em.pdf).

[10-5] Lari K,Young SJ. Applications of stochastic context-free grammars using the Inside-Outside algorithm,Computer Speech & Language,1991,5(3): 237â€“257

[10-6] Ghahramani Z. [Learning Dynamic Bayesian Networks](https://courses.cs.duke.edu//compsci662/current/pdf/ghahramani.1998.pdf). Lecture Notes in ComputerScience,Vol. 1387,1997,168â€“197

ä»¥ä¸‹æ¥è‡ª[éšé©¬å°”å¯å¤«æ¨¡å‹](http://infolab.stanford.edu/~jiali/hmm.html)

[10-7] J. Li, A. Najmi, R. M. Gray, `Image classification by a two dimensional hidden Markov model`,IEEE Transactions on Signal Processing , 48(2):517-33, February 2000. [2-D HMM] ([download](http://www.stat.psu.edu/~jiali/pub/sp00.pdf))

[10-8] J. Li, R. M. Gray, R. A. Olshen, `Multiresolution image classification by hierarchical modeling with two dimensional hidden Markov models`, IEEE Transactions on Information Theory , 46(5):1826-41, August 2000. [2-D MHMM] ([download](http://www.stat.psu.edu/~jiali/pub/it00.pdf))

[10-9] J. Li, W. Miller, `Significance of inter-species matches when evolutionary rate varies`, Journal of Computational Biology , 10(3-4):537-554, 2003. [HMMMO] ([download](http://www.stat.psu.edu/~jiali/pub/jcb03.pdf))

[10-10] J. Li, J. Z. Wang, `Studying digital imagery of ancient paintings by mixtures of stochastic models`, IEEE Transactions on Image Processing, 12(3):340-353, 2004. [Mixture of 2-D MHMMs] ([download](http://www-db.stanford.edu/~wangz/project/imsearch/ART/TIP03/li_ip.pdf))

## ç¬¬ 11 ç«  æ¡ä»¶éšæœºåœº
**æ¡ä»¶éšæœºåœº**ï¼ˆ[Conditional random field, CRF](https://en.jinzhao.wiki/wiki/Conditional_random_field)ï¼‰æ¡ä»¶éšæœºåœº(CRFs)æ˜¯ä¸€ç±»å¸¸ç”¨çš„ç»Ÿè®¡å»ºæ¨¡æ–¹æ³•ï¼ˆ[statistical modeling methods](https://en.jinzhao.wiki/wiki/Statistical_model)ï¼‰ï¼Œå¸¸ç”¨äºæ¨¡å¼è¯†åˆ«ï¼ˆ[pattern recognition](https://en.jinzhao.wiki/wiki/Pattern_recognition)ï¼‰å’Œæœºå™¨å­¦ä¹ ï¼Œå¹¶ç”¨äºç»“æ„é¢„æµ‹ï¼ˆ[structured prediction](https://en.jinzhao.wiki/wiki/Structured_prediction)ï¼‰ã€‚

ç›¸å…³çš„æœºå™¨å­¦ä¹ åº“æœ‰[PyStruct](https://github.com/pystruct/pystruct)å’Œ[python-crfsuite](https://github.com/scrapinghub/python-crfsuite)

è¿™é‡Œæ¨èå­¦ä¹ ï¼š[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åä¸ƒ)-æ¡ä»¶éšæœºåœºCRFï¼ˆConditional Random Fieldï¼‰](https://www.bilibili.com/video/BV19t411R7QU) ä»¥åŠè®ºæ–‡[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)

æ¡ä»¶éšæœºåœºæ˜¯åœ¨æ— å‘å›¾ä¸Šçš„åˆ¤åˆ«æ¨¡å‹ã€‚

æ¡ä»¶éšæœºåœºæ˜¯ç»™å®šä¸€ç»„è¾“å…¥éšæœºå˜é‡æ¡ä»¶ä¸‹å¦ä¸€ç»„è¾“å‡ºéšæœºå˜é‡çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹ï¼Œå…¶ç‰¹ç‚¹æ˜¯å‡è®¾è¾“å‡ºéšæœºå˜é‡æ„æˆé©¬å°”å¯å¤«éšæœºåœºã€‚
æ¡ä»¶éšæœºåœºå¯ä»¥ç”¨äºä¸åŒçš„é¢„æµ‹é—®é¢˜ï¼Œæœ¬ä¹¦ä»…è®ºåŠå®ƒåœ¨æ ‡æ³¨é—®é¢˜çš„åº”ç”¨ã€‚å› æ­¤ä¸»è¦è®²è¿°çº¿æ€§é“¾ï¼ˆlinear   chainï¼‰æ¡ä»¶éšæœºåœºï¼Œè¿™æ—¶ï¼Œé—®é¢˜å˜æˆäº†ç”±è¾“å…¥åºåˆ—å¯¹è¾“å‡ºåºåˆ—é¢„æµ‹çš„åˆ¤åˆ«æ¨¡å‹ï¼Œå½¢å¼ä¸ºå¯¹æ•°çº¿æ€§æ¨¡å‹ï¼Œå…¶å­¦ä¹ æ–¹æ³•é€šå¸¸æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚


**æ¡ä»¶éšæœºåœº**ï¼ˆconditional  random  fieldï¼‰æ˜¯ç»™å®šéšæœºå˜é‡Xæ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡Yçš„é©¬å°”å¯å¤«éšæœºåœºã€‚
è®¾$X$ä¸$Y$æ˜¯éšæœºå˜é‡ï¼Œ$P(Y|X)$æ˜¯åœ¨ç»™å®šXçš„æ¡ä»¶ä¸‹$Y$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚è‹¥éšæœºå˜é‡$Y$æ„æˆä¸€ä¸ªç”±æ— å‘å›¾$Gï¼(V,E)$è¡¨ç¤ºçš„é©¬å°”å¯å¤«éšæœºåœºï¼Œå³
$$p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \neq v) = p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \sim v)$$
å¯¹ä»»æ„ç»“ç‚¹$v$æˆç«‹ï¼Œåˆ™ç§°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(Y|X)$ä¸ºæ¡ä»¶éšæœºåœºã€‚å¼ä¸­$w \sim v$è¡¨ç¤ºåœ¨å›¾$Gï¼(V,E)$ä¸­ä¸ç»“ç‚¹$v$æœ‰è¾¹è¿æ¥çš„æ‰€æœ‰ç»“ç‚¹$w$ï¼Œ$w \neq v$è¡¨ç¤ºç»“ç‚¹$v$ä»¥å¤–çš„æ‰€æœ‰ç»“ç‚¹ï¼Œ$Y_v$ï¼Œ$Y_u$ä¸$Y_w$ä¸ºç»“ç‚¹$v$ï¼Œ$u$ä¸$w$å¯¹åº”çš„éšæœºå˜é‡ã€‚


**çº¿æ€§é“¾æ¡ä»¶éšæœºåœº**ï¼ˆlinear chain conditional random fieldï¼‰å‡è®¾Xå’ŒYæœ‰ç›¸åŒçš„å›¾ç»“æ„ã€‚
> æ¡ä»¶éšæœºåœºåœ¨å®šä¹‰ä¸­å¹¶æ²¡æœ‰è¦æ±‚Xå’ŒYå…·æœ‰ç›¸åŒçš„ç»“æ„ã€‚ç°å®ä¸­ï¼Œä¸€èˆ¬å‡è®¾Xå’ŒYæœ‰ç›¸åŒçš„å›¾ç»“æ„ã€‚

è®¾$Xï¼(X_1,X_2,...,X_n)ï¼ŒYï¼(Y_1ï¼ŒY_2,...,Y_n)$å‡ä¸ºçº¿æ€§é“¾è¡¨ç¤ºçš„éšæœºå˜é‡åºåˆ—ï¼Œè‹¥åœ¨ç»™å®šéšæœºå˜é‡åºåˆ—$X$çš„æ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡åºåˆ—$Y$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(Y|X)$æ„æˆæ¡ä»¶éšæœºåœºï¼Œå³æ»¡è¶³é©¬å°”å¯å¤«æ€§
$$P(Y_i|X,Y_1,...,Y_{i-1},Y_{i+1},...,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})\\ i=1,2,...,n(å½“i=1å’Œnæ—¶åªè€ƒè™‘å•è¾¹)$$
åˆ™ç§°$P(Y|X)$ä¸ºçº¿æ€§é“¾æ¡ä»¶éšæœºåœºã€‚

```mermaid
graph LR
    Y1(("Yâ‚"))
    Y2(("Yâ‚‚"))
    Yi(("Yáµ¢"))
    Yn(("Yâ‚™"))
    Xg(("Xâ‚:â‚™"))
    Y1---Y2-.-Yi-.-Yn
    Y1---Xg
    Y2---Xg
    Yi---Xg
    Xg---Yn
    style Y1 fill:#fff
    style Y2 fill:#fff
    style Yi fill:#fff
    style Yn fill:#fff
    style Xg fill:#f96
```

çº¿æ€§é“¾æ¡ä»¶éšæœºåœºå¯ä»¥ç”¨äºæ ‡æ³¨ç­‰é—®é¢˜ã€‚
åœ¨æ ‡æ³¨é—®é¢˜ä¸­ï¼Œ$X$è¡¨ç¤ºè¾“å…¥è§‚æµ‹åºåˆ—ï¼Œ$Y$è¡¨ç¤ºå¯¹åº”çš„è¾“å‡ºæ ‡è®°åºåˆ—æˆ–çŠ¶æ€åºåˆ—ã€‚

è¿™æ—¶ï¼Œåœ¨æ¡ä»¶æ¦‚ç‡æ¨¡å‹P(Y|X)ä¸­ï¼ŒYæ˜¯è¾“å‡ºå˜é‡ï¼Œè¡¨ç¤ºæ ‡è®°åºåˆ—ï¼ŒXæ˜¯è¾“å…¥å˜é‡ï¼Œè¡¨ç¤ºéœ€è¦æ ‡æ³¨çš„è§‚æµ‹åºåˆ—ã€‚ä¹ŸæŠŠæ ‡è®°åºåˆ—ç§°ä¸ºçŠ¶æ€åºåˆ—ã€‚
å­¦ä¹ æ—¶ï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®é›†é€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°æ¡ä»¶æ¦‚ç‡æ¨¡å‹$\hat{P}(Y|X)$ï¼›
é¢„æµ‹æ—¶ï¼Œå¯¹äºç»™å®šçš„è¾“å…¥åºåˆ—xï¼Œæ±‚å‡ºæ¡ä»¶æ¦‚ç‡$\hat{P}(Y|X)$æœ€å¤§çš„è¾“å‡ºåºåˆ—$\hat{y}$ã€‚


æ ¹æ®**æ— å‘å›¾çš„å› å­åˆ†è§£**ï¼Œå¾—ï¼š
$$P(Y|X) = \frac{1}{Z} exp\sum_{i=1}^K F_i(x_{ci})$$
æ ¹æ®CRFçš„æ¦‚ç‡æ— å‘å›¾è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å®é™…çš„èŠ‚ç‚¹å¸¦å…¥è¿›å»ï¼ˆæœ€å¤§å›¢$y_{t-1},y_t,x_{1:T}$ï¼‰ï¼Œï¼ˆä¸ºäº†è¡¨è¾¾çš„æ–¹ä¾¿ï¼Œå‡‘ä¸€ä¸ªy0ï¼‰æœ‰ï¼š
$$P(Y|X) = \frac{1}{Z} exp\sum_{t=1}^T F(y_{t-1},y_t,x_{1:T})$$
å°†$F(y_{t-1},y_t,x_{1:T})$åˆ†è§£ä¸º2ä¸ªéƒ¨åˆ†ï¼Œå³ï¼š$x_{1:T}$ï¼ˆå·²çŸ¥çš„ï¼‰å¯¹$y_t$çš„å½±å“ä»¥åŠ$y_{t-1},y_t$é—´çš„å½±å“ã€‚æ•°å­¦åŒ–è¡¨ç¤ºä¸ºï¼š
$$F(y_{t-1},y_t,x_{1:T})=\triangle y_{t},x_{1:T} + \triangle y_{t-1},y_{t},x_{1:T}$$
å…¶å®è¿˜æœ‰ä¸ª$\triangle y_{t-1},x_{1:T}$ï¼Œå› ä¸ºè¿™æ˜¯ä¸Šä¸€ä¸ªçš„çŠ¶æ€ï¼Œå¯¹äºtæ—¶åˆ»æ˜¯å·²çŸ¥çš„ï¼Œè¿™é‡Œå¿½ç•¥äº†ã€‚
å…¶ä¸­ï¼Œ$\triangle y_t,x_{1:T}$ä¸º**çŠ¶æ€å‡½æ•°**ï¼Œå³è¡¨ç¤ºä¸ºåœ¨$t$ä½ç½®ä¸Šçš„èŠ‚ç‚¹$y_t$çŠ¶æ€ï¼›
$\triangle y_{t-1},y_t,x_{1:T}$ä¸º**è½¬ç§»å‡½æ•°**ï¼Œå³è¡¨ç¤ºå½“å‰èŠ‚ç‚¹$y_t$ä¸ä¸Šä¸€ä¸ªèŠ‚ç‚¹$y_{t-1}$çš„ç›¸å…³æ€§ã€‚
å®šä¹‰åœ¨ğ‘Œä¸Šä¸‹æ–‡çš„**å±€éƒ¨ç‰¹å¾å‡½æ•°**$t_k$ï¼Œè¿™ç±»ç‰¹å¾å‡½æ•°åªå’Œå½“å‰èŠ‚ç‚¹å’Œä¸Šä¸€ä¸ªèŠ‚ç‚¹æœ‰å…³ï¼Œå³ä¸ºä¸Šé¢çš„
$$\triangle y_{t-1},y_t,x_{1:T} =\sum_{k=1}^K \lambda_k t_k(y_{i-1},y_i,X,i),k=1,2,..,K$$
å…¶ä¸­$K$æ˜¯å®šä¹‰åœ¨è¯¥èŠ‚ç‚¹çš„å±€éƒ¨ç‰¹å¾å‡½æ•°çš„æ€»ä¸ªæ•°ï¼Œ$i$æ˜¯å½“å‰èŠ‚ç‚¹åœ¨åºåˆ—çš„ä½ç½®ã€‚$\lambda_k$ä¸ºç‰¹å¾å‡½æ•°çš„ä¿¡ä»»åº¦ã€‚
å®šä¹‰åœ¨ğ‘ŒèŠ‚ç‚¹ä¸Šçš„**èŠ‚ç‚¹ç‰¹å¾å‡½æ•°**ï¼Œè¿™ç±»ç‰¹å¾å‡½æ•°åªå’Œå½“å‰èŠ‚ç‚¹æœ‰å…³ï¼Œå³ä¸ºä¸Šé¢çš„
$$\triangle y_t,x_{1:T} =\sum_{l=1}^L \mu_l s_l(y_i,X,i),l=1,2,â€¦,L$$
å…¶ä¸­$L$æ˜¯å®šä¹‰åœ¨è¯¥èŠ‚ç‚¹çš„èŠ‚ç‚¹ç‰¹å¾å‡½æ•°çš„æ€»ä¸ªæ•°ï¼Œ$i$æ˜¯å½“å‰èŠ‚ç‚¹åœ¨åºåˆ—çš„ä½ç½®ã€‚$\mu_l$ä¸ºç‰¹å¾å‡½æ•°çš„ä¿¡ä»»åº¦ã€‚
ä¸ºäº†ä½¿ç‰¹å¾å‡½æ•°make senseï¼ˆæœ‰é“ç†ï¼Œåˆä¹æƒ…ç†; å¯ä»¥ç†è§£;è®²å¾—é€šï¼‰ï¼Œä¸€èˆ¬æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œå³å–å€¼é0å³1ã€‚æ— è®ºæ˜¯èŠ‚ç‚¹ç‰¹å¾å‡½æ•°è¿˜æ˜¯å±€éƒ¨ç‰¹å¾å‡½æ•°ï¼Œå®ƒä»¬çš„å–å€¼åªèƒ½æ˜¯0æˆ–è€…1ã€‚å³æ»¡è¶³ç‰¹å¾æ¡ä»¶æˆ–è€…ä¸æ»¡è¶³ç‰¹å¾æ¡ä»¶ã€‚
å¦‚ï¼š
$$t_k\{y_{t-1}=åè¯, y_t=åŠ¨è¯, x_{1:T}\} = 1 \\ t_k\{y_{t-1}=åè¯, y_t=åŠ©è¯, x_{1:T}\} = 0$$
æ‰€ä»¥linear-chain-CRFçš„å‚æ•°åŒ–å½¢å¼ä¸ºï¼š
$$P(Y|X)=\frac{1}{Z(X)}exp \sum_{i=1}^ T   \bigg (\sum_{k=1}^K \lambda_k t_k (y_{i-1},y_i,X,i)  +\sum_{l=1}^L \mu_l s_l (y_i,X,i)\bigg )$$
$Y$è¡¨ç¤ºçš„æ˜¯æ ‡æ³¨åºåˆ—ï¼Œæ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œé•¿åº¦ä¸º$T$ï¼›$X = x_{1:T}$è¡¨ç¤ºçš„è¯è¯­åºåˆ—ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œé•¿åº¦ä¹Ÿä¸º$T$ã€‚
å…¶ä¸­ï¼Œ$Z(X)$ä¸ºè§„èŒƒåŒ–å› å­ï¼š
$$Z(X)=\sum_Y exp \sum_{i=1}^T \bigg(\sum_{k=1}^K\lambda_k t_k (y_{i-1},y_i,X,i) +\sum_{l}^L\mu_l s_l (y_i,X,i)\bigg)$$
**æ¨¡å‹çš„ç®€åŒ–è¡¨ç¤º-æ•°å€¼è¡¨ç¤º**
å‡è®¾ï¼Œå…±æœ‰$K=K_1+K_2$ä¸ªç‰¹å¾å‡½æ•°ï¼Œå…¶ä¸­ï¼Œ$K_1$ä¸ªå±€éƒ¨ç‰¹å¾å‡½æ•°$t_k$ï¼Œ$K_2$ä¸ªèŠ‚ç‚¹ç‰¹å¾å‡½æ•°$s_l$ã€‚æˆ‘ä»¬ç”¨1ä¸ªç‰¹å¾å‡½æ•°$f_k(y_{i-1},y_i,X,i)$æ¥ç»Ÿä¸€è¡¨ç¤ºï¼š
$$\begin{aligned}f_k(y_{i-1},y_i,X,i)=\left\{\begin{aligned} & t_k (y_{i-1},y_i,X,i)  \qquad k = 1,2,..,K_1 \\ &  s_l (y_i,X,i)  \qquad k = K_1+l,l=1,2,â€¦,K_2    \end{aligned}\right.\end{aligned}$$
å¯¹$f_k(y_{i-1},y_i,X,i)$åœ¨å„ä¸ªåºåˆ—ä½ç½®æ±‚å’Œå¾—åˆ°ï¼š
$$\begin{aligned}f_k(Y,X)=\sum_{i=1}^T f_k(y_{i-1},y_i,X,i)\end{aligned}$$
åŒæ—¶ä¹Ÿç»Ÿä¸€$f_k(y_{i-1},y_i,x,i)$å¯¹åº”çš„æƒé‡ç³»æ•°$w_k$ï¼š
$$\begin{aligned}w_k=\left\{   \begin{aligned} & \lambda_k \qquad k = 1,2,..,K_1 \\ & \mu_l   \qquad k = K_1+l,l=1,2,â€¦,K_2    \end{aligned}\right.\end{aligned}$$
è¿™æ ·ï¼ŒLinear-chain-CRFçš„ç®€åŒ–å·¥ä½œå°±åˆ°è¿™é‡Œç»“æŸå•¦ï¼š
$$\begin{aligned}P(Y|X)=\frac{1}{Z(X)}exp\sum_{k=1}^K w_kf_k(Y,X)\end{aligned}$$
å…¶ä¸­ï¼Œè§„èŒƒåŒ–å› å­ï¼š
$$\begin{aligned}Z(X)=\sum_Y exp\sum_{k=1}^Kw_kf_k(Y,X)\end{aligned}$$
**æ¨¡å‹çš„ç®€åŒ–è¡¨ç¤º-å‘é‡è¡¨ç¤º**
å¦‚æœå¯¹$f_k(Y,X)$å’Œ$w_k$è¿›è¡Œå‘é‡åŒ–è¡¨ç¤ºï¼Œ$F(Y,X)$å’Œ$W$éƒ½æ˜¯$K \times 1$çš„åˆ—å‘é‡ï¼š
$$\begin{aligned}W  =\left [ \begin{aligned}    w_1\\    w_2\\    â€¦\\    w_K \end{aligned}\right]\end{aligned}$$
$$\begin{aligned}F(Y,X) =\left[    \begin{aligned}   f_1(Y,X)\\   f_2(Y,X)\\   â€¦â€¦â€¦\\   f_K(Y,X)    \end{aligned}\right]\end{aligned}$$
é‚£ä¹ˆLinear-chain-CRFçš„å‘é‡å†…ç§¯å½¢å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$\begin{aligned}P_W(Y|X) = \frac{exp(W \bullet F(Y,X))}{Z(X,W)}\\   = \frac{exp(W \bullet F(Y,X))}{\sum_Y exp(W \bullet F(Y,X))}\end{aligned}$$
å‘é‡åŒ–çš„æ„ä¹‰ï¼š
å°±æ˜¯ä¸ºäº†å¹²æ‰è¿åŠ çš„å½¢å¼ï¼Œä¸ºåé¢çš„è®­ç»ƒæä¾›æ›´åŠ åˆç†çš„è®¡ç®—æ”¯æŒã€‚
**è¦è§£å†³çš„ä¸‰ä¸ªé—®é¢˜**
1. Inferenceï¼ˆæ¦‚ç‡è®¡ç®—é—®é¢˜ï¼‰ï¼šè®¡ç®—æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå³ç»™å®šXåºåˆ—ï¼Œç®—å‡ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ‰€å¯¹åº”æ ‡æ³¨çš„æ¦‚ç‡ï¼Œå³ï¼š$P(y_t|X)$
1. Learningï¼šæŠŠå‚æ•°å­¦ä¹ å‡ºæ¥ï¼ˆparameter estimationï¼‰ï¼Œä¹Ÿå°±æ˜¯ç»™å®š$N$ä¸ªè®­ç»ƒæ•°æ®ï¼Œæ±‚ä¸Šé¢å‘é‡è¡¨ç¤ºçš„$W$çš„å‚æ•°å€¼ï¼Œå³ï¼š$\hat{W}=argmax\prod_{i=1}^N P(Y^{(i)}|X^{(i)})$
1. Decodingï¼šç»™å®šXåºåˆ—ï¼Œæ‰¾åˆ°ä¸€ä¸ªæœ€æœ‰å¯èƒ½çš„æ ‡æ³¨åºåˆ—ï¼Œå³ï¼š$\hat{Y}=argmax P(Y|X)$ï¼Œå…¶ä¸­ï¼Œ$Y=y_1y_2..y_T$

**Inferenceï¼šæ¡ä»¶æ¦‚ç‡(å‰å‘-åå‘)**
**Learning(å‚æ•°ä¼°è®¡)**
**Decoding(Vitebi)**

- **æ¨¡å‹**ï¼š
å‘é‡å†…ç§¯å½¢å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$\begin{aligned}P_W(Y|X) = \frac{exp(W \bullet F(Y,X))}{Z(X,W)}\\ = \frac{exp(W \bullet F(Y,X))}{\sum_Y exp(W \bullet F(Y,X))}\end{aligned}$$
- **ç­–ç•¥**ï¼š
$$\hat{W}=\mathop{\arg\max}\limits_{W} \prod_{i=1}^N P(Y^{(i)}|X^{(i)})$$
- **ç®—æ³•**ï¼š
æ”¹è¿›çš„è¿­ä»£å°ºåº¦ç®—æ³•ã€æ¢¯åº¦ä¸‹é™æ³•ã€æ‹Ÿç‰›é¡¿æ³•

> å‚è€ƒ[ã€NLPã€‘ä»éšé©¬å°”ç§‘å¤«åˆ°æ¡ä»¶éšæœºåœº](https://anxiang1836.github.io/2019/11/05/NLP_From_HMM_to_CRF/) ä»¥åŠè§†é¢‘[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åä¸ƒ)-æ¡ä»¶éšæœºåœºCRFï¼ˆConditional Random Fieldï¼‰](https://www.bilibili.com/video/BV19t411R7QU)

### é™„åŠ çŸ¥è¯†

#### éšæœºåœº

**éšæœºåœº**ï¼ˆ[Random field, RF](https://en.jinzhao.wiki/wiki/Random_field)ï¼‰æ˜¯ç”±è‹¥å¹²ä¸ªä½ç½®ç»„æˆçš„æ•´ä½“ï¼Œå½“ç»™æ¯ä¸€ä¸ªä½ç½®ä¸­æŒ‰ç…§æŸç§åˆ†å¸ƒï¼ˆæˆ–è€…æ˜¯æŸç§æ¦‚ç‡ï¼‰éšæœºèµ‹äºˆä¸€ä¸ªå€¼ä¹‹åï¼Œå…¶å…¨ä½“å°±å«åšéšæœºåœºã€‚

ä»¥è¯æ€§æ ‡æ³¨ä¸ºä¾‹ï¼š

å‡å¦‚æˆ‘ä»¬æœ‰10ä¸ªè¯å½¢æˆçš„å¥å­éœ€è¦åšè¯æ€§æ ‡æ³¨ã€‚è¿™10ä¸ªè¯æ¯ä¸ªè¯çš„è¯æ€§å¯ä»¥åœ¨æˆ‘ä»¬å·²çŸ¥çš„è¯æ€§é›†åˆï¼ˆåè¯ï¼ŒåŠ¨è¯â€¦ï¼‰ä¸­å»é€‰æ‹©ã€‚å½“æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯é€‰æ‹©å®Œè¯æ€§åï¼Œè¿™å°±å½¢æˆäº†ä¸€ä¸ªéšæœºåœºã€‚

**é©¬å°”ç§‘å¤«éšæœºåœº**ï¼ˆ[Markov random field, MRF](https://en.jinzhao.wiki/wiki/Markov_random_field)ï¼‰æ˜¯éšæœºåœºçš„ç‰¹ä¾‹ï¼Œå®ƒå‡è®¾éšæœºåœºä¸­æŸä¸€ä¸ªä½ç½®çš„èµ‹å€¼ä»…ä»…ä¸å’Œå®ƒç›¸é‚»çš„ä½ç½®çš„èµ‹å€¼æœ‰å…³ï¼Œå’Œä¸å…¶ä¸ç›¸é‚»çš„ä½ç½®çš„èµ‹å€¼æ— å…³ã€‚
æ¢ä¸€ç§è¡¨ç¤ºæ–¹å¼ï¼ŒæŠŠé©¬å°”ç§‘å¤«éšæœºåœºæ˜ å°„åˆ°æ— å‘å›¾ä¸­ã€‚æ­¤æ— å‘å›¾ä¸­çš„èŠ‚ç‚¹éƒ½ä¸æŸä¸ªéšæœºå˜é‡ç›¸å…³ï¼Œè¿æ¥ç€èŠ‚ç‚¹çš„è¾¹ä»£è¡¨ä¸è¿™ä¸¤ä¸ªèŠ‚ç‚¹æœ‰å…³çš„éšæœºå˜é‡ä¹‹é—´çš„å…³ç³»ã€‚
ç»§ç»­è¯æ€§æ ‡æ³¨ä¸ºä¾‹ï¼šï¼ˆè¿˜æ˜¯10ä¸ªè¯çš„å¥å­ï¼‰
å¦‚æœæˆ‘ä»¬å‡è®¾æ‰€æœ‰è¯çš„è¯æ€§ä»…ä¸å’Œå®ƒç›¸é‚»çš„è¯çš„è¯æ€§æœ‰å…³æ—¶ï¼Œè¿™ä¸ªéšæœºåœºå°±ç‰¹åŒ–æˆä¸€ä¸ªé©¬å°”ç§‘å¤«éšæœºåœºã€‚
æ¯”å¦‚ç¬¬3ä¸ªè¯çš„è¯æ€§é™¤äº†ä¸è‡ªå·±æœ¬èº«çš„ä½ç½®æœ‰å…³å¤–ï¼Œåªä¸ç¬¬2ä¸ªè¯å’Œç¬¬4ä¸ªè¯çš„è¯æ€§æœ‰å…³ã€‚

**æ¡ä»¶éšæœºåœº**(CRF)æ˜¯é©¬å°”ç§‘å¤«éšæœºåœºçš„ç‰¹ä¾‹ï¼Œå®ƒå‡è®¾é©¬å°”ç§‘å¤«éšæœºåœºä¸­åªæœ‰ğ‘‹å’Œğ‘Œä¸¤ç§å˜é‡ï¼Œğ‘‹ä¸€èˆ¬æ˜¯ç»™å®šçš„ï¼Œè€Œğ‘Œä¸€èˆ¬æ˜¯åœ¨ç»™å®šğ‘‹çš„æ¡ä»¶ä¸‹æˆ‘ä»¬çš„è¾“å‡ºã€‚è¿™æ ·é©¬å°”ç§‘å¤«éšæœºåœºå°±ç‰¹åŒ–æˆäº†æ¡ä»¶éšæœºåœºã€‚

åœ¨æˆ‘ä»¬10ä¸ªè¯çš„å¥å­è¯æ€§æ ‡æ³¨çš„ä¾‹å­ä¸­ï¼Œğ‘‹æ˜¯è¯ï¼Œğ‘Œæ˜¯è¯æ€§ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾å®ƒæ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«éšæœºåœºï¼Œé‚£ä¹ˆå®ƒä¹Ÿå°±æ˜¯ä¸€ä¸ªCRFã€‚
å¯¹äºCRFï¼Œæˆ‘ä»¬ç»™å‡ºå‡†ç¡®çš„æ•°å­¦è¯­è¨€æè¿°ï¼š
è®¾ğ‘‹ä¸ğ‘Œæ˜¯éšæœºå˜é‡ï¼ŒP(ğ‘Œ|ğ‘‹)æ˜¯ç»™å®šğ‘‹æ—¶ğ‘Œçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œè‹¥éšæœºå˜é‡ğ‘Œæ„æˆçš„æ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«éšæœºåœºï¼Œåˆ™ç§°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(ğ‘Œ|ğ‘‹)æ˜¯æ¡ä»¶éšæœºåœºã€‚

**çº¿æ€§é“¾æ¡ä»¶éšæœºåœº**(Linear-CRF)
æ³¨æ„åœ¨CRFçš„å®šä¹‰ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¦æ±‚ğ‘‹å’Œğ‘Œæœ‰ç›¸åŒçš„ç»“æ„ã€‚å½“ğ‘‹å’Œğ‘Œæœ‰ç›¸åŒç»“æ„ï¼Œå³ï¼š
$$X=(x_1,x_2,â€¦,x_T),Y=(y_1,y_2,â€¦,y_T)$$
è¿™ä¸ªæ—¶å€™ï¼Œğ‘‹å’Œğ‘Œæœ‰ç›¸åŒçš„ç»“æ„çš„CRFå°±æ„æˆäº†çº¿æ€§é“¾æ¡ä»¶éšæœºåœºã€‚

#### MEMM(Maximum Entropy Markov Model)
åˆ¤åˆ«æ¨¡å‹
[Maximum Entropy Markov Models for Information Extraction and Segmentation](http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf)
[Maximum Entropy Markov Models](http://www.cs.cornell.edu/courses/cs778/2006fa/lectures/05-memm.pdf)
[Hidden Markov Model and Naive Bayes relationship](https://aman.ai/primers/ai/hmm-and-naive-bayes/)
[Maximum Entropy Markov Models and Logistic Regression](https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/)

[Maximum-Entropy Markov Model](https://devopedia.org/maximum-entropy-markov-model)

![](https://devopedia.org/images/article/225/8864.1570601314.png)
MEMMä¸HMM
![](https://devopedia.org/images/article/225/6824.1570601351.png)

#### æ¦‚ç‡å›¾æ¨¡å‹
ä»‹ç»æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆProbabilistic Graphical Modelï¼‰ä¹‹å‰ï¼Œå…ˆç®€å•äº†è§£ä¸‹**ç»“æ„å­¦ä¹ **ï¼ˆ[Structured Learning](https://en.jinzhao.wiki/wiki/Structured_prediction)ï¼‰ï¼Œç›¸æ¯”äºå›å½’ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡æˆ–è€…é¢„æµ‹ï¼Œè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œç»“æ„åŒ–å­¦ä¹ çš„è¾“å‡ºæ›´åŠ å¤æ‚ï¼Œå¯ä»¥æ˜¯å›¾åƒï¼Œå¯ä»¥æ˜¯è¯­å¥ï¼Œå¯ä»¥æ˜¯æ ‘ç»“æ„ï¼Œç­‰ã€‚
é‚£ä¹ˆä¸æ¦‚ç‡å›¾æ¨¡å‹æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ
æ¦‚ç‡å›¾å½¢æ¨¡å‹å½¢æˆäº†å¤§é‡çš„ç»“æ„åŒ–é¢„æµ‹æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œè´å¶æ–¯ç½‘ç»œå’Œéšæœºåœºå¾ˆå—æ¬¢è¿ã€‚[å‚è§](https://en.jinzhao.wiki/wiki/Structured_prediction#Techniques)


[ä»€ä¹ˆæ˜¯ç»“æ„åŒ–å­¦ä¹ ï¼ŸWhat is structured learning?](https://pystruct.github.io/intro.html#intro)
ç»“æ„åŒ–é¢„æµ‹æ˜¯ç›‘ç£å­¦ä¹ ã€åˆ†ç±»å’Œå›å½’æ ‡å‡†èŒƒå¼çš„æ¦‚æ‹¬ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°æ¥æœ€å°åŒ–è®­ç»ƒé›†ä¸Šçš„ä¸€äº›æŸå¤±ã€‚åŒºåˆ«åœ¨äºä½¿ç”¨çš„å‡½æ•°ç±»å‹å’ŒæŸå¤±ã€‚
åœ¨åˆ†ç±»ä¸­ï¼Œç›®æ ‡åŸŸæ˜¯ç¦»æ•£çš„ç±»æ ‡ç­¾ï¼ŒæŸå¤±é€šå¸¸æ˜¯0-1çš„æŸå¤±ï¼Œå³å¯¹è¯¯åˆ†ç±»è¿›è¡Œè®¡æ•°ã€‚åœ¨å›å½’ä¸­ï¼Œç›®æ ‡åŸŸæ˜¯å®æ•°ï¼ŒæŸå¤±é€šå¸¸æ˜¯å‡æ–¹è¯¯å·®ã€‚åœ¨ç»“æ„åŒ–é¢„æµ‹ä¸­ï¼Œç›®æ ‡åŸŸå’ŒæŸå¤±æˆ–å¤šæˆ–å°‘éƒ½æ˜¯ä»»æ„çš„ã€‚è¿™æ„å‘³ç€ç›®æ ‡ä¸æ˜¯é¢„æµ‹æ ‡ç­¾æˆ–æ•°å­—ï¼Œè€Œæ˜¯å¯èƒ½æ›´å¤æ‚çš„å¯¹è±¡ï¼Œå¦‚åºåˆ—æˆ–å›¾å½¢ã€‚


**æ¦‚ç‡å›¾æ¨¡å‹**ï¼ˆ[Probabilistic Graphical Modelï¼ŒPGM](https://en.jinzhao.wiki/wiki/Graphical_model)ï¼‰ï¼Œç®€ç§°å›¾æ¨¡å‹ï¼ˆGraphical Modelï¼ŒGMï¼‰ï¼Œæ˜¯æŒ‡ä¸€ç§ç”¨å›¾ç»“æ„æ¥æè¿°å¤šå…ƒéšæœºå˜é‡ä¹‹é—´æ¡ä»¶ç‹¬ç«‹å…³ç³»çš„æ¦‚ç‡æ¨¡å‹ï¼Œä»è€Œç»™ç ”ç©¶é«˜ç»´ç©ºé—´ä¸­çš„æ¦‚ç‡æ¨¡å‹å¸¦æ¥äº†å¾ˆå¤§çš„ä¾¿æ·æ€§ã€‚
å¾ˆå¤šæœºå™¨å­¦ä¹ æ¨¡å‹éƒ½å¯ä»¥å½’ç»“ä¸ºæ¦‚ç‡æ¨¡å‹ï¼Œå³å»ºæ¨¡è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼å› æ­¤ï¼Œå›¾æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„è§’åº¦æ¥è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸”è¿™ç§è§’åº¦æœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œæ¯”å¦‚äº†è§£ä¸åŒæœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„è”ç³»ï¼Œæ–¹ä¾¿è®¾è®¡æ–°æ¨¡å‹ï¼ˆDeveloping Bayesian networksï¼‰ç­‰ï¼åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå›¾æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°ç”¨æ¥è®¾è®¡å’Œåˆ†æå„ç§å­¦ä¹ ç®—æ³•ï¼


**å›¾æ¨¡å‹æœ‰ä¸‰ä¸ªåŸºæœ¬é—®é¢˜**ï¼š
1. è¡¨ç¤ºï¼ˆRepresentationï¼‰é—®é¢˜ï¼šå¯¹äºä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œå¦‚ä½•é€šè¿‡å›¾ç»“æ„æ¥æè¿°å˜é‡ä¹‹é—´çš„ä¾
èµ–å…³ç³»ï¼
1. å­¦ä¹ ï¼ˆLearningï¼‰é—®é¢˜ï¼šå›¾æ¨¡å‹çš„å­¦ä¹ åŒ…æ‹¬å›¾ç»“æ„çš„å­¦ä¹ å’Œå‚æ•°çš„å­¦ä¹ ï¼åœ¨æœ¬ç« ä¸­ï¼Œ
æˆ‘ä»¬åªå…³æ³¨åœ¨ç»™å®šå›¾ç»“æ„æ—¶çš„å‚æ•°å­¦ä¹ ï¼Œå³å‚æ•°ä¼°è®¡é—®é¢˜ï¼
1. æ¨æ–­ï¼ˆInferenceï¼‰é—®é¢˜ï¼šåœ¨å·²çŸ¥éƒ¨åˆ†å˜é‡æ—¶ï¼Œè®¡ç®—å…¶ä»–å˜é‡çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ

$$
\begin{cases}
   Representation(è¡¨ç¤º) &  \begin{cases} \text{æœ‰å‘å›¾ Bayesian Network} \\ \text{æ— å‘å›¾ Markov Network} \end{cases} \\
   Learning(å­¦ä¹ ) & \begin{cases} \text{å‚æ•°å­¦ä¹ } & \begin{cases} \text{å®Œå¤‡æ•°æ®} \\ \text{éšå˜é‡} \to EM \end{cases} \\ \text{ç»“æ„å­¦ä¹ } \end{cases}\\
   Inference(æ¨æ–­) & \begin{cases} \text{ç²¾ç¡®æ¨æ–­} \\ \text{è¿‘ä¼¼æ¨æ–­} & \begin{cases} \text{ç¡®å®šæ€§è¿‘ä¼¼} \to å˜åˆ†æ¨æ–­ \\ \text{éšæœºè¿‘ä¼¼} \to MCMC \end{cases} \end{cases} \\
\end{cases}
$$


**å›¾çš„è¡¨ç¤º**ï¼š
å›¾å¯ä»¥ç”¨$G=(V,E)$è¡¨ç¤ºï¼Œ$V$æ˜¯é¡¶ç‚¹vertices(nodes or points)é›†åˆï¼Œ
${\displaystyle E\subseteq \{(x,y)\mid (x,y)\in V^{2}\;{\textrm {and}}\;x\neq y\}}$æ˜¯è¾¹çš„é›†åˆedges;å¯¹äºæœ‰å‘å›¾è€Œè¨€ï¼Œè¾¹æ˜¯æœ‰å‘çš„ï¼ˆdirected edges, directed links, directed lines, arrows or arcsï¼‰å®ƒä»¬æ˜¯æœ‰åºçš„é¡¶ç‚¹å¯¹ï¼Œä»£è¡¨ç€æ–¹å‘;å¯¹äºæ— å‘å›¾è€Œè¨€ï¼Œè¾¹æ˜¯æ— å‘çš„ã€‚

ä¹Ÿæœ‰äº›åœ°æ–¹æœ‰å‘è¾¹ä¸€èˆ¬ç”¨å°–æ‹¬å·è¡¨ç¤º<>ï¼›è€Œæ— å‘è¾¹ä¸€èˆ¬ç”¨å¼§å½¢æ‹¬å·è¡¨ç¤ºï¼ˆï¼‰ï¼›å¦‚ï¼š
æœ‰å‘å›¾ï¼š
$$G1=(V,E) \\ V(G1)=\{v1,v2,v3\}\\  E(G1)=\{\braket{v1,v2},\braket{v1,v3},\braket{v2,v3}\}$$

```mermaid
graph LR
    v1(("v1"))-->v2(("v2"))
    v1-->v3(("v3"))
    v2-->v3
```

æ— å‘å›¾ï¼š
$$G2=(V,E) \\ V(G2)=\{v1,v2,v3,v4\} \\ E(G2)=\{(vl,v2),(v1,v3),(v1,v4),(v2,v3),(v2,v4),(v3,v4)\}$$
```mermaid
graph LR
    v1(("v1"))---v2(("v2"))
    v1---v3(("v3"))
    v1---v4(("v4"))
    v2---v3
    v2---v4
    v3---v4
```
##### ï¼ˆæ¦‚ç‡ï¼‰æœ‰å‘å›¾æ¨¡å‹
æœ‰å‘å›¾æ¨¡å‹ï¼ˆDirected Graphical Modelï¼‰åˆç§°è´å¶æ–¯ç½‘ç»œï¼ˆ[Bayesian Network](https://en.jinzhao.wiki/wiki/Bayesian_network)ï¼‰æˆ–ä¿¡å¿µç½‘ç»œï¼ˆBelief Networkï¼ŒBNï¼‰æˆ–ï¼ˆcausal networksï¼‰æ˜¯ä¸€ç±»ç”¨æœ‰å‘å›¾ï¼ˆ[Directed Graphical](https://en.jinzhao.wiki/wiki/Graph_(discrete_mathematics)#Directed_graph)ï¼‰æ¥æè¿°éšæœºå‘é‡æ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å‹ï¼

> è¿™é‡Œæ˜¯ æœ‰å‘æ— ç¯å›¾(DAG)

å®šä¹‰å’Œæ¦‚ç‡ Definitions and conceptsï¼š
> parent çˆ¶èŠ‚ç‚¹
> descendants åä»£
> non-descendants éåä»£ï¼ˆä¸åŒ…æ‹¬çˆ¶ä»£ï¼Œä¹Ÿå°±æ˜¯all-parent-descendantsï¼‰

- **æ¦‚ç‡åˆ†å¸ƒçš„åˆ†è§£ï¼ˆFactorization definitionï¼‰**ï¼š
$X$æ˜¯ä¸€ä¸ªå…³äº$G$çš„è´å¶æ–¯ç½‘ç»œï¼Œå¦‚æœ$X$çš„è”åˆæ¦‚ç‡åˆ†å¸ƒ(è”åˆæ¦‚ç‡å¯†åº¦å‡½æ•°)å¯ä»¥å†™æˆã€å•ä¸ªå¯†åº¦å‡½æ•°çš„ä¹˜ç§¯ï¼Œæ¡ä»¶æ˜¯å®ƒä»¬çš„çˆ¶å˜é‡ã€‘ä¹Ÿå°±æ˜¯å±€éƒ¨æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼ˆLocal Conditional Probability Distributionï¼‰çš„è¿ä¹˜å½¢å¼ï¼ˆå¹¿ä¹‰çš„ä¸€é˜¶é©¬å¯å¤«æ€§è´¨ï¼‰:
$$p(X)=\prod _{v\in V}p\left(x_{v}\,{\big |}\,x_{\operatorname {pa} (v)}\right)$$
å…¶ä¸­$x_{\operatorname {pa} (v)}$è¡¨ç¤º$x_{v}$çš„çˆ¶äº²èŠ‚ç‚¹é›†åˆã€‚
å¦‚ï¼š
```mermaid
graph LR
    x1(("xâ‚"))-->x2(("xâ‚‚"))-->x4(("xâ‚„"))
    x1-->x3(("xâ‚ƒ"))
    x2-->x3
    x3-->x5(("xâ‚…"))
```
$X=x_1,x_2,x_3,x_4,x_5$
$V=\{x_1,x_2,x_3,x_4,x_5\}$
$E=\{\braket{x_1,x_2},\braket{x_1,x_3},\braket{x_2,x_3},\braket{x_2,x_4}\},\braket{x_3,x_5}$
$G=(V,E)$
æœ‰å‘å›¾å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸º
$$p(X) = p(x_1,x_2,x_3,x_4,x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_2)p(x_5|x_3)$$

- **å› æœç½‘ç»œ(Causal networks)**ï¼š
åœ¨è´å¶æ–¯ç½‘ç»œä¸­ï¼Œå¦‚æœä¸¤ä¸ªèŠ‚ç‚¹æ˜¯ç›´æ¥è¿æ¥çš„ï¼Œå®ƒä»¬è‚¯å®šæ˜¯éæ¡ä»¶ç‹¬ç«‹çš„ï¼Œæ˜¯ç›´æ¥å› æœå…³ç³»ï¼çˆ¶èŠ‚ç‚¹æ˜¯â€œå› â€(tail)ï¼Œå­èŠ‚ç‚¹æ˜¯â€œæœâ€ï¼ˆä¹Ÿå°±æ˜¯ç®­å¤´æŒ‡å‘çš„ï¼Œä¹Ÿç§°headï¼‰$tail \rightarrow head (å› \rightarrow æœ)$ã€‚å¦‚æœä¸¤ä¸ªèŠ‚ç‚¹ä¸æ˜¯ç›´æ¥è¿æ¥çš„ï¼Œä½†å¯ä»¥ç”±ä¸€æ¡ç»è¿‡å…¶ä»–èŠ‚ç‚¹çš„è·¯å¾„æ¥è¿æ¥ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„**æ¡ä»¶ç‹¬ç«‹æ€§**å°±æ¯”è¾ƒå¤æ‚ã€‚
ä»¥ä¸‰ä¸ªèŠ‚ç‚¹çš„è´å¶æ–¯ç½‘ç»œä¸ºä¾‹

Pattern|Model | æ¡ä»¶ç‹¬ç«‹æ€§
---|---|---
Chainï¼ˆé—´æ¥å› æœå…³ç³»/tail to headï¼‰	|$X\rightarrow Y\rightarrow Z$ | å·²çŸ¥Yæ—¶,Xå’ŒZä¸ºæ¡ä»¶ç‹¬ç«‹ï¼Œå³ $X \perp \!\!\!\perp Z\mid Y$
Forkï¼ˆå…±å› å…³ç³»/tail to tailï¼‰|$X\leftarrow Y\rightarrow Z$ | å·²çŸ¥Yæ—¶,Xå’ŒZä¸ºæ¡ä»¶ç‹¬ç«‹ï¼Œå³ $X \perp \!\!\!\perp Z \mid Y$ ï¼ˆYæœªçŸ¥æ—¶ï¼ŒXå’ŒZä¸ºä¸ç‹¬ç«‹ï¼‰
Colliderï¼ˆå…±æœå…³ç³»/head to headï¼‰	|$X\rightarrow Y\leftarrow Z$ | å·²çŸ¥Yæ—¶,Xå’ŒZä¸ºä¸ç‹¬ç«‹ï¼Œå³ $X \perp \!\!\!\perp \!\!\!\!\!\!/ \;\; Z \mid Y$ï¼ˆYæœªçŸ¥æ—¶ï¼ŒXå’ŒZä¸ºç‹¬ç«‹ï¼‰


- **å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨ï¼ˆLocal Markov propertyï¼‰**ï¼š
å¯¹ä¸€ä¸ªæ›´ä¸€èˆ¬çš„è´å¶æ–¯ç½‘ç»œï¼Œå…¶å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨ä¸ºï¼šæ¯ä¸ªéšæœºå˜é‡åœ¨ç»™å®šçˆ¶èŠ‚ç‚¹çš„æƒ…å†µä¸‹ï¼Œæ¡ä»¶ç‹¬ç«‹äºå®ƒçš„éåä»£èŠ‚ç‚¹ï¼ 
$${\displaystyle X_{v}\perp \!\!\!\perp X_{V\,\smallsetminus \,\operatorname {de} (v)}\mid X_{\operatorname {pa} (v)}\quad {\text{for all }}v\in V}$$
å…¶ä¸­$X_{V\,\smallsetminus \,\operatorname {de} (v)}$è¡¨ç¤ºéåä»£é›†åˆ

- **é©¬å°”å¯å¤«æ¯¯**ï¼ˆ[Markov blanket](https://en.jinzhao.wiki/wiki/Markov_blanket)ï¼‰ï¼š
åœ¨éšæœºå˜é‡çš„å…¨é›†U UUä¸­ï¼Œå¯¹äºç»™å®šçš„å˜é‡$X\in U$å’Œå˜é‡é›†$MB\subset U(X\notin MB)$ï¼Œè‹¥æœ‰
$$X\perp \!\!\!\perp\{U-MB-\{X\}\}|MB$$
åˆ™ç§°èƒ½æ»¡è¶³ä¸Šè¿°æ¡ä»¶çš„æœ€å°å˜é‡é›†$MB$ä¸º$X$çš„é©¬å°”å¯å¤«æ¯¯(Markov Blanket)ã€‚


- **Dåˆ’åˆ†ï¼ˆd-separationï¼‰**ï¼š
dè¡¨ç¤ºæ–¹å‘ï¼ˆdirectionalï¼‰ã€‚pæ˜¯u to vçš„å»é™¤æ–¹å‘çš„è·¯å¾„ã€‚pè¢«ä¸€ç»„èŠ‚ç‚¹Zåˆ†éš”ã€‚
  - å¦‚æœpæ˜¯è¿™æ ·çš„è·¯å¾„ ${\displaystyle u\cdots \leftarrow m\leftarrow \cdots v}$ or ${\displaystyle u\cdots \rightarrow m\rightarrow \cdots v}$ å¹¶ä¸”$m \in Z$
  - å¦‚æœpæ˜¯è¿™æ ·çš„è·¯å¾„ ${\displaystyle u\cdots \leftarrow m\rightarrow \cdots v}$ å¹¶ä¸”$m \in Z$
  - å¦‚æœpæ˜¯è¿™æ ·çš„è·¯å¾„ ${\displaystyle u\cdots \rightarrow m\leftarrow \cdots v}$ å¹¶ä¸”$m \notin Z$
$$X_{u}\perp \!\!\!\perp X_{v}\mid X_{Z}$$


- **å¸¸è§çš„æœ‰å‘å›¾æ¨¡å‹**ï¼š
å¦‚æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€éšé©¬å°”å¯å¤«æ¨¡å‹ã€æ·±åº¦ä¿¡å¿µç½‘ç»œç­‰
æœ´ç´ è´å¶æ–¯ï¼šå‡è®¾è¾“å…¥Xæœ‰ä¸‰ä¸ªç‰¹å¾
```mermaid
graph TD
    y(("y"))
    y-->x1(("xâ‚"))
    y-->x2(("xâ‚‚"))
    y-->x3(("xâ‚ƒ"))
    style y fill:#fff
    style x1 fill:#f96
    style x2 fill:#f96
    style x3 fill:#f96
```
ç”±å›¾å¯å¾—
$$P(y,x_1,x_2,x_3) = P(y)P(x_1|y)P(x_2|y)P(x_3|y) = P(x_1,x_2,x_3|y)P(y) \\ \Darr\\  P(x_1,x_2,x_3|y)=P(x_1|y)P(x_2|y)P(x_3|y)$$
è¿™ä¸å°±æ˜¯æœ´ç´ è´å¶æ–¯çš„æ¡ä»¶ç›¸äº’ç‹¬ç«‹çš„å‡è®¾ä¹ˆ?$P(X|y) = \prod_{i=1}^n P(x_i|y)$
è€Œè¿™ä¸ªç‹¬ç«‹å‡è®¾å¤ªå¼ºäº†ï¼Œæ¯ä¸ªç‰¹å¾ä¹‹é—´æ²¡æœ‰ä»»ä½•å…³ç³»ï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒi.i.d.ï¼‰ï¼›
é‚£ä¹ˆæˆ‘ä»¬å‡è®¾å½“å‰åªä¸å‰ä¸€æ—¶åˆ»æœ‰å…³ï¼Œä¸å…¶å®ƒæ— å…³ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æœ‰äº†Markovå‡è®¾ï¼Œå¦‚éšé©¬å°”å¯å¤«æ¨¡å‹ï¼š
å…¶ä¸­yä¸ºéšå˜é‡ï¼Œxä¸ºè§‚æµ‹å˜é‡
```mermaid
graph LR
    y1(("yâ‚"))
    y1-->x1(("xâ‚"))
    y1-->y2(("yâ‚‚"))
    y2-->x2(("xâ‚‚"))
    y2-->y3(("yâ‚ƒ"))
    y3-->x3(("xâ‚ƒ"))
    y3-->y4(("yâ‚„"))
    y4-->x4(("xâ‚„"))
    style y1 fill:#fff
    style y2 fill:#fff
    style y3 fill:#fff
    style y4 fill:#fff
    style x1 fill:#f96
    style x2 fill:#f96
    style x3 fill:#f96
    style x4 fill:#f96
```
æˆ‘ä»¬èƒ½ä»å›¾ä¸­ç›´æ¥å¾—åˆ°
$P(y_t|y_{t-1},...,y_1,x_{t-1},...,x_1) = P(y_t|y_{t-1})$ï¼Œå³Markovå‡è®¾ï¼›
$P(x_t|x_{T},...,x_{t+1},x_{t-1},...,x_1,Y) = P(x_t|y_{t})$ï¼Œå³è§‚æµ‹ç‹¬ç«‹æ€§å‡è®¾ï¼›

##### ï¼ˆæ¦‚ç‡ï¼‰æ— å‘å›¾æ¨¡å‹
æ— å‘å›¾æ¨¡å‹ï¼ˆUndirected Graphical Modelï¼‰åˆç§°é©¬å°”å¯å¤«éšæœºåœºï¼ˆ[Markov random field, MRF](https://en.jinzhao.wiki/wiki/Markov_random_field)ï¼‰æˆ–é©¬å°”å¯å¤«ç½‘ç»œï¼ˆMarkov networkï¼‰æ˜¯ä¸€ç±»ç”¨æ— å‘å›¾ï¼ˆ[Undirected Graphical](https://en.jinzhao.wiki/wiki/Graph_(discrete_mathematics)#Undirected_graph)ï¼‰æ¥æè¿°ä¸€ç»„å…·æœ‰å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨çš„éšæœºå‘é‡ğ‘¿çš„è”åˆæ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å‹ï¼
ä»¥ä¸‹å®šä¹‰æ˜¯ç­‰ä»·çš„
$$\text{Global Markov} \iff \text{Local Markov}\iff\text{Pair Markov}\xLeftrightarrow{Hammesleyâˆ’Clifford } å› å­åˆ†è§£$$



- **å›¢åˆ†è§£ï¼Œå› å­åˆ†è§£**ï¼ˆClique factorizationï¼‰ï¼š
æ— å‘å›¾Gä¸­ä»»ä½•ä¸¤ä¸ªç»“ç‚¹å‡æœ‰è¾¹è¿æ¥çš„ç»“ç‚¹å­é›†ç§°ä¸º**å›¢**ï¼ˆcliqueï¼‰ã€‚è‹¥Cæ˜¯æ— å‘å›¾Gçš„ä¸€ä¸ªå›¢ï¼Œå¹¶ä¸”ä¸èƒ½å†åŠ è¿›ä»»ä½•ä¸€ä¸ªGçš„ç»“ç‚¹ä½¿å…¶æˆä¸ºä¸€ä¸ªæ›´å¤§çš„å›¢ï¼Œåˆ™ç§°æ­¤Cä¸º**æœ€å¤§å›¢**ï¼ˆmaximal cliqueï¼‰ã€‚
å°†æ¦‚ç‡æ— å‘å›¾æ¨¡å‹çš„è”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºä¸ºå…¶æœ€å¤§å›¢ä¸Šçš„éšæœºå˜é‡çš„å‡½æ•°çš„ä¹˜ç§¯å½¢å¼çš„æ“ä½œï¼Œç§°ä¸ºæ¦‚ç‡æ— å‘å›¾æ¨¡å‹çš„å› å­åˆ†è§£ï¼ˆfactorizationï¼‰ã€‚
ç»™å®šæ¦‚ç‡æ— å‘å›¾æ¨¡å‹ï¼Œè®¾å…¶æ— å‘å›¾ä¸ºGï¼Œéšæœºå˜é‡${\displaystyle X=(X_{v})_{v\in V}}$ï¼ŒCä¸ºGä¸Šçš„æœ€å¤§å›¢ï¼Œ$X_C$è¡¨ç¤ºCå¯¹åº”çš„éšæœºå˜é‡ã€‚é‚£ä¹ˆæ¦‚ç‡æ— å‘å›¾æ¨¡å‹çš„è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X)$å¯å†™ä½œå›¾ä¸­æ‰€æœ‰æœ€å¤§å›¢Cä¸Šçš„å‡½æ•°$\phi_C (x_C)$çš„ä¹˜ç§¯å½¢å¼ï¼Œå³
$$P(X) =\frac{1}{Z} \prod_{C \in \operatorname{cl}(G)} \phi_C (X_C) $$
$$Z=\sum_{X}\prod_{C \in \operatorname{cl}(G)} \phi_C (X_C)$$
Zæ˜¯è§„èŒƒåŒ–å› å­ï¼ˆnormalization factorï¼‰æˆ–å½’ä¸€åŒ–å› å­ä¹Ÿè¢«ç§°ä¸ºé…åˆ†å‡½æ•°ï¼ˆpartition functionï¼‰;
$\phi_C (X_C)$ç§°ä¸ºåŠ¿å‡½æ•°ï¼ˆpotential function or factor potentials or clique potentialsï¼‰ï¼ŒåŠ¿å‡½æ•°è¦æ±‚æ˜¯ä¸¥æ ¼æ­£çš„ï¼Œé€šå¸¸å®šä¹‰ä¸ºæŒ‡æ•°å‡½æ•°ï¼š
$$\phi_C (X_C) = \exp\{-E(X_C)\}$$
å…¶ä¸­Eä¸ºèƒ½é‡å‡½æ•°ï¼ˆenergy functionï¼‰ã€‚
å®é™…ä¸Šç”¨è¿™ç§å½¢å¼è¡¨è¾¾çš„p(x)ï¼Œä¸ºGibbs Distributionï¼Œæˆ–è€…åˆè¢«ç§°ä¹‹ä¸ºBoltzman Distributionã€‚å¯ä»¥å†™æˆï¼š
$$P(x) = \frac{1}{Z} \prod_{i=1}^K \phi (x_{C_{i}}) = \frac{1}{Z} \prod_{i=1}^K \exp\{-E(x_{C_{i}})\} =  \frac{1}{Z}\exp\{-\sum_{i=1}^K E(x_{C_{i}})\} = \frac{1}{Z}\exp\sum_{i=1}^K F_i(x_{ci})ï¼Œx \in \mathbb{R}^{p}$$
$x \in \mathbb{R}^p$æ˜¯ä¸ªè”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒçš„ç»´åº¦æ˜¯$p$ç»´ï¼›$\phi$è¡¨ç¤ºåŠ¿å‡½æ•°ï¼›$E$è¡¨ç¤ºèƒ½é‡å‡½æ•°ï¼›$K$è¡¨ç¤ºæœ€å¤§å›¢çš„ä¸ªæ•°ï¼›$C_i$è¡¨ç¤ºç¬¬$i$ä¸ªæœ€å¤§å›¢ã€‚
æˆ‘ä»¬å°†æŒ‡æ•°æ—åˆ†å¸ƒå’ŒåŠ¿å‡½æ•°è”ç³»èµ·æ¥ï¼š
$${\displaystyle p(x\mid {\boldsymbol {\eta }})=h(x)\,\exp {\Big (}{\boldsymbol {\eta^T }}\cdot \mathbf {T} (x)-A({\boldsymbol {\eta }}){\Big )}} = h(x) \frac{1}{Z(\eta)}\exp\{\eta^T \cdot \mathbf {T} (x)\}$$
å‘ç°åŠ¿å‡½æ•°(Gibbs Distribution)æ˜¯ä¸€ä¸ªæŒ‡æ•°æ—åˆ†å¸ƒã€‚Gibbsæ˜¯æ¥è‡ªç»Ÿè®¡ç‰©ç†å­¦ï¼Œå½¢å¼ä¸Šå’ŒæŒ‡æ•°æ—åˆ†å¸ƒæ—¶ä¸€æ ·çš„ã€‚è€ŒæŒ‡æ•°æ—åˆ†å¸ƒå®é™…ä¸Šæ˜¯ç”±æœ€å¤§ç†µåˆ†å¸ƒå¾—åˆ°çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç†è§£Gibbsåˆ†å¸ƒä¹Ÿæ˜¯æœ‰æœ€å¤§ç†µåŸç†å¾—åˆ°çš„ã€‚è€Œé©¬å°”å¯å¤«éšæœºåœº(Markov Random Field)å®é™…ä¸Šç­‰ä»·äºGibbsåˆ†å¸ƒã€‚å³ï¼š
æœ€å¤§ç†µåŸç† â‡’ æŒ‡æ•°æ—åˆ†å¸ƒ(Gibbsåˆ†å¸ƒ).
Markov Random Field â‡” Gibbs Distribution.

- **æˆå¯¹é©¬å°”å¯å¤«æ€§**ï¼ˆPairwise Markov propertyï¼‰ï¼š
ä»»æ„ä¸¤ä¸ªä¸ç›¸é‚»çš„å˜é‡åœ¨ç»™å®šå…¶ä»–å˜é‡çš„æ¡ä»¶ä¸‹æ˜¯ç‹¬ç«‹çš„:${\displaystyle X_{u}\perp \!\!\!\perp X_{v}\mid X_{V\setminus \{u,v\}}}$
- **å±€éƒ¨é©¬å°”å¯å¤«æ€§**ï¼ˆLocal Markov propertyï¼‰ï¼š
ä¸€ä¸ªå˜é‡åœ¨ç»™å®šå…¶ç›¸é‚»å˜é‡çš„æ¡ä»¶ä¸‹æ˜¯ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–å˜é‡:${\displaystyle X_{v}\perp \!\!\!\perp X_{V\setminus \operatorname {N} [v]}\mid X_{\operatorname {N} (v)}}$
å…¶ä¸­$\operatorname {N} (v)$æ˜¯vçš„é‚»å±…ï¼ˆneighborï¼‰èŠ‚ç‚¹ï¼›${\displaystyle \operatorname {N} [v]=v\cup \operatorname {N} (v)}$
- **å…¨å±€é©¬å°”å¯å¤«æ€§**ï¼ˆGlobal Markov propertyï¼‰ï¼š
ç»™å®šä¸€ä¸ªåˆ†ç¦»å­é›†ï¼ˆseparating subsetï¼‰ï¼Œä»»æ„ä¸¤ä¸ªå˜é‡å­é›†éƒ½æ˜¯æ¡ä»¶ç‹¬ç«‹çš„:$X_A \perp\!\!\!\perp X_B \mid X_S$
Aä¸­çš„èŠ‚ç‚¹åˆ°Bä¸­çš„èŠ‚ç‚¹éƒ½è¦ç»è¿‡Sï¼›

- **é“å¾·å›¾**ï¼ˆMoral graphï¼‰ï¼š
æœ‰å‘å›¾å’Œæ— å‘å›¾å¯ä»¥ç›¸äº’è½¬æ¢ï¼Œä½†å°†æ— å‘å›¾è½¬ä¸ºæœ‰å‘å›¾é€šå¸¸æ¯”è¾ƒå›°éš¾ï¼åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå°†æœ‰å‘å›¾è½¬ä¸ºæ— å‘å›¾æ›´åŠ é‡è¦ï¼Œè¿™æ ·å¯ä»¥åˆ©ç”¨æ— å‘å›¾ä¸Šçš„ç²¾ç¡®æ¨æ–­ç®—æ³•ï¼Œæ¯”å¦‚è”åˆæ ‘ç®—æ³•ï¼ˆJunction Tree Algorithmï¼‰ï¼
æœ‰å‘å›¾è½¬åŒ–æˆæ— å‘å›¾çš„è¿‡ç¨‹ç§°ä¸ºé“å¾·åŒ–ï¼ˆMoralizationï¼‰ï¼Œè½¬åŒ–åçš„æ— å‘å›¾ç§°ä¸ºé“å¾·å›¾ï¼ˆ[Moral graph](https://en.jinzhao.wiki/wiki/Moral_graph)ï¼‰ã€‚
æ¯ä¸ªæœ‰å‘å›¾åˆ†è§£çš„å› å­è¦å¤„äºä¸€ä¸ªæœ€å¤§å›¢ä¸­ï¼Œå¦‚ï¼š
$$P(X) = p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)$$
å…¶ä¸­$p(x_4|x_1,x_2,x_3)$æœ‰å››ä¸ªå˜é‡ï¼Œé‚£ä¹ˆï¼š
```mermaid
graph TD
    x1(("xâ‚"))
    x2(("xâ‚‚"))
    x3(("xâ‚ƒ"))
    x4(("xâ‚„"))
    x1-->x4
    x2-->x4
    x3-->x4
    y1(("xâ‚"))
    y2(("xâ‚‚"))
    y3(("xâ‚ƒ"))
    y4(("xâ‚„"))
    y1---y2
    y1---y3
    y1---y4
    y2---y3
    y2---y4
    y3---y4
```
> é“å¾·åŒ–çš„è¿‡ç¨‹ä¸­ï¼ŒåŸæœ‰çš„ä¸€äº›æ¡ä»¶ç‹¬ç«‹æ€§ä¼šä¸¢å¤±ã€‚

- **å› å­å›¾**ï¼ˆFactor graphï¼‰ï¼š
è¿™é‡Œä¸ä½œä»‹ç»ï¼Œç›®å‰ä¸å¤ªæ˜ç™½ç”¨å¤„ã€‚


- **å¸¸è§çš„æœ‰å‘å›¾æ¨¡å‹**ï¼š
å¯¹æ•°çº¿æ€§æ¨¡å‹ï¼ˆæœ€å¤§ç†µæ¨¡å‹ï¼‰ã€æ¡ä»¶éšæœºåœºã€ç»å°”å…¹æ›¼æœºã€å—é™ç»å°”å…¹æ›¼æœºç­‰ï¼

> ä»¥ä¸Šå†…å®¹åªæ˜¯è®²åˆ°äº†æ¦‚ç‡å›¾çš„è¡¨ç¤ºã€‚

### å‚è€ƒæ–‡çŒ®

[11-1] Bishop M. Pattern Recognition and Machine Learning. Springer-Verlag,2006

[11-2] Koller D,Friedman N. Probabilistic Graphical Models: Principles and Techniques.MIT Press,2009

[11-3] Lafferty J,McCallum A,Pereira F. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In: International Conference on Machine Learning,2001

[11-4] Sha F,Pereira F. Shallow parsing with conditional random fields. In: Proceedings ofthe 2003 Conference of the North American Chapter of Association for ComputationalLinguistics on Human Language Technology,Vol.1,2003

[11-5] McCallum A,Freitag D,Pereira F. Maximum entropy Markov models for informationextraction and segmentation. In: Proc of the International Conference on Machine Learning,2000

[11-6] Taskar B,Guestrin C,Koller D. Max-margin Markov networks. In: Proc of the NIPS2003,2003

[11-7] Tsochantaridis I,Hofmann T,Joachims T. Support vector machine learning forinterdependent and structured output spaces. In: ICML,2004

## ç¬¬ 12 ç«  ç›‘ç£å­¦ä¹ æ–¹æ³•æ€»ç»“
å‚è€ƒï¼š[ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹](../ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹.md)
| åˆ†ç±»                                                       | æ–¹æ³•                                       | é€‚ç”¨é—®é¢˜           | æ¨¡å‹ç‰¹ç‚¹                                           | æ¨¡å‹ç±»åˆ«           | å­¦ä¹ ç­–ç•¥                           | æŸå¤±å‡½æ•°             | å­¦ä¹ ç®—æ³•                               |
| ---------------------------------------------------------- | ------------------------------------------ | ------------------ | -------------------------------------------------- | ------------------ | ---------------------------------- | -------------------- | -------------------------------------- |
| ç›‘ç£                                                       | æ„ŸçŸ¥æœº                                     | äºŒåˆ†ç±»             | åˆ†ç¦»è¶…å¹³é¢                                         | åˆ¤åˆ«æ¨¡å‹           | æå°åŒ–è¯¯åˆ†ç‚¹åˆ°è¶…å¹³é¢è·ç¦»           | è¯¯åˆ†ç‚¹åˆ°è¶…å¹³é¢è·ç¦»   | éšæœºæ¢¯åº¦ä¸‹é™                           |
| ç›‘ç£                                                       | k è¿‘é‚»æ³•                                   | å¤šåˆ†ç±»ã€å›å½’       | ç‰¹å¾ç©ºé—´ã€æ ·æœ¬ç‚¹                                   | åˆ¤åˆ«æ¨¡å‹           | â€”                                  | â€”                    | â€”                                      |
| ç›‘ç£                                                       | æœ´ç´ è´å¶æ–¯                                 | å¤šåˆ†ç±»             | ç‰¹å¾ä¸ç±»åˆ«çš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œæ¡ä»¶ç‹¬ç«‹å‡è®¾             | ç”Ÿæˆæ¨¡å‹           | æå¤§ä¼¼ç„¶ä¼°è®¡ã€æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡     | å¯¹æ•°ä¼¼ç„¶æŸå¤±         | æ¦‚ç‡è®¡ç®—å…¬å¼ã€EM ç®—æ³•                  |
| ç›‘ç£                                                       | å†³ç­–æ ‘                                     | å¤šåˆ†ç±»ã€å›å½’       | åˆ†ç±»æ ‘ã€å›å½’æ ‘                                     | åˆ¤åˆ«æ¨¡å‹           | æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡               | å¯¹æ•°ä¼¼ç„¶æŸå¤±         | ç‰¹å¾é€‰æ‹©ã€ç”Ÿæˆã€å‰ªæ                   |
| ç›‘ç£                                                       | é€»è¾‘æ–¯è’‚å›å½’                               | å¤šåˆ†ç±»             | ç‰¹å¾æ¡ä»¶ä¸‹ç±»åˆ«çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå¯¹æ•°çº¿æ€§æ¨¡å‹         | åˆ¤åˆ«æ¨¡å‹           | æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡ | é€»è¾‘æ–¯è’‚æŸå¤±         | æ”¹è¿›çš„è¿­ä»£å°ºåº¦ç®—æ³•ï¼Œæ¢¯åº¦ä¸‹é™ï¼Œæ‹Ÿç‰›é¡¿æ³• |
| ç›‘ç£                                                       | æ”¯æŒå‘é‡æœº                                 | äºŒåˆ†ç±»             | åˆ†ç¦»è¶…å¹³é¢ï¼Œæ ¸æŠ€å·§                                 | åˆ¤åˆ«æ¨¡å‹           | æå°åŒ–æ­£åˆ™åŒ–åˆé¡µæŸå¤±ï¼Œè½¯é—´éš”æœ€å¤§åŒ– | åˆé¡µæŸå¤±             | åºåˆ—æœ€å°æœ€ä¼˜åŒ–ç®—æ³•ï¼ˆSMO)               |
| ç›‘ç£                                                       | æå‡æ–¹æ³• (Boosting)                        | äºŒåˆ†ç±»             | å¼±åˆ†ç±»å™¨çš„çº¿æ€§ç»„åˆ                                 | åˆ¤åˆ«æ¨¡å‹           | æå°åŒ–åŠ æ³•æ¨¡å‹çš„æŒ‡æ•°æŸå¤±           | æŒ‡æ•°æŸå¤±             | å‰å‘åˆ†å¸ƒåŠ æ³•ç®—æ³•                       |
| ç›‘ç£                                                       | EM ç®—æ³•                                    | æ¦‚ç‡æ¨¡å‹å‚æ•°ä¼°è®¡   | å«éšå˜é‡æ¦‚ç‡æ¨¡å‹                                   | â€”                  | æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡     | å¯¹æ•°ä¼¼ç„¶æŸå¤±         | è¿­ä»£ç®—æ³•                               |
| ç›‘ç£                                                       | éšé©¬å°”ç§‘å¤«æ¨¡å‹(HMM)                        | æ ‡æ³¨               | è§‚æµ‹åºåˆ—ä¸çŠ¶æ€åºåˆ—çš„è”åˆæ¦‚ç‡åˆ†å¸ƒæ¨¡å‹               | ç”Ÿæˆæ¨¡å‹           | æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡     | å¯¹æ•°ä¼¼ç„¶æŸå¤±         | æ¦‚ç‡è®¡ç®—å…¬å¼ï¼ŒEM ç®—æ³•                  |
| ç›‘ç£                                                       | æœ€å¤§ç†µé©¬å°”ç§‘å¤«æ¨¡å‹(MEMM)                                       | æ ‡æ³¨               | -                                                  | åˆ¤åˆ«æ¨¡å‹           |
| ç›‘ç£                                                       | æ¡ä»¶éšæœºåœº(CRF)                            | æ ‡æ³¨               | çŠ¶æ€åºåˆ—æ¡ä»¶ä¸‹è§‚æµ‹åºåˆ—çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå¯¹æ•°çº¿æ€§æ¨¡å‹ | åˆ¤åˆ«æ¨¡å‹           | æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæ­£åˆ™åŒ–æå¤§ä¼¼ç„¶ä¼°è®¡   | å¯¹æ•°ä¼¼ç„¶æŸå¤±         | æ”¹è¿›çš„è¿­ä»£å°ºåº¦ç®—æ³•ï¼Œæ¢¯åº¦ä¸‹é™ï¼Œæ‹Ÿç‰›é¡¿æ³• |
| ç›‘ç£                                                       | é©¬å°”å¯å¤«éšæœºåœº Markov Random Fields        | -                  | -                                                  | ç”Ÿæˆæ¨¡å‹           |


**æ¨¡å‹**
åˆ†ç±»é—®é¢˜ä¸æ ‡æ³¨é—®é¢˜çš„é¢„æµ‹æ¨¡å‹éƒ½å¯ä»¥è®¤ä¸ºæ˜¯è¡¨ç¤ºä»è¾“å…¥ç©ºé—´åˆ°è¾“å‡ºç©ºé—´çš„æ˜ å°„ã€‚å®ƒä»¬å¯ä»¥å†™æˆæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(Y|X)æˆ–å†³ç­–å‡½æ•°Yï¼f(X)çš„å½¢å¼ã€‚å‰è€…è¡¨ç¤ºç»™å®šè¾“å…¥æ¡ä»¶ä¸‹è¾“å‡ºçš„æ¦‚ç‡æ¨¡å‹ï¼Œåè€…è¡¨ç¤ºè¾“å…¥åˆ°è¾“å‡ºçš„éæ¦‚ç‡æ¨¡å‹ã€‚æœ‰æ—¶ï¼Œæ¨¡å‹æ›´ç›´æ¥åœ°è¡¨ç¤ºä¸ºæ¦‚ç‡æ¨¡å‹ï¼Œæˆ–è€…éæ¦‚ç‡æ¨¡å‹ï¼›ä½†æœ‰æ—¶æ¨¡å‹å…¼æœ‰ä¸¤ç§è§£é‡Šã€‚

æœ´ç´ è´å¶æ–¯æ³•ã€éšé©¬å°”å¯å¤«æ¨¡å‹æ˜¯æ¦‚ç‡æ¨¡å‹ã€‚æ„ŸçŸ¥æœºã€kè¿‘é‚»æ³•ã€æ”¯æŒå‘é‡æœºã€æå‡æ–¹æ³•æ˜¯éæ¦‚ç‡æ¨¡å‹ã€‚è€Œå†³ç­–æ ‘ã€é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æ¡ä»¶éšæœºåœºæ—¢å¯ä»¥çœ‹ä½œæ˜¯æ¦‚ç‡æ¨¡å‹ï¼Œåˆå¯ä»¥çœ‹ä½œæ˜¯éæ¦‚ç‡æ¨¡å‹ã€‚

ç›´æ¥å­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(Y|X)æˆ–å†³ç­–å‡½æ•°Yï¼f(X)çš„æ–¹æ³•ä¸ºåˆ¤åˆ«æ–¹æ³•ï¼Œå¯¹åº”çš„æ¨¡å‹æ˜¯åˆ¤åˆ«æ¨¡å‹ã€‚æ„ŸçŸ¥æœºã€kè¿‘é‚»æ³•ã€å†³ç­–æ ‘ã€é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æ”¯æŒå‘é‡æœºã€æå‡æ–¹æ³•ã€æ¡ä»¶éšæœºåœºæ˜¯åˆ¤åˆ«æ–¹æ³•ã€‚é¦–å…ˆå­¦ä¹ è”åˆæ¦‚ç‡åˆ†å¸ƒP(X,Y)ï¼Œä»è€Œæ±‚å¾—æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(Y|X)çš„æ–¹æ³•æ˜¯ç”Ÿæˆæ–¹æ³•ï¼Œå¯¹åº”çš„æ¨¡å‹æ˜¯ç”Ÿæˆæ¨¡å‹ã€‚æœ´ç´ è´å¶æ–¯æ³•ã€éšé©¬å°”å¯å¤«æ¨¡å‹æ˜¯ç”Ÿæˆæ–¹æ³•ã€‚


å¯ä»¥ç”¨éç›‘ç£å­¦ä¹ çš„æ–¹æ³•å­¦ä¹ ç”Ÿæˆæ¨¡å‹ã€‚å…·ä½“åœ°ï¼Œåº”ç”¨EMç®—æ³•å¯ä»¥å­¦ä¹ æœ´ç´ è´å¶æ–¯æ¨¡å‹ä»¥åŠéšé©¬å°”å¯å¤«æ¨¡å‹ã€‚

å†³ç­–æ ‘æ˜¯å®šä¹‰åœ¨ä¸€èˆ¬çš„ç‰¹å¾ç©ºé—´ä¸Šçš„ï¼Œå¯ä»¥å«æœ‰è¿ç»­å˜é‡æˆ–ç¦»æ•£å˜é‡ã€‚æ„ŸçŸ¥æœºã€æ”¯æŒå‘é‡æœºã€kè¿‘é‚»æ³•çš„ç‰¹å¾ç©ºé—´æ˜¯æ¬§æ°ç©ºé—´ï¼ˆæ›´ä¸€èˆ¬åœ°ï¼Œæ˜¯å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼‰ã€‚æå‡æ–¹æ³•çš„æ¨¡å‹æ˜¯å¼±åˆ†ç±»å™¨çš„çº¿æ€§ç»„åˆï¼Œå¼±åˆ†ç±»å™¨çš„ç‰¹å¾ç©ºé—´å°±æ˜¯æå‡æ–¹æ³•æ¨¡å‹çš„ç‰¹å¾ç©ºé—´ã€‚

æ„ŸçŸ¥æœºæ¨¡å‹æ˜¯çº¿æ€§æ¨¡å‹ï¼Œè€Œé€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æ¡ä»¶éšæœºåœºæ˜¯å¯¹æ•°çº¿æ€§æ¨¡å‹ã€‚kè¿‘é‚»æ³•ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºï¼ˆåŒ…å«æ ¸å‡½æ•°ï¼‰ã€æå‡æ–¹æ³•ä½¿ç”¨çš„æ˜¯éçº¿æ€§æ¨¡å‹ã€‚

**å­¦ä¹ ç­–ç•¥**
åœ¨äºŒç±»åˆ†ç±»çš„ç›‘ç£å­¦ä¹ ä¸­ï¼Œæ”¯æŒå‘é‡æœºã€é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æå‡æ–¹æ³•å„è‡ªä½¿ç”¨åˆé¡µæŸå¤±å‡½æ•°ã€é€»è¾‘æ–¯è°›æŸå¤±å‡½æ•°ã€æŒ‡æ•°æŸå¤±å‡½æ•°ã€‚3ç§æŸå¤±å‡½æ•°åˆ†åˆ«å†™ä¸º
$$[1-yf(x)]_+ \\ \log[1-\exp(-yf(x))] \\ \exp(-yf(x))$$
è¿™3ç§æŸå¤±å‡½æ•°éƒ½æ˜¯0-1æŸå¤±å‡½æ•°çš„ä¸Šç•Œï¼Œå…·æœ‰ç›¸ä¼¼çš„å½¢çŠ¶ã€‚æ‰€ä»¥ï¼Œå¯ä»¥è®¤ä¸ºæ”¯æŒå‘é‡æœºã€é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æå‡æ–¹æ³•ä½¿ç”¨ä¸åŒçš„ä»£ç†æŸå¤±å‡½æ•°ï¼ˆsurrogate  loss  functionï¼‰è¡¨ç¤ºåˆ†ç±»çš„æŸå¤±ï¼Œå®šä¹‰ç»éªŒé£é™©æˆ–ç»“æ„é£é™©å‡½æ•°ï¼Œå®ç°äºŒç±»åˆ†ç±»å­¦ä¹ ä»»åŠ¡ã€‚å­¦ä¹ çš„ç­–ç•¥æ˜¯ä¼˜åŒ–ä»¥ä¸‹ç»“æ„é£é™©å‡½æ•°ï¼š
$$\min_{f \in H} \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i)) +\lambda J(f)$$
è¿™é‡Œï¼Œç¬¬1é¡¹ä¸ºç»éªŒé£é™©ï¼ˆç»éªŒæŸå¤±ï¼‰ï¼Œç¬¬2é¡¹ä¸ºæ­£åˆ™åŒ–é¡¹ï¼ŒL(Y,f(X))ä¸ºæŸå¤±å‡½æ•°ï¼ŒJ(f)ä¸ºæ¨¡å‹çš„å¤æ‚åº¦ï¼Œâ‰¥0ä¸ºç³»æ•°ã€‚

æ”¯æŒå‘é‡æœºç”¨L2èŒƒæ•°è¡¨ç¤ºæ¨¡å‹çš„å¤æ‚åº¦ã€‚åŸå§‹çš„é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹æ²¡æœ‰æ­£åˆ™åŒ–é¡¹ï¼Œå¯ä»¥ç»™å®ƒä»¬åŠ ä¸ŠL2èŒƒæ•°æ­£åˆ™åŒ–é¡¹ã€‚æå‡æ–¹æ³•æ²¡æœ‰æ˜¾å¼çš„æ­£åˆ™åŒ–é¡¹ï¼Œé€šå¸¸é€šè¿‡æ—©åœæ­¢ï¼ˆearly stoppingï¼‰çš„æ–¹æ³•è¾¾åˆ°æ­£åˆ™åŒ–çš„æ•ˆæœã€‚


ä»¥ä¸ŠäºŒç±»åˆ†ç±»çš„å­¦ä¹ æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å¤šç±»åˆ†ç±»å­¦ä¹ ä»¥åŠæ ‡æ³¨é—®é¢˜ï¼Œæ¯”å¦‚æ ‡æ³¨é—®é¢˜çš„æ¡ä»¶éšæœºåœºå¯ä»¥çœ‹ä½œæ˜¯åˆ†ç±»é—®é¢˜çš„æœ€å¤§ç†µæ¨¡å‹çš„æ¨å¹¿ã€‚

æ¦‚ç‡æ¨¡å‹çš„å­¦ä¹ å¯ä»¥å½¢å¼åŒ–ä¸ºæå¤§ä¼¼ç„¶ä¼°è®¡æˆ–è´å¶æ–¯ä¼°è®¡çš„æå¤§åéªŒæ¦‚ç‡ä¼°è®¡ã€‚è¿™æ—¶ï¼Œå­¦ä¹ çš„ç­–ç•¥æ˜¯æå°åŒ–å¯¹æ•°ä¼¼ç„¶æŸå¤±æˆ–æå°åŒ–æ­£åˆ™åŒ–çš„å¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‚å¯¹æ•°ä¼¼ç„¶æŸå¤±å¯ä»¥å†™æˆ
$$-\log P(y|x)$$
æå¤§åéªŒæ¦‚ç‡ä¼°è®¡æ—¶ï¼Œæ­£åˆ™åŒ–é¡¹æ˜¯å…ˆéªŒæ¦‚ç‡çš„è´Ÿå¯¹æ•°ã€‚

å†³ç­–æ ‘å­¦ä¹ çš„ç­–ç•¥æ˜¯æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ŒæŸå¤±å‡½æ•°æ˜¯å¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œæ­£åˆ™åŒ–é¡¹æ˜¯å†³ç­–æ ‘çš„å¤æ‚åº¦ã€‚

é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æ¡ä»¶éšæœºåœºçš„å­¦ä¹ ç­–ç•¥æ—¢å¯ä»¥çœ‹æˆæ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆæˆ–æ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ï¼Œåˆå¯ä»¥çœ‹æˆæ˜¯æå°åŒ–é€»è¾‘æ–¯è°›æŸå¤±ï¼ˆæˆ–æ­£åˆ™åŒ–çš„é€»è¾‘æ–¯è°›æŸå¤±ï¼‰ã€‚

æœ´ç´ è´å¶æ–¯æ¨¡å‹ã€éšé©¬å°”å¯å¤«æ¨¡å‹çš„éç›‘ç£å­¦ä¹ ä¹Ÿæ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–æå¤§åéªŒæ¦‚ç‡ä¼°è®¡ï¼Œä½†è¿™æ—¶æ¨¡å‹å«æœ‰éšå˜é‡ã€‚

**å­¦ä¹ ç®—æ³•**

ç»Ÿè®¡å­¦ä¹ çš„é—®é¢˜æœ‰äº†å…·ä½“çš„å½¢å¼ä»¥åï¼Œå°±å˜æˆäº†æœ€ä¼˜åŒ–é—®é¢˜ã€‚æœ‰æ—¶ï¼Œæœ€ä¼˜åŒ–é—®é¢˜æ¯”è¾ƒç®€å•ï¼Œè§£æè§£å­˜åœ¨ï¼Œæœ€ä¼˜è§£å¯ä»¥ç”±å…¬å¼ç®€å•è®¡ç®—ã€‚ä½†åœ¨å¤šæ•°æƒ…å†µä¸‹ï¼Œæœ€ä¼˜åŒ–é—®é¢˜æ²¡æœ‰è§£æè§£ï¼Œéœ€è¦ç”¨æ•°å€¼è®¡ç®—çš„æ–¹æ³•æˆ–å¯å‘å¼çš„æ–¹æ³•æ±‚è§£ã€‚

æœ´ç´ è´å¶æ–¯æ³•ä¸éšé©¬å°”å¯å¤«æ¨¡å‹çš„ç›‘ç£å­¦ä¹ ï¼Œæœ€ä¼˜è§£å³æå¤§ä¼¼ç„¶ä¼°è®¡å€¼ï¼Œå¯ä»¥ç”±æ¦‚ç‡è®¡ç®—å…¬å¼ç›´æ¥è®¡ç®—ã€‚

æ„ŸçŸ¥æœºã€é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹ã€æ¡ä»¶éšæœºåœºçš„å­¦ä¹ åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€æ‹Ÿç‰›é¡¿æ³•ç­‰ã€‚è¿™äº›éƒ½æ˜¯ä¸€èˆ¬çš„æ— çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜çš„è§£æ³•ã€‚

æ”¯æŒå‘é‡æœºå­¦ä¹ ï¼Œå¯ä»¥è§£å‡¸äºŒæ¬¡è§„åˆ’çš„å¯¹å¶é—®é¢˜ã€‚æœ‰åºåˆ—æœ€å°æœ€ä¼˜åŒ–ç®—æ³•ç­‰æ–¹æ³•ã€‚

å†³ç­–æ ‘å­¦ä¹ æ˜¯åŸºäºå¯å‘å¼ç®—æ³•çš„å…¸å‹ä¾‹å­ã€‚å¯ä»¥è®¤ä¸ºç‰¹å¾é€‰æ‹©ã€ç”Ÿæˆã€å‰ªææ˜¯å¯å‘å¼åœ°è¿›è¡Œæ­£åˆ™åŒ–çš„æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚

æå‡æ–¹æ³•åˆ©ç”¨å­¦ä¹ çš„æ¨¡å‹æ˜¯åŠ æ³•æ¨¡å‹ã€æŸå¤±å‡½æ•°æ˜¯æŒ‡æ•°æŸå¤±å‡½æ•°çš„ç‰¹ç‚¹ï¼Œå¯å‘å¼åœ°ä»å‰å‘åé€æ­¥å­¦ä¹ æ¨¡å‹ï¼Œä»¥è¾¾åˆ°é€¼è¿‘ä¼˜åŒ–ç›®æ ‡å‡½æ•°çš„ç›®çš„ã€‚

EMç®—æ³•æ˜¯ä¸€ç§è¿­ä»£çš„æ±‚è§£å«éšå˜é‡æ¦‚ç‡æ¨¡å‹å‚æ•°çš„æ–¹æ³•ï¼Œå®ƒçš„æ”¶æ•›æ€§å¯ä»¥ä¿è¯ï¼Œä½†æ˜¯ä¸èƒ½ä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ã€‚

æ”¯æŒå‘é‡æœºå­¦ä¹ ã€é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹å­¦ä¹ ã€æ¡ä»¶éšæœºåœºå­¦ä¹ æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå…¨å±€æœ€ä¼˜è§£ä¿è¯å­˜åœ¨ã€‚è€Œå…¶ä»–å­¦ä¹ é—®é¢˜åˆ™ä¸æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜ã€‚

## ç¬¬ 13 ç«  æ— ç›‘ç£å­¦ä¹ æ¦‚è®º
**åŸºæœ¬é—®é¢˜**ï¼š
**èšç±»**ï¼ˆclusteringï¼‰æ˜¯å°†æ ·æœ¬é›†åˆä¸­ç›¸ä¼¼çš„æ ·æœ¬å½’åˆ°ç›¸åŒçš„ç±»ï¼Œç›¸ä¼¼çš„å®šä¹‰ä¸€èˆ¬ç”¨è·ç¦»åº¦é‡ã€‚
å¦‚æœä¸€ä¸ªæ ·æœ¬åªèƒ½å±äºä¸€ä¸ªç±»ï¼Œåˆ™ç§°ä¸ºç¡¬èšç±»ï¼ˆhard clusteringï¼‰ï¼Œå¦‚k-meansï¼›å¦‚æœä¸€ä¸ªæ ·æœ¬å¯ä»¥å±äºå¤šä¸ªç±»ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬ä»¥æ¦‚ç‡å±äºæ¯ä¸€ä¸ªç±»$\sum_{i=1}^N p(z_i|x_i) =1$ï¼Œåˆ™ç§°ä¸ºè½¯èšç±»ï¼ˆsoft clusteringï¼‰ï¼Œå¦‚GMMã€‚
èšç±»ä¸»è¦ç”¨äºæ•°æ®åˆ†æï¼Œä¹Ÿå¯ç”¨äºç›‘ç£å­¦ä¹ çš„å‰å¤„ç†ã€‚èšç±»å¯ä»¥å¸®åŠ©å‘ç°æ•°æ®ä¸­çš„ç»Ÿè®¡è§„å¾‹ã€‚


**é™ç»´**ï¼ˆdimensionality reductionï¼‰æ˜¯å°†æ ·æœ¬é›†åˆä¸­çš„æ ·æœ¬ï¼ˆå®ä¾‹ï¼‰ä»é«˜ç»´ç©ºé—´è½¬æ¢åˆ°ä½ç»´ç©ºé—´ã€‚
é«˜ç»´ç©ºé—´é€šå¸¸æ˜¯é«˜ç»´çš„æ¬§æ°ç©ºé—´ï¼Œè€Œä½ç»´ç©ºé—´æ˜¯ä½ç»´çš„æ¬§æ°ç©ºé—´æˆ–æµå½¢ï¼ˆmanifoldï¼‰ã€‚ä½ç»´ç©ºé—´æ˜¯ä»æ•°æ®ä¸­è‡ªåŠ¨å‘ç°çš„ã€‚é™ç»´æœ‰çº¿æ€§é™ç»´å’Œéçº¿æ€§é™ç»´ï¼Œé™ç»´æ–¹æ³•æœ‰ä¸»æˆåˆ†åˆ†æã€‚
é™ç»´çš„å¥½å¤„æœ‰ï¼šèŠ‚çœå­˜å‚¨ç©ºé—´ã€åŠ é€Ÿè®¡ç®—ã€è§£å†³**ç»´åº¦ç¾éš¾**ï¼ˆå‰é¢ç« èŠ‚æœ‰è®²åˆ°ï¼‰ç­‰
é™ç»´ä¸»è¦ç”¨äºæ•°æ®åˆ†æï¼Œä¹Ÿå¯ç”¨äºç›‘ç£å­¦ä¹ çš„å‰å¤„ç†ã€‚é™ç»´å¯ä»¥å¸®åŠ©å‘ç°é«˜ç»´æ•°æ®ä¸­çš„ç»Ÿè®¡è§„å¾‹ã€‚

**æ¦‚ç‡æ¨¡å‹ä¼°è®¡**ï¼ˆprobability model estimationï¼‰ï¼Œç®€ç§°æ¦‚ç‡ä¼°è®¡ï¼Œå‡è®¾è®­ç»ƒæ•°æ®ç”±ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ç”Ÿæˆï¼Œç”±è®­ç»ƒæ•°æ®å­¦ä¹ æ¦‚ç‡æ¨¡å‹çš„ç»“æ„å’Œå‚æ•°ã€‚
æ¦‚ç‡æ¨¡å‹çš„ç»“æ„ç±»å‹æˆ–è€…æ¦‚ç‡æ¨¡å‹çš„é›†åˆäº‹å…ˆç»™å®šï¼Œè€Œæ¨¡å‹çš„å…·ä½“ç»“æ„ä¸å‚æ•°ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ ã€‚å‡è®¾æ•°æ®ç”±GMMç”Ÿæˆï¼ˆå·²çŸ¥ç»“æ„ï¼‰ï¼Œå­¦ä¹ çš„ç›®æ ‡æ˜¯ä¼°è®¡è¿™ä¸ªæ¨¡å‹çš„å‚æ•°ã€‚
æ¦‚ç‡æ¨¡å‹åŒ…æ‹¬æ··åˆæ¨¡å‹ã€æ¦‚ç‡å›¾æ¨¡å‹ç­‰ã€‚æ¦‚ç‡å›¾æ¨¡å‹åˆåŒ…æ‹¬æœ‰å‘å›¾æ¨¡å‹å’Œæ— å‘å›¾æ¨¡å‹ï¼ˆå‰é¢ç« èŠ‚æœ‰è®²åˆ°ï¼‰ã€‚

**æ— ç›‘ç£å­¦ä¹ æ–¹æ³•**
- èšç±»
    - ç¡¬èšç±»ï¼š
        - k-means
    - è½¯èšç±»ï¼š
        - GMM
- é™ç»´
    - çº¿æ€§ï¼š
        - ä¸»æˆåˆ†åˆ†æ
    - éçº¿æ€§ï¼š
        - æµå½¢å­¦ä¹ 

- è¯é¢˜åˆ†æ
è¯é¢˜åˆ†ææ˜¯æ–‡æœ¬åˆ†æçš„ä¸€ç§æŠ€æœ¯ã€‚ç»™å®šä¸€ä¸ªæ–‡æœ¬é›†åˆï¼Œè¯é¢˜åˆ†ææ—¨åœ¨å‘ç°æ–‡æœ¬é›†åˆä¸­æ¯ä¸ªæ–‡æœ¬çš„è¯é¢˜ï¼Œè€Œè¯é¢˜ç”±å•è¯çš„é›†åˆè¡¨ç¤ºã€‚è¯é¢˜åˆ†ææ–¹æ³•æœ‰ï¼š
    - æ½œåœ¨è¯­ä¹‰åˆ†æ
    - æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æ
    - æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…

- å›¾åˆ†æ
å›¾åˆ†æ çš„ç›®çš„æ˜¯ å‘æ˜éšè—åœ¨å›¾ä¸­çš„ç»Ÿè®¡è§„å¾‹æˆ–æ½œåœ¨ç»“æ„ï¼›
    - é“¾æ¥åˆ†æ æ˜¯å›¾åˆ†æçš„ä¸€ç§ï¼Œä¸»è¦æ˜¯å‘ç° æœ‰å‘å›¾ä¸­çš„é‡è¦ç»“ç‚¹ï¼ŒåŒ…æ‹¬ PageRank ç®—æ³•
    - PageRank ç®—æ³•æœ€åˆæ˜¯ä¸ºäº’è”ç½‘æœç´¢è€Œæå‡ºã€‚å°†äº’è”ç½‘çœ‹ä½œæ˜¯ä¸€ä¸ªå·¨å¤§çš„æœ‰å‘å›¾ï¼Œç½‘é¡µæ˜¯ç»“ç‚¹ï¼Œç½‘é¡µçš„è¶…é“¾æ¥æ˜¯æœ‰å‘è¾¹ã€‚PageRank ç®—æ³•å¯ä»¥ç®—å‡ºç½‘é¡µçš„ PageRank å€¼ï¼Œè¡¨ç¤ºå…¶é‡è¦åº¦ï¼Œåœ¨æœç´¢å¼•æ“çš„æ’åºä¸­ç½‘é¡µçš„é‡è¦åº¦èµ·ç€é‡è¦ä½œç”¨

åŒç›‘ç£å­¦ä¹ ä¸€æ ·ï¼Œæ— ç›‘ç£å­¦ä¹ ä¹Ÿæœ‰**ä¸‰è¦ç´ **ï¼šæ¨¡å‹ã€ç­–ç•¥ã€ç®—æ³•
**æ¨¡å‹** å°±æ˜¯å‡½æ•°$z=g_\theta(x)$ï¼Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P_\theta(z |x)$ï¼Œæˆ–$P_\theta(x|z)$ï¼Œåœ¨èšç±»ã€é™ç»´ã€æ¦‚ç‡æ¨¡å‹ä¼°è®¡ä¸­æ‹¥æœ‰ä¸åŒçš„å½¢å¼
- èšç±» ä¸­æ¨¡å‹çš„è¾“å‡ºæ˜¯ ç±»åˆ«
- é™ç»´ ä¸­æ¨¡å‹çš„è¾“å‡ºæ˜¯ ä½ç»´å‘é‡
- æ¦‚ç‡æ¨¡å‹ä¼°è®¡ ä¸­çš„æ¨¡å‹å¯ä»¥æ˜¯æ··åˆæ¦‚ç‡æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ‰å‘æ¦‚ç‡å›¾æ¨¡å‹å’Œæ— å‘æ¦‚ç‡å›¾æ¨¡å‹

**ç­–ç•¥** åœ¨ä¸åŒçš„é—®é¢˜ä¸­æœ‰ä¸åŒçš„å½¢å¼ï¼Œä½†éƒ½å¯ä»¥è¡¨ç¤ºä¸ºç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–
- èšç±» ä¸­æ ·æœ¬ä¸æ‰€å±ç±»åˆ«ä¸­å¿ƒè·ç¦»çš„æœ€å°åŒ–
- é™ç»´ ä¸­æ ·æœ¬ä»é«˜ç»´ç©ºé—´è½¬æ¢åˆ°ä½ç»´ç©ºé—´è¿‡ç¨‹ä¸­ä¿¡æ¯æŸå¤±çš„æœ€å°åŒ–
- æ¦‚ç‡æ¨¡å‹ä¼°è®¡ ä¸­æ¨¡å‹ç”Ÿæˆæ•°æ®æ¦‚ç‡çš„æœ€å¤§åŒ–

**ç®—æ³•** é€šå¸¸æ˜¯è¿­ä»£ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£è¾¾åˆ°ç›®æ ‡å‡½æ•°çš„æœ€ä¼˜åŒ–ï¼Œæ¯”å¦‚ï¼Œæ¢¯åº¦ä¸‹é™æ³•ã€‚
- å±‚æ¬¡èšç±»æ³•ã€kå‡å€¼èšç±» æ˜¯ç¡¬èšç±»æ–¹æ³•
- é«˜æ–¯æ··åˆæ¨¡å‹ EMç®—æ³•æ˜¯è½¯èšç±»æ–¹æ³•
- ä¸»æˆåˆ†åˆ†æã€æ½œåœ¨è¯­ä¹‰åˆ†æ æ˜¯é™ç»´æ–¹æ³•
- æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æã€æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é… æ˜¯æ¦‚ç‡æ¨¡å‹ä¼°è®¡æ–¹æ³•



### å‚è€ƒæ–‡çŒ®
[13-1] Hastie T, Tibshirani R, Friedman J. The elements of statistical learning:data mining, inference, and prediction. Springer. 2001. 

[13-2] Bishop M. Pattern Recognition and Machine Learning. Springer, 2006.

[13-3] Koller D, Friedman N. Probabilistic graphical models: principles and techniques. Cambridge, MA: MIT Press, 2009.

[13-4] Goodfellow I,Bengio Y,Courville A. Deep learning. Cambridge, MA: MIT Press, 2016.

[13-5] Michelle T M. Machine Learning. McGraw-Hill Companies, Inc. 1997.ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬ï¼šæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003.ï¼‰

[13-6] Barber D. Bayesian reasoning and machine learning, Cambridge, UK:Cambridge University Press, 2012.

[13-7] å‘¨å¿—å. æœºå™¨å­¦ä¹ . åŒ—äº¬ï¼šæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ2017.

## ç¬¬ 14 ç«  èšç±»æ–¹æ³•

èšç±»åˆ†æï¼ˆ[Cluster analysis or clustering](https://en.jinzhao.wiki/wiki/Cluster_analysis)ï¼‰æ˜¯é’ˆå¯¹ç»™å®šçš„æ ·æœ¬ï¼Œæ ¹æ®å®ƒä»¬ç‰¹å¾ç‚¹çš„ç›¸ä¼¼åº¦ï¼ˆè·ç¦»ï¼‰ï¼Œå°†å…¶å½’å¹¶åˆ°è‹¥å¹²ä¸ªç±»ï¼ˆç°‡ï¼‰ä¸­çš„åˆ†æé—®é¢˜ã€‚èšç±»åˆ†ææœ¬èº«ä¸æ˜¯ä¸€ç§ç‰¹å®šçš„ç®—æ³•ï¼Œè€Œæ˜¯è¦è§£å†³çš„ä¸€èˆ¬ä»»åŠ¡ã€‚

### ç®—æ³•åˆ†ç±»

Hard clustering
Soft clustering (also: [fuzzy clustering](https://en.jinzhao.wiki/wiki/Fuzzy_clustering))

---

Connectivity modelsï¼šå¦‚ hierarchical clustering åŸºäºè·ç¦»è¿é€šæ€§ï¼ˆbased on distance connectivityï¼‰
Centroid modelsï¼šå¦‚ k-means
Distribution modelsï¼šå¦‚GMM
Density modelsï¼šå¦‚ DBSCAN and OPTICS
Subspace modelsï¼šå¦‚ biclustering
Group modelsï¼šå¦‚
Graph-based modelsï¼šå¦‚
Signed graph modelsï¼šå¦‚
Neural modelsï¼šå¦‚


èšç±»åˆ†æç®—æ³•ï¼ˆ[Cluster analysis algorithms](https://en.jinzhao.wiki/wiki/Category:Cluster_analysis_algorithms)ï¼‰ï¼š[sklearnä¸­çš„èšç±»ç®—æ³•å’Œä»‹ç»](https://scikit-learn.org/stable/modules/clustering.html#clustering)
**åŸºäºè¿æ¥çš„èšç±»ï¼ˆå±‚æ¬¡èšç±»ï¼‰** Connectivity-based clusteringï¼ˆ[Hierarchical clustering](https://en.jinzhao.wiki/wiki/Hierarchical_clustering)ï¼‰ï¼š
å±‚æ¬¡èšç±»æœ‰èšåˆAgglomerativeï¼ˆè‡ªä¸‹è€Œä¸Š"bottom-up"ï¼‰å’Œåˆ†è£‚ Divisiveï¼ˆè‡ªä¸Šè€Œä¸‹"top-down"ï¼‰ä¸¤ç§æ–¹æ³•ã€‚
- [sklearn.cluster.AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)
ä»ä¸‹è€Œä¸Šåœ°æŠŠå°çš„clusteråˆå¹¶èšé›†,å¼€å§‹æ—¶å°†æ¯ä¸ªæ ·æœ¬å„è‡ªåˆ†åˆ°ä¸€ä¸ªclusterï¼Œä¹‹åå°†è·ç¦»æœ€çŸ­çš„ä¸¤ä¸ªclusteråˆå¹¶æˆä¸€ä¸ªæ–°çš„clusterï¼Œé‡å¤æ­¤æ­¥éª¤ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼›
ä¹¦ä¸­æåˆ°ä¸‰ä¸ªè¦ç´ ï¼š
1. è·ç¦»åº¦é‡æˆ–ç›¸ä¼¼åº¦
1. åˆå¹¶è§„åˆ™ï¼ˆclusteré—´çš„è·ç¦»è§„åˆ™ï¼Œclusteré—´çš„è·ç¦»å¯ä»¥æ˜¯æœ€çŸ­ã€æœ€é•¿ã€ä¸­å¿ƒè·ç¦»ã€å¹³å‡è·ç¦»ç­‰ï¼‰
1. åœæ­¢æ¡ä»¶ï¼ˆè¾¾åˆ°kå€¼ï¼Œä¹Ÿå°±æ˜¯clusterçš„ä¸ªæ•°è¾¾åˆ°é˜ˆå€¼ï¼›clusterçš„ç›´å¾„è¾¾åˆ°é˜ˆå€¼ï¼›ï¼‰

- DIANA (DIvisive ANAlysis Clustering) algorithm
å¼€å§‹å°†æ‰€æœ‰æ ·æœ¬å½’ä¸ºä¸€ä¸ªclusterï¼Œä¹‹åå°†è·ç¦»æœ€è¿œçš„ä¸¤ä¸ªæ ·æœ¬åˆ†åˆ°ä¸¤ä¸ªæ–°çš„clusterï¼Œé‡å¤æ­¤æ­¥éª¤ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼›

**åŸºäºè´¨å¿ƒçš„èšç±»** Centroid-based clusteringï¼š
- Kå‡å€¼èšç±»ï¼ˆ[k-means clustering](https://en.jinzhao.wiki/wiki/K-means_clustering)ï¼‰ï¼Œ[sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
KMeans å¯ä»¥çœ‹ä½œæ˜¯é«˜æ–¯æ··åˆæ¨¡å‹çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œæ¯ä¸ªåˆ†é‡çš„åæ–¹å·®ç›¸ç­‰ã€‚

- å‡å€¼ç§»ä½èšç±»ï¼ˆ[Mean shift clustering](https://en.jinzhao.wiki/wiki/Mean-shift)ï¼‰ ï¼Œ[sklearn.cluster.MeanShift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html)
Mean shift clustering using a flat kernel.  based on [kernel density estimation](https://en.jinzhao.wiki/wiki/Kernel_density_estimation).
sklearnä¸­æœ‰è¯´æ˜¯centroid-basedã€‚ç»´åŸºç™¾ç§‘æŠŠå®ƒæ”¾åœ¨åŸºäºå¯†åº¦çš„èšç±»ä¸­ï¼ˆæ ¸å¯†åº¦ä¼°è®¡ï¼‰ã€‚

-  äº²å’ŒåŠ›ä¼ æ’­ï¼ˆ[Affinity Propagation (AP)](https://en.jinzhao.wiki/wiki/Affinity_propagation)ï¼‰ï¼Œ[sklearn.cluster.AffinityPropagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)
åŸºäºæ•°æ®ç‚¹ä¹‹é—´â€œæ¶ˆæ¯ä¼ é€’â€çš„æ¦‚å¿µã€‚based on the concept of "message passing" between data points.

**åŸºäºåˆ†å¸ƒçš„èšç±»** Distribution-based clusteringï¼š
- é«˜æ–¯æ··åˆæ¨¡å‹èšç±»ï¼ˆ[Gaussian mixture model](https://en.jinzhao.wiki/wiki/Mixture_model#Gaussian_mixture_model)ï¼‰ï¼Œ[sklearn.mixture](https://scikit-learn.org/stable/modules/mixture.html)

**åŸºäºå¯†åº¦çš„èšç±»** Density-based clusteringï¼š
- åŸºäºå¯†åº¦çš„å¸¦å™ªå£°åº”ç”¨ç©ºé—´èšç±»ï¼ˆ[Density-Based Spatial Clustering of Applications with Noise (DBSCAN)](https://en.jinzhao.wiki/wiki/DBSCAN)ï¼‰ï¼Œ[sklearn.cluster.DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
- å¯¹è±¡æ’åºè¯†åˆ«èšç±»ç»“æ„ï¼ˆ[Ordering Points To Identify the Clustering Structure (OPTICS)](https://en.jinzhao.wiki/wiki/OPTICS_algorithm)ï¼‰ ï¼Œ[sklearn.cluster.OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html)


**åŸºäºç½‘æ ¼çš„èšç±»** Grid-based clusteringï¼š
**å…¶å®ƒèšç±»**ï¼š
- è°±èšç±»ï¼ˆ[Spectral clustering](https://en.jinzhao.wiki/wiki/Spectral_clustering)ï¼‰ï¼Œ[sklearn.cluster.SpectralClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)
- åŒèšç±»ï¼ˆ[biclustering](https://en.jinzhao.wiki/wiki/Biclustering)ï¼‰ï¼Œ[è°±åŒèšç±»sklearn.cluster.SpectralBiclustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralBiclustering.html)ï¼Œ[è°±ååŒèšç±»sklearn.cluster.SpectralCoclustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralCoclustering.html)

**é«˜æ•ˆèšç±»**ï¼š

- ä½¿ç”¨å±‚æ¬¡ç»“æ„å¹³è¡¡è¿­ä»£å‡å°‘å’Œèšç±» ï¼ˆ[Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)](https://en.jinzhao.wiki/wiki/BIRCH)ï¼‰ ï¼Œ[sklearn.cluster.Birch](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)
å®ƒæ˜¯ä¸€ç§é«˜æ•ˆå†…å­˜çš„åœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Œå¯ä½œä¸ºMiniBatchKMeansçš„æ›¿ä»£æ–¹æ¡ˆã€‚å®ƒæ„é€ äº†ä¸€ä¸ªæ ‘çŠ¶æ•°æ®ç»“æ„ï¼Œå…¶ä¸­é›†ç¾¤ä¸­å¿ƒè¢«ä»å¶å­ä¸­è¯»å–ã€‚è¿™äº›å¯ä»¥æ˜¯æœ€ç»ˆçš„èšç±»ä¸­å¿ƒï¼Œä¹Ÿå¯ä»¥ä½œä¸ºè¾“å…¥æä¾›ç»™å…¶ä»–èšç±»ç®—æ³•ï¼Œå¦‚ AgglomerativeClusteringã€‚

**é«˜ç»´æ•°æ®çš„èšç±»**ï¼ˆ[Clustering high-dimensional data](https://en.jinzhao.wiki/wiki/Clustering_high-dimensional_data)ï¼‰ï¼š
å­ç©ºé—´èšç±»Subspace clustering
- [SUBCLU](https://en.jinzhao.wiki/wiki/SUBCLU)

æŠ•å½±èšç±»Projected clustering
åŸºäºæŠ•å½±çš„èšç±»Projection-based clustering
æ··åˆæ–¹æ³•Hybrid approaches

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)

### ç®—æ³•è¯„ä¼°Evaluation
èšç±»ç»“æœçš„è¯„ä¼°ä¸èšç±»æœ¬èº«ä¸€æ ·å›°éš¾ï¼ˆå¹¶ä¸åƒè®¡ç®—é”™è¯¯æ•°é‡æˆ–ç›‘ç£åˆ†ç±»ç®—æ³•çš„ç²¾åº¦å’Œå¬å›ç‡é‚£ä¹ˆç®€å•ï¼‰ã€‚ä¸€èˆ¬åˆ†ä¸ºInternal evaluationå’ŒExternal evaluationï¼Œå½“ä¸¤ç§è¯„ä¼°æ•ˆæœéƒ½ä¸å¥½æ—¶å°±éœ€è¦human evaluationã€‚

---
å…ˆä½œä¸€äº›å®šä¹‰ï¼š

æ•°æ®é›†$D=\{x_1,x_2,...,x_i,...,x_j,...,x_n\}$

è®¾$T$ä¸ºç»™å®šçš„æ­£æ•°ï¼Œè‹¥é›†åˆ$C_p$ä¸­ä»»æ„ä¸¤ä¸ªæ ·æœ¬é—´çš„è·ç¦»$dist(x_i,x_j) \leq T$ï¼Œåˆ™ç§°$C_p$ä¸ºä¸€ä¸ªç±»æˆ–ç°‡ï¼ˆclusterï¼‰

$C = \{C_1,C_2,...,C_k\}$è¡¨ç¤ºï¼ˆé¢„æµ‹çš„ï¼‰ç±»æˆ–ç°‡ï¼ˆclusterï¼‰
$C^* = \{C_1^*,C_2^*,...,C_s^*\}$è¡¨ç¤ºå‚è€ƒæ¨¡å‹çš„ç±»æˆ–ç°‡ï¼ˆclusterï¼‰
$\lambda$è¡¨ç¤ºç°‡$C$çš„æ ‡è®°(é¢„æµ‹)å‘é‡ï¼Œå¦‚ï¼š$\lambda = [0,1,...,k],\lambda^* = [0,1,...,s]$ï¼Œé•¿åº¦ä¸ºæ ·æœ¬æ•°é‡$n$
$\lambda_i$ä¸ºæ ·æœ¬$x_i$çš„é¢„æµ‹æˆ–æ ‡è®°å€¼
$a = TP, TP=\{(x_i,x_j)\mid\lambda_i = \lambda_j, \lambda_i^* = \lambda_j^* ,i \lt j\}$ ï¼Œè¡¨ç¤ºâ€œæ ·æœ¬å¯¹â€åœ¨$C$ä¸­å±äºç›¸åŒçš„ç°‡ä¸”åœ¨$C^*$ä¸­ä¹Ÿå±äºç›¸åŒçš„ç°‡çš„æ•°é‡(true positive)
$b = TN, TN=\{(x_i,x_j)\mid\lambda_i = \lambda_j, \lambda_i^* \neq \lambda_j^* ,i \lt j\}$ ï¼Œè¡¨ç¤ºâ€œæ ·æœ¬å¯¹â€åœ¨$C$ä¸­å±äºç›¸åŒçš„ç°‡ä¸”åœ¨$C^*$ä¸­ä¹Ÿå±äºä¸åŒçš„ç°‡çš„æ•°é‡(true negative)
$c = FP, FP=\{(x_i,x_j)\mid\lambda_i \neq \lambda_j, \lambda_i^* = \lambda_j^* ,i\lt j\}$ ï¼Œè¡¨ç¤ºâ€œæ ·æœ¬å¯¹â€åœ¨$C$ä¸­å±äºä¸åŒçš„ç°‡ä¸”åœ¨$C^*$ä¸­ä¹Ÿå±äºç›¸åŒçš„ç°‡çš„æ•°é‡(false positive)
$d = FN, FN=\{(x_i,x_j) \mid \lambda_i \neq \lambda_j, \lambda_i^* \neq \lambda_j^* ,i\lt j\}$ ï¼Œè¡¨ç¤ºâ€œæ ·æœ¬å¯¹â€åœ¨$C$ä¸­å±äºä¸åŒçš„ç°‡ä¸”åœ¨$C^*$ä¸­ä¹Ÿå±äºä¸åŒçš„ç°‡çš„æ•°é‡(false negative)

> æ³¨æ„ï¼šlabels_pred = [0, 0, 1, 1] ä¸ labels_true = [0, 0, 1, 1] ä»¥åŠ labels_pred = [0, 0, 1, 1] ä¸ labels_true = [1, 1, 0, 0] æ˜¯æ²¡æœ‰åŒºåˆ«çš„ï¼Œä»–ä»¬éƒ½æ­£ç¡®çš„èšç±»äº†ã€‚

æ ·æœ¬å¯¹çš„æ•°é‡ä¸º$C_n^2 = \binom{n}{2} =n(n-1)/2 = a+b+c+d$ï¼Œè¿™é‡Œçš„$C$æ˜¯æ’åˆ—ç»„åˆçš„æ„æ€

$d_{ij} = dist(x_i,x_j)$è¡¨ç¤ºæ ·æœ¬$x_i,x_j$ä¹‹é—´çš„è·ç¦»


$n_p = |C_p|$è¡¨ç¤ºç°‡$C_p$ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œ
$\bar{x}_p = \frac{1}{n_p}\sum_{x_i \in C_p}x_i$åˆ†åˆ«è¡¨ç¤ºç°‡$C_p$çš„è´¨å¿ƒï¼ˆä¸­å¿ƒã€å‡å€¼ã€ä¸­å¿ƒç‚¹ã€centroidï¼‰
$diam(C_p) = \max \{dist(x_i,x_j) \mid x_i,x_j \in C_p\}$è¡¨ç¤ºç°‡çš„ç›´å¾„diamæˆ–è€…ç°‡ç±»æ ·æœ¬é—´çš„æœ€è¿œè·ç¦»
$avg(C_p) = \frac{2}{n_p(n_p-1)} \sum_{1\leq i \lt j\leq n_p }dist(x_i,x_j)$è¡¨ç¤ºç°‡ç±»æ ·æœ¬é—´çš„å¹³å‡è·ç¦»
$A_{C_p} = \sum_{x_i \in C_p} (x_i-\bar{x}_p)(x_i-\bar{x}_p)^T$è¡¨ç¤ºç°‡ç±»æ ·æœ¬æ•£å¸ƒçŸ©é˜µï¼ˆ[scatter matrix](https://en.jinzhao.wiki/wiki/Scatter_matrix)ï¼‰
$S_{C_p} = \frac{1}{n_p-1}A_{C_p}$è¡¨ç¤ºç°‡ç±»æ ·æœ¬åæ–¹å·®çŸ©é˜µï¼ˆ[covariance matrix](https://en.jinzhao.wiki/wiki/Covariance_matrix)ï¼‰

ä¸¤ä¸ªç°‡ä¹‹é—´çš„è·ç¦»ä¸»è¦æœ‰ä»¥ä¸‹è¡¨ç¤ºæ–¹æ³•ï¼š
$d_{min}(C_p,C_q) = \min\{dist(x_i,x_j) \mid x_i \in C_p,x_j \in C_q\}$è¡¨ç¤ºä¸¤ä¸ªç°‡é—´çš„æœ€çŸ­è·ç¦»
$d_{max}(C_p,C_q) = \max\{dist(x_i,x_j) \mid x_i \in C_p,x_j \in C_q\}$è¡¨ç¤ºä¸¤ä¸ªç°‡é—´çš„æœ€é•¿è·ç¦»
$d_{cen}(C_p,C_q) = dist(\bar{x}_p,\bar{x}_q)$è¡¨ç¤ºä¸¤ä¸ªç°‡ä¸­å¿ƒé—´çš„è·ç¦»
$d_{mean}(C_p,C_q) = \frac{1}{n_p n_q}\sum_{x_i \in G_p}\sum_{x_j \in G_q} dist(x_i,x_j)$è¡¨ç¤ºä¸¤ä¸ªç°‡ ä»»æ„ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´è·ç¦»çš„å¹³å‡å€¼ ä¸ºä¸¤ä¸ªç°‡çš„è·ç¦»

---

èšç±»æ ‡å‡†ï¼ˆ[Clustering criteria](https://en.jinzhao.wiki/wiki/Category:Clustering_criteria)ï¼‰ï¼šç°‡å†…ç›¸ä¼¼åº¦ï¼ˆintra-cluster similarityï¼‰é«˜ï¼Œç°‡é—´ç›¸ä¼¼åº¦ï¼ˆinter-cluster similarityï¼‰ä½

**ç¡®å®šæ•°æ®é›†ä¸­çš„ç°‡æ•°**ï¼ˆ[Determining the number of clusters in a data set](https://en.jinzhao.wiki/wiki/Determining_the_number_of_clusters_in_a_data_set)ï¼‰ï¼Œä¹Ÿå°±æ˜¯Kå€¼çš„é€‰å–ã€‚
å¯¹äºæŸç±»èšç±»ç®—æ³•ï¼ˆç‰¹åˆ«æ˜¯k-means, k-medoidsï¼‰ï¼Œæœ‰ä¸€ä¸ªé€šå¸¸ç§°ä¸ºkçš„å‚æ•°æŒ‡å®šè¦æ£€æµ‹çš„èšç±»æ•°ã€‚å…¶ä»–ç®—æ³•å¦‚DBSCANå’ŒOPTICS ç®—æ³•ä¸éœ€è¦æŒ‡å®šè¯¥å‚æ•°ï¼›å±‚æ¬¡èšç±»å®Œå…¨é¿å…äº†è¿™ä¸ªé—®é¢˜ã€‚
ç°‡æ•°æ˜¯æ•°æ®é›†ä¸­é‡è¦çš„æ¦‚æ‹¬ç»Ÿè®¡é‡ã€‚ç»éªŒå€¼ï¼š$k \thickapprox \sqrt{n/2}$
- è‚˜æ–¹æ³•ï¼ˆ[Elbow method](https://en.jinzhao.wiki/wiki/Elbow_method_(clustering))ï¼‰
ç»™å®šk>0,ä½¿ç”¨åƒK-å‡å€¼è¿™æ ·çš„ç®—æ³•å¯¹æ•°æ®é›†èšç±»ï¼Œå¹¶è®¡ç®—ç°‡å†…æ–¹å·®å’Œvar(k)ã€‚ç„¶åï¼Œç»˜åˆ¶varå…³äºkçš„æ›²çº¿ã€‚æ›²çº¿çš„ç¬¬ä¸€ä¸ªï¼ˆæˆ–æœ€æ˜¾è‘—çš„ï¼‰æ‹ç‚¹æš—ç¤ºâ€œæ­£ç¡®çš„â€ç°‡æ•°ã€‚

- X-means clustering
- Information criterion approach
- Informationâ€“theoretic approach
- è½®å»“ç³»æ•°ï¼ˆ[Silhouette method](https://en.jinzhao.wiki/wiki/Silhouette_(clustering))ï¼‰
- Cross-validation
- Finding number of clusters in text databases
- Analyzing the kernel matrix

**Internal evaluation**: 
æ— ç›‘ç£çš„æ–¹æ³•ï¼Œæ— éœ€åŸºå‡†æ•°æ®ã€‚ç±»å†…èšé›†ç¨‹åº¦å’Œç±»é—´ç¦»æ•£ç¨‹åº¦ã€‚ç°‡å†…ç›¸ä¼¼åº¦ï¼ˆintra-cluster similarityï¼‰é«˜ï¼Œç°‡é—´ç›¸ä¼¼åº¦ï¼ˆinter-cluster similarityï¼‰ä½ã€‚

- **DBæŒ‡æ•°**ï¼ˆ[Davies-Bouldin Index, DBI](https://en.jinzhao.wiki/wiki/Davies%E2%80%93Bouldin_index)ï¼‰
$$DBI={\frac {1}{k}}\sum _{i=1}^{k}\max _{j\neq i}\left({\frac {avg(C_i)+avg(C_j)}{d_{cen}(C_{i},C_{j})}}\right)$$
DBIå€¼è¶Šå°è¶Šå¥½ï¼Œä¹Ÿå°±æ˜¯$avg(C_i)$è¶Šå°ï¼Œç°‡å†…ç›¸ä¼¼åº¦è¶Šé«˜ï¼›$d_{cen}(C_{i},C_{j})$è¶Šå¤§ï¼Œç°‡é—´ç›¸ä¼¼åº¦è¶Šä½ã€‚
> metrics.davies_bouldin_score

- **è½®å»“ç³»æ•°**ï¼ˆ[Silhouette coefficient](https://en.jinzhao.wiki/wiki/Silhouette_(clustering)))
$${\displaystyle a(x_i)={\frac {1}{|C_{p}|-1}}\sum _{x_i,x_j\in C_{p},i\neq j}dist(x_i,x_j)}$$
$a(x_i)$è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬ç‚¹ä¸ç°‡$C_{p}$å†…å…¶å®ƒæ ·æœ¬ç‚¹çš„å¹³å‡è·ç¦»(å€¼è¶Šå°è¡¨ç¤ºåˆ†é…çš„è¶Šå¥½)
$${\displaystyle b(x_i)=\min _{q\neq p}{\frac {1}{|C_{q}|}}\sum _{x_i\in C_{p},x_j\in C_{q}}dist(x_i,x_j) }$$
å¹³å‡ç›¸å¼‚åº¦ï¼šç°‡$C_{p}$ä¸­æ ·æœ¬ç‚¹$x_i$åˆ°å…¶å®ƒç°‡$C_{q}$ä¸­æ‰€æœ‰çš„æ ·æœ¬ç‚¹çš„å¹³å‡è·ç¦»ï¼ˆè¶Šå¤§è¯´æ˜ç°‡é—´ç›¸ä¼¼åº¦è¶Šä½ï¼‰
$b(x_i)$è¡¨ç¤ºæœ€å°å¹³å‡ç›¸å¼‚åº¦ã€‚æœ€å°å¹³å‡å·®å¼‚çš„é›†ç¾¤è¢«ç§°ä¸ºç›¸é‚»ç°‡ï¼ˆneighboring clusterï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¸‹ä¸€ä¸ªæœ€è¿‘çš„ç°‡
ä¸‹é¢æˆ‘ä»¬å®šä¹‰æ ·æœ¬ç‚¹$x_i$çš„è½®å»“å€¼silhouette 
$${\displaystyle s(x_i)={\frac {b(x_i)-a(x_i)}{\max\{a(x_i),b(x_i)\}}}} ,x_i \in C_p , \text{if } |C_p| > 1$$
ä¹Ÿå¯ä»¥å†™ä½œ(è¿™é‡Œç®€å†™äº†æ ·æœ¬ç‚¹)
$$s(i) = \begin{cases}
  1-a(i)/b(i), & \text{if } a(i) < b(i) \\
  0,  & \text{if } a(i) = b(i) \\
  b(i)/a(i)-1, & \text{if } a(i) > b(i) \\
\end{cases}$$
è½®å»“ç³»æ•°ï¼ˆsilhouette coefficient ï¼‰çš„å®šä¹‰
$${\displaystyle SC=\max _{k}{\tilde {s}}\left(k\right)}$$
${\displaystyle {\tilde {s}}\left(k\right)}$è¡¨ç¤ºä¸€ä¸ªç°‡çš„å¹³å‡è½®å»“å€¼ã€‚
è½®å»“ç³»æ•°çš„å€¼åœ¨-1å’Œ1ä¹‹é—´ï¼Œè¶Šå¤§è¶Šå¥½
> metrics.silhouette_score

- **DunnæŒ‡æ•°**ï¼ˆ[Dunn index, DI](https://en.jinzhao.wiki/wiki/Dunn_index)ï¼‰
$$DI=\min_{1\leq i\leq k}\bigg\{ \min_{j\neq i}\bigg(\frac{d_{min}(C_i,C_j)}{\max_{1\leq l \leq k}diam(C_l)}\bigg) \bigg\}$$
DIå€¼è¶Šå¤§è¶Šå¥½ï¼Œè§£é‡ŠåŒDBI


- **CHæŒ‡æ•°**ï¼ˆ[Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)ï¼‰
è¯¥æŒ‡æ•°æ˜¯æ‰€æœ‰ç°‡çš„ç°‡é—´ç¦»æ•£åº¦ï¼ˆbetween-clusters dispersionï¼‰å’Œç°‡å†…ç¦»æ•£åº¦ï¼ˆwithin-cluster dispersionï¼‰ä¹‹å’Œçš„æ¯”å€¼ï¼ˆå…¶ä¸­ç¦»æ•£åº¦å®šä¹‰ä¸ºè·ç¦»å¹³æ–¹å’Œï¼‰
$$s = \frac{\mathrm{tr}(B_k)/{k - 1}}{\mathrm{tr}(W_k) / {n_E - k}}  = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1} $$
å…¶ä¸­ä¸¤ä¸ªçŸ©é˜µï¼ˆç±»é—´ç¦»æ•£åº¦çŸ©é˜µ$B_k$å’Œç±»å†…ç¦»æ•£åº¦çŸ©é˜µ$W_k$ï¼‰çš„å®šä¹‰
$$W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T$$
$$B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T$$
$\mathrm{tr}(B_k)$è¡¨ç¤ºçŸ©é˜µçš„è¿¹ï¼›
å¤§$C_q$è¡¨ç¤ºç°‡$q$çš„é›†åˆ;å°$c_q$è¡¨ç¤ºç°‡$q$çš„ä¸­å¿ƒï¼›å°$c_E$è¡¨ç¤ºæ•°æ®é›†$E$çš„ä¸­å¿ƒï¼›
$n_q$è¡¨ç¤ºç°‡$q$çš„æ•°é‡ï¼›$E$è¡¨ç¤ºæ•°æ®é›†ï¼›$n_E$è¡¨ç¤ºæ•°æ®é›†$E$çš„æ•°é‡ï¼›$k$è¡¨ç¤ºç°‡çš„ä¸ªæ•°ï¼›
è¯¥æŒ‡æ•°è¶Šå¤§è¶Šå¥½
> metrics.calinski_harabasz_score

**External evaluation**ï¼ˆå°±æ˜¯éœ€è¦äººä¸ºæ ‡è®°æ¯ä¸ªæ ·æœ¬æ‰€å±çš„ç±»ï¼‰
æœ‰ç›‘ç£çš„æ–¹æ³•ï¼Œéœ€è¦åŸºå‡†æ•°æ®æˆ–è€…å‚è€ƒæ¨¡å‹ã€‚ç”¨ä¸€å®šçš„åº¦é‡è¯„åˆ¤èšç±»ç»“æœä¸åŸºå‡†æ•°æ®çš„ç¬¦åˆç¨‹åº¦ã€‚ï¼ˆåŸºå‡†æ˜¯ä¸€ç§ç†æƒ³çš„èšç±»ï¼Œé€šå¸¸ç”±ä¸“å®¶æ„å»ºï¼‰

- çº¯åº¦ï¼ˆPurityï¼‰
æˆ‘ä»¬å‡å®šæ•°æ®é›†æœ‰Nä¸ªæ•°æ®ã€‚åˆ†ç±»classesä½¿ç”¨$C = \{c_i|i=1,â€¦,n \}$ï¼›èšç±»clustersç»“æœ$K= \{k_i|1,â€¦,m\}$ï¼›
purityæ–¹æ³•æ˜¯æä¸ºç®€å•çš„ä¸€ç§èšç±»è¯„ä»·æ–¹æ³•ï¼Œåªéœ€è®¡ç®—æ­£ç¡®èšç±»çš„æ–‡æ¡£æ•°å æ€»æ–‡æ¡£æ•°çš„æ¯”ä¾‹ï¼š
$${\displaystyle {\frac {1}{N}}\sum _{k_i\in K}\max _{c_j\in C}{|k_i\cap c_j|}}$$
å€¼åœ¨[0,1]ä¹‹é—´ï¼Œå®Œå…¨é”™è¯¯çš„èšç±»æ–¹æ³•å€¼ä¸º0ï¼Œå®Œå…¨æ­£ç¡®çš„æ–¹æ³•å€¼ä¸º1
![](img/purity.jpg)

- [F-score](https://en.jinzhao.wiki/wiki/F-score)
F1 score, also known as balanced F-score or F-measure
$$P(\text{precision rate})={\frac {TP}{TP+FP}}, R(\text{recall rate})={\frac {TP}{TP+FN}}$$
$$F_{\beta }={\frac {(\beta ^{2}+1)\cdot P\cdot R}{\beta ^{2}\cdot P+R}}$$
F1 scoreå°±æ˜¯$\beta = 1$æ—¶çš„F-measureï¼›å½“$\beta = 0, F_0=P$ï¼›
> [Precision and recall](https://en.jinzhao.wiki/wiki/Precision_and_recall)
> metrics.f1_score

- [Jaccard index](https://en.jinzhao.wiki/wiki/Jaccard_coefficient)
$${\displaystyle {\text{Jaccard index}}=J(A,B)={\frac {|A\cap B|}{|A\cup B|}}={\frac {TP}{TP+FP+FN}}}$$
> metrics.jaccard_score

- [Dice index](https://en.jinzhao.wiki/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)
$${\displaystyle DSC={\frac {2TP}{2TP+FP+FN}}}$$

- [Homogeneity, completeness and V-measure](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure)
åŒè´¨æ€§ï¼ˆHomogeneityï¼‰ï¼šæ¯ä¸€ä¸ªcluster(èšç±»ç»“æœç°‡)ä¸­æ‰€åŒ…å«çš„æ•°æ®åº”å½’å±äºä¸€ä¸ªclass(ç±»)ã€‚
å®Œæ•´æ€§ï¼ˆcompletenessï¼‰ï¼šæ‰€æœ‰å±äºåŒä¸€ä¸ªclassçš„æ•°æ®åº”è¯¥è¢«å½’åˆ°ç›¸åŒçš„clusterä¸­ã€‚
æˆ‘ä»¬å‡å®šæ•°æ®é›†æœ‰Nä¸ªæ•°æ®ã€‚åˆ†ç±»classesä½¿ç”¨$C = \{c_i|i=1,â€¦,n \}$ï¼›èšç±»clustersç»“æœ$K= \{k_i|1,â€¦,m\}$ï¼›
$A = [a_{ij}]_{n \times m}$ï¼Œå…¶ä¸­ $a_{ij}$ è¡¨ç¤º$c_i$å±äº$k_j$çš„æ•°é‡ï¼ˆå°±æ˜¯contingency tableï¼Œä¸‹é¢æœ‰è®²ï¼‰
$$h = 1 - \frac{H(C|K)}{H(C)} , c = 1 - \frac{H(K|C)}{H(K)}$$
$$H(C|K) = - \sum_{k=1}^{|K|} \sum_{c=1}^{|C|} \frac{a_{ck}}{N}
\cdot \log\left(\frac{a_{ck}}{\sum_{c=1}^{|C|}a_{ck}}\right) \quad,\quad H(C) = - \sum_{c=1}^{|C|} \frac{\sum_{k=1}^{|K|}a_{ck}}{N} \cdot \log\left(\frac{\sum_{k=1}^{|K|}a_{ck}}{N}\right)$$
$$H(K|C) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{a_{ck}}{N}
\cdot \log\left(\frac{a_{ck}}{\sum_{k=1}^{|K|}a_{ck}}\right) \quad,\quad H(K) = - \sum_{k=1}^{|K|} \frac{\sum_{c=1}^{|C|}a_{ck}}{N} \cdot \log\left(\frac{\sum_{c=1}^{|C|}a_{ck}}{N}\right)$$
$å½“H(C)=0æ—¶ï¼Œh=1ï¼›å½“H(K)=0æ—¶ï¼Œc=1;$
ä¸¤è€…çš„è°ƒå’Œå¹³å‡v_measure_scoreï¼š
$$v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}$$
$\beta$é»˜è®¤ä¸º1ï¼›å½“$\beta>1$æ—¶completenesså½±å“æ›´å¤§ï¼Œæ›´å¤š[å‚è€ƒ](https://aclanthology.org/D07-1043.pdf)
ä¸‰ä¸ªæŒ‡æ ‡éƒ½åœ¨[0,1]åŒºé—´ï¼Œè¶Šå¤§è¶Šå¥½
> metrics.homogeneity_score, metrics.completeness_score, metrics.v_measure_score
> ä¸‰ä¸ªæŒ‡æ•°ä¸€èµ·è¿”å›metrics.homogeneity_completeness_v_measure

- **RandæŒ‡æ•°**ï¼ˆ[Rand index](https://en.jinzhao.wiki/wiki/Rand_index)ï¼‰
$$RI={\frac {TP+TN}{TP+FP+FN+TN}}=\frac{a+d}{n(n-1)/2}$$
[0,1]åŒºé—´ï¼Œå€¼è¶Šå¤§è¶Šå¥½
> è¡¡é‡ä¸¤ä¸ªæ•°æ®èšç±»ä¹‹é—´ç›¸ä¼¼æ€§çš„åº¦é‡ï¼ˆä¹Ÿå¯ä»¥æ˜¯æ ‡è®°æ•°æ®å’Œé¢„æµ‹æ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼‰
> 1.0 æ˜¯å®Œç¾åŒ¹é…åˆ†æ•°ã€‚æœªè°ƒæ•´ Rand æŒ‡æ•°çš„å¾—åˆ†èŒƒå›´ä¸º [0, 1]ï¼Œè°ƒæ•´å(adjusted)çš„ Rand æŒ‡æ•°ä¸º [-1, 1]
> metrics.rand_score ã€metrics.adjusted_rand_score

- FMæŒ‡æ•°ï¼ˆ[Fowlkesâ€“Mallows index](https://en.jinzhao.wiki/wiki/Fowlkes%E2%80%93Mallows_Index)ï¼‰
$$\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}$$
> metrics.fowlkes_mallows_score

- **æƒ…å½¢åˆ†æè¡¨**ï¼ˆ[Contingency Matrix](https://en.jinzhao.wiki/wiki/Contingency_table)ï¼‰

```
from sklearn.metrics.cluster import contingency_matrix
x = ["a", "a", "a", "b", "b", "b"]
y = [0, 0, 1, 1, 2, 2]
contingency_matrix(x, y)
array([[2, 1, 0],
       [0, 1, 2]])
```
åˆ—æ•°ä»£è¡¨yä¸­ä¸é‡å¤çš„ä¸ªæ•°ï¼›è¡Œä»£è¡¨xä¸­ä¸é‡å¤çš„ä¸ªæ•°ï¼›
ç¬¬ä¸€è¡Œåˆ†åˆ«è¡¨ç¤ºï¼š2ä¸ªaå±äº0ï¼Œ1ä¸ªaå±äº1ï¼Œ0ä¸ªaå±äº2
ç¬¬äºŒè¡Œåˆ†åˆ«è¡¨ç¤ºï¼š0ä¸ªbå±äº0ï¼Œ1ä¸ªbå±äº1ï¼Œ2ä¸ªbå±äº2
> metrics.cluster.contingency_matrix
- **æ··æ·†çŸ©é˜µ**ï¼ˆ[Confusion matrix](https://en.jinzhao.wiki/wiki/Confusion_matrix)ï¼‰

è¡ŒPredicted (expectation)
åˆ—Actual (observation)
TNè¡¨ç¤ºé¢„æµ‹negativeæ­£ç¡®
TPè¡¨ç¤ºé¢„æµ‹positiveæ­£ç¡®
FNè¡¨ç¤ºé¢„æµ‹negativeé”™è¯¯
FPè¡¨ç¤ºé¢„æµ‹positiveé”™è¯¯

Actual\Predicted |P|N
---|---|---
P(positive)|TP	|FN
N(negative)|FP	|TN

![https://en.jinzhao.wiki/wiki/Template:Diagnostic_testing_diagram](img/confusion_matrix.png)

> æ³¨æ„æ­£å¸¸çš„confusion matrixä¸­çš„å››ä¸ªå…ƒç´ ç›¸åŠ ä¸º$C_n^2=n(n-1)/2$ï¼Œè€Œpair_confusion_matrixæ˜¯$(n-1)$ï¼Œå¹¶ä¸”æ··æ·†çŸ©é˜µ
$$\begin{split}C = \left[\begin{matrix}
C_{00}(FN) & C_{01}(FP) \\
C_{10}(TN) & C_{11}(TP)
\end{matrix}\right]\end{split}$$
> metrics.cluster.pair_confusion_matrix
> metrics.plot_confusion_matrixå¯ä»¥ç»˜åˆ¶æ··æ·†çŸ©é˜µ

- **äº’ä¿¡æ¯**ï¼ˆ[mutual information](https://en.jinzhao.wiki/wiki/Mutual_information)ï¼‰
æ•°æ®é›†$S=\{s_{1},s_{2},\ldots s_{N}\}$, ç°‡$U=\{U_{1},U_{2},\ldots ,U_{R}\}$ä»¥åŠç°‡$V=\{V_{1},V_{2},\ldots ,V_{C}\}$,æ»¡è¶³${\displaystyle U_{i}\cap U_{j}=\varnothing =V_{i}\cap V_{j}}$ä»¥åŠ$\cup _{{i=1}}^{R}U_{i}=\cup _{{j=1}}^{C}V_{j}=S$
æœ‰è¿™æ ·ä¸€ä¸ªè¡¨(R*C)$M=[n_{{ij}}]_{{j=1\ldots C}}^{{i=1\ldots R}}$,ç§°ä¸º[contingency table](https://en.jinzhao.wiki/wiki/Contingency_table),å…¶ä¸­$n_{{ij}}=\left|U_{i}\cap V_{j}\right|$
${\displaystyle P_{U}(i)={\frac {|U_{i}|}{N}}}$è¡¨ç¤ºéšæœºé€‰å–ä¸€ä¸ªæ•°æ®ï¼Œå±äº$U_i$ç°‡çš„æ¦‚ç‡ã€‚
${\displaystyle P_{UV}(i,j)={\frac {|U_{i}\cap V_{j}|}{N}}}$è¡¨ç¤ºéšæœºé€‰å–ä¸€ä¸ªæ•°æ®ï¼ŒåŒæ—¶å±äº$U_i, V_j$ç°‡çš„æ¦‚ç‡ã€‚
${\displaystyle H(U)=-\sum _{i=1}^{R}P_{U}(i)\log P_{U}(i)}$è¡¨ç¤º$U$çš„ç†µã€‚
$${\displaystyle MI(U,V)=\sum _{i=1}^{R}\sum _{j=1}^{C}P_{UV}(i,j)\log {\frac {P_{UV}(i,j)}{P_{U}(i)P_{V}(j)}}}$$
$$\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}$$
$$AMI(U,V)={\frac {MI(U,V)-E\{MI(U,V)\}}{\max {\{H(U),H(V)\}}-E \{MI(U,V)\}}}$$
å…¶ä¸­
$${\begin{aligned}E\{MI(U,V)\}=&\sum _{{i=1}}^{R}\sum _{{j=1}}^{C}\sum _{{n_{{ij}}=(a_{i}+b_{j}-N)^{+}}}^{{\min(a_{i},b_{j})}}{\frac  {n_{{ij}}}{N}}\log \left({\frac  {N\cdot n_{{ij}}}{a_{i}b_{j}}}\right)\times \\&{\frac  {a_{i}!b_{j}!(N-a_{i})!(N-b_{j})!}{N!n_{{ij}}!(a_{i}-n_{{ij}})!(b_{j}-n_{{ij}})!(N-a_{i}-b_{j}+n_{{ij}})!}}\\\end{aligned}}$$
$E\{MI(U,V)\} ä¸ºMI(U,V)$çš„æœŸæœ›ï¼›$(a_{i}+b_{j}-N)^{+} = \max(1,a_{i}+b_{j}-N)$;
$a_{i}=\sum _{{j=1}}^{C}n_{{ij}}$;$b_{j}=\sum _{{i=1}}^{R}n_{{ij}}$
è¶Šå¤§è¶Šå¥½ï¼Œæœ€å¥½ä¸º1
> å‚è€ƒ[Adjusted mutual information](https://en.jinzhao.wiki/wiki/Adjusted_mutual_information)
> metrics.adjusted_mutual_info_scoreï¼Œmetrics.normalized_mutual_info_scoreï¼Œmetrics.mutual_info_score

**Cluster tendency**ï¼ˆèšç±»è¶‹åŠ¿ï¼‰
- **éœæ™®é‡‘æ–¯ç»Ÿè®¡é‡**ï¼ˆ[Hopkins statistic](https://en.jinzhao.wiki/wiki/Hopkins_statistic)ï¼‰
èšç±»è¶‹åŠ¿ï¼ˆèšç±»å¯è¡Œæ€§ï¼‰ï¼šåº”ç”¨èšç±»ç®—æ³•ä¹‹å‰ï¼Œåº”è¯¥è€ƒè™‘èšç±»å¯è¡Œæ€§ï¼›å¦‚ï¼šå³ä½¿æ•°æ®ä¸åŒ…å«ä»»ä½•clusterï¼Œèšç±»æ–¹æ³•ä¹Ÿä¼šè¿”å›clusterï¼›å› æ­¤ï¼Œè¯„ä¼°æ•°æ®é›†æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„clusterï¼ˆå³ï¼šééšæœºç»“æ„ï¼‰æœ‰æ—¶ä¼šå˜å¾—æœ‰å¿…è¦ã€‚æ­¤è¿‡ç¨‹è¢«å®šä¹‰ä¸º èšç±»è¶‹åŠ¿çš„è¯„ä¼° æˆ– èšç±»å¯è¡Œæ€§çš„åˆ†æã€‚
ä¸ééšæœºç»“æ„ç›¸å¯¹çš„æ˜¯å‡åŒ€åˆ†å¸ƒï¼ˆéšæœºç»“æ„ï¼‰ï¼Œéœæ™®é‡‘æ–¯ç»Ÿè®¡é‡çš„è®¡ç®—åŸç†ï¼Œä¾¿æ˜¯æ£€æŸ¥æ•°æ®æ˜¯å¦ç¬¦åˆå‡åŒ€åˆ†å¸ƒï¼ˆæˆ–è€…è¯´éšæœºæ€§ï¼‰ã€‚
æœ‰æ•°æ®é›†$X=\{x_1,x_2,...,x_n\},x_i \in \mathbb{R}^d$
ç”Ÿæˆéšæœºæ•°æ®é›†$Y=\{y_1,y_2,...,y_m\}, m \ll n $,å³ä»æ ·æœ¬çš„å¯èƒ½å–å€¼èŒƒå›´å†…éšæœºç”Ÿæˆmä¸ªç‚¹
$u_{i} = \min dist(y_i,x_j\in X)$å°±æ˜¯ä¸€ä¸ªéšæœºç‚¹ä¸æ•°æ®é›†Xä¸­çš„ç‚¹çš„æœ€å°è·ç¦»ï¼›
ä»æ‰€æœ‰æ ·æœ¬ä¸­éšæœºæ‰¾mä¸ªç‚¹ï¼Œ$w_{i} = \min dist(x_i,x_j\in X_{i\neq j})$å°±æ˜¯æ¯ä¸ªç‚¹åœ¨æ ·æœ¬ç©ºé—´(X)ä¸­æ‰¾åˆ°ä¸€ä¸ªç¦»ä»–æœ€è¿‘çš„ç‚¹ä¹‹é—´çš„è·ç¦»ï¼›
$${\displaystyle H={\frac {\sum _{i=1}^{m}{u_{i}^{d}}}{\sum _{i=1}^{m}{u_{i}^{d}}+\sum _{i=1}^{m}{w_{i}^{d}}}}\,,}$$
éšæœºç”Ÿæˆçš„ç‚¹ï¼ˆæ ·æœ¬å¯èƒ½çš„å–å€¼èŒƒå›´å†…ï¼‰ä¸ä»æ ·æœ¬ä¸­æ‰¾å‡ºç‚¹çš„ç©ºé—´æ¯”å€¼
æ ¹æ®è¿™ä¸ªå®šä¹‰ï¼Œå‡åŒ€éšæœºæ•°æ®çš„å€¼åº”è¯¥è¶‹å‘äºæ¥è¿‘0.5ï¼ˆä¸å¯è¡Œï¼‰ï¼Œè€Œèšé›†æ•°æ®çš„å€¼åº”è¯¥è¶‹å‘äºæ¥è¿‘1ï¼ˆå¯è¡Œï¼‰ã€$\sum u \gg \sum w$ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœèšç±»è¶‹åŠ¿æ˜æ˜¾ï¼Œåˆ™éšæœºç”Ÿæˆçš„æ ·æœ¬ç‚¹è·ç¦»åº”è¯¥è¿œå¤§äºå®é™…æ ·æœ¬ç‚¹çš„è·ç¦»ã€‘ã€‚

> å‚è€ƒ[Clustering performance evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)ä»¥åŠ[Evaluation and assessment](https://en.jinzhao.wiki/wiki/Cluster_analysis#Evaluation_and_assessment)


k-means: æ ·æœ¬é›†åˆ$X=\{x_1,x_2,...,x_n\},x_i \in \mathbb{R}^m$ï¼Œç®—æ³•çš„ç›®æ ‡æ˜¯å°†nä¸ªæ ·æœ¬åˆ†åˆ°ä¸åŒçš„clusterä¸­$C = \{ C_1,...,C_k\},k \lt n,C_i \cap C_j =\empty , \cup_{i=1}^kC_i =X$;
ç”¨$F: x_i \to l,l\in \{1,...,k\}$è¡¨ç¤ºåˆ’åˆ†å‡½æ•°ï¼Œè¾“å…¥æ ·æœ¬ï¼Œè¾“å‡ºæ‰€åœ¨çš„cluster
- **æ¨¡å‹**ï¼š
$$l = F(x_i) = F(i) ,i \in \{1,...,n\} $$

- **ç­–ç•¥**ï¼š
$$F^* = \argmin_{F} W(F) = \argmin_{F} \sum_{l=1}^k \sum_{F(i)=l}^{n_l} \|x_i - \bar{x}_l\|^2$$
å…¶ä¸­æŸå¤±å‡½æ•°$W(F)$ä¸ºæ ·æœ¬ä¸å…¶æ‰€å±clusterçš„ä¸­å¿ƒä¹‹é—´çš„è·ç¦»çš„æ€»å’Œï¼›
$n_l = \sum_{i=1}^n I(F(i)=l)$ï¼›

nä¸ªæ ·æœ¬åˆ†åˆ°kä¸ªclusterçš„æ‰€æœ‰åˆ†æ³•çš„ç§ç±»æœ‰$S(n,k)$ç§ï¼Œè¿™ä¸ªæ•°å­—æ˜¯æŒ‡æ•°çº§çš„ï¼Œæ‰€ä»¥æœ€ä¼˜é—®é¢˜æ˜¯ä¸ªNPå›°éš¾é—®é¢˜
$$S(n,k) = \frac{1}{k!}\sum_{l=1}^k(-1)^{k-l}\dbinom{k}{l}k^n$$

- **ç®—æ³•**ï¼š
è¿­ä»£ç®—æ³•,ä¸èƒ½ä¿è¯å…¨å±€æœ€ä¼˜
1. éšæœºé€‰æ‹©kä¸ªä¸­å¿ƒ$(m_1,m_2,...,m_k)$,
1. å°†æ ·æœ¬åˆ†åˆ«åˆ’åˆ†åˆ°ä¸å…¶æœ€è¿‘çš„ä¸­å¿ƒçš„clusterä¸­
1. æ›´æ–°æ¯ä¸ªclusterçš„å‡å€¼$(m_1,m_2,...,m_k)$ä½œä¸ºclusterçš„æ–°çš„ä¸­å¿ƒ
1. é‡å¤2ï¼Œ3ç›´åˆ°æ”¶æ•›ï¼ˆä¸­å¿ƒå˜åŒ–å¾ˆå°ï¼‰

---

K-meansç®—æ³•æœ‰ä»¥ä¸‹ä¸è¶³ï¼š
1. ç®—æ³•å¯¹åˆå§‹å€¼çš„é€‰å–ä¾èµ–æ€§æå¤§ã€‚åˆå§‹å€¼ä¸åŒï¼Œå¾€å¾€å¾—åˆ°ä¸åŒçš„å±€éƒ¨æå°å€¼ã€‚
1. ç”±äºå°†å‡å€¼ç‚¹ä½œä¸ºèšç±»ä¸­å¿ƒè¿›è¡Œæ–°ä¸€è½®è®¡ç®—ï¼Œè¿œç¦»æ•°æ®å¯†é›†åŒºçš„å­¤ç«‹ç‚¹å’Œå™ªå£°ç‚¹ä¼šå¯¼è‡´èšç±»ä¸­å¿ƒåç¦»çœŸæ­£çš„æ•°æ®å¯†é›†åŒºï¼Œæ‰€ä»¥K-å‡å€¼ç®—æ³•å¯¹å™ªå£°ç‚¹å’Œå­¤ç«‹ç‚¹å¾ˆæ•æ„Ÿã€‚


K-mediodsç®—æ³•ä¼˜ç¼ºç‚¹
K-ä¸­å¿ƒç‚¹è½®æ¢ç®—æ³•æ˜¯ä¸€ç§ä½¿ç›®æ ‡å‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹æ³•ï¼Œå®ƒå±äºå¯å‘å¼æœç´¢ç®—æ³•ï¼Œèƒ½ä»nä¸ªå¯¹è±¡ä¸­æ‰¾å‡ºä»¥kä¸ªä¸­å¿ƒç‚¹ä¸ºä»£è¡¨çš„ä¸€ä¸ªå±€éƒ¨ä¼˜åŒ–åˆ’åˆ†èšç±»ã€‚ä¸K-å‡å€¼ç®—æ³•æ¯”è¾ƒï¼ŒK-ä¸­å¿ƒç‚¹è½®æ¢ç®—æ³•è§£å†³äº†K-å‡å€¼ç®—æ³•æœ¬èº«çš„ç¼ºé™·ï¼š
1. è§£å†³äº†K-å‡å€¼ç®—æ³•å¯¹åˆå§‹å€¼é€‰æ‹©ä¾èµ–åº¦å¤§çš„é—®é¢˜ã€‚K-å‡å€¼ç®—æ³•å¯¹äºä¸åŒçš„åˆå§‹å€¼ï¼Œç»“æœå¾€å¾€å¾—åˆ°ä¸åŒçš„å±€éƒ¨æå°å€¼ã€‚è€ŒK-ä¸­å¿ƒç‚¹è½®æ¢ç®—æ³•é‡‡ç”¨è½®æ¢æ›¿æ¢çš„æ–¹æ³•æ›¿æ¢ä¸­å¿ƒç‚¹ï¼Œä»è€Œä¸åˆå§‹å€¼çš„é€‰æ‹©æ²¡æœ‰å…³ç³»ã€‚
1. è§£å†³äº†K-å‡å€¼ç®—æ³•å¯¹å™ªå£°å’Œç¦»ç¾¤ç‚¹çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚ç”±äºè¯¥ç®—æ³•ä¸ä½¿ç”¨å¹³å‡å€¼æ¥æ›´æ”¹ä¸­å¿ƒç‚¹è€Œæ˜¯é€‰ç”¨ä½ç½®æœ€é è¿‘ä¸­å¿ƒçš„å¯¹è±¡ä½œä¸ºä¸­å¿ƒä»£è¡¨ç‚¹ï¼Œå› æ­¤å¹¶ä¸å®¹æ˜“å—æç«¯æ•°æ®çš„å½±å“ï¼Œå…·æœ‰å¾ˆå¥½çš„é²æ£’æ€§ã€‚

K-ä¸­å¿ƒç‚¹è½®æ¢ç®—æ³•ä¹Ÿå­˜æœ‰ä»¥ä¸‹ç¼ºç‚¹ï¼š
1. ç”±äºK-ä¸­å¿ƒç‚¹è½®æ¢ç®—æ³•æ˜¯åŸºäºåˆ’åˆ†çš„ä¸€ç§èšç±»ç®—æ³•ï¼Œä»ç„¶è¦æ±‚è¾“å…¥è¦å¾—åˆ°çš„ç°‡çš„æ•°ç›®kï¼Œæ‰€ä»¥å½“kçš„å–å€¼ä¸æ­£ç¡®æ—¶ï¼Œå¯¹èšç±»çš„ç»“æœå½±å“ç”šå¤§ã€‚
1. ä»ä»¥ä¸Šçš„æ—¶é—´å¤æ‚åº¦ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œå½“nå’Œkè¾ƒå¤§æ—¶ï¼Œè®¡ç®—ä»£ä»·å¾ˆé«˜ï¼Œæ‰€ä»¥å°†è¯¥ç®—æ³•åº”ç”¨äºå¤§æ•°æ®é›†æ—¶ä¸æ˜¯å¾ˆç†æƒ³ã€‚

### å‚è€ƒæ–‡çŒ®
[14-1] Jain A, Dubes R. Algorithms for clustering data. Prentice-Hall, 1988.

[14-2] Aggarwal C C, Reddy C K. Data clustering: algorithms and applications. CRC Press, 2013.

[14-3] MacQueen J B. Some methods for classification and analysis of multivariate observations. Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability. Volume 1,pp.396-410. 1967.

[14-4] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[14-5] Pelleg D, Moore A W. X-means:  Extending K-means with Efficient Estimation of the Number of Clusters. Proceedings of ICML, pp. 727-734, 2000.

[14-6] Ester M, Kriegel H, Sander J, et al. [A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf). Proceedings of ACM SIGKDD, pp. 226-231, 1996.

[14-7] Shi J, Malik J. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000,22(8):888-905.

[14-8] Dhillon I S. Co-clustering documents and words using bipartite spectral graph partitioning. Proceedings of ACM SIGKDD, pp. 269-274. 2001.


## ç¬¬ 15 ç«  å¥‡å¼‚å€¼åˆ†è§£
å¥‡å¼‚å€¼åˆ†è§£([Singular Value Decomposition, SVD](https://en.jinzhao.wiki/wiki/Singular_value_decomposition))æ˜¯åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¹¿æ³›åº”ç”¨çš„ç®—æ³•ï¼Œå®ƒä¸å…‰å¯ä»¥ç”¨äºé™ç»´ç®—æ³•ä¸­çš„ç‰¹å¾åˆ†è§£ï¼Œè¿˜å¯ä»¥ç”¨äºæ¨èç³»ç»Ÿï¼Œä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚æ˜¯å¾ˆå¤šæœºå™¨å­¦ä¹ ç®—æ³•çš„åŸºçŸ³ã€‚ä¹Ÿæ˜¯çŸ©é˜µåˆ†è§£ï¼ˆ[Matrix decomposition](https://en.jinzhao.wiki/wiki/Category:Matrix_decompositions)ï¼‰çš„ä¸€ç§ã€‚

å…ˆäº†è§£ä¸‹ç‰¹å¾å€¼åˆ†è§£ï¼ˆ[Eigenvalues and eigenvectors](https://en.jinzhao.wiki/wiki/Eigenvalues_and_eigenvectors)ä»¥åŠ[Eigendecomposition of a matrix](https://en.jinzhao.wiki/wiki/Eigendecomposition_of_a_matrix)ï¼‰ä»¥åŠå¯¹è§’åŒ–ï¼ˆ[Diagonalizable matrix](https://en.jinzhao.wiki/wiki/Diagonalizable_matrix)ï¼‰
ç‰¹å¾å€¼ï¼ˆæœ‰äº›æ–¹é˜µæ˜¯æ²¡æœ‰ç‰¹å¾å€¼çš„ï¼‰ï¼š
$${\displaystyle A\mathbf {u} =\lambda \mathbf {u} \implies (A-I\lambda)\mathbf {u} = 0}$$
ç‰¹å¾å€¼åˆ†è§£ï¼š
è®¾$A_{n\times n}$ï¼Œæ˜¯å…·æœ‰nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡$q_i$ï¼ˆä¸ä¸€å®šæ˜¯ä¸åŒç‰¹å¾å€¼,å¯ä»¥æœ‰ç›¸åŒçš„ç‰¹å¾å€¼ï¼‰ï¼Œé‚£ä¹ˆAå¯ä»¥åˆ†è§£ä¸ºï¼š
$${\displaystyle A=Q\Lambda Q^{-1}}$$
å…¶ä¸­Aæ˜¯æ–¹é˜µï¼Œ$\Lambda$æ˜¯ç”±ç‰¹å¾å€¼ç»„æˆçš„å¯¹è§’çŸ©é˜µï¼ˆ[diagonal matrix](https://en.jinzhao.wiki/wiki/Diagonal_matrix)ï¼‰ï¼›
$q_i$é€šå¸¸æ˜¯æ ‡å‡†åŒ–çš„ï¼Œä½†ä¸æ˜¯å¿…é¡»çš„;
å› ä¸ºQçš„åˆ—æ˜¯çº¿æ€§æ— å…³çš„ï¼Œæ‰€ä»¥ Q æ˜¯å¯é€†çš„;
å¦‚æœAçš„ç‰¹å¾å€¼éƒ½ä¸ä¸º0é‚£ä¹ˆAæ˜¯å¯é€†çš„ï¼ˆ[invertible](https://en.jinzhao.wiki/wiki/Invertible_matrix)ï¼‰${\displaystyle \mathbf {A} ^{-1}=\mathbf {Q} \mathbf {\Lambda } ^{-1}\mathbf {Q} ^{-1}}$

æ³¨æ„åªæœ‰[å¯å¯¹è§’åŒ–](https://en.jinzhao.wiki/wiki/Diagonalizable_matrix)çš„çŸ©é˜µåˆ†è§£æ‰èƒ½ç”¨è¿™ç§æ–¹å¼ï¼šå¦‚ä»¥ä¸‹çŸ©é˜µä¸å¯è¢«å¯¹è§’åŒ–
$${\displaystyle A = \left[{\begin{matrix}1&1\\0&1\end{matrix}}\right]}$$
å…¶ç‰¹å¾å€¼ä¸º$[1,1]$,ç‰¹å¾å‘é‡ä¸º$[1,0]^T , [-1,0]^T$

å¦‚æœ $\mathbf {A}$  æ˜¯å¯¹ç§°çŸ©é˜µï¼ˆ[symmetric matrix](https://en.jinzhao.wiki/wiki/Symmetric_matrix)ï¼‰ï¼Œå› ä¸º$\mathbf {Q}$  ç”±ç‰¹å¾å‘é‡æ„æˆ $\mathbf {A}$ å®ƒä¿è¯æ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼ˆ[orthogonal matrix](https://en.jinzhao.wiki/wiki/Orthogonal_matrix)ï¼‰,æœ‰${\displaystyle \mathbf {Q} ^{-1}=\mathbf {Q} ^{\mathrm {T} }}$

> Qå…¶å®ä¹Ÿæ˜¯é…‰çŸ©é˜µï¼ˆ[Unitary Matrix](https://en.jinzhao.wiki/wiki/Unitary_matrix)ï¼‰ï¼Œå®ƒæ˜¯ æ­£äº¤çŸ©é˜µ åœ¨å¤æ•°ä¸Šçš„æ¨å¹¿ã€‚

> ä¸å¯å¯¹è§’åŒ–çš„çŸ©é˜µç§°ä¸ºæœ‰ç¼ºé™·çš„ï¼ˆ[defective](https://en.jinzhao.wiki/wiki/Defective_matrix)ï¼‰ã€‚å¯¹äºæœ‰ç¼ºé™·çš„çŸ©é˜µï¼Œç‰¹å¾å‘é‡çš„æ¦‚å¿µæ¨å¹¿åˆ°å¹¿ä¹‰ç‰¹å¾å‘é‡ï¼ˆ[generalized eigenvectors](https://en.jinzhao.wiki/wiki/Generalized_eigenvector)ï¼‰ï¼Œç‰¹å¾å€¼çš„å¯¹è§’çŸ©é˜µæ¨å¹¿åˆ°Jordan èŒƒå¼ï¼ˆ[Jordan normal form](https://en.jinzhao.wiki/wiki/Jordan_normal_form)ï¼‰ã€‚åœ¨ä»£æ•°é—­åŸŸä¸Šï¼Œä»»ä½•çŸ©é˜µAéƒ½å…·æœ‰Jordan èŒƒå¼ï¼Œå› æ­¤å…è®¸å¹¿ä¹‰ç‰¹å¾å‘é‡çš„åŸºå’Œåˆ†è§£ä¸ºå¹¿ä¹‰ç‰¹å¾ç©ºé—´ï¼ˆ[generalized eigenspaces](https://en.jinzhao.wiki/wiki/Generalized_eigenspace)ï¼‰ã€‚



- **æ¨¡å‹**ï¼š
å¯¹äºå¤çŸ©é˜µ $M_{m \times n} = {\displaystyle \mathbf {U\Sigma V^{H}} }$
å¯¹äºå®çŸ©é˜µ $M_{m \times n} = {\displaystyle \mathbf {U\Sigma V^{T}} }$
å…¶ä¸­$U$æ˜¯${\displaystyle m\times m}$å¤é…‰çŸ©é˜µ(æ­£äº¤çŸ©é˜µ)
$\Sigma = diag(\sigma_1,\sigma_2,...,\sigma_p)$æ˜¯çŸ©å½¢å¯¹è§’çŸ©é˜µï¼ˆrectangular diagonal matrixï¼‰ï¼Œå¯¹è§’å…ƒç´ æ˜¯éè´Ÿçš„å®æ•°å¹¶ä¸”é™åºæ’åˆ—,$p=\min(m,n), \sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0$
$V$æ˜¯ä¸€ä¸ª${\displaystyle n\times n}$å¤é…‰çŸ©é˜µ(æ­£äº¤çŸ©é˜µ)
$\sigma_i$ç§°ä¸ºçŸ©é˜µMçš„**å¥‡å¼‚å€¼**
$U$çš„åˆ—å‘é‡ç§°ä¸ºå·¦å¥‡å¼‚å‘é‡left-singular vector
$V$çš„åˆ—å‘é‡ç§°ä¸ºå³å¥‡å¼‚å‘é‡right-singular vector
å¦‚æœçŸ©é˜µ$M$çš„ç§©ä¸º$r = rank(M),r \le \min(m,n)$
$M$çš„ç´§SVDï¼ˆcompact SVDï¼‰ä¸º$M_{m \times n} = U_{m \times r} \Sigma_{r \times r} V_{n \times r}^T$ï¼Œ$rank(\Sigma_{r \times r}) = rank(M) = r$ï¼ˆå¯ä»¥æ— æŸå‹ç¼©ï¼‰
$M$çš„æˆªæ–­SVDï¼ˆtruncated SVDï¼‰ä¸º$M_{m \times n} \approx U_{m \times k} \Sigma_{k \times k} V_{n \times k}^T , 0 \lt k \lt r$ï¼ˆæœ‰æŸå‹ç¼©ï¼‰
çŸ©é˜µ$M$çš„ä¼ªé€†ï¼ˆ[pseudoinverse](https://en.jinzhao.wiki/wiki/Moore%E2%80%93Penrose_inverse)ï¼‰$M^{+} = V\Sigma^{+}U^{H}$
$M$çš„å¥‡å¼‚å€¼$\sigma_i$æ˜¯$M^TM$çš„ç‰¹å¾å€¼çš„å¹³æ–¹æ ¹$\sqrt{\lambda_i}$,$V$çš„åˆ—å‘é‡æ˜¯$M^TM$çš„ç‰¹å¾å‘é‡ï¼Œ$U$çš„åˆ—å‘é‡æ˜¯$MM^T$çš„ç‰¹å¾å‘é‡
$M^TM$å¾ˆæ˜æ˜¾è¿˜æ˜¯ä¸€ä¸ªå¯¹ç§°çŸ©é˜µï¼Œå…¶ç‰¹å¾å€¼ä¸ºéè´Ÿï¼Œè¯æ˜ï¼šå‡è®¾$\lambda$æ˜¯$M^TM$çš„ä¸€ä¸ªç‰¹å¾å€¼
$\|Mx\|^2 = x^TA^TAx = x^T \lambda x= \lambda x^Tx = \lambda\|x\|^2 \Rightarrow \lambda = \frac{\|Mx\|^2}{\|x\|^2} \ge 0$
$${\begin{aligned}\mathbf {M} ^{T}\mathbf {M} &=\mathbf {V} {\boldsymbol {\Sigma }}^{T}\mathbf {U} ^{T}\,\mathbf {U} {\boldsymbol {\Sigma }}\mathbf {V} ^{T}=\mathbf {V} ({\boldsymbol {\Sigma }}^{T}{\boldsymbol {\Sigma }})\mathbf {V} ^{T}\\\mathbf {M} \mathbf {M} ^{T}&=\mathbf {U} {\boldsymbol {\Sigma }}\mathbf {V} ^{T}\,\mathbf {V} {\boldsymbol {\Sigma }}^{T}\mathbf {U} ^{T}=\mathbf {U} ({\boldsymbol {\Sigma }}{\boldsymbol {\Sigma }}^{T})\mathbf {U} ^{T}\end{aligned}}$$
è¿™ä¸å°±æ˜¯çŸ©é˜µçš„ç‰¹å¾å€¼åˆ†è§£å—ã€ä¸Šé¢æœ‰å°†ç‰¹å¾å€¼åˆ†è§£æœ‰è®²åˆ°å¯¹ç§°çŸ©é˜µçš„å¯¹è§’åŒ–ï¼ˆç‰¹å¾å€¼åˆ†è§£ï¼‰ã€‘

- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š
### é™„åŠ çŸ¥è¯†


#### çŸ©é˜µæ€§è´¨
è¿™é‡Œä»‹ç»ä¸€äº›å‚è§çš„[çŸ©é˜µ](https://en.jinzhao.wiki/wiki/Category:Matrices)ï¼Œä»¥åŠå…¶æ€§è´¨ã€‚

##### å…±è½­è½¬ç½®ï¼ˆ[Conjugate transpose](https://en.jinzhao.wiki/wiki/Conjugate_transpose)ï¼‰

å…±è½­ï¼ˆ[Complex conjugate](https://en.jinzhao.wiki/wiki/Complex_conjugate)ï¼‰æ˜¯å¤æ•°ä¸Šçš„æ¦‚å¿µ
å¯¹äºä¸€ä¸ªå¤æ•°$z =a + bi$ï¼Œå…¶å…±è½­ä¸º$\bar{z} = a-bi$ï¼Œæ‰€ä»¥æœ‰$z\bar{z} = a^2 + b^2$
å…±è½­è½¬ç½®ä¹Ÿæœ‰å…¶å®ƒå«æ³•ï¼Œå¦‚ï¼šHermitian conjugate, bedaggered matrix, adjoint matrix or transjugateã€‚å€¼å¾—æ³¨æ„çš„æ˜¯adjoint matrixè€Œä¸æ˜¯ è¿™ä¸ª[Adjugate matrix](https://en.jinzhao.wiki/wiki/Adjugate_matrix)ï¼Œè™½ç„¶æœ‰æ—¶å€™ä»–ä»¬éƒ½ç”¨$A^*$è¡¨ç¤ºã€‚è¿™é‡Œä¸ºäº†ç»Ÿä¸€æˆ‘ç”¨$A^H$è¡¨ç¤ºAçš„å…±è½­è½¬ç½®çŸ©é˜µã€‚å¯ä»¥å‚è€ƒ[å…±è½­è½¬ç½®çŸ©é˜µä¸ä¼´éšçŸ©é˜µéƒ½ç”¨A*è¡¨ç¤ºåˆç†å—ï¼Ÿ](https://zhuanlan.zhihu.com/p/87330558)
> æœ‰ä¸ªç¥å¥‡çš„å…¬å¼ï¼šæ¬§æ‹‰å…¬å¼ $e^{\pi i}+1=0$ï¼Œå‡†ç¡®çš„è¯´æ¬§æ‹‰å…¬å¼ä¸ºï¼š$e^{ix}=\cos x + i\sin x$ï¼Œå‰é¢åªæ˜¯å½“$x=\pi$æ—¶çš„ç»“æœã€‚
> è¿™ä¸ªå…¬å¼é‡Œæ—¢æœ‰è‡ªç„¶åº•æ•°eï¼Œè‡ªç„¶æ•°1å’Œ0ï¼Œè™šæ•°iè¿˜æœ‰åœ†å‘¨ç‡piï¼Œå®ƒæ˜¯è¿™ä¹ˆç®€æ´ï¼Œè¿™ä¹ˆç¾ä¸½å•Šï¼

$${\displaystyle \left({\boldsymbol {A}}^{\mathrm {H} }\right)_{ij}={\overline {{\boldsymbol {A}}_{ji}}}}$$

$${\displaystyle {\boldsymbol {A}}^{\mathrm {H} }=\left({\overline {\boldsymbol {A}}}\right)^{\mathsf {T}}={\overline {{\boldsymbol {A}}^{\mathsf {T}}}}}$$

ä¾‹å¦‚ï¼š
$${\displaystyle {\boldsymbol {A}}={\begin{bmatrix}1&-2-i&5\\1+i&i&4-2i\end{bmatrix}}} , {\displaystyle {\boldsymbol {A}}^{\mathsf {T}}={\begin{bmatrix}1&1+i\\-2-i&i\\5&4-2i\end{bmatrix}}} , {\displaystyle {\boldsymbol {A}}^{\mathrm {H} }={\begin{bmatrix}1&1-i\\-2+i&-i\\5&4+2i\end{bmatrix}}}$$

æ€§è´¨ï¼š
- ${\displaystyle ({\boldsymbol {A}}+{\boldsymbol {B}})^{\mathrm {H} }={\boldsymbol {A}}^{\mathrm {H} }+{\boldsymbol {B }}^{\mathrm {H} }}$ 
- ${\displaystyle (z{\boldsymbol {A}})^{\mathrm {H} }={\overline {z}}{\boldsymbol {A}}^{\mathrm {H} }}$
- ${\displaystyle ({\boldsymbol {A}}{\boldsymbol {B}})^{\mathrm {H} }={\boldsymbol {B}}^{\mathrm {H} }{\boldsymbol {A}} ^{\mathrm {H} }}$
- ${\displaystyle \left({\boldsymbol {A}}^{\mathrm {H} }\right)^{\mathrm {H} }={\boldsymbol {A}}}$
- å¦‚æœ$A$å¯é€†ï¼Œå½“ä¸”ä»…å½“$A^H$å¯é€†ï¼Œæœ‰${\displaystyle \left({\boldsymbol {A}}^{\mathrm {H} }\right)^{-1}=\left({\boldsymbol {A}}^{-1}\right)^{ \mathrm {H} }}$
- $A^H$çš„ç‰¹å¾å€¼æ˜¯$A$ç‰¹å¾å€¼çš„å¤å…±è½­
- å†…ç§¯æ€§è´¨${\displaystyle \left\langle {\boldsymbol {A}}x,y\right\rangle _{m}=\left\langle x,{\boldsymbol {A}}^{\mathrm {H} }y\right\rangle _{n}}$ï¼Œ`Aæ˜¯m*n,xæ˜¯n*1,yæ˜¯m*1`ï¼Œä¸‹æ ‡mè¡¨ç¤ºæ˜¯mç»´å‘é‡ä½œå†…ç§¯ã€‚
- ï¼ˆAæ˜¯æ–¹é˜µï¼‰è¡Œåˆ—å¼æ€§è´¨${\displaystyle \det \left({\boldsymbol {A}}^{\mathrm {H} }\right)={\overline {\det \left({\boldsymbol {A}}\right)}}}$
- ï¼ˆAæ˜¯æ–¹é˜µï¼‰è¿¹çš„æ€§è´¨${\displaystyle \operatorname {tr} \left({\boldsymbol {A}}^{\mathrm {H} }\right)={\overline {\operatorname {tr} ({\boldsymbol {A}})}}}$

##### åŸƒå°”ç±³ç‰¹çŸ©é˜µï¼ˆ[Hermitian matrix](https://en.jinzhao.wiki/wiki/Hermitian_matrix)ï¼‰
Hermitian matrix (or self-adjoint matrix)
Aæ˜¯å¤**æ–¹é˜µ**
$${\displaystyle A{\text{ Hermitian}}\quad \iff \quad A=A^{\mathsf {H}}}$$
ä¾‹å­ï¼š
$$A = {\displaystyle {\begin{bmatrix}0&a-ib&c-id\\a+ib&1&m-in\\c+id&m+in&2\end{bmatrix}}}$$
**åŸƒå°”ç±³ç‰¹çŸ©é˜µæ˜¯å¯¹ç§°çŸ©é˜µåœ¨å¤æ•°ä¸Šçš„æ¨å¹¿**ã€‚

å…¶çŸ©é˜µæœ‰å¾ˆå¤šæ€§è´¨ï¼Œå…·ä½“è§ç»´åŸºç™¾ç§‘ã€‚

Skew-Hermitian matrixï¼š${\displaystyle A{\text{ skew-Hermitian}}\quad \iff \quad A^{\mathsf {H}}=-A}$

##### [Normal matrix](https://en.jinzhao.wiki/wiki/Normal_matrix)
Aæ˜¯å¤**æ–¹é˜µ**
$${\displaystyle A{\text{ normal}}\quad \iff \quad A^{H}A=AA^{H}}$$

ä¾‹å­ï¼š
$${\displaystyle A={\begin{bmatrix}1&1&0\\0&1&1\\1&0&1\end{bmatrix}}} , {\displaystyle AA^{H}={\begin{bmatrix}2&1&1\\1&2&1\\1&1&2\end{bmatrix}}=A^{H}A.}$$

Normal matrixä¸€å®šæ˜¯å¯å¯¹è§’åŒ–çš„$A = U\Lambda U^H$ï¼Œ$U$æ˜¯é…‰çŸ©é˜µï¼Œ$\Lambda = diag(\lambda_1,...)$æ˜¯$A$çš„ç‰¹å¾å€¼ç»„æˆçš„å¯¹è§’çŸ©é˜µ

> å¯¹äºå¤çŸ©é˜µï¼Œæ‰€æœ‰çš„unitary, Hermitian, and skew-Hermitian çŸ©é˜µéƒ½æ˜¯normalçŸ©é˜µ
> å¯¹åº”çš„å¯¹äºå®çŸ©é˜µï¼Œæ‰€æœ‰çš„ orthogonal, symmetric, and skew-symmetric çŸ©é˜µä¹Ÿéƒ½æ˜¯normalçŸ©é˜µ

##### é…‰çŸ©é˜µï¼ˆ[Unitary matrix](https://en.jinzhao.wiki/wiki/Unitary_matrix)ï¼‰
Uæ˜¯å¤**æ–¹é˜µ**
$$U^{H} = U^{-1}$$
æ€§è´¨ï¼š
- ${\displaystyle U^{H}U=UU^{H}=I,}$
- $\left\langle Ux,Uy\right\rangle = \left\langle x,y\right\rangle$
- Uæ˜¯å¯å¯¹è§’åŒ–çš„${\displaystyle U=VDV^{H},}$where V is unitary, and D is diagonal and unitary.
- ${\displaystyle \left|\det(U)\right|=1}$
- å…¶ç‰¹å¾å‘é‡æ˜¯ç›¸äº’æ­£äº¤çš„ï¼ˆåºŸè¯ï¼Œæ­£äº¤çŸ©é˜µçš„æ¨å¹¿ï¼‰

**é…‰çŸ©é˜µå®ƒæ˜¯æ­£äº¤çŸ©é˜µåœ¨å¤æ•°ä¸Šçš„æ¨å¹¿**ã€‚

#### çŸ©é˜µåˆ†è§£(å› å­åˆ†è§£)

> sympy.Matrixé™¤äº†åˆ†è§£è¿˜æœ‰diagonalizeå¯¹è§’åŒ–ï¼ˆä¹Ÿæ˜¯ä¸€ç§çŸ©é˜µåˆ†è§£ï¼‰ï¼Œeigç‰¹å¾å€¼ï¼ˆå…¶å®ä¹Ÿå¯ä»¥ç‰¹å¾å€¼åˆ†è§£ï¼‰ï¼Œrrefè¡Œç®€åŒ–é˜¶æ¢¯å‹ï¼Œdetè¡Œåˆ—å¼ï¼Œinvé€†çŸ©é˜µï¼Œå¹¿ä¹‰é€†çŸ©é˜µpinvï¼›æ›´å¤š[å‚è€ƒ](https://docs.sympy.org/latest/modules/matrices/matrices.html#linear-algebra)
> scipy.linalgä¸­ä¹Ÿæœ‰å¾ˆå¤šå…³äºçº¿æ€§ä»£æ•°çš„æ–¹æ³•ï¼šscipy.linalg.svdï¼Œä»¥åŠå„ç§çŸ©é˜µåˆ†è§£çš„æ–¹æ³•ï¼›æ›´å¤š[å‚è€ƒ](http://scipy.github.io/devdocs/reference/linalg.html)
> numpy.linalgä¸­ä¹Ÿæœ‰å¾ˆå¤šå…³äºçº¿æ€§ä»£æ•°çš„æ–¹æ³•ï¼šnp.linalg.svdï¼›æ›´å¤š[å‚è€ƒ](https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.linalg.html)


é™¤äº†SVDå’ŒPCAï¼Œè¿˜æœ‰å¾ˆå¤šçŸ©é˜µåˆ†è§£ï¼ˆ[Matrix decomposition](https://en.jinzhao.wiki/wiki/Matrix_decomposition)ï¼‰çš„æ–¹æ³•ã€‚ä¸è¿‡æœ‰å¾ˆå¤šåˆ†è§£æ˜¯æœ‰è¦æ±‚çš„ï¼Œæ¯”å¦‚å¿…é¡»æ˜¯æ–¹é˜µï¼ˆç‰¹å¾å€¼åˆ†è§£å°±è¦æ±‚å¿…é¡»æ˜¯æ–¹é˜µï¼‰ç­‰ã€‚
- **LUåˆ†è§£**ï¼ˆ[LU decomposition](https://en.jinzhao.wiki/wiki/LU_decomposition)ï¼‰
$${\displaystyle A=LU.}$$
Læ˜¯ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆ[lower triangular matrix](https://en.jinzhao.wiki/wiki/Triangular_matrix)ï¼‰
Uæ˜¯ä¸Šä¸‰è§’çŸ©é˜µï¼ˆ[upper triangular matrix](https://en.jinzhao.wiki/wiki/Triangular_matrix)ï¼‰
æœ‰æ—¶è¿˜ä¼šåŒ…å«ä¸€ä¸ªç½®æ¢çŸ©é˜µï¼ˆ[permutation matrix](https://en.jinzhao.wiki/wiki/Permutation_matrix)ï¼‰ï¼Œå®ƒåœ¨æ¯è¡Œå’Œæ¯åˆ—ä¸­åªæœ‰ä¸€ä¸ª1ï¼Œè€Œåœ¨å…¶ä»–åœ°æ–¹åˆ™ä¸º0ã€‚
$${\displaystyle A=PLU}$$

- **QRåˆ†è§£**ï¼ˆ[QR decomposition](https://en.jinzhao.wiki/wiki/QR_decomposition)ï¼‰
$$A = QR$$
Qæ˜¯æ­£äº¤çŸ©é˜µï¼ˆ[Orthogonal Matrix](https://en.jinzhao.wiki/wiki/Orthogonal_matrix)ï¼‰ï¼›
Ræ˜¯ä¸Šä¸‰è§’çŸ©é˜µï¼ˆ[right(upper) triangular matrix](https://en.jinzhao.wiki/wiki/Triangular_matrix)ï¼‰
> ç±»ä¼¼çš„å¯ä»¥å®šä¹‰QLã€RQ å’Œ LQï¼ŒLæ˜¯ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆ[left(lower) triangular matrix](https://en.jinzhao.wiki/wiki/Triangular_matrix)ï¼‰


- **éè´ŸçŸ©é˜µåˆ†è§£**ï¼ˆ[Non-negative matrix factorization (NMF or NNMF)](https://en.jinzhao.wiki/wiki/Non-negative_matrix_factorization)ï¼‰
$$\mathbf {A} =\mathbf {W} \mathbf {H} \,.$$
å°†çŸ©é˜µ$A$åˆ†è§£ä¸ºä¸¤ä¸ªéè´ŸçŸ©é˜µçš„ä¹˜ç§¯(è¿‘ä¼¼ç›¸ç­‰)
$$minimize  {\displaystyle \left\|V-WH\right\|_{F},} \\ s.t. W\geq 0,H\geq 0.$$
è¿™é‡Œæœ‰è®²åˆ°[ä¸åŒçš„è¡¨ç¤ºæ–¹æ³•å¯¹åº”è¿™ä¸åŒè¯´æ³•](https://en.jinzhao.wiki/wiki/Non-negative_matrix_factorization#Clustering_property)ï¼Œè¿™é‡Œæœ‰[ä¸åŒçš„è¡¨ç¤ºæ–¹æ³•](https://scikit-learn.org/stable/modules/decomposition.html#nmf-with-a-beta-divergence)

### å‚è€ƒæ–‡çŒ®

[15-1] Leon S J. Linear algebra with applications. Pearson, 2009(ä¸­è¯‘æœ¬ï¼šçº¿æ€§ä»£æ•°ã€‚å¼ æ–‡åšï¼Œå¼ ä¸½é™ è¯‘. åŒ—äº¬ï¼šæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾)

[15-2] Strang G. Introduction to linear algebra. Fourth Edition. Wellesley-Cambridge Press, 2009.

[15-3] Cline A K. Dhillon I S. Computation of the singular value decomposition, Handbook of linear algebra. CRC Press, 2006.

[15-4] å¾æ ‘æ–¹. çŸ©é˜µè®¡ç®—çš„ç†è®ºä¸æ–¹æ³•ã€‚åŒ—äº¬ï¼šåŒ—äº¬å¤§å­¦å‡ºç‰ˆç¤¾, 1995.

[15-5] Kolda T G,Bader B W. [Tensor decompositions and applications](https://old-www.sandia.gov/~tgkolda/pubs/pubfiles/SAND2007-6702.pdf). SIAM Review, 2009, 51(3):455-500.

## ç¬¬ 16 ç«  ä¸»æˆåˆ†åˆ†æ

ä¸»æˆåˆ†åˆ†æï¼ˆ[Principal component analysis, PCA](https://en.jinzhao.wiki/wiki/Principal_component_analysis)ï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒPCAåˆ©ç”¨æ­£äº¤å˜æ¢æŠŠç”±çº¿æ€§ç›¸å…³å˜é‡è¡¨ç¤ºçš„è§‚æµ‹æ•°æ®è½¬æ¢ä¸ºå°‘æ•°å‡ ä¸ªç”±çº¿æ€§æ— å…³å˜é‡è¡¨ç¤ºçš„æ•°æ®ï¼Œçº¿æ€§æ— å…³çš„å˜é‡ç§°ä¸ºä¸»æˆåˆ†ã€‚

ä¸»æˆåˆ†åˆ†ææ­¥éª¤å¦‚ä¸‹ï¼š
1. å¯¹ç»™å®šæ•°æ®è¿›è¡Œè§„èŒƒåŒ–ï¼Œä½¿å¾—æ•°æ®æ¯ä¸€å˜é‡çš„å¹³å‡å€¼ä¸º0,æ–¹å·®ä¸º1ï¼ˆStandardScalerï¼‰ã€‚

1. å¯¹æ•°æ®è¿›è¡Œæ­£äº¤å˜æ¢ï¼ŒåŸæ¥ç”±çº¿æ€§ç›¸å…³å˜é‡è¡¨ç¤ºçš„æ•°æ®,é€šè¿‡æ­£äº¤å˜æ¢å˜æˆç”±è‹¥å¹²ä¸ªçº¿æ€§æ— å…³çš„æ–°å˜é‡è¡¨ç¤ºçš„æ•°æ®ã€‚

æ–°å˜é‡æ˜¯å¯èƒ½çš„æ­£äº¤å˜æ¢ä¸­å˜é‡çš„æ–¹å·®çš„å’Œ(ä¿¡æ¯ä¿å­˜)æœ€å¤§çš„ï¼Œæ–¹å·®è¡¨ç¤ºåœ¨æ–°å˜é‡ä¸Šä¿¡æ¯çš„å¤§å°ã€‚å°†æ–°å˜é‡ä¾æ¬¡ç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ç­‰ã€‚

> æˆ‘ä»¬é€šå¸¸è¡¨ç¤ºä¸€ä¸ªæ ·æœ¬æ˜¯åœ¨å®æ•°ç©ºé—´ä¸­ç”¨æ­£äº¤åæ ‡ç³»è¡¨ç¤ºï¼Œè§„èŒƒåŒ–çš„æ•°æ®åˆ†å¸ƒåœ¨åŸç‚¹é™„è¿‘

ä¸»æˆåˆ†åˆ†æå°±æ˜¯å¯¹æ•°æ®è¿›è¡Œæ­£äº¤å˜æ¢ï¼Œå¯¹åŸåæ ‡ç³»è¿›è¡Œæ—‹è½¬å˜æ¢ï¼Œå¹¶å°†æ•°æ®åœ¨æ–°åæ ‡ç³»ä¸­è¡¨ç¤ºï¼›æˆ‘ä»¬å°†é€‰æ‹©æ–¹å·®æœ€å¤§çš„æ–¹å‘ä½œä¸ºæ–°åæ ‡ç³»çš„ç¬¬ä¸€åæ ‡è½´ã€‚æ–¹å·®æœ€å¤§ä»£è¡¨ç€åœ¨è¯¥æ–¹å‘ä¸Šçš„æŠ•å½±ï¼ˆä¸å°±æ˜¯åœ¨è¿™ä¸ªåæ ‡ç³»çš„åæ ‡è½´ä¸Šçš„è¡¨ç¤ºä¹ˆï¼‰åˆ†æ•£çš„æœ€å¼€ã€‚

æ ¹æ®æ–¹å·®çš„å®šä¹‰ï¼Œæ¯ä¸ªæ–¹å‘ä¸Šæ–¹å·®å°±æ˜¯åœ¨è¯¥åæ ‡ç³»ï¼ˆå˜æ¢åçš„æ–°åæ ‡ç³»ï¼‰ä¸Šè¡¨ç¤ºæ‰€å¯¹åº”çš„ç»´åº¦çš„æ–¹å·®$var(a) = \frac{1}{N-1}\sum_{i=1}^N (a_i - \mu)^2$ï¼ˆç”¨ç¬¬ä¸€ä¸ªæ–¹å‘æ¥è¯´æ˜, Nä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªç»´åº¦ç»„æˆå‘é‡$a$ï¼‰ï¼›ç”±äºæˆ‘ä»¬å·²ç»å¯¹æ•°æ®è¿›è¡Œçš„è§„èŒƒåŒ–ï¼Œæ‰€ä»¥å‡å€¼ä¸º0ï¼›$var(a) = \frac{1}{N-1}\sum_{i=1}^N (a_i)^2$ ;$a_i$å°±æ˜¯ç¬¬$i$ä¸ªæ ·æœ¬$x^{(i)}$ä¸ç¬¬ä¸€ä¸ªæ–¹å‘çš„å†…ç§¯ã€‚

æˆ‘ä»¬çš„ç›®çš„å°±æ˜¯ä¸ºäº†$var(a)$æœ€å¤§ï¼Œæˆ‘ä»¬è¦æ±‚çš„å°±æ˜¯æ‰¾åˆ°å˜æ¢åçš„æ–°åæ ‡ç³»ï¼Œå‡è®¾æ–¹å·®æœ€å¤§çš„æ–¹å‘çš„å•ä½å‘é‡ä¸º$v_1$ï¼Œæ•°æ®é›†$T = \{x^{(1)},x^{(2)},...,x^{(N)}\} , x=\{x_1,...,x_m\}^T$ï¼Œmç»´

$$\max \frac{1}{N-1}\sum_{i=1}^N \braket{x^{(i)},v_1}^2 = \frac{1}{N-1}\sum_{i=1}^N \|{x^{(i)}}^{T}.v_1\|^2 \\= \frac{1}{N-1}\sum_{i=1}^N ({x^{(i)}}^{T}.v_1)^T({x^{(i)}}^{T}.v_1) \\= \frac{1}{N-1}\sum_{i=1}^N v_1^T{x^{(i)}}{x^{(i)}}^{T}v_1 \\= \frac{1}{N-1} v_1^T \sum_{i=1}^N[{x^{(i)}}{x^{(i)}}^{T}]v_1$$
è®¾çŸ©é˜µ$X = [x^{(1)},x^{(2)},...,x^{(N)}]$é‚£ä¹ˆ$XX^T =\sum_{i=1}^N[{x^{(i)}}{x^{(i)}}^{T}]$ï¼Œå¾—åˆ°
$$\max \frac{1}{N-1} v_1^T XX^T v_1 \\ s.t. \quad v_1^Tv_1 =1$$
æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼ˆå‚è§[çŸ©é˜µæ±‚å¯¼](https://en.jinzhao.wiki/wiki/Matrix_calculus#Scalar-by-vector_identities)ï¼‰
$$L = - \frac{1}{N-1} v_1^T XX^T v_1 + \lambda_1(v_1^Tv_1 - 1)$$
$$\frac{\partial L}{\partial v_1} = -2\frac{1}{N-1}v_1^T XX^T + 2\lambda_1 v_1^T =0 $$
$$\frac{1}{N-1}v_1^T XX^T = \lambda_1 v_1^T \implies  \frac{1}{N-1}XX^Tv_1 = \lambda_1 v_1$$
å…¶å®$\frac{1}{N-1}XX^T$å°±æ˜¯$X_{m \times N}$æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µ$\Sigma_{m \times m}$

$\lambda_1$æ˜¯$\Sigma_{m \times m}$çš„ç‰¹å¾å€¼ï¼Œ$v_1$(åˆ—å‘é‡)æ˜¯å…¶å¯¹åº”çš„ç‰¹å¾å€¼å‘é‡ï¼›

æ¥ç€æ±‚ç¬¬äºŒä¸ªä¸»æˆåˆ†$v_2$,ä¸»æˆåˆ†æ˜¯ç›¸äº’æ­£äº¤çš„
$$\max \frac{1}{N-1} v_2^T XX^T v_2 \\ s.t. \quad v_2^Tv_2 =1 ,v_2^Tv_1 =0$$
æ³¨æ„åˆ°
$$v_1^T XX^T v_2 = \lambda_1 v_1^T v_2 = 0 = v_2^T XX^T v_1 =  \lambda_1 v_2^T v_1$$
ä¾æ¬¡æ±‚å¾—å…¶å®ƒæˆåˆ†ã€‚

æœ€ç»ˆæœ‰ä¸»æˆåˆ†ç»„æˆçš„çŸ©é˜µ
$V_{m \times m } = [v_1,v_2,...,v_m]$
é™ç»´åˆ°kç»´å°±æ˜¯ä¸€æ¬¡å–å‰kä¸ªå‘é‡ç»„æˆçš„çŸ©é˜µä¸Xä½œä¹˜ç§¯ï¼Œé‚£ä¹ˆé™ç»´åçš„æ•°æ®ï¼š

$$Y_{k \times N} = V_{m \times k }^T X_{m \times N}$$

> å‰é¢å­¦ä¹ äº†SVDéœ€è¦æ±‚$A^TA$çš„ç‰¹å¾å€¼åˆ†è§£;è€ŒPCAéœ€è¦æ±‚$\frac{1}{N-1}XX^T$çš„ç‰¹å¾å€¼åˆ†è§£;
> åªéœ€è¦å–$A = \frac{X^T}{\sqrt{N-1}}$å°±å¯ä»¥å°†PCAé—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºSVDé—®é¢˜æ±‚è§£
> å…¶å®ï¼ŒPCAåªä¸SVDçš„å³å¥‡å¼‚å‘é‡çš„å‹ç¼©æ•ˆæœç›¸åŒã€‚
> ä¸€èˆ¬ $X$ çš„ç»´åº¦å¾ˆé«˜ï¼Œ$XX^T$ çš„è®¡ç®—é‡å¾ˆå¤§ï¼Œå¹¶ä¸”æ–¹é˜µçš„ç‰¹å¾å€¼åˆ†è§£è®¡ç®—æ•ˆç‡ä¸é«˜ï¼ŒSVDé™¤äº†ç‰¹å¾å€¼åˆ†è§£è¿™ç§æ±‚è§£æ–¹å¼å¤–ï¼Œè¿˜æœ‰æ›´é«˜æ•ˆä¸”æ›´å‡†ç¡®çš„è¿­ä»£æ±‚è§£æ³•ï¼Œé¿å…äº†$XX^T$çš„è®¡ç®—

- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š

**ç¨€ç–ä¸»æˆåˆ†åˆ†æ**ï¼ˆ[Sparse PCA](https://en.jinzhao.wiki/wiki/Sparse_PCA)ï¼‰
ç¨€ç– PCA é—®é¢˜æœ‰è®¸å¤šä¸åŒçš„å…¬å¼ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨[Structured Sparse Principal Component Analysis](https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf)ä»¥åŠ[Online Dictionary Learning for Sparse Coding](https://www.di.ens.fr/sierra/pdfs/icml09.pdf)

$$\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2}
             ||X-UV||_2^2+\alpha||V||_1 \\
             \text{subject to } & ||U_k||_2 = 1 \text{ for all }
             0 \leq k < n_{components}\end{split}$$

> æ„æ€å°±æ˜¯æ±‚UVè®©å…¶è¿‘ä¼¼ç­‰äºXï¼Œç„¶åå¾—åˆ°ä¸€ä¸ªç¨€ç–çŸ©é˜µV
> sklearn.decomposition.SparsePCA.components_ å°±æ˜¯å…¶ç¨€ç–çš„çŸ©é˜µ$V$
> SPCAçš„å«ä¹‰å‚è€ƒ [Matrix decomposition](https://github.com/kingreatwill/files/tree/main/ebook/Matrix%20decomposition%20.pdf)

### é™„åŠ çŸ¥è¯†

#### åŸºå˜æ¢
æˆ‘ä»¬å¸¸è¯´çš„å‘é‡(3,2)å…¶å®éšå¼å¼•å…¥äº†ä¸€ä¸ªå®šä¹‰ï¼šä»¥ x è½´å’Œ y è½´ä¸Šæ­£æ–¹å‘é•¿åº¦ä¸º 1 çš„å‘é‡ä¸ºæ ‡å‡†ã€‚å‘é‡ (3,2) å®é™…æ˜¯è¯´åœ¨ x è½´æŠ•å½±ä¸º 3 è€Œ y è½´çš„æŠ•å½±ä¸º 2ã€‚æ³¨æ„æŠ•å½±æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œæ‰€ä»¥å¯ä»¥ä¸ºè´Ÿã€‚
è€Œx è½´å’Œ y è½´çš„æ–¹å‘çš„å•ä½å‘é‡å°±æ˜¯(1,0)å’Œ(0,1)ï¼Œæ‰€ä»¥(1,0)å’Œ(0,1)å°±æ˜¯åæ ‡ç³»çš„åŸº

å¦‚ï¼šå¦ä¸€ç»„åŸº(å•ä½å‘é‡)$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$å’Œ$(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$
é‚£ä¹ˆ(3,2)åœ¨è¯¥åæ ‡ç³»ä¸­å¦‚ä½•è¡¨ç¤ºå‘¢ï¼Ÿæˆ‘ä»¬çŸ¥é“ä¸€ä¸ªå‘é‡$a$åœ¨ä¸€ä¸ªæ–¹å‘ï¼ˆ$å•ä½å‘é‡x$ï¼‰ä¸Šçš„æŠ•å½±å¯ä»¥ç”¨å†…ç§¯è¡¨ç¤º$\braket{a,x} = \|a\|.\|x\|\cos \theta = \|a\|\cos \theta$ï¼Œå…¶ä¸­$\theta$è¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„å¤¹è§’ã€‚

$a=(3,2)$åœ¨$x=(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$è¿™ä¸ªæ–¹å‘çš„æŠ•å½±ä¸º$\braket{a,x} = \frac{5}{\sqrt{2}}$
$a=(3,2)$åœ¨$y=(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$è¿™ä¸ªæ–¹å‘çš„æŠ•å½±ä¸º$\braket{a,y} = -\frac{1}{\sqrt{2}}$

æ‰€ä»¥æ–°åæ ‡ä¸º$(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}})$

æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨çŸ©é˜µæ¥è¡¨ç¤º(x,yæ˜¯è¡Œå‘é‡;aç”¨åˆ—å‘é‡)
$$\begin{bmatrix} x \\  y\end{bmatrix}\begin{bmatrix} 3 \\ 2\end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}} \\  -\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\end{bmatrix}\begin{bmatrix} 3 \\ 2\end{bmatrix} = \begin{bmatrix} \frac{5}{\sqrt{2}} \\  -\frac{1}{\sqrt{2}}\end{bmatrix}$$

#### åæ–¹å·®
åæ–¹å·®ï¼ˆ[Covariance](https://en.jinzhao.wiki/wiki/Covariance)ï¼‰çš„å®šä¹‰ï¼š

$${\displaystyle \operatorname {cov} (X,Y)=\operatorname {E} {{\big [}(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){\big ]}}} \\ {\displaystyle \operatorname {cov} (X,Y)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-E(X))(y_{i}-E(Y)).}$$

æ€§è´¨ï¼š
$${\displaystyle {\begin{aligned}\operatorname {cov} (X,Y)&=\operatorname {E} \left[\left(X-\operatorname {E} \left[X\right]\right)\left(Y-\operatorname {E} \left[Y\right]\right)\right]\\&=\operatorname {E} \left[XY-X\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]Y+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right],\end{aligned}}}$$


$${\displaystyle {\begin{aligned}\operatorname {cov} (X,a)&=0\\\operatorname {cov} (X,X)&=\operatorname {var} (X)\\\operatorname {cov} (X,Y)&=\operatorname {cov} (Y,X)\\\operatorname {cov} (aX,bY)&=ab\,\operatorname {cov} (X,Y)\\\operatorname {cov} (X+a,Y+b)&=\operatorname {cov} (X,Y)\\\operatorname {cov} (aX+bY,cW+dV)&=ac\,\operatorname {cov} (X,W)+ad\,\operatorname {cov} (X,V)+bc\,\operatorname {cov} (Y,W)+bd\,\operatorname {cov} (Y,V)\end{aligned}}}$$

##### åæ–¹å·®çŸ©é˜µ
åæ–¹å·®çŸ©é˜µï¼ˆ[Covariance matrix](https://en.jinzhao.wiki/wiki/Covariance_matrix)ï¼‰çš„å®šä¹‰ï¼š**å¯¹ç§°çš„æ–¹é˜µ**
$X$æ˜¯ä¸ªéšæœºå‘é‡ï¼ˆrandom vectorï¼‰ï¼Œæ¯ä¸ªå®ä½“ï¼ˆéšæœºå˜é‡ï¼‰å°±æ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œå°±æ˜¯çŸ©é˜µç”¨åˆ—å‘é‡è¡¨ç¤ºï¼›
$${\displaystyle \mathbf {X} =(X_{1},X_{2},...,X_{n})^{\mathrm {T} }}$$
$X$çš„åæ–¹å·®çŸ©é˜µç”¨${\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }}$è¡¨ç¤ºï¼ŒçŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ ${\displaystyle \operatorname {K} _{X_{i}X_{j}}=\operatorname {cov} [X_{i},X_{j}]=\operatorname {E} [(X_{i}-\operatorname {E} [X_{i}])(X_{j}-\operatorname {E} [X_{j}])]}$

$${\displaystyle \operatorname {K} _{\mathbf {X} \mathbf {X} }={\begin{bmatrix}\mathrm {E} [(X_{1}-\operatorname {E} [X_{1}])(X_{1}-\operatorname {E} [X_{1}])]&\mathrm {E} [(X_{1}-\operatorname {E} [X_{1}])(X_{2}-\operatorname {E} [X_{2}])]&\cdots &\mathrm {E} [(X_{1}-\operatorname {E} [X_{1}])(X_{n}-\operatorname {E} [X_{n}])]\\\\\mathrm {E} [(X_{2}-\operatorname {E} [X_{2}])(X_{1}-\operatorname {E} [X_{1}])]&\mathrm {E} [(X_{2}-\operatorname {E} [X_{2}])(X_{2}-\operatorname {E} [X_{2}])]&\cdots &\mathrm {E} [(X_{2}-\operatorname {E} [X_{2}])(X_{n}-\operatorname {E} [X_{n}])]\\\\\vdots &\vdots &\ddots &\vdots \\\\\mathrm {E} [(X_{n}-\operatorname {E} [X_{n}])(X_{1}-\operatorname {E} [X_{1}])]&\mathrm {E} [(X_{n}-\operatorname {E} [X_{n}])(X_{2}-\operatorname {E} [X_{2}])]&\cdots &\mathrm {E} [(X_{n}-\operatorname {E} [X_{n}])(X_{n}-\operatorname {E} [X_{n}])]\end{bmatrix}}}$$

**æ ·æœ¬çš„åæ–¹å·®**ï¼ˆæ— åï¼‰
$$cov(X,Y) = \frac{1}{n - 1}\sum_{i=1}^{n}\left ( X_{i} - \bar{X} \right )\left ( Y_{i} - \bar{Y} \right )$$

**æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µ**ï¼š
$$cov[X_{n \times p}]_{n \times n} = cov[X_1,...,X_n] = \frac{1}{n-1}{K} _{\mathbf {X} \mathbf {X} }$$
è¿™é‡Œå¾ˆå¤šåæ–¹å·®å‡½æ•°éƒ½æœ‰å‚æ•°ï¼Œå¯ä»¥è®¾ç½®åˆ°åº•æ˜¯æŒ‰è¡Œå‘é‡è¿˜æ˜¯åˆ—å‘é‡è®¡ç®—åæ–¹å·®ã€‚
ä¹Ÿæœ‰äº›åœ°æ–¹æ˜¯ç”¨$\frac{1}{n}$ï¼Œå°±æ˜¯æ— åä¸æœ‰åçš„åŒºåˆ«ã€‚

> $\text{np.cov}(X_{3 \times 2},rowvar = False)$è¾“å‡º$2 \times 2$ï¼ˆrowvar = Falseè¡¨ç¤ºä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ï¼‰;é»˜è®¤æ˜¯è¾“å‡º$3 \times 3$ï¼ˆé»˜è®¤æ˜¯è¡Œè¡¨ç¤ºä¸€ä¸ªç‰¹å¾ï¼‰
> $\text{np.cov}(x,y,z)$è¾“å‡º$3 \times 3$

##### æœŸæœ›
æœŸæœ›ï¼ˆ[Expectation](https://en.jinzhao.wiki/wiki/Expected_value)ï¼‰çš„å®šä¹‰ï¼š
$${\displaystyle \operatorname {E} [X]=\sum _{i=1}^{k}x_{i}\,p_{i}=x_{1}p_{1}+x_{2}p_{2}+\cdots +x_{k}p_{k}.} \\ {\displaystyle p_{1}+p_{2}+\cdots +p_{k}=1,}$$

æ€§è´¨ï¼š
$${\displaystyle {\begin{aligned}\operatorname {E} [X+Y]&=\operatorname {E} [X]+\operatorname {E} [Y],\\\operatorname {E} [aX]&=a\operatorname {E} [X],\end{aligned}}}$$

å¦‚æœ$X,Y$æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆ${\displaystyle \operatorname {E} [XY]=\operatorname {E} [X]\operatorname {E} [Y]}$

å¸¸æ•°çš„æœŸæœ›ç­‰äºå¸¸æ•°æœ¬èº«$E(a) = a$

$x_i$æ˜¯éšæœºå˜é‡$X$çš„ä¸€ä¸ªå®ä¾‹ï¼Œ$X$æœä»ä»€ä¹ˆåˆ†å¸ƒï¼Œ$x_i$ä¹Ÿæ˜¯æœä»ä»€ä¹ˆåˆ†å¸ƒçš„ï¼Œæ‰€ä»¥$E(x_i) = E(X),D(x_i) = D(X)$

$E(X)$ä¸ºä¸€é˜¶çŸ©
$E(X^2)$ä¸ºäºŒé˜¶çŸ©

åŸç‚¹çŸ©ï¼ˆ[raw moment](https://en.jinzhao.wiki/wiki/Moment_(mathematics))ï¼‰å’Œä¸­å¿ƒçŸ©ï¼ˆ[central moment](https://en.jinzhao.wiki/wiki/Central_moment)ï¼‰
$E(X^k)$ä¸ºké˜¶è¿œç‚¹çŸ©ï¼Œä¸€é˜¶åŸç‚¹çŸ©æ˜¯æ•°å­¦æœŸæœ›
$E(X-E(X))^k$ä¸ºké˜¶ä¸­å¿ƒçŸ©ï¼ŒäºŒé˜¶åŸç‚¹çŸ©æ˜¯æ–¹å·®ï¼ˆä»¥$E(X)$ä¸ºä¸­å¿ƒï¼‰

##### æ–¹å·®
æ–¹å·®ï¼ˆ[Variance](https://en.jinzhao.wiki/wiki/Variance)ï¼‰çš„å®šä¹‰ï¼š
$$\operatorname {Var} (X)=\operatorname {E} \left[(X-\mu )^{2}\right] \\ {\displaystyle \mu =\operatorname {E} [X]}$$

æ€§è´¨ï¼š
$D(X)$ å’Œ $Var(X)$ éƒ½æ˜¯è¡¨ç¤ºæ–¹å·®ï¼›æ–¹å·®å¤§äºç­‰äº0ï¼›å‚æ•°çš„æ–¹å·®ä¸º0ï¼›

$\operatorname {Var} (X)=\operatorname {Cov} (X,X).$
$${\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E} \left[(X-\operatorname {E} [X])^{2}\right]\\[4pt]&=\operatorname {E} \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\[4pt]&=\operatorname {E} \left[X^{2}\right]-2\operatorname {E} [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\[4pt]&=\operatorname {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}}$$

$${\displaystyle {\begin{aligned}\operatorname {Var} (X+Y)&=\operatorname {E} \left[(X+Y)^{2}\right]-(\operatorname {E} [X+Y])^{2}\\[5pt]&=\operatorname {E} \left[X^{2}+2XY+Y^{2}\right]-(\operatorname {E} [X]+\operatorname {E} [Y])^{2} \\&=\operatorname {E} \left[X^{2}\right]+2\operatorname {E} [XY]+\operatorname {E} \left[Y^{2}\right]-\left(\operatorname {E} [X]^{2}+2\operatorname {E} [X]\operatorname {E} [Y]+\operatorname {E} [Y]^{2}\right)\\[5pt]&=\operatorname {E} \left[X^{2}\right]+\operatorname {E} \left[Y^{2}\right]-\operatorname {E} [X]^{2}-\operatorname {E} [Y]^{2}\\[5pt]&=\operatorname {Var} (X)+\operatorname {Var} (Y)\end{aligned}}}$$

$\operatorname {Var} (X+a)=\operatorname {Var} (X).$
$\operatorname {Var} (aX)=a^{2}\operatorname {Var} (X).$
$\operatorname {Var} (aX+bY)=a^{2}\operatorname {Var} (X)+b^{2}\operatorname {Var} (Y)+2ab\,\operatorname {Cov} (X,Y),$
$\operatorname {Var} (aX-bY)=a^{2}\operatorname {Var} (X)+b^{2}\operatorname {Var} (Y)-2ab\,\operatorname {Cov} (X,Y),$

${\displaystyle \operatorname {Var} (XY)=\operatorname {E} \left(X^{2}\right)\operatorname {E} \left(Y^{2}\right)-[\operatorname {E} (X)]^{2}[\operatorname {E} (Y)]^{2}.}$

#### æœ‰åä¼°è®¡å’Œæ— åä¼°è®¡

**å‚æ•°ä¼°è®¡**éœ€è¦æœªçŸ¥å‚æ•°çš„ä¼°è®¡é‡å’Œä¸€å®šç½®ä¿¡åº¦
ä¼°è®¡æ–¹æ³•ï¼šç”¨ç‚¹ä¼°è®¡ä¼°è®¡ä¸€ä¸ªå€¼ï¼›ç”¨åŒºé—´ä¼°è®¡ä¼°è®¡å€¼çš„å¯èƒ½åŒºé—´å’Œæ˜¯è¯¥å€¼çš„å¯èƒ½æ€§ã€‚
ä¼°è®¡çš„**åå·®**çš„å®šä¹‰ï¼š
$$bias(\hat{\theta}_m) = E(\hat{\theta}_m)-\theta$$
$\theta$æ˜¯æ•°æ®åˆ†å¸ƒçœŸå®å‚æ•°ï¼ˆå®Œç¾ï¼‰
$\theta$çš„ä¼°è®¡é‡æˆ–ç»Ÿè®¡é‡$\hat{\theta}_m$

å¯¹ä¼°è®¡å€¼çš„è¯„ä»·æ ‡å‡†ï¼š
- æ— åæ€§ï¼ˆUnbiasednessï¼‰ï¼šæ˜¯ä¼°è®¡é‡ï¼ˆä¸ä¸€å®šæ˜¯æ ·æœ¬å‡å€¼ï¼‰æŠ½æ ·åˆ†å¸ƒçš„æ•°å­¦æœŸæœ›ç­‰ä¸æ€»ä½“å‚æ•°çš„çœŸå€¼ã€‚
$m$ä¸ºæ ·æœ¬æ•°é‡ï¼ˆæŠ½æ ·æ•°é‡ï¼‰
å¦‚æœ$bias(\hat{\theta}_m) = 0$é‚£ä¹ˆä¼°è®¡é‡$\hat{\theta}_m$è¢«ç§°ä¸º**æ— å**(unbiased)çš„,æ„å‘³ç€$E(\hat{\theta}_m) = \theta$
å¦‚æœ$\lim_{m \to \infty}bias(\hat{\theta}_m) = 0$é‚£ä¹ˆä¼°è®¡é‡$\hat{\theta}_m$è¢«ç§°ä¸º **æ¸è¿‘æ— å** (asymptotically unbiased) çš„,æ„å‘³ç€ $\lim_{m \to \infty}E(\hat{\theta}_m) = \theta$

- æœ‰æ•ˆæ€§ï¼ˆEfficiencyï¼‰ï¼šæ˜¯æœ‰æ—¶å‡ ç»„æ•°æ®éƒ½æ˜¯æ— åçš„ï¼Œä½†æ˜¯æ­¤æ—¶æœ‰æ•ˆæ•°æ˜¯æ–¹å·®æœ€å°çš„ã€‚
å¦‚ï¼šæ ·æœ¬$(x_1,...,x_m)$ï¼Œå…¶å‡å€¼ä¸º$\mu$ï¼Œæ–¹å·®ä¸º$\sigma^2$
ç¬¬ä¸€ç§æƒ…å†µï¼šéšæœºå–ä¸€ä¸ªæ ·æœ¬$x_i$ï¼Œé‚£ä¹ˆ$E(x_i)=E(x_1)=...=\mu$ï¼Œæ–¹å·®ä¸º$D(x_i)=D(x_1)=...=\sigma^2$ï¼ˆå› ä¸ºæ¯ä¸ªå€¼éƒ½æœ‰å¯èƒ½å–åˆ°ï¼Œæ‰€ä»¥éšæœºå–ä¸€ä¸ªæ ·æœ¬å¾—æœŸæœ›å°±æ˜¯å‡å€¼ï¼Œæ–¹å·®å°±æ˜¯$\sigma^2$ï¼‰
ç¬¬äºŒç§æƒ…å†µï¼šå–å¹³å‡å€¼$\bar{x}$,é‚£ä¹ˆ$E(\bar{x}) = E[\frac{1}{m}\sum_m x_i] =\frac{1}{m}E[\sum_mx_i] = \frac{1}{m}[\sum_mE(x_i)] = \frac{1}{m}[\sum_m \mu] = \mu$;
$D(\bar{x}) = D(\frac{1}{m}\sum_m x_i) = \frac{1}{m^2}D(\sum_m x_i) = \frac{1}{m^2}\sum_m D(x_i) = \frac{1}{m^2} m\sigma^2 = \frac{\sigma^2}{m}$
å¾ˆæ˜æ˜¾ç¬¬äºŒç§çš„æ–¹å·®å°

- ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰ï¼šæ˜¯æŒ‡æ ·æœ¬å˜å¤§ï¼Œä¼°è®¡è¶Šå‡†ã€‚
$\lim_{m \to \infty}P(|\hat{\theta}_m - \theta | \lt \epsilon) = 1$


**æ— åä¼°è®¡**
ä¾‹å¦‚æ ·æœ¬å‡å€¼çš„ä¼°è®¡ä¸º$\hat{\mu} = \sum_{i=1}^m x_i$ï¼ŒçœŸå®çš„å‡å€¼ä¸º$\mu$ï¼Œå¦‚ä½•çŸ¥é“è¿™ä¸ªä¼°è®¡æ˜¯æœ‰åè¿˜æ˜¯æ— åï¼Ÿæ ¹æ®å®šä¹‰åˆ¤æ–­åå·®æ˜¯å¦ä¸º0ï¼›$bias(\hat{\mu}) = E(\hat{\mu}) - \mu$
è¿™é‡Œçš„è¯æ˜å†æœ‰æ•ˆæ€§å·²ç»è¯æ˜è¿‡äº†ã€‚

**æœ‰åä¼°è®¡**
å¦‚æœæ ·æœ¬æ–¹å·®çš„ä¼°è®¡ä¸º$\hat{\sigma}^2 = \frac{1}{m}\sum_{i=1}^m(x_i-\hat{\mu})^2$,$\hat{\mu} = \sum_{i=1}^m x_i$ï¼Œé‚£ä¹ˆåå·®$bias(\hat{\sigma}^2) = E[\hat{\sigma}^2] - \sigma^2$ä¸ä¸º0ï¼Œå°±è¯æ˜è¿™ä¸ªä¼°è®¡æ˜¯æœ‰åçš„ã€‚
æˆ‘ä»¬æ¥æ±‚$E[\hat{\sigma}^2]$
$$ E[\hat{\sigma}^2] = E[\frac{1}{m}\sum_{i=1}^m(x_i-\hat{\mu})^2] \\= E[\frac{1}{m}\sum_{i=1}^m(x_i^2- 2\hat{\mu}x_i +\hat{\mu}^2)] \\= E[\frac{1}{m}\sum_{i=1}^mx_i^2-\frac{1}{m}\sum_{i=1}^m 2\hat{\mu}x_i +\frac{1}{m}\sum_{i=1}^m\hat{\mu}^2]\\= E[\frac{1}{m}\sum_{i=1}^mx_i^2-2\hat{\mu}\frac{1}{m}\sum_{i=1}^m x_i +\frac{1}{m}\sum_{i=1}^m\hat{\mu}^2]\\= E[\frac{1}{m}\sum_{i=1}^mx_i^2-2\hat{\mu}\hat{\mu} +\hat{\mu}^2]= E[\frac{1}{m}\sum_{i=1}^mx_i^2-\hat{\mu}^2] \\= E[\frac{1}{m}\sum_{i=1}^mx_i^2]-E[\hat{\mu}^2] = \frac{1}{m}E[\sum_{i=1}^mx_i^2]-E[\hat{\mu}^2]$$

$E[\hat{\mu}^2] = D(\hat{\mu}) + E(\hat{\mu})^2$
è€Œ$D(\hat{\mu}) = \frac{1}{m} \sigma^2 å’Œ E(\hat{\mu}) = \mu$åœ¨ä¸Šé¢çš„æœ‰æ•ˆæ€§ä¸­å·²ç»è¯æ˜äº†
æ‰€ä»¥$E[\hat{\mu}^2] = \frac{1}{m} \sigma^2 + \mu^2$

$E[{x_i}^2] = D(x_i) + E(x_i)^2$
è€Œ$D(x_i) = \sigma^2 å’Œ E(x_i) = \mu$åœ¨ä¸Šé¢çš„æœ‰æ•ˆæ€§ä¸­å·²ç»è¯æ˜äº†
æ‰€ä»¥$ \frac{1}{m}E[\sum_{i=1}^mx_i^2] = \frac{1}{m}\sum_{i=1}^m E[x_i^2]= \sigma^2 + \mu^2$

æ‰€ä»¥$E[\hat{\sigma}^2] = \sigma^2 + \mu^2 - (\frac{1}{m} \sigma^2 + \mu^2) =\frac{m-1}{m}\sigma^2$
æ‰€ä»¥ä¼°è®¡æ˜¯æœ‰åä¼°è®¡ã€‚

æ‰€ä»¥æ–¹å·®çš„æ— åä¼°è®¡ä¸º$\tilde{\sigma}^2 = \frac{1}{m-1}\sum_{i=1}^m(x_i-\hat{\mu})^2$

å½“$\lim_{m \to \infty}\frac{1}{m}\sum_{i=1}^m(x_i-\hat{\mu})^2 = \frac{1}{m-1}\sum_{i=1}^m(x_i-\hat{\mu})^2$ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰åä¼°è®¡æ˜¯ä¸€ä¸ªæ¸è¿‘æ— åä¼°è®¡ã€‚

æ— åä¼°è®¡ä¸ä¸€å®šæ˜¯æœ€å¥½çš„ä¼°è®¡ï¼

#### å› å­åˆ†æFA

å› å­åˆ†æï¼ˆ[Factor analysis, FA](https://en.jinzhao.wiki/wiki/Factor_analysis)ï¼‰

æ¯ä¸ªå˜é‡éƒ½å¯ä»¥è¡¨ç¤ºæˆå…¬å…±å› å­çš„çº¿æ€§å‡½æ•°ä¸ç‰¹æ®Šå› å­ä¹‹å’Œ
$$X_i = a_{i1}F_1 + a_{i2}F_2 +...++ a_{im}F_m  + \epsilon_i  ,(i=1,2,...,p) $$
å¼ä¸­çš„F1ï¼ŒF2ï¼Œâ€¦ï¼ŒFmç§°ä¸ºå…¬å…±å› å­ï¼ŒÎµiç§°ä¸ºXiçš„ç‰¹æ®Šå› å­ã€‚è¯¥æ¨¡å‹å¯ç”¨çŸ©é˜µè¡¨ç¤ºä¸ºï¼š`X = AF+Îµ`
X è¡¨ç¤ºåŸå§‹æ•°æ®ï¼ŒçŸ©é˜µAç§°ä¸ºå› å­è½½è·çŸ©é˜µ,Fè¡¨ç¤ºå…¬å…±å› å­ï¼Œ Îµæ˜¯ç‰¹æ®Šå› å­
aijç§°ä¸ºå› å­â€œè½½è·â€ï¼Œæ˜¯ç¬¬iä¸ªå˜é‡åœ¨ç¬¬jä¸ªå› å­ä¸Šçš„è´Ÿè·ï¼Œå¦‚æœæŠŠå˜é‡Xiçœ‹æˆmç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œåˆ™aijè¡¨ç¤ºå®ƒåœ¨åæ ‡è½´Fjä¸Šçš„æŠ•å½±ã€‚
$X = [X_1,X_2...X_p]^T$
$A= \begin{bmatrix}
a_{11} & a_{12} & ... & a_{1m} \\\\
a_{21} & a_{22} & ... & a_{2m} \\\\
... & ... & ... & ... \\\\
a_{p1} & a_{p2} & ... & a_{pm} \\\\
\end{bmatrix}$
$F = [F_1,F_2...F_m]^T$
$\epsilon = [\epsilon_1,\epsilon_2...\epsilon_p]^T$

> ä¸»æˆåˆ†åˆ†æï¼Œæ˜¯åˆ†æç»´åº¦å±æ€§çš„ä¸»è¦æˆåˆ†è¡¨ç¤ºã€‚
> å› å­åˆ†æï¼Œæ˜¯åˆ†æå±æ€§ä»¬çš„å…¬å…±éƒ¨åˆ†çš„è¡¨ç¤ºã€‚
> äºŒè€…å‡åº”ç”¨äºé«˜æ–¯åˆ†å¸ƒçš„æ•°æ®ï¼Œéé«˜æ–¯åˆ†å¸ƒçš„æ•°æ®é‡‡ç”¨ç‹¬ç«‹æˆåˆ†åˆ†æICAç®—æ³•

#### ç‹¬ç«‹æˆåˆ†åˆ†æICA
ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆ[Independent component analysis, ICA](https://en.jinzhao.wiki/wiki/Independent_component_analysis)ï¼‰

`X=AS`
`Y=WX=WAS  ï¼Œ A = inv(W)`
ICA(Independent Component Correlation Algorithm)æ˜¯ä¸€ç§å‡½æ•°ï¼ŒXä¸ºnç»´è§‚æµ‹ä¿¡å·çŸ¢é‡ï¼ŒSä¸ºç‹¬ç«‹çš„mï¼ˆm<=n)ç»´æœªçŸ¥æºä¿¡å·çŸ¢é‡ï¼ŒçŸ©é˜µAè¢«ç§°ä¸ºæ··åˆçŸ©é˜µã€‚
ICAçš„ç›®çš„å°±æ˜¯å¯»æ‰¾è§£æ··çŸ©é˜µWï¼ˆAçš„é€†çŸ©é˜µï¼‰ï¼Œç„¶åå¯¹Xè¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¾—åˆ°è¾“å‡ºå‘é‡Yã€‚

ICAæ˜¯æ‰¾å‡ºæ„æˆä¿¡å·çš„ç›¸äº’ç‹¬ç«‹éƒ¨åˆ†(ä¸éœ€è¦æ­£äº¤)ï¼Œå¯¹åº”é«˜é˜¶ç»Ÿè®¡é‡åˆ†æã€‚ICAç†è®ºè®¤ä¸ºç”¨æ¥è§‚æµ‹çš„æ··åˆæ•°æ®é˜µXæ˜¯ç”±ç‹¬ç«‹å…ƒSç»è¿‡Açº¿æ€§åŠ æƒè·å¾—ã€‚
ICAç†è®ºçš„ç›®æ ‡å°±æ˜¯é€šè¿‡Xæ±‚å¾—ä¸€ä¸ªåˆ†ç¦»çŸ©é˜µWï¼Œä½¿å¾—Wä½œç”¨åœ¨Xä¸Šæ‰€è·å¾—çš„ä¿¡å·Yæ˜¯ç‹¬ç«‹æºSçš„æœ€ä¼˜é€¼è¿‘ï¼Œ


[ç‹¬ç«‹æˆåˆ†åˆ†æ (ICA) åº”ç”¨å‚è€ƒ(Originæ¥åšICAåˆ†æ)](https://www.bilibili.com/video/BV1w54y1G7bw)

[ç‹¬ç«‹æˆåˆ†åˆ†æ - è®²è§£çš„åŸç†](https://www.bilibili.com/video/BV1mQ4y1M7wB)

[Independent Component Analysis (ICA)](http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_ICA09.pdf)

### å‚è€ƒæ–‡çŒ®
[16-1] æ–¹å¼€æ³°. å®ç”¨å¤šå…ƒç»Ÿè®¡åˆ†æ. ä¸Šæµ·ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å‡ºç‰ˆç¤¾, 1989.
[16-2] å¤ç»ç®ï¼Œæ¨å®¶æœ¬ï¼Œæ¨æŒ¯æ–Œ. ç³»ç»Ÿå·¥ç¨‹æ¦‚è®º. åŒ—äº¬ï¼šæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ1995.
[16-3] Jolliffe I. Principal component analysis, Sencond Edition. John Wiley & Sons, 2002.
[16-4] Shlens J. A tutorial on principal component analysis. arXiv preprint arXiv: 14016.1100, 2014.
[16-5] SchÃ¶lkopf B, Smola A, MÃ¼ller K-R. Kernel principal component analysis. Artificial Neural Networks-ICANN'97. Springer, 1997:583-588.
[16-6] Hardoon D R, Szedmak S, Shawe-Taylor J. Canonical correlation analysis: an overview with application to learning methods. Neural Computation, 2004, 16(12):2639-2664.
[16-7] Candes E J, Li X D, Ma Y, et al. Robust Principal component analysis? Journal of the ACM(JACM), 2011, 58(3):11.



## ç¬¬ 17 ç«  æ½œåœ¨è¯­ä¹‰åˆ†æ
æˆ‘ä»¬å…ˆä»‹ç»ä¸‹æ–‡æœ¬ä¿¡æ¯å¤„ç†ä¸­çš„ä¸€äº›é—®é¢˜ï¼š
1. ä¸€è¯å¤šä¹‰ï¼ˆå¤šä¹‰ç°è±¡ï¼‰[polysemy](https://en.jinzhao.wiki/wiki/Polysemy)
åˆ†ç±»æ—¶ï¼šæ¯”å¦‚bank è¿™ä¸ªå•è¯å¦‚æœå’Œmortgage, loans, rates è¿™äº›å•è¯åŒæ—¶å‡ºç°æ—¶ï¼Œbank å¾ˆå¯èƒ½è¡¨ç¤ºé‡‘èæœºæ„çš„æ„æ€ã€‚å¯æ˜¯å¦‚æœbank è¿™ä¸ªå•è¯å’Œlures, casting, fishä¸€èµ·å‡ºç°ï¼Œé‚£ä¹ˆå¾ˆå¯èƒ½è¡¨ç¤ºæ²³å²¸çš„æ„æ€ã€‚

1. ä¸€ä¹‰å¤šè¯ï¼ˆåŒä¹‰ç°è±¡ï¼‰[synonymy](https://en.jinzhao.wiki/wiki/Synonym)
æ£€ç´¢æ—¶ï¼šæ¯”å¦‚ç”¨æˆ·æœç´¢â€œautomobileâ€ï¼Œå³æ±½è½¦ï¼Œä¼ ç»Ÿå‘é‡ç©ºé—´æ¨¡å‹ä»…ä»…ä¼šè¿”å›åŒ…å«â€œautomobileâ€å•è¯çš„é¡µé¢ï¼Œè€Œå®é™…ä¸ŠåŒ…å«â€œcarâ€å•è¯çš„é¡µé¢ä¹Ÿå¯èƒ½æ˜¯ç”¨æˆ·æ‰€éœ€è¦çš„ã€‚

> LSAèƒ½è§£å†³åŒä¹‰ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰é—®é¢˜ï¼šå‘ç°å•è¯ä¸ä¸»é¢˜ä¹‹é—´çš„å…³ç³»ï¼Œè¿™é‡Œä¸»é¢˜æ˜¯æ±½è½¦ï¼›ä¹Ÿèƒ½è§£å†³ä¸€å®šç¨‹åº¦çš„å¤šä¹‰é—®é¢˜ï¼ŒåŒä¸€ä¸ªå•è¯åœ¨ä¸åŒæ–‡æ¡£ä¸­è¡¨ç¤ºä¸åŒè¯é¢˜

æ½œåœ¨è¯­ä¹‰åˆ†æï¼ˆ[Latent semantic analysis, LSA](https://en.jinzhao.wiki/wiki/Latent_semantic_analysis)ï¼‰æ—¨åœ¨ è§£å†³è¿™ç§æ–¹æ³•ä¸èƒ½å‡†ç¡®è¡¨ç¤ºè¯­ä¹‰çš„é—®é¢˜ï¼Œè¯•å›¾ä»å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸­å‘ç°æ½œåœ¨çš„è¯é¢˜ï¼Œä»¥è¯é¢˜å‘é‡è¡¨ç¤ºæ–‡æœ¬çš„è¯­ä¹‰å†…å®¹ï¼Œä»¥è¯é¢˜å‘é‡ç©ºé—´çš„åº¦é‡æ›´å‡†ç¡®åœ°è¡¨ç¤ºæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

æ–‡æœ¬docé›†åˆ$D = \{d_1,d_2,...,d_n\}$
æ–‡æœ¬é›†åˆä¸­å‡ºç°çš„å•è¯wordé›†åˆ$W = \{w_1,w_2,...,w_m\}$
å•è¯-æ–‡æœ¬çŸ©é˜µ(word-document matrix)
$$X = \begin{bmatrix}
   x_{11} & x_{12} & \cdots & x_{1n} \\
   x_{21} & x_{22} & \cdots & x_{2n} \\
   \vdots & \vdots &  & \vdots \\
   x_{m1} & x_{m2} & \cdots & x_{mn} \\
\end{bmatrix}$$
æ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªæ–‡æœ¬;$x_{ij}$è¡¨ç¤ºå•è¯$w_i$åœ¨æ–‡æœ¬$d_j$ä¸­å‡ºç°çš„é¢‘æ•°æˆ–æƒå€¼ã€‚
æ¯ä¸ªæ–‡æœ¬ä¸­ä¸å¯èƒ½å‡ºç°æ‰€æœ‰å•è¯ï¼Œæ‰€ä»¥è¯¥çŸ©é˜µæ˜¯ç¨€ç–çŸ©é˜µã€‚

æƒå€¼é€šå¸¸ç”¨**å•è¯é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡**ï¼ˆ[term frequencyâ€“inverse document frequencyï¼ŒTFIDF](https://en.jinzhao.wiki/wiki/Tf%E2%80%93idf)ï¼‰è¡¨ç¤ºï¼Œå®šä¹‰ä¸ºï¼š
$${\displaystyle \text {tf-idf} (t,d,D)=\mathrm {tf} (t,d)\cdot \mathrm {idf} (t,D)}$$
tä¸ºæŸä¸€ä¸ªå•è¯ï¼ˆtermï¼Œwordï¼‰ï¼›dä¸ºæŸä¸€ä¸ªæ–‡æ¡£ï¼ˆdocumentï¼‰ï¼›$x_{ij} = \text {tf-idf} (w_i,d_j,D)$;Dè¡¨ç¤ºæ–‡æ¡£é›†åˆï¼Œ$N = |D|$è¡¨ç¤ºæ–‡æ¡£æ€»æ•°ï¼›
$${\displaystyle \mathrm {tf} (t,d)={\frac {f_{t,d}}{\sum _{t'\in d}{f_{t',d}}}} = \frac{tåœ¨dä¸­å‡ºç°çš„é¢‘æ•°}{dä¸­å‡ºç°çš„æ‰€æœ‰å•è¯çš„é¢‘æ•°å’Œ}}$$
$$ \mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}| } = \log \frac{æ–‡æ¡£æ€»æ•°}{å«æœ‰å•è¯tçš„æ–‡æœ¬æ€»æ•°}$$

ç›´è§‚ä¸Šç†è§£ï¼š
ä¸€ä¸ªå•è¯åœ¨ä¸€ä¸ªæ–‡æœ¬ä¸­å‡ºç°çš„é¢‘æ•°è¶Šé«˜ï¼Œè¿™ä¸ªå•è¯åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­çš„é‡è¦åº¦ï¼ˆTFï¼‰å°±è¶Šé«˜;
ä¸€ä¸ªå•è¯åœ¨æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­å‡ºç°çš„æ–‡æ¡£æ•°è¶Šå°‘ï¼Œè¿™ä¸ªå•è¯å°±è¶Šèƒ½è¡¨ç¤ºå…¶æ‰€åœ¨æ–‡æ¡£çš„ç‰¹ç‚¹ï¼Œé‡è¦åº¦ï¼ˆTDFï¼‰å°±è¶Šé«˜ï¼›
ä¸¤ç§é‡è¦åº¦çš„ç§¯ï¼Œè¡¨ç¤ºç»¼åˆé‡è¦åº¦ã€‚
æ„æ€å°±æ˜¯é‡è¦çš„å•è¯åœ¨ä¸€ä¸ªæ–‡æœ¬ä¸­å‡ºç°çš„è¶Šå¤šè¶Šé‡è¦ï¼Œåœ¨è¶Šå°‘çš„æ–‡æœ¬ä¸­å‡ºç°è¶Šé‡è¦ï¼›å¦‚ï¼šçš„ï¼Œå¯èƒ½åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­å‡ºç°éƒ½å¾ˆå¤šï¼ˆTFå¤§ï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªæ–‡æ¡£éƒ½æœ‰å‡ºç°è¿‡ï¼ˆTDFå°ï¼‰ï¼Œæ‰€ä»¥åè€Œä¸é‡è¦äº†ã€‚

**ç›¸ä¼¼åº¦(ä½™å¼¦ç›¸ä¼¼åº¦)** ï¼ˆ[Cosine similarity](https://en.jinzhao.wiki/wiki/Cosine_similarity)ï¼‰å¯ä»¥è¡¨ç¤ºä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ã€‚
æˆ‘ä»¬çŸ¥é“è®­ç»ƒç‚¹ç§¯${\displaystyle \mathbf {A} \cdot \mathbf {B} =\left\|\mathbf {A} \right\|\left\|\mathbf {B} \right\|\cos \theta }$,è€Œç›¸ä¼¼åº¦å°±æ˜¯å‘é‡ä¹‹é—´çš„å¤¹è§’${\displaystyle {\text{similarity}}=\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},}$

æ–‡æ¡£ç”¨å‘é‡è¡¨ç¤ºï¼š$d_i = x_{.i} = \begin{bmatrix}  x_{1i} \\  x_{2i} \\ \vdots \\  x_{mi} \end{bmatrix}$
é‚£ä¹ˆ$d_i,d_j$ä¹‹é—´çš„ç›¸ä¼¼åº¦ä¸º
$$\text{similarity} = \frac{x_{.i}\cdot x_{.j}}{\|x_{.i}\|\|x_{.j}\|}$$
è¿™é‡Œæ¯”è¾ƒç›¸ä¼¼åº¦ç”¨åœ¨äº†**å•è¯å‘é‡ç©ºé—´**(word vector space model)ä¸­ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å°±æ˜¯å¤šä¹‰å’ŒåŒä¹‰ç°è±¡ï¼Œè¿™æ—¶æˆ‘ä»¬å°±å¯ä»¥è€ƒè™‘**è¯é¢˜å‘é‡ç©ºé—´**(topic vector space model):
å‡è®¾ç”¨ä¸€ä¸ªå‘é‡è¡¨ç¤ºæ–‡æ¡£ï¼Œè¯¥å‘é‡çš„æ¯ä¸€ä¸ªåˆ†é‡è¡¨ç¤ºä¸€ä¸ªè¯é¢˜ï¼Œå…¶æ•°å€¼ä¸ºè¯¥è¯é¢˜åœ¨è¯¥æ–‡æœ¬ä¸­çš„æƒå€¼ï¼Œç„¶åæ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£ç›¸ä¼¼åº¦ï¼ˆä¸€èˆ¬è¯é¢˜æ•°è¿œå°äºå•è¯æ•°ï¼‰ã€‚ 
æ½œåœ¨è¯­ä¹‰åˆ†æå°±æ˜¯æ„å»ºè¿™æ ·ä¸€ä¸ªè¯é¢˜å‘é‡ç©ºé—´çš„æ–¹æ³•ã€‚

å•è¯-æ–‡æœ¬çŸ©é˜µ
$$X = \begin{bmatrix}
   x_{11} & x_{12} & \cdots & x_{1n} \\
   x_{21} & x_{22} & \cdots & x_{2n} \\
   \vdots & \vdots &  & \vdots \\
   x_{m1} & x_{m2} & \cdots & x_{mn} \\
\end{bmatrix}$$
å‡è®¾æ‰€æœ‰æ–‡æ¡£å…±å«æœ‰kä¸ªè¯é¢˜ï¼Œè¯é¢˜å‘é‡ç©ºé—´$T$(å•è¯-è¯é¢˜çŸ©é˜µ)
$$T = \begin{bmatrix}
   t_{11} & t_{12} & \cdots & t_{1k} \\
   t_{21} & t_{22} & \cdots & t_{2k} \\
   \vdots & \vdots &  & \vdots \\
   t_{m1} & t_{m2} & \cdots & t_{mk} \\
\end{bmatrix}$$


é‚£ä¹ˆæ–‡æ¡£åœ¨è¯é¢˜å‘é‡ç©ºé—´çš„è¡¨ç¤ºï¼ˆè¯é¢˜-æ–‡æœ¬çŸ©é˜µï¼‰
$$Y = \begin{bmatrix}
   y_{11} & y_{12} & \cdots & y_{1n} \\
   y_{21} & y_{22} & \cdots & y_{2n} \\
   \vdots & \vdots &  & \vdots \\
   y_{k1} & y_{k2} & \cdots & y_{kn} \\
\end{bmatrix}$$

- **æ¨¡å‹**ï¼š
$$X_{m \times n} \approx T_{m \times k}Y_{k \times n}$$
å…¶ä¸­$X_{m \times n}$æ˜¯å•è¯-æ–‡æœ¬çŸ©é˜µï¼ˆå°±æ˜¯å•è¯å‘é‡ç©ºé—´ï¼‰ï¼›$T_{m \times k}$æ˜¯å•è¯-è¯é¢˜çŸ©é˜µï¼ˆå°±æ˜¯è¯é¢˜å‘é‡ç©ºé—´ï¼‰ï¼›$Y_{k \times n}$æ˜¯è¯é¢˜-æ–‡æœ¬çŸ©é˜µï¼Œ**å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„è¾“å‡º**ï¼›
må•è¯æ€»æ•°ï¼Œnæ˜¯æ–‡æ¡£æ€»æ•°ï¼Œkæ˜¯è¯é¢˜æ€»æ•°ï¼›
- **ç­–ç•¥**ï¼š
$$minimize \|X - TY\|^2$$
å¯ä»¥çœ‹åˆ°è·ŸNMFéå¸¸åƒï¼Œä¹Ÿå¯ä»¥ç”¨TruncatedSVD
- **ç®—æ³•**ï¼š
TruncatedSVD:
$$X_{m \times n} \approx X_{rank(k)} = U_{m \times k}\Sigma_{k \times k} V^T_{n \times k}$$
é‚£ä¹ˆè¯é¢˜ç©ºé—´$T=U_k$ä»¥åŠæœ¬æ–‡åœ¨è¯é¢˜ç©ºé—´çš„è¡¨ç¤º$Y = \Sigma_kV_k^T$
NMF:
$$X \approx WH$$
é‚£ä¹ˆè¯é¢˜ç©ºé—´$T=W$ä»¥åŠæœ¬æ–‡åœ¨è¯é¢˜ç©ºé—´çš„è¡¨ç¤º$Y = H$

### å‚è€ƒæ–‡çŒ®
[17-1] Deerwester S C, Dumais S T, Landauer T K, et al. Indexing by latent semantic analysis. Journal of the Association for Information Science and Technology ,1990, 41: 391-407.
[17-2] Landauer T K. Latent semantic analysis. In: Encyclopedia of Cognitive Science, Wiley, 2006.
[17-3] Lee D D, Seung H S. [Learning the parts of objects by non-negative matrix factorization](http://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf). Nature, 1999, 401(6755):788-791.
[17-4] Lee D D, Seung H S. Algorithms for non-negative matrix factorization. Advances in Neural Information Processing Systems, 2001: 556-562.
[17-5] Xu W, Liu X, Gong Y. Document clustering based on non-negative matrix factorization. Proceedings of the 26th Annual International ACM [SIGIR](https://en.jinzhao.wiki/wiki/Special_Interest_Group_on_Information_Retrieval) Conference in Research and Development in [Information Retrieval](https://en.jinzhao.wiki/wiki/Information_retrieval), 2003.
[17-6] Wang Q, Xu J, Li H, et al. Regularized latent semantic indexing. Proceedings of the 34th International ACM [SIGIR](https://en.jinzhao.wiki/wiki/Special_Interest_Group_on_Information_Retrieval) Conference in Research and Development in [Information Retrieval](https://en.jinzhao.wiki/wiki/Information_retrieval), 2011.


## ç¬¬ 18 ç«  æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æ
> ç”Ÿæˆæ¨¡å‹ï¼Œç”¨éšå˜é‡è¡¨ç¤ºè¯é¢˜

æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æï¼ˆ[Probabilistic latent semantic analysis, PLSA](https://en.jinzhao.wiki/wiki/Probabilistic_latent_semantic_analysis)ï¼‰

æ¦‚ç‡æœ‰å‘å›¾æ¨¡å‹ï¼š
![](./img/2020050116564379.png)
é˜´å½±åœ†è¡¨ç¤ºè§‚æµ‹å˜é‡ï¼Œç©ºå¿ƒåœ†è¡¨ç¤ºéšå˜é‡ï¼›ç®­å¤´è¡¨ç¤ºæ¦‚ç‡å…³ç³»ï¼›æ–¹æ¡†è¡¨ç¤ºå¤šæ¬¡é‡å¤ï¼Œæ–¹æ¡†å†…çš„å­—æ¯è¡¨ç¤ºé‡å¤æ¬¡æ•°ï¼›
æ–‡æ¡£dæ˜¯ä¸€ä¸ªè§‚æµ‹å˜é‡ï¼›è¯é¢˜å˜é‡zæ˜¯éšå˜é‡(è¯é¢˜çš„ä¸ªæ•°æ˜¯è¶…å‚æ•°)ï¼›å•è¯å˜é‡wæ˜¯ä¸€ä¸ªè§‚æµ‹å˜é‡ï¼›

- **æ¨¡å‹**ï¼š
å•è¯é›†åˆ$W = \{w_1,...,w_M\}$ï¼›
æ–‡æœ¬é›†åˆ$D=\{d_1,...,d_N\}$ï¼›
è¯é¢˜é›†åˆ$Z=\{z_1,...,z_K\}$ï¼ŒKæ˜¯è¶…å‚æ•°ï¼›
$$P(w,d) =P(d)P(w|d) = P(d)\sum_{z} P(w,z|d)=P(d)\sum _{z}P(z|d)P(w|z)$$
$P(w,z|d) = P(z|d)P(w|z,d) = P(z|d)P(w|z)$å³ç»™å®šzçš„æƒ…å†µä¸‹wå’Œdç›¸äº’ç‹¬ç«‹$w \perp d |z$
$P(w,d)$æ˜¯â€œæ¯ä¸ªå•è¯-æ–‡æœ¬å¯¹(w,d)â€çš„ç”Ÿæˆæ¦‚ç‡
P(d)è¡¨ç¤ºç”Ÿæˆæ–‡æœ¬dçš„æ¦‚ç‡,
æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(z|d)è¡¨ç¤ºæ–‡æœ¬dç”Ÿæˆè¯é¢˜zçš„æ¦‚ç‡,
æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(w|z)è¡¨ç¤ºè¯é¢˜zç”Ÿæˆå•è¯w çš„æ¦‚ç‡ã€‚
ç”Ÿæˆï¼ˆæˆæ–‡æœ¬-å•è¯å…±ç°æ•°æ®ï¼‰æ­¥éª¤ï¼š
1. ä¾æ®æ¦‚ç‡åˆ†å¸ƒP(d)ï¼Œä»æ–‡æœ¬é›†åˆä¸­éšæœºé€‰å–ä¸€ä¸ªæ–‡æœ¬dï¼Œå…±ç”ŸæˆNä¸ªæ–‡æœ¬ï¼›é’ˆå¯¹æ¯ä¸ªæ–‡æœ¬ï¼Œæ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œ
1. åœ¨æ–‡æœ¬dç»™å®šæ¡ä»¶ä¸‹ï¼Œä¾æ®æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(z|d)ï¼Œä»è¯é¢˜é›†åˆä¸­éšæœºé€‰å–ä¸€ä¸ªè¯é¢˜zï¼Œå…±ç”ŸæˆLä¸ªè¯é¢˜ï¼Œè¿™é‡ŒLæ˜¯æ–‡æœ¬é•¿åº¦ï¼ˆæ¯ä¸ªè¯é¢˜ç”Ÿæˆä¸€ä¸ªå•è¯ï¼Œæ‰€ä»¥ç”Ÿæˆçš„æ–‡æœ¬dçš„é•¿åº¦æ˜¯Lï¼‰
1. åœ¨è¯é¢˜zç»™å®šæ¡ä»¶ä¸‹ï¼Œä¾æ®æ¦‚ç‡åˆ†å¸ƒP(w|z)ï¼Œä»å•è¯é›†åˆä¸­éšæœºé€‰å–ä¸€ä¸ªå•è¯w


æ–‡æœ¬-å•è¯å…±ç°æ•°æ®(çŸ©é˜µT)çš„ç”Ÿæˆæ¦‚ç‡ä¸ºæ‰€æœ‰å•è¯-æ–‡æœ¬å¯¹ï¼ˆwï¼Œdï¼‰çš„ç”Ÿæˆæ¦‚ç‡ä¹˜ç§¯ï¼Œ
$$P(T) = \prod_{(w,d)} P(w,d)^{n(w,d)} \\ T = [n(w_i,d_j)] i=1,2,...,M; j=1,2,...,N$$
$n(w,d)$è¡¨ç¤ºï¼ˆwï¼Œdï¼‰å‡ºç°çš„æ¬¡æ•°;
çŸ©é˜µTçš„è¡Œè¡¨ç¤ºå•è¯ï¼Œåˆ—è¡¨ç¤ºæ–‡æœ¬ï¼Œå…ƒç´ è¡¨ç¤ºï¼ˆwï¼Œdï¼‰å‡ºç°çš„æ¬¡æ•°ï¼Œç”¨$n(w,d)$è¡¨ç¤ºï¼›
å•è¯-æ–‡æœ¬å¯¹ å‡ºç°çš„æ€»æ¬¡æ•°ä¸ºNÃ—L 

> è¿™é‡Œå‡è®¾æ–‡æœ¬çš„é•¿åº¦éƒ½æ˜¯ç­‰é•¿çš„ï¼Œæ­£å¸¸æƒ…å†µæ˜¯ç¬¬ä¸€ä¸ªæ–‡æœ¬çš„é•¿åº¦æ˜¯L1,...ï¼Œç¬¬Nä¸ªæ–‡æœ¬çš„é•¿åº¦æ˜¯LN;
> æ­£å¸¸æƒ…å†µä¸‹å•è¯-æ–‡æœ¬å¯¹ å‡ºç°çš„æ€»æ¬¡æ•°ä¸º$\sum_{i=1}^N L_i$


ä¹¦ä¸­è¿˜è®²åˆ°äº†ç­‰ä»·çš„**å…±ç°æ¨¡å‹ï¼ˆå¯¹ç§°æ¨¡å‹ï¼‰**
$$P(w,d)=\sum _{z}P(z)P(d|z)P(w|z)$$
![](./img/20200501171318598.png)

- **ç­–ç•¥**ï¼š
$$ L =\log P(T) = \log \prod_{i=1}^{M} \prod_{j=1}^{N} {P\left(w_{i}, d_{j}\right)}^{n\left(w_{i}, d_{j}\right) } \\=\sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \log P\left(w_{i}, d_{j}\right) \\ =\sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \left[\log P(d)+\log \sum_{k=1}^{K} P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)\right]$$
å› ä¸ºlogP(d)å¯¹éœ€è¦æ±‚çš„æ¨¡å‹å‚æ•°æ— å…³ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶çœå»ï¼Œäºæ˜¯å¾—åˆ°ï¼š
$$L =\sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \log \left[ \sum_{k=1}^{K} P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)\right]$$
æå¤§åŒ–ï¼Œå¾—åˆ°æœ€ä¼˜å‚æ•°
$$\arg \max _{\theta} L(\theta)=\arg \max _{\theta} \sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \log \left[ \sum_{k=1}^{K} P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right) \right]$$

- **ç®—æ³•**ï¼š
EMï¼š
1. ç°åœ¨è¿›è¡ŒEæ­¥ï¼Œè®¡ç®—Qå‡½æ•°
$$\arg \max _{\theta} L(\theta)=\arg \max _{\theta} \sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \log \sum_{k=1}^{K} P\left(z_{k} \mid w_{i}, d_{j}\right) \frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}$$
å…¶ä¸­logå³è¾¹ä¸ºå…³äºzçš„æœŸæœ›ï¼š
$$E_z \left[\frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}\right] = \sum_{k=1}^{K} P\left(z_{k} \mid w_{i}, d_{j}\right) \frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}$$
æ‰€ä»¥ï¼š
$$\arg \max _{\theta} L(\theta)=\arg \max _{\theta} \sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \log E_z \left[\frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}\right]$$
æ ¹æ®jensenä¸ç­‰å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š
$$\log E_{z}\left[\frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}\right] \geq E_{z}\left[\log \frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}\right]$$
å¾—åˆ°L(Î¸)çš„ä¸‹ç•Œï¼š
$$\sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right)  E_z \left[\log \frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{P\left(z_{k} \mid w_{i}, d_{j}\right)}\right] \\ =\sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \sum_{k=1}^{K} P\left(z_{k} \mid w_{i}, d_{j}\right)\left[\log P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)-\log P\left(z_{k} \mid w_{i}, d_{j}\right)\right]$$
æœ€åï¼Œæˆ‘ä»¬å°†Kçš„ç´¯åŠ é¡¹æ‹†å¼€ï¼Œå¯ä»¥å¾—åˆ°ä¸€é¡¹ $\sum_{k=1}^{K} P\left(z_{k} \mid w_{i}, d_{j}\right) \log P\left(z_{k} \mid w_{i}, d_{j}\right)$ ï¼Œè¿™ä¸€é¡¹åœ¨Mæ­¥ä¸­æ²¡æœ‰ä½œç”¨ï¼Œå¯ä»¥çœå»ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥å¾—åˆ°Qå‡½æ•°ä¸ºï¼š
$$Q=\sum_{i=1}^{M} \sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) \sum_{k=1}^{K} P\left(z_{k} \mid w_{i}, d_{j}\right) \log \left[P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)\right]$$
éœ€è¦ä¼˜åŒ–çš„å‚æ•°ä¸º $P\left(z_{k} \mid w_{i}, d_{j}\right)ï¼ŒP\left(w_{i} \mid z_{k}\right)ï¼Œ  P\left(z_{k} \mid d_{j}\right)$ è¿™ä¸‰é¡¹ï¼Œåœ¨Qæ­¥ä¸­ï¼Œç¬¬ä¸€é¡¹æ˜¯å˜é‡ï¼Œåä¸¤é¡¹æ˜¯å¸¸é‡ï¼Œäºæ˜¯å¯ä»¥ç”±è´å¶æ–¯å…¬å¼è·å¾—ï¼š
$$P\left(z_{k} \mid w_{i}, d_{j}\right)=\frac{P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}{\sum_{k=1}^{K} P\left(w_{i} \mid z_{k}\right) P\left(z_{k} \mid d_{j}\right)}$$

1. Mæ­¥
åœ¨Mæ­¥ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¼˜åŒ–çš„æ˜¯$P\left(w_{i} \mid z_{k}\right)ï¼Œ  P\left(z_{k} \mid d_{j}\right)$ è¿™ä¸¤é¡¹ï¼ˆä¸¤é¡¹çš„ä¹˜ç§¯ä»£è¡¨çš„å®Œå…¨æ•°æ®ï¼Œæ˜¯æœªçŸ¥å˜é‡ï¼‰ï¼Œæ­¤æ—¶ $P\left(z_{k} \mid w_{i}, d_{j}\right)$ä¸ºå¸¸é‡ï¼ˆä»£è¡¨ä¸å®Œå…¨æ•°æ®ï¼Œæ˜¯å·²çŸ¥å˜é‡ï¼‰ï¼Œæå¤§åŒ–Qå‡½æ•°çš„Mæ­¥å¯ä»¥ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ¥ä¼˜åŒ–ä¸¤ä¸ªå‚æ•°ï¼Œå³ï¼š
$$\max Q \\s.t. \quad \begin{array}{l}\sum_{i=1}^{M} P\left(w_{i} \mid z_{k}\right)=1, \quad k=1,2, \cdots, K \\ \sum_{k=1}^{K} P\left(z_{k} \mid d_{j}\right)=1, \quad j=1,2, \cdots, N\end{array}$$
æ ¹æ®ä¸Šè¿°çº¦æŸæ¡ä»¶æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š
$$\Lambda=Q^{\prime}+\sum_{k=1}^{K} \tau_{k}\left(1-\sum_{i=1}^{M} P\left(w_{i} \mid z_{k}\right)\right)+\sum_{j=1}^{N} \rho_{j}\left(1-\sum_{k=1}^{K} P\left(z_{k} \mid d_{j}\right)\right)$$
åˆ†åˆ«å¯¹ä¸¤ä¸ªå‚æ•°$P\left(w_{i} \mid z_{k}\right)ï¼Œ  P\left(z_{k} \mid d_{j}\right)$æ±‚åå¯¼ï¼Œå¹¶ä»¤åå¯¼æ•°ä¸º0ï¼š
$$\begin{array}{l}\sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) P\left(z_{k} \mid w_{i}, d_{j}\right)-\tau_{k} P\left(w_{i} \mid z_{k}\right)=0, \quad i=1,2, \cdots, M ; \quad k=1,2, \cdots, K \\ \sum_{i=1}^{M} n\left(w_{i}, d_{j}\right) P\left(z_{k} \mid w_{i}, d_{j}\right)-\rho_{j} P\left(z_{k} \mid d_{j}\right)=0, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K\end{array}$$
æ±‚è§£ä¸Šé¢çš„æ–¹ç¨‹ç»„ï¼Œå°±å¯ä»¥å¾—åˆ°Mæ­¥çš„å‚æ•°ä¼°è®¡ï¼š
$$P\left(w_{i} \mid z_{k}\right)=\frac{\sum_{j=1}^{N} n\left(w_{i}, d_{j}\right) P\left(z_{k} \mid w_{i}, d_{j}\right)}{\sum_{m=1}^{M} \sum_{j=1}^{N} n\left(w_{m}, d_{j}\right) P\left(z_{k} \mid w_{m}, d_{j}\right)}$$
$$P\left(z_{k} \mid d_{j}\right)=\frac{\sum_{i=1}^{M} n\left(w_{i}, d_{j}\right) P\left(z_{k} \mid w_{i}, d_{j}\right)}{n\left(d_{j}\right)}$$
æœ€åï¼Œåœ¨Eæ­¥å’ŒMæ­¥é—´ä¸åœè¿­ä»£ï¼Œç›´åˆ°å¾—åˆ°ä¼˜åŒ–åçš„ä¸¤ä¸ªå‚æ•°
$n(d_j) = \sum_{i=1}^M n(w_i,d_j)$è¡¨ç¤ºæ–‡æœ¬$d_j$ä¸­çš„å•è¯ä¸ªæ•°ï¼Œ$n(w_i,d_j)$è¡¨ç¤ºå•è¯$w_i$åœ¨æ–‡æœ¬$d_j$ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚

### å‚è€ƒæ–‡çŒ®
[18-1] Hofmann T. Probabilistic Latent Semantic analysis. Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, 1999: 289-296.
[18-2] Hofmann T. [Probabilistic Latent Semantic Indexing](https://arxiv.org/abs/1301.6705). Proceedings of the 22nd Annual International ACM [SIGIR](https://en.jinzhao.wiki/wiki/Special_Interest_Group_on_Information_Retrieval) Conference in Research and Development in [Information Retrieval](https://en.jinzhao.wiki/wiki/Information_retrieval), 1999.
[18-3] Hofmann T. [Unsupervised learning by probabilistic latent semantic analysis](https://link.springer.com/content/pdf/10.1023%2FA%3A1007617005950.pdf). Machine Learning, 2001, 42: 177-196.
[18-4] Ding C, Li T, Peng W. On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing. Computational Statistics & Data Analysis, 2008, 52(8): 3913-3927.

## ç¬¬ 19 ç«  é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•

**é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—**ï¼ˆ[Markov Chain Monte Carlo, MCMC](https://en.jinzhao.wiki/wiki/Markov_chain_Monte_Carlo)ï¼‰ç”±ä¸¤ä¸ªMCç»„æˆï¼Œå³**è’™ç‰¹å¡ç½—æ–¹æ³•**ï¼ˆ[Monte Carlo Simulation, MC](https://en.jinzhao.wiki/wiki/Monte_Carlo_method)ï¼‰å’Œ**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov Chain, MC](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚

è¦å¼„æ‡‚MCMCçš„åŸç†æˆ‘ä»¬é¦–å…ˆå¾—ææ¸…æ¥šè’™ç‰¹å¡ç½—æ–¹æ³•å’Œé©¬å°”å¯å¤«é“¾çš„åŸç†ã€‚

é©¬å°”å¯å¤«é“¾åœ¨å‰é¢çš„ç« èŠ‚æœ‰è®²åˆ°ï¼Œå†ç»“åˆä¹¦ä¸­çš„å†…å®¹ã€‚è¿™é‡Œè¡¥å……ä¸‹å‡ ä¸ªçŸ¥è¯†ï¼š
**é©¬å°”å¯å¤«é“¾çš„éå†å®šç†**ï¼š
ä¹¦ä¸­å•°å—¦äº†å¾ˆå¤šï¼Œæˆ‘çš„ç†è§£æ˜¯éå†è¶³å¤Ÿå¤šèƒ½è¾¾åˆ°å¹³ç¨³åˆ†å¸ƒçš„é©¬å°”å¯å¤«é“¾ã€‚å¹¶ä¸”è¾¾åˆ°ä»»ä½•ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ä¸èƒ½ä¸º0ï¼›ï¼ˆä¸å¯çº¦ï¼Œéå‘¨æœŸä¸”æ­£å¸¸è¿”ï¼‰

**å¯é€†é©¬å°”å¯å¤«é“¾**ï¼ˆreversible Markov chainï¼‰ï¼š
è®¾æœ‰é©¬å°”å¯å¤«é“¾$X=\{X_0,X_1,...,X_t,...\}$ï¼ŒçŠ¶æ€ç©ºé—´Sï¼Œè½¬ç§»çŸ©é˜µPï¼Œå¦‚æœæœ‰çŠ¶æ€åˆ†å¸ƒ$\pi = (\pi_1,\pi_2,...)^T$ï¼Œå¯¹ä»»æ„çŠ¶æ€$i,j \in S$ï¼Œå¯¹ä»»æ„æ—¶åˆ»tæ»¡è¶³
$$P(X_t=i|X_{t-1}=j)\pi_j = P(X_{t-1}=j|X_t=i)\pi_i  ,\quad i,j =1,2,...$$
æˆ–è€…ç®€å†™
$$p_{ij}\pi_j = p_{ji}\pi_i ,\quad i,j =1,2,...$$
åˆ™ç§°æ­¤é©¬å°”å¯å¤«é“¾ä¸ºå¯é€†é©¬å°”å¯å¤«é“¾ï¼›ç®€å†™çš„ç­‰å¼ç§°ä¸º**ç»†è‡´å¹³è¡¡æ–¹ç¨‹**ï¼ˆdetailed balance equationï¼‰ï¼Œå¹¶ä¸”æ»¡è¶³ç»†è‡´å¹³è¡¡æ–¹ç¨‹çš„çŠ¶æ€åˆ†å¸ƒ$\pi$å°±æ˜¯è¯¥é©¬å°”å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒï¼ˆå¹¶ä¸æ˜¯æ‰€æœ‰çš„é©¬å°”å¯å¤«é“¾éƒ½æ˜¯å¯é€†çš„ï¼‰ã€‚
å¯é€†é©¬å°”å¯å¤«é“¾æ»¡è¶³éå†å®šç†ã€‚




**é‡‡æ ·æ³•**ï¼ˆ[Sampling Method](https://en.jinzhao.wiki/wiki/Sampling_(statistics))ï¼‰ä¹Ÿç§°ä¸ºè’™ç‰¹å¡ç½—æ–¹æ³•ï¼ˆ[Monte Carlo Method, MC](https://en.jinzhao.wiki/wiki/Monte_Carlo_method)ï¼‰æˆ–ç»Ÿè®¡æ¨¡æ‹Ÿæ–¹æ³•ï¼ˆStatistical Simulation  Methodï¼‰

è’™ç‰¹å¡ç½—æ–¹æ³•è¯ç”Ÿäº20 ä¸–çºª 40 å¹´ä»£ç¾å›½çš„â€œæ›¼å“ˆé¡¿è®¡åˆ’â€ï¼Œå…¶åå­—æ¥æºäºæ‘©çº³å“¥çš„ä¸€ä¸ªä»¥èµŒåšä¸šé—»åçš„åŸå¸‚è’™ç‰¹å¡ç½—ï¼Œè±¡å¾æ¦‚ç‡ï¼
è’™ç‰¹å¡ç½—æ–¹æ³•æ˜¯ä¸€ç§é€šè¿‡éšæœºé‡‡æ ·æ¥è¿‘ä¼¼ä¼°è®¡ä¸€äº›è®¡ç®—é—®é¢˜**æ•°å€¼è§£**ï¼ˆNumerical solutionä¸å…¶å¯¹åº”çš„æ˜¯é—­å¼è§£Closed-form solutionæˆ–è§£æè§£Analytical solutionï¼‰çš„æ–¹æ³•ï¼

æœ€æ—©çš„è’™ç‰¹å¡ç½—æ–¹æ³•éƒ½æ˜¯ä¸ºäº†æ±‚è§£ä¸€äº›ä¸å¤ªå¥½æ±‚è§£çš„æ±‚å’Œæˆ–è€…ç§¯åˆ†é—®é¢˜ã€‚æ¯”å¦‚ç§¯åˆ†ï¼š$\theta = \int_a^b f(x)dx$ æˆ–è€…ä¼°è®¡$\pi$å€¼æˆ–åœ†çš„é¢ç§¯ï¼ˆç§¯åˆ†ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è’™ç‰¹å¡ç½—æ–¹æ³•æ¥æ¨¡æ‹Ÿæ±‚è§£è¿‘ä¼¼å€¼ã€‚å¦‚ä½•æ¨¡æ‹Ÿå‘¢ï¼Ÿ

**éšæœºé‡‡æ ·**æŒ‡ä»ç»™å®šæ¦‚ç‡å¯†åº¦å‡½æ•° ğ‘(ğ‘¥) ä¸­æŠ½å–å‡ºç¬¦åˆå…¶æ¦‚ç‡åˆ†å¸ƒçš„æ ·æœ¬ï¼

éšæœºé‡‡æ · é‡‡æ ·æ³•çš„éš¾ç‚¹æ˜¯å¦‚ä½•è¿›è¡Œéšæœºé‡‡æ ·ï¼Œå³å¦‚ä½•è®©è®¡ç®—æœºç”Ÿæˆæ»¡è¶³æ¦‚ç‡å¯†åº¦å‡½æ•° ğ‘(ğ‘¥) çš„æ ·æœ¬ï¼æˆ‘ä»¬çŸ¥é“ï¼Œè®¡ç®—æœºå¯ä»¥æ¯”è¾ƒå®¹æ˜“åœ°éšæœºç”Ÿæˆä¸€ä¸ªåœ¨ [0, 1]åŒºé—´ä¸Šå‡å¸ƒåˆ†å¸ƒçš„æ ·æœ¬ ğœ‰ï¼å¦‚æœè¦éšæœºç”Ÿæˆæœä»æŸä¸ªéå‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬ï¼Œå°±éœ€è¦ä¸€äº›é—´æ¥çš„é‡‡æ ·æ–¹æ³•ï¼
å¦‚æœä¸€ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º ğ‘(ğ‘¥)ï¼Œå…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•° cdf(ğ‘¥) ä¸ºè¿ç»­çš„ä¸¥æ ¼å¢å‡½æ•°ï¼Œä¸”å­˜åœ¨é€†å‡½æ•°$cdf^{âˆ’1}(ğ‘¦), ğ‘¦ âˆˆ [0, 1]$ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åˆ©ç”¨**ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†å‡½æ•°**ï¼ˆinverse CDFï¼‰æ¥ç”Ÿæˆæœä»è¯¥éšæœºåˆ†å¸ƒçš„æ ·æœ¬ï¼å‡è®¾ ğœ‰ æ˜¯ [0, 1] åŒºé—´ä¸Šå‡åŒ€åˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œåˆ™ $cdf^{âˆ’1}(\xi)$ æœä»æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºğ‘(ğ‘¥)çš„åˆ†å¸ƒï¼ä½†å½“ ğ‘(ğ‘¥) éå¸¸å¤æ‚ï¼Œå…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†å‡½æ•°éš¾ä»¥è®¡ç®—ï¼Œæˆ–è€…ä¸çŸ¥é“ ğ‘(ğ‘¥)çš„ç²¾ç¡®å€¼ï¼ŒåªçŸ¥é“æœªå½’ä¸€åŒ–çš„åˆ†å¸ƒ Ì‚ğ‘(ğ‘¥)æ—¶ï¼Œå°±éš¾ä»¥ç›´æ¥å¯¹ğ‘(ğ‘¥)è¿›è¡Œé‡‡æ ·ï¼Œå¾€å¾€éœ€è¦ä½¿ç”¨ä¸€äº›é—´æ¥çš„é‡‡æ ·ç­–ç•¥ï¼Œæ¯”å¦‚**æ‹’ç»é‡‡æ ·ã€é‡è¦æ€§é‡‡æ ·ã€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—é‡‡æ ·**ç­‰ï¼è¿™äº›æ–¹æ³•ä¸€èˆ¬æ˜¯å…ˆæ ¹æ®ä¸€ä¸ªæ¯”è¾ƒå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œç„¶åé€šè¿‡ä¸€äº›ç­–ç•¥æ¥é—´æ¥å¾—åˆ°ç¬¦åˆğ‘(ğ‘¥)åˆ†å¸ƒçš„æ ·æœ¬ï¼

> rejection sampling, inverse CDF, Box-Muller,  Ziggurat algorithm

**æ‹’ç»é‡‡æ ·**ï¼ˆRejection Samplingï¼‰æ˜¯ä¸€ç§é—´æ¥é‡‡æ ·æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºæ¥å—-æ‹’ç»é‡‡æ ·ï¼ˆAcceptance-Rejection Samplingï¼‰ï¼
å‡è®¾åŸå§‹åˆ†å¸ƒğ‘(ğ‘¥)éš¾ä»¥ç›´æ¥é‡‡æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒğ‘(ğ‘¥)ï¼Œä¸€èˆ¬ç§°ä¸ºæè®®åˆ†å¸ƒï¼ˆProposal Distributionï¼‰ï¼Œç„¶åä»¥æŸä¸ªæ ‡å‡†æ¥æ‹’ç»ä¸€éƒ¨åˆ†çš„æ ·æœ¬ä½¿å¾—æœ€ç»ˆé‡‡é›†çš„æ ·æœ¬æœä»åˆ†å¸ƒ ğ‘(ğ‘¥)ã€‚æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæè®®åˆ†å¸ƒ ğ‘(ğ‘¥) å’Œä¸€ä¸ªå¸¸æ•° ğ‘˜ï¼Œä½¿å¾— ğ‘˜ğ‘(ğ‘¥) å¯ä»¥è¦†ç›–å‡½æ•°ğ‘(ğ‘¥)ï¼Œå³ğ‘˜ğ‘(ğ‘¥) â‰¥ ğ‘(ğ‘¥),
![](./img/1042406-20170327143755811-993574578.png)
å¯¹äºæ¯æ¬¡æŠ½å–çš„æ ·æœ¬ $\^{x}$ è®¡ç®—æ¥å—æ¦‚ç‡ï¼ˆAcceptance Probabilityï¼‰ï¼š$\alpha(\^{x}) = \frac{p(\^{x})}{kq(\^{x})}$ï¼Œå¹¶ä»¥æ¦‚ç‡$\alpha(\^{x})$)æ¥æ¥å—æ ·æœ¬ $\^{x}$

æ‹’ç»é‡‡æ ·çš„é‡‡æ ·è¿‡ç¨‹
```
è¾“å…¥: æè®®åˆ†å¸ƒğ‘(ğ‘¥)ï¼Œå¸¸æ•°ğ‘˜ï¼Œæ ·æœ¬é›†åˆğ’± = âˆ…;
1 repeat
2   æ ¹æ®ğ‘(ğ‘¥)éšæœºç”Ÿæˆä¸€ä¸ªæ ·æœ¬ ;Ì‚ğ‘¥
3   è®¡ç®—æ¥å—æ¦‚ç‡ğ›¼( Ì‚ğ‘¥);
4   ä»(0, 1)çš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºç”Ÿæˆä¸€ä¸ªå€¼ğ‘§;
5   if ğ‘§ â‰¤ ğ›¼( Ì‚ğ‘¥) then // ä»¥ğ›¼(ğ‘¥)Ì‚ çš„æ¦‚ç‡æ¥å—ğ’™Ì‚
6       ğ’± = ğ’± âˆª { Ì‚ğ‘¥};
7   end
8 until è·å¾— ğ‘ ä¸ªæ ·æœ¬(|ğ’±| = ğ‘);
è¾“å‡º: æ ·æœ¬é›†åˆğ’±
```

åˆ¤æ–­ä¸€ä¸ªæ‹’ç»é‡‡æ ·æ–¹æ³•çš„å¥½åå°±æ˜¯çœ‹å…¶é‡‡æ ·æ•ˆç‡ï¼Œå³æ€»ä½“çš„æ¥å—ç‡ï¼å¦‚æœå‡½æ•°ğ‘˜ğ‘(ğ‘¥)è¿œå¤§äºåŸå§‹åˆ†å¸ƒå‡½æ•° Ì‚ğ‘(ğ‘¥)ï¼Œæ‹’ç»ç‡ä¼šæ¯”è¾ƒé«˜ï¼Œé‡‡æ ·æ•ˆç‡ä¼šéå¸¸ä¸ç†æƒ³ï¼ä½†è¦æ‰¾åˆ°ä¸€ä¸ªå’Œ Ì‚ğ‘(ğ‘¥)æ¯”è¾ƒæ¥è¿‘çš„æè®®åˆ†å¸ƒå¾€å¾€æ¯”è¾ƒå›°éš¾ï¼ç‰¹åˆ«æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œå…¶é‡‡æ ·ç‡ä¼šéå¸¸ä½ï¼Œå¯¼è‡´å¾ˆéš¾åº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­ï¼

**é‡è¦æ€§é‡‡æ ·**ï¼ˆImportance samplingï¼‰
å¦‚æœé‡‡æ ·çš„ç›®çš„æ˜¯è®¡ç®—åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹å‡½æ•°ğ‘“(ğ‘¥)çš„æœŸæœ›ï¼Œé‚£ä¹ˆå®é™…ä¸ŠæŠ½å–çš„æ ·æœ¬ä¸éœ€è¦ä¸¥æ ¼æœä»åˆ†å¸ƒ ğ‘(ğ‘¥)ï¼ä¹Ÿå¯ä»¥é€šè¿‡å¦ä¸€ä¸ªåˆ†å¸ƒï¼Œå³æè®®åˆ†å¸ƒ ğ‘(ğ‘¥)ï¼Œç›´æ¥é‡‡æ ·å¹¶ä¼°è®¡ğ”¼ğ‘[ğ‘“(ğ‘¥)]ï¼
å‡½æ•°ğ‘“(ğ‘¥)åœ¨åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹çš„æœŸæœ›å¯ä»¥å†™ä¸º
$$E_p[f(x)] = \int_x f(x)p(x)dx = \int_x f(x)\frac{p(x)}{q(x)}q(x)dx = \int_x f(x)w(x)q(x)dx = E_q[f(x)w(x)]$$
å…¶ä¸­ğ‘¤(ğ‘¥)ç§°ä¸ºé‡è¦æ€§æƒé‡ï¼

é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Samplingï¼‰æ˜¯é€šè¿‡å¼•å…¥é‡è¦æ€§æƒé‡ï¼Œå°†åˆ†å¸ƒ ğ‘(ğ‘¥)ä¸‹ğ‘“(ğ‘¥)çš„æœŸæœ›å˜ä¸ºåœ¨åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹ğ‘“(ğ‘¥)ğ‘¤(ğ‘¥)çš„æœŸæœ›ï¼Œä»è€Œå¯ä»¥è¿‘ä¼¼ä¸º
$$E_p[f(x)] = E_q[f(x)w(x)] =\frac{1}{N} \sum_{i=1}^N f(x_i)w(x_i) =\frac{1}{N} \sum_{i=1}^N f(x_i)\frac{p(x_i)}{q(x_i)}$$
å…¶ä¸­$\{x_1,...,x_N\}$æ˜¯ç‹¬ç«‹ä»$q(x)$ä¸­éšæœºé‡‡æ ·æ¥çš„ï¼ˆp(x)æ˜¯å·²çŸ¥çš„ï¼Œåªæ˜¯ä¸å¥½é‡‡æ ·ï¼Œä½†æ˜¯èƒ½è®¡ç®—ï¼‰ã€‚

**é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³•**
åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ‹’ç»é‡‡æ ·å’Œé‡è¦æ€§é‡‡æ ·çš„æ•ˆç‡éšç©ºé—´ç»´æ•°çš„å¢åŠ è€ŒæŒ‡æ•°é™ä½ï¼é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ï¼ˆMarkov Chain Monte Carloï¼ŒMCMCï¼‰æ–¹æ³•æ˜¯ä¸€ç§æ›´å¥½çš„é‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å¯¹é«˜ç»´å˜é‡è¿›è¡Œé‡‡æ ·ï¼

å‡è®¾å¤šå…ƒéšæœºå˜é‡$x$,æ»¡è¶³$x \in \mathcal{X}$,å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º$p(x)$ï¼›$f(x)$ä¸ºå®šä¹‰åœ¨$x \in \mathcal{X}$ä¸Šçš„å‡½æ•°ï¼Œç›®æ ‡æ˜¯è·å¾—æ¦‚ç‡åˆ†å¸ƒ$p(x)$çš„æ ·æœ¬é›†åˆä»¥åŠæ±‚å‡½æ•°$f(x)$çš„æ•°å­¦æœŸæœ›$E_{p(x)}[f(x)]$ã€‚

åº”ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•è§£å†³è¿™ä¸ªé—®é¢˜çš„**åŸºæœ¬æƒ³æ³•**æ˜¯ï¼š
åœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\mathcal{S}$ä¸Šå®šä¹‰ä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å°”å¯å¤«é“¾$X=\{X_0,X_1,...,X_t,...\}$,ä½¿å…¶å¹³ç¨³åˆ†å¸ƒå°±æ˜¯æŠ½æ ·çš„çš„ç›®æ ‡åˆ†å¸ƒ$p(x)$ï¼ˆè®¾è®¡ä¸€ä¸ªéšæœºçŸ©é˜µï¼ˆè¿ç»­éšæœºå˜é‡å°±æ˜¯è½¬ç§»æ ¸å‡½æ•°ï¼‰ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒç­‰äºç›®æ ‡åˆ†å¸ƒï¼‰ã€‚ç„¶ååœ¨è¿™ä¸ªé©¬å°”å¯å¤«é“¾ä¸Šè¿›è¡Œéšæœºæ¸¸èµ°ï¼Œæ¯ä¸ªæ—¶åˆ»å¾—åˆ°ä¸€ä¸ªæ ·æœ¬ã€‚æ ¹æ®éå†å®šç†ï¼Œå½“æ—¶é—´è¶‹äºæ— ç©·æ—¶ï¼Œæ ·æœ¬çš„åˆ†å¸ƒè¶‹è¿‘å¹³ç¨³åˆ†å¸ƒï¼Œæ ·æœ¬çš„å‡½æ•°å‡å€¼è¶‹è¿‘å‡½æ•°çš„æ•°å­¦æœŸæœ›ã€‚æ‰€ä»¥ï¼Œå½“æ—¶é—´è¶³å¤Ÿé•¿æ—¶ï¼ˆæ—¶åˆ»å¤§äºæŸä¸ªæ­£æ•´æ•°mï¼‰ï¼Œåœ¨ä¹‹åçš„æ—¶é—´ï¼ˆæ—¶åˆ»å°äºç­‰äºæŸä¸ªæ­£æ•´æ•°n,n>mï¼‰é‡Œéšæœºæ¸¸èµ°å¾—åˆ°çš„æ ·æœ¬é›†åˆ$\left\{x_{m+1}, x_{m+2}, \cdots, x_{n}\right\}$å°±æ˜¯ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒçš„æŠ½æ ·ç»“æœï¼Œå¾—åˆ°çš„å‡½æ•°å‡å€¼å°±æ˜¯è¦è®¡ç®—çš„æ•°å­¦æœŸæœ›å€¼ï¼š
$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left(x_{i}\right)$$
ä»æ—¶åˆ»0åˆ°æ—¶åˆ»mä¸ºæ­¢çš„è¿™æ®µæ—¶é—´ç§°ä¸ºç‡ƒçƒ§æœŸï¼ˆBurn-in Periodï¼‰ã€‚ç‡ƒçƒ§æœŸçš„æ ·æœ¬ä¹Ÿæ˜¯è¦è¢«ä¸¢å¼ƒçš„ã€‚

**åŸºæœ¬æ­¥éª¤**
1. é¦–å…ˆï¼Œåœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\mathcal{S}$ä¸Šæ„é€ ä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å°”å¯å¤«é“¾ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒä¸ºç›®æ ‡åˆ†å¸ƒ$p(x)$;
1. ä»çŠ¶æ€ç©ºé—´çš„æŸä¸€ç‚¹$x_0$å‡ºå‘ï¼Œç”¨æ„é€ çš„é©¬å°”å¯å¤«é“¾è¿›è¡Œéšæœºæ¸¸èµ°ï¼Œäº§ç”Ÿæ ·æœ¬åºåˆ—$x_0,x_1,...,x_t,...$ã€‚
1. åº”ç”¨é©¬å°”å¯å¤«é“¾çš„éå†åŸç†ï¼Œç¡®å®šæ­£æ•´æ•°må’Œnï¼ˆm < nï¼‰ï¼Œå¾—åˆ°æ ·æœ¬é›†åˆ$\{x_{m+1},x_{m+2},...,x_{n}\}$ï¼Œæ±‚å¾—å‡½æ•°$f(x)$çš„å‡å€¼ï¼ˆéå†å‡å€¼ï¼‰
$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left(x_{i}\right)$$

è¿™é‡Œæœ‰å‡ ä¸ª**é‡è¦é—®é¢˜**
1. å¦‚ä½•ç¬¬ä¸€é©¬å°”å¯å¤«é“¾ï¼Œä¿è¯é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•çš„æ¡ä»¶æˆç«‹ã€‚
1. å¦‚ä½•ç¡®å®šæ”¶æ•›æ­¥æ•°$m$ï¼Œä¿è¯æ ·æœ¬æŠ½æ ·çš„æ— åæ€§ã€‚
1. å¦‚ä½•ç¡®å®šè¿­ä»£æ­¥æ•°$n$ï¼Œä¿è¯éå†å‡å€¼è®¡ç®—çš„ç²¾åº¦ã€‚

**MCMCé‡‡æ ·**
ç”±äºä¸€èˆ¬æƒ…å†µä¸‹ï¼Œç›®æ ‡å¹³ç¨³åˆ†å¸ƒ $\pi(x)$ å’ŒæŸä¸€ä¸ªé©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ ä¸æ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼Œå³ï¼š

$$\pi(i)Q(i,j)\neq\pi(j)Q(j,i)$$

**è¿™äº›è®°å·è¡¨è¾¾åŒä¸€ä¸ªæ„æ€**ï¼š $Q(i,j)\Leftrightarrow Q(j|i)\Leftrightarrow Q(i\rightarrow j)$ ï¼ˆçŠ¶æ€è½¬ç§»åˆ†å¸ƒæˆ–æè®®åˆ†å¸ƒï¼‰

æˆ‘ä»¬å¼•å…¥ä¸€ä¸ª $\alpha(i,j)$ ,ä½¿ä¸Šå¼å¯ä»¥å–ç­‰å·ã€‚

$$\pi(i)Q(i,j)\alpha(i,j)=\pi(j)Q(j,i)\alpha(j,i)$$

æ€æ ·æ‰èƒ½æˆç«‹å‘¢ï¼Œå…¶å®å¾ˆç®€å•ï¼ŒæŒ‰ç…§å¯¹ç§°æ€§ï¼š

$$\alpha(i,j)=\pi(j)Q(j,i); \pi(i)Q(i,j)=\alpha(j,i)$$

ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°äº†åˆ†å¸ƒ $\pi(x)$ å¯¹è®©é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $P(i,j)=Q(i,j)\alpha(i,j)$

å…¶ä¸­ $\alpha(i,j)$ ä¸€èˆ¬ç§°ä¹‹ä¸ºæ¥å—ç‡ï¼Œå–å€¼åœ¨ $[0,1]$ ä¹‹é—´ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ¦‚ç‡å€¼ã€‚è¿™å¾ˆåƒæ¥å—-æ‹’ç»é‡‡æ ·ï¼Œé‚£é‡Œæ˜¯ä»¥ä¸€ä¸ªå¸¸ç”¨åˆ†å¸ƒé€šè¿‡ä¸€å®šçš„æ¥å—-æ‹’ç»æ¦‚ç‡å¾—åˆ°ä¸€ä¸ªéå¸¸è§åˆ†å¸ƒï¼Œ è¿™é‡Œæ˜¯ä»¥ä¸€ä¸ªå¸¸è§çš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ é€šè¿‡ä¸€å®šçš„æ¥å—-æ‹’ç»æ¦‚ç‡å¾—åˆ°ç›®æ ‡è½¬ç§»çŸ©é˜µ $P$ ,ä¸¤è€…çš„è§£å†³é—®é¢˜æ€è·¯æ˜¯ç±»ä¼¼çš„ã€‚

MCMCé‡‡æ ·ç®—æ³•å¦‚ä¸‹ï¼š
1. è¾“å…¥ä»»æ„ç»™å®šçš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ ï¼Œç›®æ ‡å¹³ç¨³åˆ†å¸ƒ $\pi(x)$ ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼ $n_1$ ï¼Œéœ€è¦çš„æ ·æœ¬æ•° $n_2$;
1. ä»ä»»æ„ç®€å•æ¦‚ç‡åˆ†å¸ƒå¾—åˆ°åˆå§‹çŠ¶æ€å€¼ $x_0$ ï¼›
1. for $t=0 ~in ~n_{1}+n_{2}-1$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ $Q(x|x_{t})$ å¾—åˆ°æ ·æœ¬å€¼ $x_{*}$
    1. ä»å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ · $u\sim uniform[0,1]$
    1. å¦‚æœ $u<\alpha(x_{t},x_{*})=\pi(x_{*})Q(x_{*},x_{t})$ ï¼Œåˆ™æ¥å— $x_{t}\rightarrow x_{*}$ ï¼Œå³ $x_{t+1}= x_{*}$
    1. å¦åˆ™ä¸æ¥å—è½¬ç§»ï¼Œ å³$x_{t+1}= x_{t}$

ä¸Šè¿°è¿‡ç¨‹ä¸­ $p(x),q(x|y)$ è¯´çš„éƒ½æ˜¯ç¦»æ•£çš„æƒ…å½¢ï¼Œäº‹å®ä¸Šå³ä¾¿è¿™ä¸¤ä¸ªåˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œä»¥ä¸Šç®—æ³•ä»ç„¶æ˜¯æœ‰æ•ˆï¼Œäºæ˜¯å°±å¾—åˆ°æ›´ä¸€èˆ¬çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒ $p(x)$ çš„é‡‡æ ·ç®—æ³•ï¼Œè€Œ $q(x|y)$ å°±æ˜¯ä»»æ„ä¸€ä¸ªè¿ç»­äºŒå…ƒæ¦‚ç‡åˆ†å¸ƒå¯¹åº”çš„æ¡ä»¶åˆ†å¸ƒã€‚

ä½†æ˜¯è¿™ä¸ªé‡‡æ ·ç®—æ³•è¿˜æ˜¯æ¯”è¾ƒéš¾åœ¨å®é™…ä¸­åº”ç”¨ï¼Œå› ä¸ºåœ¨ç¬¬ä¸‰æ­¥ä¸­ï¼Œç”±äº $\alpha(x_{t},x_{*})$ å¯èƒ½éå¸¸çš„å°ï¼Œæ¯”å¦‚0.1ï¼Œå¯¼è‡´æˆ‘ä»¬å¤§éƒ¨åˆ†çš„é‡‡æ ·å€¼éƒ½è¢«æ‹’ç»è½¬ç§»ï¼Œé‡‡æ ·æ•ˆç‡å¾ˆä½ã€‚æœ‰å¯èƒ½æˆ‘ä»¬é‡‡æ ·äº†ä¸Šç™¾ä¸‡æ¬¡é©¬å°”å¯å¤«é“¾è¿˜æ²¡æœ‰æ”¶æ•›ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢è¿™ä¸ª $n_1$ è¦éå¸¸éå¸¸çš„å¤§ï¼Œè¿™è®©äººéš¾ä»¥æ¥å—ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿè¿™æ—¶å°±è½®åˆ°æˆ‘ä»¬çš„M-Hé‡‡æ ·å‡ºåœºäº†ã€‚

**Metropolis-Hastingsç®—æ³•**ï¼š
M-Hé‡‡æ ·æ˜¯Metropolis-Hastingsé‡‡æ ·çš„ç®€ç§°ï¼Œè¿™ä¸ªç®—æ³•é¦–å…ˆç”±Metropolisæå‡ºï¼Œè¢«Hastingsæ”¹è¿›ï¼Œå› æ­¤è¢«ç§°ä¹‹ä¸ºMetropolis-Hastingsé‡‡æ ·æˆ–M-Hé‡‡æ ·ã€‚

> ä¹¦ä¸­çš„ä»‹ç»æ›´è¯¦ç»†ï¼Œè¿™é‡Œç®€å•ä»‹ç»åŸç†

æˆ‘ä»¬å›åˆ°MCMCé‡‡æ ·çš„ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼š$\pi(i)Q(i,j)\alpha(i,j) = \pi(j)Q(j,i)\alpha(j,i)$
æˆ‘ä»¬é‡‡æ ·æ•ˆç‡ä½çš„åŸå› æ˜¯$\alpha(i,j)$å¤ªå°äº†ï¼Œæ¯”å¦‚ä¸º0.1ï¼Œè€Œ$\alpha(j,i)$ä¸º0.2ã€‚å³ï¼š$\pi(i)Q(i,j)\times 0.1 = \pi(j)Q(j,i)\times 0.2$
è¿™æ—¶æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœä¸¤è¾¹åŒæ—¶æ‰©å¤§äº”å€ï¼Œæ¥å—ç‡æé«˜åˆ°äº†0.5ï¼Œä½†æ˜¯ç»†è‡´å¹³ç¨³æ¡ä»¶å´ä»ç„¶æ˜¯æ»¡è¶³çš„ï¼Œå³ï¼š$\pi(i)Q(i,j)\times 0.5 = \pi(j)Q(j,i)\times 1$
è¿™æ ·æˆ‘ä»¬çš„æ¥å—ç‡å¯ä»¥åšå¦‚ä¸‹æ”¹è¿›ï¼Œå³ï¼š$\alpha(i,j) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}$

æ‰€ä»¥ä¿®æ­£è½¬ç§»æ¦‚ç‡çš„$\hat{Q}(i,j) = Q(i,j)\alpha(i,j)$ï¼Œå¹¶ä¸”å¹³ç¨³åˆ†å¸ƒå°±æ˜¯$\pi(x)$

**Metropolisç®—æ³•**ä¸­çš„æè®®åˆ†å¸ƒæ˜¯å¯¹ç§°çš„(å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬é€‰æ‹©å¯¹ç§°çš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ$Q$)ï¼Œå³$Q(i,j)=Q(j,i)$ï¼Œæ‰€ä»¥æ¥å—æ¦‚ç‡$\alpha(i,j) = min\{ \frac{\pi(j)}{\pi(i)},1\}$

M-Hé‡‡æ ·ç®—æ³•è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è¾“å…¥æˆ‘ä»¬ä»»æ„é€‰å®šçš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ$Q$ï¼Œå¹³ç¨³åˆ†å¸ƒ$\pi(x)$ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼$n_1$ï¼Œéœ€è¦çš„æ ·æœ¬ä¸ªæ•°$n_2$
1. ä»ä»»æ„ç®€å•æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·å¾—åˆ°åˆå§‹çŠ¶æ€å€¼$x_0$
1. for $t=0 ~to ~n_1+n_2âˆ’1$: 
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$Q(x|x_t)$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_âˆ—$
    1. ä»å‡åŒ€åˆ†å¸ƒé‡‡æ ·$uâˆ¼uniform[0,1]$
    1. å¦‚æœ$u < \alpha(x_t,x_{*}) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}$, åˆ™æ¥å—è½¬ç§»$x_t \to x_{*}$ï¼Œå³$x_{t+1}=x_âˆ—$
    1. å¦åˆ™ä¸æ¥å—è½¬ç§»ï¼Œå³$x_{t+1}=x_t$


æ ·æœ¬é›†$(x_{n_1}, x_{n_1+1},..., x_{n_1+n_2-1})$å³ä¸ºæˆ‘ä»¬éœ€è¦çš„å¹³ç¨³åˆ†å¸ƒå¯¹åº”çš„æ ·æœ¬é›†ã€‚


M-Hé‡‡æ ·å®Œæ•´è§£å†³äº†ä½¿ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•éœ€è¦çš„ä»»æ„æ¦‚ç‡åˆ†å¸ƒæ ·æœ¬é›†çš„é—®é¢˜ï¼Œå› æ­¤åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚
ä½†æ˜¯åœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œ**M-Hé‡‡æ ·é¢ä¸´ç€ä¸¤å¤§éš¾é¢˜**ï¼š
1. æˆ‘ä»¬çš„æ•°æ®ç‰¹å¾éå¸¸çš„å¤šï¼ŒM-Hé‡‡æ ·ç”±äºæ¥å—ç‡è®¡ç®—å¼$\frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)}$çš„å­˜åœ¨ï¼Œåœ¨é«˜ç»´æ—¶éœ€è¦çš„è®¡ç®—æ—¶é—´éå¸¸çš„å¯è§‚ï¼Œç®—æ³•æ•ˆç‡å¾ˆä½ã€‚åŒæ—¶$\alpha(i,j)$ä¸€èˆ¬å°äº1ï¼Œæœ‰æ—¶å€™è¾›è‹¦è®¡ç®—å‡ºæ¥å´è¢«æ‹’ç»äº†ã€‚èƒ½ä¸èƒ½åšåˆ°ä¸æ‹’ç»è½¬ç§»å‘¢ï¼Ÿ
1. ç”±äºç‰¹å¾ç»´åº¦å¤§ï¼Œå¾ˆå¤šæ—¶å€™æˆ‘ä»¬ç”šè‡³å¾ˆéš¾æ±‚å‡ºç›®æ ‡çš„å„ç‰¹å¾ç»´åº¦è”åˆåˆ†å¸ƒï¼Œä½†æ˜¯å¯ä»¥æ–¹ä¾¿æ±‚å‡ºå„ä¸ªç‰¹å¾ä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚è¿™æ—¶å€™æˆ‘ä»¬èƒ½ä¸èƒ½åªæœ‰å„ç»´åº¦ä¹‹é—´æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„æƒ…å†µä¸‹æ–¹ä¾¿çš„é‡‡æ ·å‘¢ï¼Ÿ

**å‰å¸ƒæ–¯é‡‡æ ·**ï¼ˆGibbs Samplingï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆåœ°å¯¹é«˜ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·çš„ MCMC æ–¹æ³•ï¼Œå¯ä»¥çœ‹ä½œ Metropolis-Hastings ç®—æ³•çš„ç‰¹ä¾‹ï¼å‰å¸ƒæ–¯é‡‡æ ·ä½¿ç”¨å…¨æ¡ä»¶æ¦‚ç‡ï¼ˆFull Conditional Probabilityï¼‰ä½œä¸ºæè®®åˆ†å¸ƒæ¥ä¾æ¬¡å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œé‡‡æ ·ï¼Œå¹¶è®¾ç½®æ¥å—ç‡ä¸º$\alpha = 1$ï¼

å‰é¢è®²åˆ°äº†ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼šå¦‚æœéå‘¨æœŸé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$P$å’Œæ¦‚ç‡åˆ†å¸ƒ$\pi(x)$å¯¹äºæ‰€æœ‰çš„$i,j$æ»¡è¶³ï¼š$\pi(i)P(i,j) = \pi(j)P(j,i)$ï¼Œåˆ™ç§°æ¦‚ç‡åˆ†å¸ƒ$\pi(x)$æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µ$P$çš„å¹³ç¨³åˆ†å¸ƒã€‚

åœ¨M-Hé‡‡æ ·ä¸­æˆ‘ä»¬é€šè¿‡å¼•å…¥æ¥å—ç‡ä½¿ç»†è‡´å¹³ç¨³æ¡ä»¶æ»¡è¶³ã€‚ç°åœ¨æˆ‘ä»¬æ¢ä¸€ä¸ªæ€è·¯ã€‚
ä»äºŒç»´çš„æ•°æ®åˆ†å¸ƒå¼€å§‹ï¼Œå‡è®¾$\pi(x_1,x_2)$æ˜¯ä¸€ä¸ªäºŒç»´è”åˆæ•°æ®åˆ†å¸ƒï¼Œè§‚å¯Ÿç¬¬ä¸€ä¸ªç‰¹å¾ç»´åº¦ç›¸åŒçš„ä¸¤ä¸ªç‚¹$A(x_1^{(1)},x_2^{(1)})$å’Œ$B(x_1^{(1)},x_2^{(2)})$ï¼Œå®¹æ˜“å‘ç°ä¸‹é¢ä¸¤å¼æˆç«‹(å°±æ˜¯è”åˆæ¦‚ç‡å…¬å¼ï¼Œä»”ç»†çœ‹ï¼Œå¾ˆç®€å•)ï¼š
$$\pi(x_1^{(1)},x_2^{(1)}) \pi(x_2^{(2)} | x_1^{(1)}) = \pi(x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)}) \pi(x_2^{(2)} | x_1^{(1)})$$
$$\pi(x_1^{(1)},x_2^{(2)}) \pi(x_2^{(1)} | x_1^{(1)}) = \pi(x_1^{(1)}) \pi(x_2^{(2)} | x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)})$$
ç”±äºä¸¤å¼çš„å³è¾¹ç›¸ç­‰ï¼Œå› æ­¤æˆ‘ä»¬æœ‰ï¼š
$$\pi(x_1^{(1)},x_2^{(1)}) \pi(x_2^{(2)} | x_1^{(1)})  = \pi(x_1^{(1)},x_2^{(2)}) \pi(x_2^{(1)} | x_1^{(1)})$$
ä¹Ÿå°±æ˜¯ï¼š
$$\pi(A) \pi(x_2^{(2)} | x_1^{(1)})  = \pi(B) \pi(x_2^{(1)} | x_1^{(1)})$$
è§‚å¯Ÿä¸Šå¼å†è§‚å¯Ÿç»†è‡´å¹³ç¨³æ¡ä»¶çš„å…¬å¼ï¼Œæˆ‘ä»¬å‘ç°åœ¨$x_1 = x_1^{(1)}$è¿™æ¡ç›´çº¿ä¸Šï¼Œå¦‚æœç”¨æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$\pi(x_2| x_1^{(1)})$ä½œä¸ºé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œåˆ™ä»»æ„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è½¬ç§»æ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼è¿™çœŸæ˜¯ä¸€ä¸ªå¼€å¿ƒçš„å‘ç°ï¼ŒåŒæ ·çš„é“ç†ï¼Œåœ¨åœ¨$x_2 = x_2^{(1)}$è¿™æ¡ç›´çº¿ä¸Šï¼Œå¦‚æœç”¨æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$\pi(x_1| x_2^{(1)})$ä½œä¸ºé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œåˆ™ä»»æ„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è½¬ç§»ä¹Ÿæ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ã€‚é‚£æ˜¯å› ä¸ºå‡å¦‚æœ‰ä¸€ç‚¹$C(x_1^{(2)},x_2^{(1)})$,æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š
$$\pi(A) \pi(x_1^{(2)} | x_2^{(1)})  = \pi(C) \pi(x_1^{(1)} | x_2^{(1)})$$
åŸºäºä¸Šé¢çš„å‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æ„é€ åˆ†å¸ƒ$\pi(x_1,x_2)$çš„é©¬å°”å¯å¤«é“¾å¯¹åº”çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$P$ï¼š
$$P(A \to B) = \pi(x_2^{(B)}|x_1^{(1)})\;\; if\; x_1^{(A)} = x_1^{(B)} =x_1^{(1)} \\ P(A \to C) = \pi(x_1^{(C)}|x_2^{(1)})\;\; if\; x_2^{(A)} = x_2^{(C)} =x_2^{(1)} \\ P(A \to D) = 0\;\; else$$
äºæ˜¯è¿™ä¸ªäºŒç»´ç©ºé—´ä¸Šçš„é©¬æ°é“¾å°†æ”¶æ•›åˆ°å¹³ç¨³åˆ†å¸ƒ$\pi(x,y)$

äºŒç»´Gibbsé‡‡æ ·ï¼Œè¿™ä¸ªé‡‡æ ·éœ€è¦ä¸¤ä¸ªç»´åº¦ä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡ã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è¾“å…¥å¹³ç¨³åˆ†å¸ƒ$\pi(x_1,x_2)$ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼$n_1$ï¼Œéœ€è¦çš„æ ·æœ¬ä¸ªæ•°$n_2$
1. éšæœºåˆå§‹åŒ–åˆå§‹çŠ¶æ€å€¼$x_1^{(0)}$å’Œ$x_2^{(0)}$
1. for $t=0 ~to ~n_1 +n_2-1$: 
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_2|x_1^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_2^{t+1}$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_1|x_2^{(t+1)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_1^{t+1}$

æ ·æœ¬é›†$\{(x_1^{(n_1)}, x_2^{(n_1)}), (x_1^{(n_1+1)}, x_2^{(n_1+1)}), ...,  (x_1^{(n_1+n_2-1)}, x_2^{(n_1+n_2-1)})\}$å³ä¸ºæˆ‘ä»¬éœ€è¦çš„å¹³ç¨³åˆ†å¸ƒå¯¹åº”çš„æ ·æœ¬é›†ã€‚


æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è½®æ¢åæ ‡è½´ï¼Œé‡‡æ ·çš„è¿‡ç¨‹ä¸ºï¼š
$$(x_1^{(1)}, x_2^{(1)}) \to  (x_1^{(1)}, x_2^{(2)}) \to (x_1^{(2)}, x_2^{(2)}) \to ... \to (x_1^{(n_1+n_2-1)}, x_2^{(n_1+n_2-1)})$$

é©¬æ°é“¾æ”¶æ•›åï¼Œæœ€ç»ˆå¾—åˆ°çš„æ ·æœ¬å°±æ˜¯ $p(x_1,x_2)$çš„æ ·æœ¬ï¼Œè€Œæ”¶æ•›ä¹‹å‰çš„é˜¶æ®µç§°ä¸º burn-in periodã€‚é¢å¤–è¯´æ˜ä¸€ä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°æ•™ç§‘ä¹¦ä¸Šçš„ Gibbs Sampling ç®—æ³•å¤§éƒ½æ˜¯åæ ‡è½´è½®æ¢é‡‡æ ·çš„ï¼Œä½†æ˜¯è¿™å…¶å®æ˜¯ä¸å¼ºåˆ¶è¦æ±‚çš„ã€‚æœ€ä¸€èˆ¬çš„æƒ…å½¢å¯ä»¥æ˜¯ï¼Œåœ¨ $t$ æ—¶åˆ»ï¼Œå¯ä»¥åœ¨ $x_1$ è½´å’Œ $x_2$ è½´ä¹‹é—´éšæœºçš„é€‰ä¸€ä¸ªåæ ‡è½´ï¼Œç„¶åæŒ‰æ¡ä»¶æ¦‚ç‡åšè½¬ç§»ï¼Œé©¬æ°é“¾ä¹Ÿæ˜¯ä¸€æ ·æ”¶æ•›çš„ã€‚è½®æ¢ä¸¤ä¸ªåæ ‡è½´åªæ˜¯ä¸€ç§æ–¹ä¾¿çš„å½¢å¼ã€‚

ä¸Šé¢çš„è¿™ä¸ªç®—æ³•æ¨å¹¿åˆ°å¤šç»´çš„æ—¶å€™ä¹Ÿæ˜¯æˆç«‹çš„ã€‚æ¯”å¦‚ä¸€ä¸ª$n$ç»´çš„æ¦‚ç‡åˆ†å¸ƒ$\pi(x_1,x_2,...x_n)$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨$n$ä¸ªåæ ‡è½´ä¸Šè½®æ¢é‡‡æ ·ï¼Œæ¥å¾—åˆ°æ–°çš„æ ·æœ¬ã€‚å¯¹äºè½®æ¢åˆ°çš„ä»»æ„ä¸€ä¸ªåæ ‡è½´$x_i$ä¸Šçš„è½¬ç§»ï¼Œé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ä¸º$P(x_i|x_1,x_2,...,x_{i-1},x_{i+1},...,x_n)$ï¼Œå³å›ºå®š$nâˆ’1$ä¸ªåæ ‡è½´ï¼Œåœ¨æŸä¸€ä¸ªåæ ‡è½´ä¸Šç§»åŠ¨ã€‚

å¤šç»´Gibbsé‡‡æ ·è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è¾“å…¥å¹³ç¨³åˆ†å¸ƒ$\pi(x_1,x_2ï¼Œ...,x_n)$æˆ–è€…å¯¹åº”çš„æ‰€æœ‰ç‰¹å¾çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼$n_1$ï¼Œéœ€è¦çš„æ ·æœ¬ä¸ªæ•°$n_2$
1. éšæœºåˆå§‹åŒ–åˆå§‹çŠ¶æ€å€¼$(x_1^{(0)},x_2^{(0)},...,x_n^{(0)})$
1. for $t=0 ~to ~n_1 +n_2-1$: 
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_1|x_2^{(t)}, x_3^{(t)},...,x_n^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_1^{t+1}$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_2|x_1^{(t+1)}, x_3^{(t)}, x_4^{(t)},...,x_n^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_2^{t+1}$
    1. ...
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_j|x_1^{(t+1)}, x_2^{(t+1)},..., x_{j-1}^{(t+1)},x_{j+1}^{(t)}...,x_n^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_j^{t+1}$
    1. ...
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_n|x_1^{(t+1)}, x_2^{(t+1)},...,x_{n-1}^{(t+1)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_n^{t+1}$

æ ·æœ¬é›†$\{(x_1^{(n_1)}, x_2^{(n_1)},...,  x_n^{(n_1)}), ...,  (x_1^{(n_1+n_2-1)}, x_2^{(n_1+n_2-1)},...,x_n^{(n_1+n_2-1)})\}$å³ä¸ºæˆ‘ä»¬éœ€è¦çš„å¹³ç¨³åˆ†å¸ƒå¯¹åº”çš„æ ·æœ¬é›†ã€‚

æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹å’ŒLassoå›å½’çš„[åæ ‡è½´ä¸‹é™æ³•ç®—æ³•](https://www.cnblogs.com/pinard/p/6018889.html)éå¸¸ç±»ä¼¼ï¼Œåªä¸è¿‡Lassoå›å½’æ˜¯å›ºå®š$nâˆ’1$ä¸ªç‰¹å¾ï¼Œå¯¹æŸä¸€ä¸ªç‰¹å¾æ±‚æå€¼ã€‚è€ŒGibbsé‡‡æ ·æ˜¯å›ºå®š$nâˆ’1$ä¸ªç‰¹å¾åœ¨æŸä¸€ä¸ªç‰¹å¾é‡‡æ ·ã€‚

- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š
### å‚è€ƒæ–‡ç« 
[é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ç®—æ³•ï¼ˆMCMCï¼‰](https://zhuanlan.zhihu.com/p/37121528)
[MCMC(ä¸‰)MCMCé‡‡æ ·å’ŒM-Hé‡‡æ ·](https://www.cnblogs.com/pinard/p/6638955.html)
[ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹  - é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³• 296 é¡µ](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)

### å‚è€ƒæ–‡çŒ®
[19-1] Serfozo R. [Basics of applied stochastic processes](http://www.stat.yale.edu/~jtc5/251/readings/Basics%20of%20Applied%20Stochastic%20Processes_Serfozo.pdf). Springer, 2009.
[19-2] Metropolis N, Rosenbluth A W, Rosenbluth M N, et al. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 1953,21(6):1087-1092. 
[19-3] Geman S, Geman D. Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984,6:721-741
[19-4] Bishop C M. Pattern recognition and machine learning. Springer, 2006.
[19-5] Gilks W R, Richardson S, Spiegelhalter, DJ. Introducing Markov chain Monte Carlo. Markov Chain Monte Carlo in Practice, 1996.
[19-6] Andrieu C, De Freitas N, Doucet A, et al. An introduction to MCMC for machine learning. Machine Learning, 2003,50(1-2): 5-43.
[19-7] Hoff P. [A first course in Bayesian statistical methods](https://esl.hohoweiya.xyz/references/A_First_Course_in_Bayesian_Statistical_Methods.pdf). Springer, 2009.
[19-8] èŒ†è¯—æ¾ï¼Œç‹é™é¾™ï¼Œæ¿®æ™“é¾™. é«˜ç­‰æ•°ç†ç»Ÿè®¡. åŒ—äº¬ï¼šé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼Œ 1998.


## ç¬¬ 20 ç«  æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…
> ç”Ÿæˆæ¦‚ç‡æ¨¡å‹

æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…ï¼ˆ[latent Dirichlet allocationï¼ŒLDA](https://en.jinzhao.wiki/wiki/Latent_Dirichlet_allocation)ï¼‰ï¼Œä½œä¸ºåŸºäº è´å¶æ–¯å­¦ä¹ çš„è¯é¢˜æ¨¡å‹ï¼Œæ˜¯æ½œåœ¨è¯­ä¹‰åˆ†æã€æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æçš„æ‰©å±•ï¼Œäº2002å¹´ç”±Bleiç­‰æå‡ºã€‚LDAåœ¨æ–‡æœ¬æ•°æ®æŒ–æ˜ã€å›¾åƒå¤„ç†ã€ç”Ÿç‰©ä¿¡æ¯å¤„ç†ç­‰é¢†åŸŸè¢«å¹¿æ³›ä½¿ç”¨ã€‚
ä¹¦ä¸­çš„æ¨¡å‹ä»¥åŠå‚æ•°ï¼ˆæ¨å¯¼å‚è€ƒ[Latent Dirichlet allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)ä»¥åŠä¹¦ä¸­çš„æ¨å¯¼ï¼‰
![](https://www.researchgate.net/publication/326140642/figure/fig1/AS:644129876873217@1530583938944/Graphical-model-of-latent-Dirichlet-allocation-LDA.png)
![](https://www.researchgate.net/profile/Diego-Buenano-Fernandez/publication/339368709/figure/fig1/AS:860489982689280@1582168207260/Schematic-of-LDA-algorithm.png)

sklearnä¸­çš„æ¨¡å‹ä»¥åŠå‚æ•°ï¼ˆä¸‹é¢çš„ä»‹ç»ä»¥æ­¤å›¾ä¸ºå‡†ï¼‰
![https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation](https://scikit-learn.org/stable/_images/lda_model_graph.png)

> ä¸Šå›¾æ¥æºä»¥åŠè§£é‡Šï¼šâ€œ[Stochastic Variational Inference](http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf)â€ M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013

> åœ¨ä»‹ç»æ¦‚ç‡å›¾æ—¶æœ‰è®²è¿‡å„ç§å›¾å½¢ä»£è¡¨çš„å«ä¹‰ï¼Œè¿™é‡Œçš„å›¾æ›´å…¨é¢ä¹Ÿå¾ˆæ ‡å‡†ï¼šå®å¿ƒåœ†ç‚¹ä»£è¡¨è¶…å‚æ•°ï¼›ç®­å¤´ä»£è¡¨å› æœå…³ç³»ï¼›æ–¹æ¡†ä»£è¡¨é‡å¤ï¼›æ–¹æ¡†å³ä¸‹è§’çš„å­—æ¯ä»£è¡¨é‡å¤æ¬¡æ•°ï¼›æ— é˜´å½±åœ†åœˆä»£è¡¨éšå˜é‡ï¼›é˜´å½±åœ†åœˆä»£è¡¨è§‚æµ‹å˜é‡ï¼›

å‡è®¾æ¯ä¸ªæ–‡æœ¬ç”±è¯é¢˜çš„ä¸€ä¸ªå¤šé¡¹åˆ†å¸ƒè¡¨ç¤ºï¼Œæ¯ä¸ªè¯é¢˜ç”±å•è¯çš„ä¸€ä¸ªå¤šé¡¹åˆ†å¸ƒè¡¨ç¤ºï¼›
ç‰¹åˆ«å‡è®¾æ–‡æœ¬çš„è¯é¢˜åˆ†å¸ƒçš„å…ˆéªŒåˆ†å¸ƒæ˜¯ç‹„åˆ©å…‹é›·åˆ†å¸ƒï¼Œè¯é¢˜çš„å•è¯åˆ†å¸ƒçš„å…ˆéªŒåˆ†å¸ƒä¹Ÿæ˜¯ç‹„åˆ©å…‹é›·åˆ†å¸ƒã€‚


$D$ä¸ªæ–‡æ¡£ç»„æˆçš„è¯­æ–™åº“ï¼ˆcorpusï¼‰ï¼Œæ¯ä¸ªæ–‡æ¡£æœ‰$N_d$ä¸ªå•è¯ï¼›
å‡è®¾æ•´ä¸ªè¯­æ–™åº“æœ‰$K$ä¸ªè¯é¢˜ï¼ˆsklearnå‚æ•°n_componentsï¼‰ï¼›æ–¹æ¡†ä»£è¡¨é‡å¤æŠ½æ ·ï¼›
$w_{d,n}$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£ä¸­çš„ç¬¬$n$ä¸ªå•è¯ï¼ˆæ³¨æ„æ•´ä¸ªé›†åˆï¼ˆåœ†åœˆï¼‰æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼›
$N_d$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£ä¸­çš„å•è¯ä¸ªæ•°ï¼›
$Z_{d,n}$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£ä¸­çš„ç¬¬$n$ä¸ªè¯é¢˜ï¼›
$\theta_d$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£çš„è¯é¢˜åˆ†å¸ƒï¼ˆdocument topic distributionï¼‰å‚æ•°ï¼›
$\beta_k$è¡¨ç¤ºç¬¬$k$ä¸ªè¯é¢˜çš„å•è¯åˆ†å¸ƒï¼ˆtopic word distributionï¼‰å‚æ•°ï¼›
$\eta$ï¼ˆsklearnå‚æ•°topic_word_priorï¼‰
$\alpha$ï¼ˆsklearnå‚æ•°doc_topic_priorï¼‰

$${\displaystyle {\begin{aligned}{\boldsymbol {\beta }}_{k=1\dots K}&\sim \operatorname {Dirichlet} _{V}({\boldsymbol {\eta }})\\{\boldsymbol {\theta }}_{d=1\dots D}&\sim \operatorname {Dirichlet} _{K}({\boldsymbol {\alpha }})\\z_{d=1\dots D,n=1\dots N_{d}}&\sim \operatorname {Categorical} _{K}({\boldsymbol {\theta }}_{d})\\w_{d=1\dots D,n=1\dots N_{d}}&\sim \operatorname {Categorical} _{V}({\boldsymbol {\beta }}_{z_{dn}})\end{aligned}}}$$

å…¶ä¸­$V$å•è¯æ€»æ•°

æ¯ä¸ªdocçš„æ¯ä¸ªwordï¼Œéƒ½æ˜¯é€šè¿‡ä¸€å®šæ¦‚ç‡é€‰æ‹©äº†æŸä¸ªtopicå¹¶ä»è¿™ä¸ªtopicä¸­ä»¥ä¸€å®šæ¦‚ç‡é€‰æ‹©äº†æŸä¸ªwordã€‚
å…·ä½“ç”Ÿæˆï¼ˆdocï¼‰è¿‡ç¨‹ï¼š
1. æ ¹æ®è¶…å‚æ•°$\eta, K$ç”Ÿæˆ$K$ä¸ªtopicçš„wordåˆ†å¸ƒçš„å‚æ•°$\beta_k$
$\beta_k âˆ¼ Dir(\eta) \text{ for } k \in \{1,...,K\}$
1. For each doc $d \in \{1,...,D\}$
    ---$\theta_d âˆ¼ Dir(\alpha)$ æ¯ä¸ªdocçš„topicåˆ†å¸ƒ
    ---For each word $n \in \{1,...,N_d\}$
    ------$Z_{d,n} âˆ¼ Mult(\theta_d)$ ç¡®å®šä¸€ä¸ªtopic
    ------$w_{d,n} âˆ¼ Mult(\beta_{Z_{d,n}})$ æ ¹æ®topicç”Ÿæˆword($Z_{d,n} \in \{1,...,K\}$)




Var| Type |Conditional|Param|Relevant Expectations
---|---|---|---|---
$Z_{d,n}$|Multinomial|$\log \theta_{dk} + \log \beta_{k,w_{d,n}}$|$\phi_{dn}$|$E[Z^k_{dn}]=\phi_{dn}^k$
$\theta_d$|Dirichlet|$\alpha + \sum_{n=1}^{N_d} Z_{dn}$|$\gamma_d$|$E[\log \theta_{dk}]=\Psi(\gamma_{dk}) -\sum_{j=1}^K \Psi(\gamma_{dj})$
$\beta_k$|Dirichlet|$\eta + \sum_{d=1}^D\sum_{n=1}^{N_d} Z_{dn}^k w_{dn}$|$\lambda_k$|$$E[\log \beta_{kv}]=\Psi(\lambda_{kv}) -\sum_{y=1}^V \Psi(\lambda_{ky})$


- **æ¨¡å‹**ï¼š
åéªŒåˆ†å¸ƒposterior distributionï¼š
$$p(z, \theta, \beta |w, \alpha, \eta) =  \frac{p(z, \theta, \beta, w|\alpha, \eta)}{p(w|\alpha, \eta)}$$
å·²çŸ¥$w, \alpha, \eta$ï¼Œæ±‚$z, \theta, \beta$
- **ç­–ç•¥**ï¼š
å‡è®¾ä¸‰ä¸ªéšå˜é‡$(z, \theta, \beta)$åˆ†åˆ«ç”±ç‹¬ç«‹åˆ†å¸ƒ$(\phi,\gamma,\lambda)$å½¢æˆï¼Œåˆ™è”åˆçš„å˜åˆ†åˆ†å¸ƒä¸º$q(z, \theta, \beta |\phi,\gamma,\lambda)$ï¼Œå˜åˆ†æ¨æ–­çš„ç›®çš„å°±æ˜¯ç”¨$q(z, \theta, \beta |\phi,\gamma,\lambda)$æ¥è¿‘ä¼¼$p(z, \theta, \beta |w, \alpha, \eta)$
$$(\phi^*,\gamma^*,\lambda^*) = \argmin_{\phi,\gamma,\lambda} KL(q\|p)$$
ç›´æ¥æ±‚è§£ä¸å¥½æ±‚ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹è¯æ®ï¼ˆæ•°æ®ï¼‰
$$\log p(w|\alpha, \eta) = \log p(z, \theta, \beta, w|\alpha, \eta) - \log p(z, \theta, \beta |w, \alpha, \eta) \\= \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}  - \log \frac{ p(z, \theta, \beta |w, \alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}$$
ç­‰å¼ä¸¤è¾¹åŒæ—¶å¯¹$q(z, \theta, \beta |\phi,\gamma,\lambda)$æ±‚æœŸæœ›
$$LHS = E_q[\log p(w|\alpha, \eta)] = \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log p(w|\alpha, \eta) dz d\theta d\beta = \log p(w|\alpha, \eta)$$
$$RHS = E_q[\log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}  - \log \frac{ p(z, \theta, \beta |w, \alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}] \\= \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}dz d\theta d\beta - \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{ p(z, \theta, \beta |w, \alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)} dz d\theta d\beta \\= \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}dz d\theta d\beta + KL(q\|p)$$
ä»¤
$$ELBO = \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}dz d\theta d\beta \\=   E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]$$

$$\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
  E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]$$
æˆ‘ä»¬å¸Œæœ›æœ€å¤§è¯æ®ï¼Œè€ŒKLæœ€å°ï¼Œæ‰€ä»¥æœ€ç»ˆæ˜¯ï¼š
$$\max ELBO$$
æ¥ç€å°±ç”¨EMç®—æ³•
- **ç®—æ³•**ï¼š
Gibbsé‡‡æ ·å’Œå˜åˆ†EMç®—æ³•

### å‚è€ƒæ–‡ç« 
[spark-ml LDA](/BigData/spark-ml-source-analysis/èšç±»/LDA/lda.md)

[å¾äº¦è¾¾æœºå™¨å­¦ä¹ ï¼šVariational Inference for LDA ç”¨å˜åˆ†æ¨æ–­åšLDAã€2015å¹´ç‰ˆ-å…¨é›†ã€‘](https://www.bilibili.com/video/BV1pp411d7US)

[ä¸»é¢˜æ¨¡å‹-æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…-Latent Dirichlet Allocation(LDA)](https://www.bilibili.com/video/BV1t54y127U8)


### é™„åŠ çŸ¥è¯†
#### Dirichlet Process ç‹„åˆ©å…‹é›·è¿‡ç¨‹
[å¾äº¦è¾¾æœºå™¨å­¦ä¹ ï¼šDirichlet Process ç‹„åˆ©å…‹é›·è¿‡ç¨‹ã€2015å¹´ç‰ˆ-å…¨é›†ã€‘](https://www.bilibili.com/video/BV1Tp411R7Sf)

#### æŒ‡æ•°æ—åˆ†å¸ƒ
å‚è€ƒï¼š[æ¦‚ç‡åˆ†å¸ƒ](../å›¾è§£æ•°å­¦/æ¦‚ç‡åˆ†å¸ƒ.md)

**ç‹„åˆ©å…‹é›·åˆ†å¸ƒ**ï¼ˆ[Dirichlet distribution](https://en.jinzhao.wiki/wiki/Dirichlet_distribution)ï¼‰

**å•çº¯å½¢**ï¼ˆ[Simplex](https://en.jinzhao.wiki/wiki/Simplex)ï¼‰

```mermaid
graph LR
    x1["ç‹„åˆ©å…‹é›·åˆ†å¸ƒ"]
    x2["å¤šé¡¹åˆ†å¸ƒ"]
    x3["ç±»åˆ«åˆ†å¸ƒ"]
    x4["è´å¡”åˆ†å¸ƒ"]
    x5["äºŒé¡¹åˆ†å¸ƒ"]
    x6["ä¼¯åŠªåˆ©åˆ†å¸ƒ"]
    x1--"å…±è½­å…ˆéªŒ"-->x2
    x1--"åŒ…å«"-->x4
    x2--"åŒ…å«"-->x3
    x2--"åŒ…å«"-->x5
    x3--"åŒ…å«"-->x6
    x4--"å…±è½­å…ˆéªŒ"-->x5
    x5--"åŒ…å«"-->x6
```

å‰é¢è®²è´å¶æ–¯ä¼°è®¡æ—¶æœ‰æåˆ°**å…±è½­å…ˆéªŒ**ï¼ˆ[Conjugate prior](https://en.jinzhao.wiki/wiki/Conjugate_prior)ï¼‰
å¦‚æœå…ˆéªŒåˆ†å¸ƒ prior å’ŒåéªŒåˆ†å¸ƒ posterior å±äºåŒä¸€åˆ†å¸ƒç°‡ï¼Œåˆ™ prior ç§°ä¸º likehood çš„å…±è½­å…ˆéªŒã€‚

$$\underbrace{p(\theta|data)}_{\text{åéªŒåˆ†å¸ƒposterior distribution}} = \frac{\overbrace{p(data|\theta)}^{\text{ä¼¼ç„¶(æ•°æ®)likelihood }}\overbrace{p(\theta)}^{\text{ å…ˆéªŒåˆ†å¸ƒprior distribution}}}{\underbrace{p(data)}_{\text{è¯æ®evidence}}}$$

å¸¸è§çš„å…±è½­å…ˆéªŒï¼š
- likehood ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œprior ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸ºé«˜æ–¯åˆ†å¸ƒã€‚
- likehood ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹å¼åˆ†å¸ƒï¼‰ï¼Œprior ä¸º beta åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸º beta åˆ†å¸ƒã€‚
- likehood ä¸ºå¤šé¡¹å¼åˆ†å¸ƒï¼Œprior ä¸º Dirichlet åˆ†å¸ƒï¼ˆbeta åˆ†å¸ƒçš„ä¸€ä¸ªæ‰©å±•ï¼‰ï¼Œåˆ™ posterior ä¹Ÿä¸º Dirichletï¼ˆç‹„åˆ©å…‹é›·ï¼‰åˆ†å¸ƒã€‚beta åˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯ dirichlet åˆ†å¸ƒçš„ç‰¹æ®Šæƒ…å†µã€‚

### å‚è€ƒæ–‡çŒ®
[20-1] Blei D M, Ng A Y, Jordan M I. Latent Dirichlet allocation. In: Advances in Neural Information Processing Systems 14. MIT Press, 2002.
[20-2] Blei D M, Ng A Y, Jordan M I. [Latent Dirichlet allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). Journal of Machine Learning Research, 2003, 3:933-1022.
[20-3] Griffiths T L, Steyvers M. Finding scientific topics. Proceedings of the National Academy of Science, 2004, 101:5288-5235.
[20-4] Steyvers M, Griffiths T. Probabilistic topic models. In: Landauer T, McNamara D, Dennis S, et al. (eds.) Handbook of Latent Semantic Analysis, Psychology Press, 2014.
[20-5] Gregor Heinrich. Parameter estimation for text analysis. Techniacl note, 2004.
[20-6] Blei D M, Kucukelbir A, McAuliffe J D. Variational inference: a review for statisticians. Journal of the American Statistical Association, 2017, 112(518).
[20-7] Newman D, Smyth P, Welling M, Asuncion A U. Distributed inference for latent Dirichlet allocation. In: Advances in Neural Information Processing Systems, 2008: 1081-1088
[20-8] Porteous I, Newman D, Ihler A, et al. Fast collapsed Gibbs sampling for latent Dirichlet allocation. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2008: 569-577.
[20-9] Hoffiman M, Bach F R, Blei D M. [Online learning for latent Dirichlet allocation](https://papers.nips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf). In: Advances in Neural Information Processing Systems, 2010:856-864.

## ç¬¬ 21 ç«  PageRankç®—æ³•
[PageRank](https://en.jinzhao.wiki/wiki/PageRank)æ˜¯è¡¡é‡ç½‘ç«™é¡µé¢é‡è¦æ€§çš„ä¸€ç§æ–¹å¼ã€‚PageRank çš„å·¥ä½œåŸç†æ˜¯è®¡ç®—é¡µé¢é“¾æ¥çš„æ•°é‡å’Œè´¨é‡ï¼Œä»¥ç¡®å®šå¯¹ç½‘ç«™é‡è¦æ€§çš„ç²—ç•¥ä¼°è®¡ã€‚
ç›®å‰ï¼ŒPageRank å¹¶ä¸æ˜¯ Google ç”¨äºå¯¹æœç´¢ç»“æœè¿›è¡Œæ’åºçš„å”¯ä¸€ç®—æ³•ï¼Œä½†å®ƒæ˜¯è¯¥å…¬å¸ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªç®—æ³•ï¼Œå¹¶ä¸”æ˜¯æœ€è‘—åçš„ç®—æ³•ã€‚

PageRank æ˜¯ä¸€ç§é“¾æ¥åˆ†æç®—æ³•ï¼Œå®ƒä¸ºè¶…é“¾æ¥æ–‡æ¡£é›†ï¼ˆä¾‹å¦‚ä¸‡ç»´ç½‘ï¼‰çš„æ¯ä¸ªå…ƒç´ åˆ†é…æ•°å­—æƒé‡ï¼Œç›®çš„æ˜¯â€œè¡¡é‡â€å…¶åœ¨é›†åˆä¸­çš„ç›¸å¯¹é‡è¦æ€§ã€‚è¯¥ç®—æ³•å¯ä»¥åº”ç”¨äºå…·æœ‰ç›¸äº’å¼•ç”¨å’Œå¼•ç”¨çš„ä»»ä½•å®ä½“é›†åˆã€‚å®ƒåˆ†é…ç»™ä»»ä½•ç»™å®šå…ƒç´ $E$çš„æ•°å­—æƒé‡ç§°ä¸º$E$çš„PageRankï¼Œè¡¨ç¤ºä¸º ${\displaystyle PR(E).}$ã€‚

ä¸€ä¸ªçŠ¶æ€è½¬ç§»çŸ©é˜µçš„å¹³ç¨³åˆ†å¸ƒå°±å¯¹åº”å„ä¸ªå…ƒç´ çš„PageRank

- **æ¨¡å‹**ï¼š
$$MR = R$$
å«æœ‰nä¸ªèŠ‚ç‚¹çš„æœ‰å‘å›¾æ˜¯å¼ºè¿é€šä¸”éå‘¨æœŸçš„ï¼Œåœ¨å…¶åŸºç¡€ä¸Šå®šä¹‰éšæœºæ¸¸èµ°æ¨¡å‹ï¼ˆå³ä¸€é˜¶é©¬å°”å¯å¤«é“¾å…·æœ‰å¹³ç¨³åˆ†å¸ƒï¼‰ï¼›
$M=[m_{ij}]_{n \times n}$ æ˜¯é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œå…¶ä¸­çš„å…ƒç´  $m_{ij}$è¡¨ç¤ºèŠ‚ç‚¹$j$è·³åˆ°èŠ‚ç‚¹$i$çš„æ¦‚ç‡ï¼›$R$æ˜¯å¹³ç¨³åˆ†å¸ƒå‘é‡ï¼Œç§°ä¸ºè¿™ä¸ªæœ‰å‘å›¾çš„PageRank
- **ç­–ç•¥**ï¼š
$$\lim_{t \to \infty} M^tR_0 = R$$
- **ç®—æ³•**ï¼š
è¿­ä»£ï¼Œå¹‚æ³•ï¼Œä»£æ•°ç®—æ³•

### å‚è€ƒæ–‡çŒ®
[21-1] Page L, Brin S, Motwani R, et al. The PageRank citation ranking: bringing order to the Web. Stanford University, 1999.
[21-2] Rajaraman A, Ullman J D. Mining of massive datasets. Cambridge University Press, 2014.
[21-3] Liu B. Web data mining: exploring Hyperlinks, contents, and usage data. Springer Science & Business Media, 2007.
[21-4] Serdozo R. Basics of applied stochastic processes. Springer, 2009.
[21-5] Kleinberg J M. Authoritative sources in a hyperlinked environment. Journal of the ACM(JACM), 1999,46(5):604-632.
[21-6] Liu Y, Gao B, Liu T Y, et al. BrowseRank: letting Web users vote for page importance. Proceedings of the 31st SIGIR Conference, 2008:451-458 
[21-7] Jeh G, Widom J. Scaling Personalized Web search. Proceedings of the 12th WWW Conference, 2003: 271-279.
[21-8] Haveliwala T H. Topic-sensitive PageRank. Proceedings of the 11th WWW Conference, 2002: 517-526.
[21-9] GyÃ¶ngyi Z, Garcia-Molina H, Pedersen J. Combating Web spam with TrustRank. Proceedings of VLDB Conference, 2004:576-587.

## ç¬¬ 22 ç«  æ— ç›‘ç£å­¦ä¹ æ–¹æ³•æ€»ç»“

æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ä¹‹é—´çš„å…³ç³»
![](https://img-blog.csdnimg.cn/20200601152905315.png)

æ— ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ç‰¹ç‚¹
.|æ–¹æ³•|æ¨¡å‹|ç­–ç•¥|ç®—æ³•
---|---|---|---|---
èšç±»| å±‚æ¬¡èšç±»|èšç±»æ ‘|ç±»å†…æ ·æœ¬è·ç¦»æœ€å°|å¯å‘å¼ç®—æ³•
èšç±»|kå‡å€¼èšç±»|kä¸­å¿ƒèšç±»|æ ·æœ¬ä¸ç±»ä¸­å¿ƒè·ç¦»æœ€å°|è¿­ä»£ç®—æ³•
èšç±»|é«˜æ–¯æ··åˆæ¨¡å‹|é«˜æ–¯æ··åˆæ¨¡å‹|ä¼¼ç„¶å‡½æ•°æœ€å¤§|EMç®—æ³•
é™ç»´|PCA|ä½ç»´æ­£äº¤ç©ºé—´|æ–¹å·®æœ€å¤§|SVD
è¯é¢˜åˆ†æ|LSA|çŸ©é˜µåˆ†è§£æ¨¡å‹|å¹³æ–¹æŸå¤±æœ€å°|SVD
è¯é¢˜åˆ†æ|NMF|çŸ©é˜µåˆ†è§£æ¨¡å‹|å¹³æ–¹æŸå¤±æœ€å°|éè´ŸçŸ©é˜µåˆ†è§£
è¯é¢˜åˆ†æ|PLSA|PLSAæ¨¡å‹|ä¼¼ç„¶å‡½æ•°æœ€å¤§|EMç®—æ³•
è¯é¢˜åˆ†æ|LDA|LDAæ¨¡å‹|åéªŒæ¦‚ç‡ä¼°è®¡|å‰å¸ƒæ–¯æŠ½æ ·ï¼Œå˜åˆ†æ¨ç†
å›¾åˆ†æ|PageRank|æœ‰å‘å›¾ä¸Šçš„é©¬å°”å¯å¤«é“¾|å¹³ç¨³åˆ†å¸ƒæ±‚è§£|å¹‚æ³•

å«æœ‰éšå˜é‡æ¦‚ç‡æ¨¡å‹çš„å­¦ä¹ æ–¹æ³•çš„ç‰¹ç‚¹
ç®—æ³•|åŸºæœ¬åŸç†|æ”¶æ•›æ€§|æ”¶æ•›é€Ÿåº¦|å®ç°éš¾æ˜“åº¦|é€‚åˆé—®é¢˜
---|---|---|---|---|---
EMç®—æ³•| è¿­ä»£è®¡ç®—ã€åéªŒæ¦‚ç‡ä¼°è®¡|æ”¶æ•›äºå±€éƒ¨æœ€ä¼˜|è¾ƒå¿«|å®¹æ˜“|ç®€å•æ¨¡å‹
å˜åˆ†æ¨ç†|è¿­ä»£è®¡ç®—ã€åéªŒæ¦‚ç‡è¿‘ä¼¼ä¼°è®¡|æ”¶æ•›äºå±€éƒ¨æœ€ä¼˜|è¾ƒæ…¢|è¾ƒå¤æ‚|å¤æ‚æ¨¡å‹
å‰å¸ƒæ–¯æŠ½æ ·|éšæœºæŠ½æ ·ã€åéªŒæ¦‚ç‡ä¼°è®¡|ä»¥æ¦‚ç‡æ”¶æ•›äºå…¨å±€æœ€ä¼˜|è¾ƒæ…¢|å®¹æ˜“|å¤æ‚æ¨¡å‹


çŸ©é˜µåˆ†è§£çš„è§’åº¦çœ‹è¯é¢˜æ¨¡å‹(Bè¡¨ç¤º[Bregmanæ•£åº¦](https://en.jinzhao.wiki/wiki/Bregman_divergence))
æ–¹æ³•|ä¸€èˆ¬æŸå¤±å‡½æ•°$B(D\|UV)$|çŸ©é˜µ$U$çš„çº¦æŸæ¡ä»¶|çŸ©é˜µ$V$çš„çº¦æŸæ¡ä»¶
---|---|---|---
LSA| $\|D-UV\|^2_F$|$U^TU = I$|$VV^T=\Lambda^2$
NMF| $\|D-UV\|^2_F$|$u_{mk} \geq 0$|$u_{kn} \geq 0$
PLSA|$\sum_{mn}d_{mn} \log\frac{d_{mn}}{(UV)_{mn}}$| $U^T1=1 \\ u_{mk} \geq 0$ |$V^T1=1 \\ u_{kn} \geq 0$

### é™„åŠ çŸ¥è¯†
#### çº¿æ€§ä»£æ•°
[ã€å®Œæ•´ç‰ˆ-éº»çœç†å·¥-çº¿æ€§ä»£æ•°ã€‘å…¨34è®²+é…å¥—æ•™æ](https://www.bilibili.com/video/BV1ix411f7Yp)

[ä¸­ç§‘å¤§-å‡¸ä¼˜åŒ–](https://www.bilibili.com/video/BV1Jt411p7jE)

#### å¾äº¦è¾¾æœºå™¨å­¦ä¹ 
EMï¼Œ MCMCï¼ŒHMMï¼Œ LDA, å˜åˆ†æ¨æ–­ï¼Œ æŒ‡æ•°æ—åˆ†å¸ƒç­‰è®²çš„éƒ½éå¸¸å¥½ï¼Œ[Bç«™](https://space.bilibili.com/97068901/)
#### ã€æœºå™¨å­¦ä¹ ã€‘ç™½æ¿æ¨å¯¼ç³»åˆ—
å¼ºçƒˆå»ºè®®çœ‹å®Œï¼Œ[Bç«™](https://space.bilibili.com/327617676)

### å‚è€ƒæ–‡çŒ®
[22-1] Singh A P, Gordon G J. A unified view of matrix factorization models. In: Daelemans W, Goethals B, Morik K,(eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2008. Lecture Notes in Computer Science, vol 5212. Berlin: Springer, 2008.
[22-2] Wang Q, Xu J, Li H, et al. Regularized latent semantic indexing:a new approach to large-scale topic modeling. ACM Transactions on Information Systems (TOIS), 2013, 31(1), 5.

<!-- {% endraw %} -->