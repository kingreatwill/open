[TOC]

# ç»Ÿè®¡å­¦ä¹ æ–¹æ³•

[ç¬¬ä¸€ç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-first_edition)

[ç¬¬äºŒç‰ˆ](https://github.com/kingreatwill/files/tree/main/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/book/Lihang-second_edition)

## ç¬¬ 1 ç«  ç»Ÿè®¡å­¦ä¹ åŠç›‘ç£å­¦ä¹ æ¦‚è®º

**ç»Ÿè®¡å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹æ˜¯**ï¼š

1. ç»Ÿè®¡å­¦ä¹ ä»¥è®¡ç®—æœºåŠç½‘ç»œä¸ºå¹³å°ï¼Œæ˜¯å»ºç«‹åœ¨è®¡ç®—æœºåŠç½‘ç»œä¹‹ä¸Šçš„ï¼›
2. ç»Ÿè®¡å­¦ä¹ ä»¥æ•°æ®ä¸ºç ”ç©¶å¯¹è±¡ï¼Œæ˜¯æ•°æ®é©±åŠ¨çš„å­¦ç§‘ï¼›
3. ç»Ÿè®¡å­¦ä¹ çš„ç›®çš„æ˜¯å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
4. ç»Ÿè®¡å­¦ä¹ ä»¥æ–¹æ³•ä¸ºä¸­å¿ƒï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•æ„å»ºæ¨¡å‹å¹¶åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ä¸åˆ†æï¼›
5. ç»Ÿè®¡å­¦ä¹ æ˜¯æ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€è®¡ç®—ç†è®ºã€æœ€ä¼˜åŒ–ç†è®ºåŠè®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªé¢†åŸŸçš„äº¤å‰å­¦ç§‘ï¼Œå¹¶ä¸”åœ¨å‘å±•ä¸­é€æ­¥å½¢æˆç‹¬è‡ªçš„ç†è®ºä½“ç³»ä¸æ–¹æ³•è®ºã€‚

**å‡è®¾ç©ºé—´(hypothesis space)**ï¼š
$$\mathcal H = \{ f(x;\theta) | \theta \in \mathbb{R}^D\} \\ or \quad \mathcal F = \{P|P(Y|X;\theta),\theta \in \mathbb{R}^D\}$$
å…¶ä¸­$f(x; \theta)$æ˜¯å‚æ•°ä¸º$\theta$ çš„å‡½æ•°ï¼ˆ**å†³ç­–å‡½æ•°**ï¼‰ï¼Œä¹Ÿç§°ä¸ºæ¨¡å‹ï¼ˆModelï¼‰ï¼Œå‚æ•°å‘é‡$\theta$å–å€¼ä¸$D$ç»´æ¬§å¼ç©ºé—´$\mathbb{R}^D$,ä¹Ÿç§°ä¸ºå‚æ•°ç©ºé—´(parameter space)ï¼Œ$D$ ä¸ºå‚æ•°çš„æ•°é‡(ç»´åº¦)

æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæˆ–å†³ç­–å‡½æ•°

**ç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰**ï¼š
æ¯ä¸ªå…·ä½“çš„è¾“å…¥æ˜¯ä¸€ä¸ªå®ä¾‹ï¼ˆinstanceï¼‰ï¼Œé€šå¸¸ç”±ç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰è¡¨ç¤ºã€‚è¿™
æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾å‘é‡å­˜åœ¨çš„ç©ºé—´ç§°ä¸ºç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰ã€‚ç‰¹å¾ç©ºé—´çš„æ¯ä¸€ç»´å¯¹åº”äº
ä¸€ä¸ªç‰¹å¾ã€‚

> è¾“å…¥ç©ºé—´ä¸­çš„ä¸€ä¸ªè¾“å…¥å‘é‡$x = (x_1,x_2)$ï¼Œåœ¨å¤šé¡¹å¼æ¨¡å‹ä¸­ç‰¹å¾å‘é‡æ˜¯($x_1^2,x_1x_2,x_2^2,...$)
> ä¸€èˆ¬è¯´çš„çº¿æ€§æ¨¡å‹ï¼ŒæŒ‡çš„æ˜¯ç‰¹å¾å‘é‡çš„çº¿æ€§ç»„åˆï¼Œè€Œä¸æ˜¯æŒ‡è¾“å…¥å‘é‡ï¼Œæ‰€ä»¥è¯´æ¨¡å‹éƒ½æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„

**ç»Ÿè®¡å­¦ä¹ çš„ä¸‰è¦ç´ **ï¼š

1. æ¨¡å‹çš„å‡è®¾ç©ºé—´(hypothesis space)ï¼Œç®€ç§°ï¼šæ¨¡å‹(model)ã€‚å‡è®¾ç©ºé—´å³æˆ‘ä»¬å¯¹æ¨¡å‹å½¢å¼çš„å…ˆéªŒå‡è®¾ï¼Œæœ€ç»ˆæˆ‘ä»¬æ±‚å¾—çš„æ¨¡å‹å¿…å®šç¬¦åˆæˆ‘ä»¬å¯¹æ¨¡å‹å½¢å¼çš„å…ˆéªŒå‡è®¾ã€‚
2. æ¨¡å‹é€‰æ‹©çš„å‡†åˆ™(evaluation criterion)ï¼Œç®€ç§°ï¼šç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™ã€‚å³æˆ‘ä»¬ç”¨ä»€ä¹ˆæ ‡å‡†æ¥è¯„ä»·ä¸€ä¸ªæ¨¡å‹çš„å¥½åã€‚ç­–ç•¥å†³å®šäº†æˆ‘ä»¬ä»å‡è®¾ç©ºé—´ä¸­é€‰æ‹©æ¨¡å‹çš„åå¥½ã€‚
3. æ¨¡å‹å­¦ä¹ çš„ç®—æ³•(algorithm)ï¼Œç®€ç§°ï¼šç®—æ³•(algorithm)ã€‚ä¼˜åŒ–ç®—æ³•æŒ‡çš„æ˜¯é€šè¿‡ä»€ä¹ˆæ ·çš„æ–¹å¼è°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ç»“æ„æˆ–æ¨¡å‹è¶…å‚æ•°å–å€¼ï¼Œä½¿å¾—æ¨¡å‹çš„ç›®æ ‡å‡½æ•°å–å€¼ä¸æ–­é™ä½ã€‚ä¼˜åŒ–ç®—æ³•å†³å®šäº†æˆ‘ä»¬ç”¨ä»€ä¹ˆæ ·çš„æ­¥éª¤åœ¨å‡è®¾ç©ºé—´ä¸­å¯»æ‰¾åˆé€‚çš„æ¨¡å‹ã€‚

> ä»¥çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ä¸ºä¾‹ï¼š
> æ¨¡å‹ï¼š $f(x;w,b) = w^Tx +b$
> ç­–ç•¥(strategy)æˆ–è€…å­¦ä¹ å‡†åˆ™: å¹³æ–¹æŸå¤±å‡½æ•° $\mathcal L(y,\hat{y}) = (y-f(x,\theta))^2$
> ç®—æ³•ï¼šè§£æè§£ analytical solution(é—­å¼è§£ closed-form solution)å’Œæ•°å€¼è§£ numerical solutionï¼Œå¦‚ï¼šclosed-form çš„æœ€å°äºŒä¹˜çš„è§£ä»¥åŠæ¢¯åº¦ä¸‹é™æ³•

**æœºå™¨å­¦ä¹ çš„å®šä¹‰**ï¼š

```mermaid
graph LR;
    F(["æœªçŸ¥çš„ç›®æ ‡å‡½æ•°(ç†æƒ³ä¸­å®Œç¾çš„å‡½æ•°)ï¼šğ‘“: ğ’™âŸ¶ğ‘¦"])-->D["è®­ç»ƒæ ·æœ¬D:{(ğ’™Â¹,ğ‘¦Â¹),...,(ğ’™â¿,ğ‘¦â¿)}"];
    D-->A{{"ç®—æ³•"}}
    H{{"å‡è®¾ç©ºé—´"}}-->A
    A-->G["æ¨¡å‹ gâ‰ˆf"]
```

ä½¿ç”¨è®­ç»ƒæ•°æ®æ¥è®¡ç®—æ¥è¿‘ç›®æ ‡ ğ‘“ çš„å‡è®¾ï¼ˆhypothesis ï¼‰g ï¼ˆæ¥è‡ªï¼š[Machine Learning Foundationsï¼ˆæœºå™¨å­¦ä¹ åŸºçŸ³ï¼‰- the learning problem,25 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/01_handout.pdf)ï¼‰

**ç›‘ç£å­¦ä¹ **ï¼š
ç›‘ç£å­¦ä¹ (supervised learning)æ˜¯æŒ‡ä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„çš„ç»Ÿè®¡è§„å¾‹**ã€‚

è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºè¿ç»­å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**å›å½’é—®é¢˜**ï¼›
è¾“å‡ºå˜é‡ä¸ºæœ‰é™ä¸ªç¦»æ•£å˜é‡çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**åˆ†ç±»é—®é¢˜**ï¼›
è¾“å…¥å˜é‡ä¸è¾“å‡ºå˜é‡å‡ä¸ºå˜é‡åºåˆ—çš„é¢„æµ‹é—®é¢˜ç§°ä¸º**æ ‡æ³¨é—®é¢˜**(åˆ†ç±»é—®é¢˜çš„æ¨å¹¿ï¼Œå¦‚ï¼šéšé©¬å°”å¯å¤«æ¨¡å‹ HMMï¼Œæ¡ä»¶éšæœºåœº CRF)ã€‚

ç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥æ˜¯æ¦‚ç‡æ¨¡å‹æˆ–éæ¦‚ç‡æ¨¡å‹ï¼Œç”±**æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ**$P(Y|X)$æˆ–**å†³ç­–å‡½æ•°ï¼ˆdecision functionï¼‰**$Y=f(X)$è¡¨ç¤ºï¼Œéšå…·ä½“å­¦ä¹ æ–¹æ³•è€Œå®šã€‚å¯¹å…·ä½“çš„è¾“å…¥è¿›è¡Œç›¸åº”çš„è¾“å‡ºé¢„æµ‹æ—¶ï¼Œå†™ä½œ$P(y|x)$æˆ–$Y=f(x)$ã€‚
$$y =\displaystyle\argmax_{y}  P(y|x)$$

**è”åˆæ¦‚ç‡åˆ†å¸ƒ**ï¼š
ç›‘ç£å­¦ä¹ å‡è®¾è¾“å…¥ä¸è¾“å‡ºçš„éšæœºå˜é‡ X å’Œ Y éµå¾ªè”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ã€‚$P(X,Y)$è¡¨ç¤ºåˆ†å¸ƒå‡½æ•°ï¼Œæˆ–åˆ†å¸ƒå¯†åº¦å‡½æ•°ã€‚æ³¨æ„ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå‡å®šè¿™ä¸€è”åˆæ¦‚ç‡åˆ†å¸ƒå­˜åœ¨ï¼Œä½†å¯¹å­¦ä¹ ç³»ç»Ÿæ¥è¯´ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å…·ä½“å®šä¹‰æ˜¯æœªçŸ¥çš„ã€‚**è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®è¢«çœ‹ä½œæ˜¯ä¾è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(X,Y)$ç‹¬ç«‹åŒåˆ†å¸ƒäº§ç”Ÿçš„**ã€‚
ç»Ÿè®¡å­¦ä¹ å‡è®¾æ•°æ®å­˜åœ¨ä¸€å®šçš„ç»Ÿè®¡è§„å¾‹ï¼Œ$X$å’Œ$Y$å…·æœ‰è”åˆæ¦‚ç‡åˆ†å¸ƒçš„å‡è®¾å°±æ˜¯ç›‘ç£å­¦ä¹ å…³äºæ•°æ®çš„åŸºæœ¬å‡è®¾ã€‚

**éç›‘ç£å­¦ä¹ **ï¼š
éç›‘ç£å­¦ä¹ (unsupervised learning)æ˜¯æŒ‡ä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æœ¬è´¨æ˜¯**å­¦ä¹ æ•°æ®ä¸­çš„ç»Ÿè®¡è§„å¾‹æˆ–æ½œåœ¨ç»“æ„**ã€‚

éç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•°$z = g(x)$æˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(z|x)$ ï¼ˆè¾“å‡º$z$å¯ä»¥æ˜¯**èšç±»**æˆ–è€…**é™ç»´**ï¼‰
$$z =\displaystyle\argmax_{z}  P(z|x)$$
ä»¥åŠ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x|z)$ ï¼ˆç”¨æ¥åš**æ¦‚ç‡å¯†åº¦ä¼°è®¡**ï¼Œæ¯”å¦‚ GMM ä¸­$P(x|z)$å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå¦‚æœå‡è®¾çŸ¥é“æ•°æ®æ¥è‡ªå“ªä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå³çŸ¥é“$z$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼°è®¡ç›¸å…³å‚æ•°ï¼‰ã€‚

[æ ¸å¯†åº¦ä¼°è®¡ Kernel Density Estimation.](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) - åº”ç”¨å¯†åº¦ä¼°è®¡æ£€æµ‹ç¦»ç¾¤å€¼ï¼ˆoutlierï¼‰çš„[LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)

**æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰ä¸éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰æˆ–è€…ç¡®å®šæ€§æ¨¡å‹ï¼ˆdeterministic modelï¼‰**ï¼š

æ¦‚ç‡æ¨¡å‹ï¼ˆprobabilistic modelï¼‰- æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ P(y|x)å’Œ éæ¦‚ç‡æ¨¡å‹ï¼ˆnon-probabilistic modelï¼‰ - å‡½æ•° y=f(x)å¯ä»¥**ç›¸äº’è½¬åŒ–**ï¼Œæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæœ€å¤§åŒ–åå¾—åˆ°å‡½æ•°ï¼Œå‡½æ•°å½’ä¸€åŒ–åå¾—åˆ°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚æ‰€ä»¥æ¦‚ç‡æ¨¡å‹ä¸éæ¦‚ç‡æ¨¡å‹çš„åŒºåˆ«ä¸åœ¨äºè¾“å…¥è¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œè€Œåœ¨äºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼šæ¦‚ç‡æ¨¡å‹ä¸€å®šå¯ä»¥è¡¨ç¤ºä¸ºè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œè€Œéæ¦‚ç‡æ¨¡å‹åˆ™ä¸ä¸€å®šå­˜åœ¨è¿™æ ·çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚

æ¦‚ç‡æ¨¡å‹çš„ä»£è¡¨æ˜¯**æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆprobabilistic graphical modelï¼‰**$^{å‚è€ƒæ–‡çŒ®[1-3]}$ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒå¯ä»¥æ ¹æ®å›¾çš„ç»“æ„åˆ†è§£ä¸ºå› å­ä¹˜ç§¯çš„å½¢å¼ï¼Œå¯ä»¥ç”¨æœ€åŸºæœ¬çš„åŠ æ³•è§„åˆ™å’Œä¹˜æ³•è§„åˆ™è¿›è¡Œæ¦‚ç‡æ¨ç†ï¼š
$$P(x) = \sum_yP(x,y) \\ P(x,y) = P(x)P(y|x)$$

**å‚æ•°åŒ–æ¨¡å‹ï¼ˆparametric modelï¼‰å’Œéå‚æ•°åŒ–æ¨¡å‹ï¼ˆnon-parametric modelï¼‰**ï¼š

å‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„ç»´åº¦å›ºå®šï¼Œæ¨¡å‹å¯ä»¥ç”±æœ‰é™ç»´å‚æ•°å®Œå…¨åˆ»ç”»ï¼Œä¸éšæ•°æ®ç‚¹çš„å˜åŒ–è€Œå˜åŒ–ã€‚(å¦‚ï¼šæ„ŸçŸ¥æœºã€GMMã€logistic regressionã€æœ´ç´ è´å¶æ–¯ã€k å‡å€¼èšç±»ã€æ½œåœ¨è¯­ä¹‰åˆ†æã€æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æã€æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…)
éå‚æ•°åŒ–æ¨¡å‹å‡è®¾æ¨¡å‹å‚æ•°çš„å”¯ç‹¬ä¸å›ºå®šæˆ–è€…è¯´æ— ç©·å¤§ï¼Œéšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ è€Œä¸æ–­å¢å¤§ã€‚(å¦‚ï¼šå†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€AdaBoostã€k è¿‘é‚»)

> éå‚æ•°åŒ–æ¨¡å‹æ„å‘³ç€å†³ç­–æ ‘æ²¡æœ‰å‡è®¾ç©ºé—´åˆ†å¸ƒå’Œåˆ†ç±»å™¨ç»“æ„?

**åœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰å’Œæ‰¹é‡å­¦ä¹ ï¼ˆbatch learningï¼‰**ï¼š

åœ¨çº¿å­¦ä¹ æ¯æ¬¡æ¥å—ä¸€ä¸ªæ ·æœ¬ï¼Œé¢„æµ‹åå­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸æ–­é‡å¤è¯¥æ“ä½œã€‚
æ‰¹é‡å­¦ä¹ ä¸€æ¬¡æ¥å—æ‰€æœ‰æ•°æ®ï¼Œå­¦ä¹ æ¨¡å‹ä¹‹åè¿›è¡Œé¢„æµ‹ã€‚

åœ¨çº¿å­¦ä¹ æ¯”æ‰¹é‡å­¦ä¹ æ›´éš¾ï¼Œå› ä¸ºæ¯æ¬¡æ¨¡å‹æ›´æ–°ä¸­å¯åˆ©ç”¨çš„æ•°æ®æœ‰é™ã€‚

**è´å¶æ–¯å­¦ä¹ ï¼ˆBayesian learningï¼‰/ è´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰**ï¼š
$$\mathrm{Bayes \; Rule:} \\ \underbrace{P(X|Y)}_{\mathrm{posterior}} = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{P(Y)}_{\mathrm{evidence}}}   = \frac{\overbrace{P(Y|X)}^{\mathrm{likelihood}}\overbrace{P(X)}^{\mathrm{prior}}}{\underbrace{\sum_{x}P(Y|X)P(X)}_{\mathrm{evidence}}}$$

**æ ¸æŠ€å·§ï¼ˆkernel trickï¼‰/ æ ¸æ–¹æ³•ï¼ˆkernel methodï¼‰**ï¼š

**æ ¸æ–¹æ³•**æ˜¯ä¸€ç±»æŠŠä½ç»´ç©ºé—´çš„éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œè½¬åŒ–ä¸ºé«˜ç»´ç©ºé—´çš„çº¿æ€§å¯åˆ†é—®é¢˜çš„æ–¹æ³•ã€‚
**æ ¸æŠ€å·§**æ˜¯ä¸€ç§åˆ©ç”¨æ ¸å‡½æ•°ç›´æ¥è®¡ç®— $\lang \phi(x),\phi(z) \rang$ ï¼Œä»¥é¿å¼€åˆ†åˆ«è®¡ç®— $\phi(x)$ å’Œ $\phi(z)$ ï¼Œä»è€ŒåŠ é€Ÿæ ¸æ–¹æ³•è®¡ç®—çš„æŠ€å·§ã€‚

**æ ¸å‡½æ•°**ï¼š[Kernel function](https://en.jinzhao.wiki/wiki/Positive-definite_kernel)
è®¾ $\mathcal X$ æ˜¯è¾“å…¥ç©ºé—´ï¼ˆå³ $x_i \in \mathcal X $ ï¼Œ $\mathcal X$ æ˜¯ $\mathbb R^n$ çš„å­é›†æˆ–ç¦»æ•£é›†åˆ ï¼‰ï¼Œåˆè®¾ $\mathcal H$ ä¸ºç‰¹å¾ç©ºé—´ï¼ˆâ€‹ å¸Œå°”ä¼¯ç‰¹ç©ºé—´$^{é™„åŠ çŸ¥è¯†:å„ç§ç©ºé—´ä»‹ç»}$ï¼‰ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªä» $\mathcal X$ åˆ° $\mathcal H$ çš„æ˜ å°„

$$\phi(x) : \mathcal X \to \mathcal H$$

ä½¿å¾—å¯¹æ‰€æœ‰ $x,z \in \mathcal X$ ï¼Œå‡½æ•° $K(x,z)$ æ»¡è¶³æ¡ä»¶

$$K(x,z) = \phi(x).\phi(z) = \lang \phi(x),\phi(z) \rang$$

åˆ™ç§° $K(x,z)$ ä¸ºæ ¸å‡½æ•°ã€‚å…¶ä¸­ $\phi(x) $ ä¸ºæ˜ å°„å‡½æ•°ï¼Œ $\lang \phi(x),\phi(z) \rang$ ä¸ºå†…ç§¯ã€‚

[æ ¸æŠ€å·§](https://en.jinzhao.wiki/wiki/Kernel_method)çš„æƒ³æ³•æ˜¯ï¼Œåœ¨å­¦ä¹ å’Œé¢„æµ‹ä¸­åªå®šä¹‰æ ¸å‡½æ•° $K(x,z)$ ï¼Œè€Œä¸æ˜¾å¼åœ°å®šä¹‰æ˜ å°„å‡½æ•° $\phi $ã€‚é€šå¸¸ç›´æ¥è®¡ç®—$K(x,z)$æ¯”è¾ƒå®¹æ˜“ï¼Œè€Œé€šè¿‡$\phi(x) $å’Œ$\phi(z) $è®¡ç®—$K(x,z)$å¹¶ä¸å®¹æ˜“ã€‚

> æ³¨æ„ï¼š$\phi $æ˜¯è¾“å…¥ç©ºé—´$\mathbb{R}^n$åˆ°ç‰¹å¾ç©ºé—´$\mathcal H$çš„æ˜ å°„ï¼Œç‰¹å¾ç©ºé—´$\mathcal H$ä¸€èˆ¬æ˜¯é«˜ç»´çš„ï¼Œç”šè‡³æ˜¯æ— ç©·ç»´çš„ã€‚æ‰€ä»¥$\phi$ä¸å¥½è®¡ç®—ï¼Œç”šè‡³ä¼šå¸¦æ¥**ç»´åº¦ç¾éš¾**åˆç§°**ç»´åº¦è¯…å’’ï¼ˆCurse of Dimensionalityï¼‰**$^{é™„åŠ çŸ¥è¯†:ç»´åº¦è¯…å’’}$ã€‚

### é™„åŠ çŸ¥è¯†

#### æ­£åˆ™åŒ–

æ­£åˆ™åŒ–ç¬¦åˆå¥¥å¡å§†å‰ƒåˆ€ï¼ˆOccam's razorï¼‰åŸç†ã€‚

å‚è€ƒï¼š[L1L2 æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)

#### æ¨¡å‹é€‰æ‹©

å‚è€ƒï¼š[æ¨¡å‹é€‰æ‹©](../Model-Selection.md)

#### ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹

å‚è€ƒï¼š[ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹](../ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹.md)

#### å„ç§ç©ºé—´ä»‹ç»

**çº¿æ€§ç©ºé—´**å°±æ˜¯å®šä¹‰äº†**åŠ æ³•å’Œæ•°ä¹˜**çš„ç©ºé—´(ç©ºé—´é‡Œçš„ä¸€ä¸ªå…ƒç´ å°±å¯ä»¥ç”±å…¶ä»–å…ƒç´ çº¿æ€§è¡¨ç¤º)ã€‚

---

**åº¦é‡ç©ºé—´**å°±æ˜¯å®šä¹‰äº†**è·ç¦»**çš„ç©ºé—´ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼Œæ¬§æ°è·ç¦»ï¼Œé—µå¯å¤«æ–¯åŸºè·ç¦»ï¼Œé©¬æ°è·ç¦»ï¼Œåˆ‡æ¯”é›ªå¤«è·ç¦»ï¼‰ã€‚
å®šä¹‰è·ç¦»æ—¶ï¼Œæœ‰ä¸‰æ¡å…¬ç†å¿…é¡»éµå®ˆï¼š

1. éè´Ÿæ€§ã€åŒä¸€æ€§ï¼š$dist(x_i,x_j) \geq 0$(éè´Ÿæ€§)ï¼Œ$dist(x_i,x_j) = 0$å½“ä¸”ä»…å½“$x_i=x_j$(åŒä¸€æ€§)
2. å¯¹ç§°æ€§ï¼š$dist(x_i,x_j) = dist(x_j,x_i)$
3. ä¸‰è§’ä¸ç­‰å¼(ä¹Ÿå«ç›´é€’æ€§)ï¼š$dist(x_i,x_j) \leq dist(x_i,x_k) + dist(x_k,x_j)$
   å¸Œå°”ä¼¯ç‰¹ç©ºé—´(Hilbert)
   > æ–‡å­—è§£é‡Šï¼šã€ä¸¤ç‚¹ä¹‹é—´è·ç¦»ä¸ä¸ºè´Ÿï¼›ä¸¤ä¸ªç‚¹åªæœ‰åœ¨ ç©ºé—´ ä¸Šé‡åˆæ‰å¯èƒ½è·ç¦»ä¸ºé›¶ï¼›a åˆ° b çš„è·ç¦»ç­‰äº b åˆ° a çš„è·ç¦»;a åˆ° c çš„è·ç¦»åŠ ä¸Š c åˆ° b çš„è·ç¦»å¤§äºç­‰äº a ç›´æ¥åˆ° b çš„è·ç¦»;ã€‘

---

**èµ‹èŒƒç©ºé—´**å°±æ˜¯å®šä¹‰äº†**èŒƒæ•°**çš„ç©ºé—´ã€‚
x çš„èŒƒæ•°||x||å°±æ˜¯ x çš„**é•¿åº¦**ã€‚é‚£ä¹ˆè¿™é‡Œçš„é•¿åº¦å’Œä¸Šä¸€èŠ‚ä¸­è¯´çš„è·ç¦»åˆ°åº•æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ã€‚**è·ç¦»çš„æ¦‚å¿µæ˜¯é’ˆå¯¹ä¸¤ä¸ªå…ƒç´ æ¥è¯´çš„**ï¼Œä¾‹å¦‚ d(x,y)æŒ‡çš„æ˜¯ x ä¸ y ä¸¤ä¸ªå…ƒç´ ä¹‹é—´çš„è·ç¦»ï¼Œè€Œ**èŒƒæ•°æ˜¯é’ˆå¯¹ä¸€ä¸ªå…ƒç´ æ¥è¯´çš„**ï¼Œæ¯ä¸€ä¸ªå…ƒç´ éƒ½å¯¹åº”ä¸€ä¸ªèŒƒæ•°ï¼Œå¯ä»¥å°†èŒƒæ•°ç†è§£ä¸ºä¸€ä¸ªå…ƒç´ åˆ°é›¶ç‚¹çš„è·ç¦»ï¼ˆè¿™åªæ˜¯ä¸€ç§ç†è§£ï¼Œå¹¶ä¸æ˜¯å®šä¹‰ï¼‰ï¼Œä¹Ÿå°±æ˜¯å®ƒè‡ªå·±çš„é•¿åº¦ã€‚
å®šä¹‰ï¼š
ç§° æ˜ å°„$||.|| : \mathbb{R}^n \to \mathbb{R}$ä¸º $\mathbb{R}^n$ ä¸Šçš„èŒƒæ•°ï¼Œå½“ä¸”ä»…å½“ï¼š

1. éè´Ÿæ€§ï¼š $\forall x \in \mathbb{R}^n ,||x|| \geq 0$ ,$||x|| = 0$å½“ä¸”ä»…å½“$x=0$
2. æ•°ä¹˜ï¼š$\forall x \in \mathbb{R}^n ,a \in \mathbb{R}^n, ||ax|| = |a|.||x||$
3. ä¸‰è§’ä¸ç­‰å¼: $\forall x,y \in \mathbb{R}^n ,||x+y|| \leq ||x|| + ||y||$

å¦‚æœæˆ‘ä»¬å®šä¹‰äº†èŒƒæ•°ï¼Œå¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šå®šä¹‰è·ç¦»ï¼šdist(x,y)=||x-y||ã€‚æ ¹æ®èŒƒæ•°çš„ä¸‰æ¡æ€§è´¨ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜æˆ‘ä»¬è¿™æ ·å®šä¹‰çš„è·ç¦»ä¹Ÿæ»¡è¶³è·ç¦»çš„å®šä¹‰ï¼Œèªæ˜çš„ä½ å¯ä»¥è‡ªå·±è¯æ˜ä¸€ä¸‹ï¼ˆå¯¹ç§°æ€§çš„è¯æ˜ï¼Œæä¸€ä¸ª-1 å‡ºæ¥ï¼Œä¸€åŠ ç»å¯¹å€¼å°±æ˜¯ 1 äº†ï¼‰ã€‚

ä¹Ÿå°±æ˜¯è¯´èŒƒæ•°å…¶å®æ˜¯ä¸€ä¸ªæ›´åŠ å…·ä½“çš„æ¦‚å¿µï¼Œ**æœ‰äº†èŒƒæ•°ä¸€å®šèƒ½åˆ©ç”¨èŒƒæ•°å®šä¹‰è·ç¦»ï¼Œä½†æ˜¯æœ‰è·ç¦»ä¸èƒ½å®šä¹‰èŒƒæ•°**ã€‚

ä¹Ÿè®¸ä½ ä¼šé—®ï¼Œä½ ä¸æ˜¯è¯´ç†è§£èŒƒæ•°å°±æ˜¯ä¸€ä¸ªå…ƒç´ åˆ°é›¶ç‚¹çš„è·ç¦»å—ï¼Œé‚£å®šä¹‰èŒƒæ•°ä¸º||x||=dist(x,0) ä¸å°±è¡Œäº†å—ã€‚è¿™æ ·çš„è¯ï¼Œå¯¹äºèŒƒæ•°çš„ç¬¬äºŒæ¡æ€§è´¨å°±ä¸ä¸€å®šä¼šæ»¡è¶³ï¼Œ||ax||=dist(ax,0)ï¼Œè€Œ dist(ax,0)ä¸ä¸€å®šç­‰äº|a|dist(x,0)ï¼Œå…·ä½“ç­‰ä¸ç­‰äºè¿˜è¦çœ‹ä½ çš„è·ç¦»æ˜¯æ€ä¹ˆå®šä¹‰çš„ã€‚

ä¾‹å¦‚ï¼šL<sub>p</sub>èŒƒæ•°
æ¬§å¼è·ç¦»å¯¹åº” L2 èŒƒæ•°
æ›¼å“ˆé¡¿è·ç¦»å¯¹åº” L1 èŒƒæ•°
åˆ‡æ¯”é›ªå¤«è·ç¦»å¯¹åº” Lâˆ èŒƒæ•°
L<sub>p</sub>èŒƒæ•°ï¼šå½“ p>=1 æ—¶ï¼Œå‘é‡çš„ L<sub>p</sub>èŒƒæ•°æ˜¯å‡¸çš„ã€‚(è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä¸€èˆ¬ä¸ç”¨ L0 èŒƒæ•°çš„åŸå› ä¹‹ä¸€)

---

**çº¿æ€§èµ‹èŒƒç©ºé—´**å°±æ˜¯å®šä¹‰äº†åŠ æ³•ã€æ•°ä¹˜å’ŒèŒƒæ•°çš„ç©ºé—´ã€‚

---

**å·´æ‹¿èµ«ç©ºé—´**å°±æ˜¯**å®Œå¤‡çš„èµ‹èŒƒçº¿æ€§ç©ºé—´**ã€‚(Banach space)
**å®Œå¤‡çš„ç©ºé—´**çš„å®šä¹‰ï¼šå¦‚æœä¸€ä¸ªç©ºé—´æ˜¯å®Œå¤‡çš„ï¼Œé‚£ä¹ˆè¯¥ç©ºé—´ä¸­çš„ä»»ä½•ä¸€ä¸ªæŸ¯è¥¿åºåˆ—éƒ½æ”¶æ•›åœ¨è¯¥ç©ºé—´ä¹‹å†…ã€‚

é¦–å…ˆæ¥è¯´ä¸€ä¸‹æŸ¯è¥¿åºåˆ—æ˜¯ä»€ä¹ˆï¼ŒæŸ¯è¥¿åºåˆ—å°±æ˜¯éšç€åºæ•°å¢åŠ ï¼Œå€¼ä¹‹é—´çš„è·ç¦»è¶Šæ¥è¶Šå°çš„åºåˆ—ã€‚æ¢ä¸€ç§è¯´æ³•æ˜¯ï¼ŒæŸ¯è¥¿åºåˆ—å¯ä»¥åœ¨å»æ‰æœ‰é™ä¸ªå€¼ä¹‹åï¼Œä½¿ä»»æ„ä¸¤ä¸ªå€¼ä¹‹é—´çš„$\underline{\mathrm{è·ç¦»}}$éƒ½å°äºä»»æ„ç»™å®šæ­£å¸¸æ•°ï¼ˆå…¶å®è¿™å°±æ˜¯å®šä¹‰äº†ä¸€ä¸ªæé™è€Œå·²ï¼‰ã€‚

é‚£ä¹ˆä»»æ„ä¸€ä¸ªæŸ¯è¥¿åºåˆ—éƒ½æ”¶æ•›åœ¨è¯¥ç©ºé—´å†…æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Œä¸¾ä¸ªä¾‹å­ä½ å°±æ˜ç™½äº†ã€‚

è®¾å®šä¹‰åœ¨æœ‰ç†æ•°ç©ºé—´ Q ä¸Šçš„åºåˆ—ï¼š$x_n = \frac{[\sqrt{2}n]}{n}$ï¼Œå…¶ä¸­[x]è¡¨ç¤º x å–æ•´æ•°éƒ¨åˆ†ã€‚
å¯¹äºè¿™ä¸ªæ•°åˆ—æ¥è¯´ï¼Œæ¯ä¸€ä¸ªå…ƒç´ çš„åˆ†å­åˆ†æ¯éƒ½æ˜¯æ•´æ•°ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ª$x_n$éƒ½åœ¨æœ‰ç†æ•°ç©ºé—´ Q ä¸Šï¼Œé‚£è¿™ä¸ªåºåˆ—çš„æé™å‘¢ï¼Œç¨æœ‰å¸¸è¯†çš„äººéƒ½èƒ½çœ‹å‡ºï¼Œè¿™ä¸ªåºåˆ—çš„æé™æ˜¯$\sqrt{2}$ï¼Œè€Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæœ‰ç†æ•°ï¼Œæ‰€ä»¥è¿™ä¸ªæŸ¯è¥¿åºåˆ—çš„æé™ä¸åœ¨è¯¥ç©ºé—´é‡Œé¢ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰ç†æ•°ç©ºé—´ Q æ˜¯ä¸å®Œå¤‡çš„ã€‚

æ‰€ä»¥å®Œå¤‡çš„æ„ä¹‰æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ï¼Œé‚£å°±æ˜¯**åœ¨ä¸€ä¸ªç©ºé—´ä¸Šæˆ‘ä»¬å®šä¹‰äº†æé™ï¼Œä½†æ˜¯ä¸è®ºä½ æ€ä¹ˆå–æé™ï¼Œå®ƒçš„æé™çš„å€¼éƒ½ä¸ä¼šè·‘å‡ºè¿™ä¸ªç©ºé—´ï¼Œé‚£ä¹ˆè¿™ä¸ªç©ºé—´å°±æ˜¯å®Œå¤‡ç©ºé—´**ã€‚

å¦å¤–ï¼Œä¸çŸ¥é“ä½ æœ‰æ²¡æœ‰å‘ç°ï¼Œä¸Šé¢åœ¨è§£é‡Šä»€ä¹ˆæ˜¯æŸ¯è¥¿åºåˆ—çš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªè¯æˆ‘åŠ äº†ä¸‹åˆ’çº¿ï¼Œé‚£å°±æ˜¯è·ç¦»ï¼Œä¹Ÿå°±è¯´è¯´åœ¨å®šä¹‰å®Œå¤‡ç©ºé—´ä¹‹å‰ï¼Œè¦å…ˆæœ‰è·ç¦»çš„æ¦‚å¿µã€‚æ‰€ä»¥**å®Œå¤‡ç©ºé—´ï¼Œå…¶å®ä¹Ÿæ˜¯å®Œå¤‡åº¦é‡ç©ºé—´**ã€‚

æ‰€ä»¥ï¼Œå·´æ‹¿èµ«ç©ºé—´æ»¡è¶³å‡ æ¡ç‰¹æ€§å‘¢ï¼šè·ç¦»ã€èŒƒæ•°ã€å®Œå¤‡ã€‚

---

**å†…ç§¯ç©ºé—´**å°±æ˜¯å®šä¹‰äº†å†…ç§¯çš„ç©ºé—´ã€‚[Inner product space](https://en.jinzhao.wiki/wiki/Inner_product_space)
æœ‰æ—¶ä¹Ÿç§°å‡†å¸Œå°”ä¼¯ç‰¹ç©ºé—´ã€‚
å†…ç§¯å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„ç‚¹ä¹˜ã€æ ‡ç§¯ï¼Œå®ƒçš„å®šä¹‰æ–¹å¼ä¹Ÿä¸æ˜¯å”¯ä¸€çš„ï¼Œä½†å¦‚åŒè·ç¦»èŒƒæ•°çš„å®šä¹‰ä¸€æ ·ï¼Œå†…ç§¯çš„å®šä¹‰ä¹Ÿè¦æ»¡è¶³æŸäº›æ¡ä»¶ï¼Œä¸èƒ½éšä¾¿å®šä¹‰ã€‚

å®šä¹‰æ˜ å°„$\lang .,. \rang : V \times V \to \mathbb{F}$, å…¶ä¸­$V$æ˜¯å‘é‡ï¼Œ$\mathbb{F}$æ˜¯æ ‡é‡
æœ‰$x,y,z \in V ,s \in \mathbb{F}$ï¼Œé‚£ä¹ˆå†…ç§¯æ»¡è¶³

1. ç¬¬ä¸€ä¸ªå‚æ•°ä¸­çš„çº¿æ€§:
   $$\lang sx,y \rang = s\lang x,y \rang \\ \lang x+y,z \rang = \lang x,z \rang + \lang y,z \rang \\ \lang 0,x \rang = 0$$

2. å…±è½­å¯¹ç§°:$\lang x,y \rang = \overline{\lang y,x \rang }$

3. æ­£å®šæ€§:$\lang x,x \rang > 0 \quad\mathrm{if}\; x \neq 0$

4. æ­£åŠå®šæ€§æˆ–éè´Ÿå®šæ€§:$\forall{x}, \lang x,x \rang \geq 0 $

5. ç¡®å®šæ€§ï¼š$\lang x,x \rang = 0 å¿…ç„¶æœ‰ x=0$

3ï¼Œ4ï¼Œ5 å¯ä»¥è·Ÿä¸Šé¢å®šä¹‰èŒƒæ•°å’Œè·ç¦»ä¸€æ ·å†™æˆä¸€ä¸ª

ä¾‹å­-æ¬§å‡ é‡Œå¾—å‘é‡ç©ºé—´:
$ x,y \in \mathbb{R}^n , \lang x,y \rang = x^Ty=\sum\_{i=1}^n{x_iy_i}$

**åªæœ‰å®šä¹‰äº†å†…ç§¯ï¼Œæ‰ä¼šæœ‰å¤¹è§’çš„æ¦‚å¿µï¼Œæ‰ä¼šæœ‰æ­£äº¤çš„æ¦‚å¿µï¼Œå¦å¤–å†…ç§¯ä¹Ÿå¯ä»¥å®šä¹‰èŒƒæ•°ï¼Œä¹Ÿå°±æ˜¯è¯´å†…ç§¯æ˜¯æ¯”èŒƒæ•°æ›´å…·ä½“çš„ä¸€ä¸ªæ¦‚å¿µã€‚**

---

**æ¬§å¼ç©ºé—´**å°±æ˜¯å®šä¹‰äº†å†…ç§¯çš„æœ‰é™ç»´å®çº¿æ€§ç©ºé—´ã€‚

---

**å¸Œå°”ä¼¯ç‰¹ç©ºé—´**å°±æ˜¯å®Œå¤‡çš„å†…ç§¯ç©ºé—´ã€‚(Hilbert space)
å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„å…ƒç´ ä¸€èˆ¬æ˜¯å‡½æ•°ï¼Œå› ä¸ºä¸€ä¸ªå‡½æ•°å¯ä»¥è§†ä¸ºä¸€ä¸ªæ— ç©·ç»´çš„å‘é‡ã€‚

```mermaid
graph LR;
    LS(("Linear Space"))-->NLS(("Normed Linear Space"));
    NLS-->BS(("Banach Space"))
    NLS-->IPS(("Inner Product Space"))
    IPS-->HS(("Hilbert Space"))
    IPS-->ES(("Euclid Space"))
```

![](https://pic2.zhimg.com/80/v2-be26b2ba1df2edc9636647a28b22238d_720w.jpg?source=1940ef5c)

å‚è€ƒï¼š[ä¸€ç‰‡æ–‡ç« å¸¦ä½ ç†è§£å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰ä»¥åŠå„ç§ç©ºé—´](https://blog.csdn.net/ChangHengyi/article/details/80577318)

#### ç»´åº¦è¯…å’’

ç»´åº¦è¯…å’’é€šå¸¸æ˜¯æŒ‡åœ¨æ¶‰åŠåˆ°å‘é‡çš„è®¡ç®—çš„é—®é¢˜ä¸­ï¼Œéšç€ç»´æ•°çš„å¢åŠ ï¼Œè®¡ç®—é‡å‘ˆæŒ‡æ•°å€å¢é•¿çš„ä¸€ç§ç°è±¡ã€‚é«˜ç»´åº¦æœ‰æ›´å¤§çš„ç‰¹å¾ç©ºé—´ï¼Œéœ€è¦æ›´å¤šçš„æ•°æ®æ‰å¯ä»¥è¿›è¡Œè¾ƒå‡†ç¡®çš„ä¼°è®¡ã€‚

> è‹¥ç‰¹å¾æ˜¯äºŒå€¼çš„ï¼Œåˆ™æ¯å¢åŠ ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€éœ€æ•°æ®é‡éƒ½åœ¨ä»¥ 2 çš„æŒ‡æ•°çº§è¿›è¡Œå¢é•¿ï¼Œæ›´ä½•å†µå¾ˆå¤šç‰¹å¾ä¸åªæ˜¯äºŒå€¼çš„ã€‚

å‡ ä½•è§’åº¦ 1ï¼š

<svg width="52" height="52" xmlns="http://www.w3.org/2000/svg">
 <!-- Created with Method Draw - http://github.com/duopixel/Method-Draw/ -->
 <g>
  <title>background</title>
  <rect fill="#fff" id="canvas_background" height="54" width="54" y="-1" x="-1"/>
  <g display="none" overflow="visible" y="0" x="0" height="100%" width="100%" id="canvasGrid">
   <rect fill="url(#gridpattern)" stroke-width="0" y="0" x="0" height="100%" width="100%"/>
  </g>
 </g>
 <g>
  <title>Layer 1</title>
  <rect stroke="#000" id="svg_1" height="50" width="50" y="1.134891" x="1.227186" stroke-width="1.5" fill="#fff"/>
  <ellipse stroke="#000" ry="25" rx="25" id="svg_2" cy="26.316708" cx="25.727185" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" fill="#fff"/>
  <line stroke-linecap="null" stroke-linejoin="null" id="svg_3" y2="26.363651" x2="49.090879" y1="26.363651" x1="23.636325" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="none"/>
  <text stroke="#000" transform="matrix(0.8454890517551235,0,0,0.38060957631270753,66.36433546231878,120.48066499237646) " xml:space="preserve" text-anchor="start" font-family="Helvetica, Arial, sans-serif" font-size="24" id="svg_4" y="-262.016546" x="-56.089448" fill-opacity="null" stroke-opacity="null" stroke-width="0" fill="#000000">0.5</text>
 </g>
</svg>

ä¸Šå›¾è¡¨ç¤ºä¸€ä¸ªå¤šç»´ç©ºé—´ï¼ˆä»¥äºŒç»´ä¸ºä¾‹ï¼‰ï¼Œè®¾æ­£æ–¹å½¢è¾¹é•¿ä¸º 1ï¼Œåˆ™å…¶å†…åˆ‡åœ†åŠå¾„ä¸º$r=0.5$ï¼Œåˆ™æ­£æ–¹å½¢é¢ç§¯ä¸º 1ï¼Œå†…åˆ‡åœ†é¢ç§¯ä¸º$\pi(0.5)^2$ ã€‚è‹¥å°†æ­¤å˜ä¸ºä¸‰ç»´æƒ…å†µä¸‹ï¼Œæ­£æ–¹ä½“ä½“ç§¯ä¸º 1ï¼Œå†…åˆ‡çƒä½“ç§¯ä¸º$\frac{4}{3}\pi(0.5)^3$ã€‚

å› æ­¤çƒä½“çš„ä½“ç§¯å¯ä»¥è¡¨ç¤ºä¸º$V(d) = \frac{\pi^{d/2}}{\varGamma(\frac{d}{2}+1)}0.5^d = k(0.5)^d$(d ä¸ºç»´åº¦),åˆ™ $\lim_{d \to \infty}k(0.5)^d = 0$ï¼Œå…¶å†…åˆ‡è¶…çƒä½“çš„ä½“ç§¯ä¸º 0ã€‚ç”±æ­¤å¯çŸ¥ï¼Œ**é«˜ç»´æƒ…å†µä¸‹ï¼Œæ•°æ®å¤§éƒ½åˆ†å¸ƒåœ¨å››è§’ï¼ˆæ­£æ–¹å½¢å†…ï¼Œå†…åˆ‡åœ†å¤–ï¼‰**ï¼Œç¨€ç–æ€§å¤ªå¤§ï¼Œä¸å¥½åˆ†ç±»ã€‚

> ç»´åº¦è¶Šå¤§ï¼Œè¶…çƒä½“ä½“ç§¯è¶Šå°ã€‚è¯´æ˜è½åœ¨è¶…çƒä½“å†…çš„æ ·æœ¬è¶Šå°‘ï¼Œå› ä¸ºè¶…çƒä½“æ˜¯è¶…ç«‹æ–¹ä½“çš„å†…åˆ‡çƒã€‚ä¸åœ¨çƒå†…,é‚£åªèƒ½åœ¨è§’è½ï¼

å‡ ä½•è§’åº¦ 2ï¼š

<svg width="52" height="52" xmlns="http://www.w3.org/2000/svg">
 <!-- Created with Method Draw - http://github.com/duopixel/Method-Draw/ -->
 <g>
  <title>background</title>
  <rect fill="#fff" id="canvas_background" height="54" width="54" y="-1" x="-1"/>
  <g display="none" overflow="visible" y="0" x="0" height="100%" width="100%" id="canvasGrid">
   <rect fill="url(#gridpattern)" stroke-width="0" y="0" x="0" height="100%" width="100%"/>
  </g>
 </g>
 <g>
  <title>Layer 1</title>
  <ellipse stroke="#000" ry="25" rx="25" id="svg_5" cy="25" cx="25" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" fill="#fff"/>
  <ellipse id="svg_6" cy="24.593763" cx="34.636353" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="#fff"/>
  <ellipse ry="20" rx="20" id="svg_7" cy="25" cx="25" fill-opacity="null" stroke-opacity="null" stroke-width="1.5" stroke="#000" fill="#fff"/>
 </g>
</svg>

ä¸Šå›¾ä¹Ÿè¡¨ç¤ºä¸€ä¸ªå¤šç»´ç©ºé—´ï¼ˆä»¥äºŒç»´ä¸ºä¾‹ï¼‰ï¼Œåˆ™å…¶ä¸­å›¾å½¢çš„ä½“ç§¯æœ‰å¦‚ä¸‹å…³ç³»ï¼šå¤–åœ†åŠå¾„$r=1$ï¼Œå†…åœ†åŠå¾„ä¸º$râˆ’\varepsilon$ ã€‚åŒæ ·åœ¨é«˜ç»´æƒ…å†µä¸‹ï¼Œå¤–åœ†ä½“ç§¯ä¸º$V_{å¤–åœ†} = k.1^d = k$ï¼Œä¸­é—´çš„åœ†ç¯ä½“ç§¯ä¸º$V_{åœ†ç¯} = k - k(1-\varepsilon)^d$ï¼Œåˆ™ï¼š
$$\lim_{d \to \infty}\frac{V_{åœ†ç¯}}{V_{å¤–åœ†}} = \lim_{d \to \infty}\frac{ k - k(1-\varepsilon)^d}{k} = \lim_{d \to \infty}(1-(1-\varepsilon)^d) = 1$$

> é«˜ç»´æƒ…å†µä¸‹ï¼Œæ— è®º$\varepsilon$å¤šå°ï¼Œåªè¦ d è¶³å¤Ÿå¤§ï¼Œåœ†ç¯å‡ ä¹å æ®äº†æ•´ä¸ªå¤–åœ†ï¼Œå†…åœ†ä½“ç§¯è¶‹å‘äº 0ï¼Œå¯¼è‡´æ•°æ®**ç¨€ç–**ã€‚

å‚è€ƒï¼š
[The Curse of Dimensionality in classification](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(äº”)-é™ç»´ï¼ˆDimensionality Reductionï¼‰](https://www.bilibili.com/video/BV1vW411S7tH)

#### ä¸ç­‰å¼(Inequality)

[æ‰€æœ‰ä¸ç­‰å¼](https://en.jinzhao.wiki/wiki/Category:Inequalities) ä»¥åŠ[æ‰€æœ‰æ¦‚ç‡ï¼ˆProbabilisticï¼‰ä¸ç­‰å¼](https://en.jinzhao.wiki/wiki/Category:Probabilistic_inequalities)

- **[ç»å¯¹å€¼ä¸ç­‰å¼](https://chi.jinzhao.wiki/wiki/%E7%BB%9D%E5%AF%B9%E5%80%BC%E4%B8%8D%E7%AD%89%E5%BC%8F) - Absolute value inequality**

- **å¹‚å¹³å‡å€¼ä¸ç­‰å¼- [Power-Mean Inequality](https://artofproblemsolving.com/wiki/index.php/Power_Mean_Inequality)**

- **[ä¸‰è§’å½¢å†…è§’çš„åµŒå…¥ä¸ç­‰å¼](https://chi.jinzhao.wiki/wiki/%E4%B8%89%E8%A7%92%E5%BD%A2%E5%86%85%E8%A7%92%E7%9A%84%E5%B5%8C%E5%85%A5%E4%B8%8D%E7%AD%89%E5%BC%8F) - æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º Wolstenholme ä¸ç­‰å¼**

- **ä¼¯åŠªåˆ©ä¸ç­‰å¼ - [Bernoulli's inequality](https://en.jinzhao.wiki/wiki/Bernoulli%27s_inequality)**
- **æ’åºä¸ç­‰å¼ - [Rearrangement inequality](https://en.jinzhao.wiki/wiki/Rearrangement_inequality)**
- **å‡å€¼ä¸ç­‰å¼ - [Inequality of arithmetic and geometric means](https://en.jinzhao.wiki/wiki/Inequality_of_arithmetic_and_geometric_means)**

- **èˆ’å°”ä¸ç­‰å¼ - [Schur's inequality](https://en.jinzhao.wiki/wiki/Schur%27s_inequality)**

- **é—µå¯å¤«æ–¯åŸº (Minkowski) ä¸ç­‰å¼ - [Minkowski inequality](https://en.jinzhao.wiki/wiki/Minkowski_inequality)**

- **å‰å¸ƒæ–¯ (Gibbs) ä¸ç­‰å¼ - [Gibbs' inequality](https://en.jinzhao.wiki/wiki/Gibbs%27_inequality)**
  $${\displaystyle -\sum _{i=1}^{n}p_{i}\log p_{i}\leq -\sum _{i=1}^{n}p_{i}\log q_{i}}$$

ç”± KL divergence å°±èƒ½è¯æ˜
$${\displaystyle D_{\mathrm {KL} }(P\|Q)\equiv \sum _{i=1}^{n}p_{i}\log {\frac {p_{i}}{q_{i}}}\geq 0.}$$

##### æ¦‚ç‡ä¸ç­‰å¼ Probabilistic inequalities

- **æŸ¯è¥¿-æ–½ç“¦èŒ¨ (Cauchyâ€“Schwarz) ä¸ç­‰å¼ - [Cauchyâ€“Schwarz inequality](https://en.jinzhao.wiki/wiki/Cauchy%E2%80%93Schwarz_inequality)**
  $$[\sum_{i=1}^{n}{a_ib_i}]^2  \leq [\sum_{i=1}^{n}a_i^2].[\sum_{i=1}^{n}b_i^2] ç­‰å¼æˆç«‹ï¼šb_i=ka_i \\ å‘é‡å½¢å¼ï¼š|\braket{u,v}| \leq ||u||.||v|| \\ æ¦‚ç‡ä¸­ï¼š|E(XY)|^2 \leq E(X^2)E(Y^2)$$
  è¯æ˜ï¼š
  $$\vec{A} = (a_1,...,a_n),  \vec{B} = (b_1,...,b_n) \\ \vec{A}.\vec{B} = (a_1b_1,...,a_nb_n) = ||\vec{A}||.||\vec{B}||\cos\theta \leq ||\vec{A}||.||\vec{B}|| = \sqrt{a_1^2+...+a_n^2}.\sqrt{b_1^2+...+b_n^2}$$
  åº”ç”¨:

  1. è¯æ˜ covariance inequalityï¼š$Var(Y) \geq \frac{Cov(Y,X)^2}{Var(X)}$,æœ‰$\braket{X,Y} := E(XY)$
     $$|Cov(Y,X)|^2 = |E((X-\mu)(Y-v))|^2 = |\braket{X-\mu,Y-v}|^2 \\ \leq \braket{X-\mu,X-\mu}\braket{Y-v,Y-v} = E((X-\mu)^2)E((Y-v)^2) = Var(X)Var(Y)$$

- **èµ«å°”å¾· (Holder) ä¸ç­‰å¼ - [HÃ¶lder's inequality](https://en.jinzhao.wiki/wiki/H%C3%B6lder%27s_inequality)**

- **ç´ç”Ÿ (Jensen) ä¸ç­‰å¼ - [Jensen's inequality](https://en.jinzhao.wiki/wiki/Jensen%27s_inequality)**
  $$f(tx_1 +(1-t)x_2) \leq tf(x_1) + (1-t)f(x_2), \text{f is convex function} \\ æ¨å¹¿ï¼šf(a_1x_1 +...+ a_nx_n) \leq a_1f(x_1) +...+ a_nf(x_n), a_1+...+a_n = 1 , a_i \geq 0 \\ or: f(\sum_{i=1}^n{a_ix_i}) \leq \sum_{i=1}^n{a_if(x_i)} , \sum_{i=1}^n{a_i} = 1, a_i \geq 0$$

  æ¦‚ç‡ä¸­ï¼šå¦‚æœ$X$æ˜¯éšæœºå˜é‡ï¼Œè€Œ$\varphi$æ˜¯å‡¸å‡½æ•°ï¼Œåˆ™:$\varphi(E[X]) \leq E[\varphi(X)]$,ä¸ç­‰å¼ä¸¤è¾¹çš„å·®ï¼Œ$ E[\varphi(X)] - \varphi(E[X]) $ç§°ä¸º Jensen gap(é—´éš™)ï¼›
  åº”ç”¨ï¼š

  1. EM ç®—æ³•ä¸­æœ‰ç”¨åˆ°(log å‡½æ•°æ˜¯å‡¹å‡½æ•°æ­£å¥½ä¸å‡¸å‡½æ•°ç›¸å);
  2. è¯æ˜ KL æ•£åº¦>=0;

- **é©¬å°”å¯å¤«ä¸ç­‰å¼ - [Markov's inequality](https://en.jinzhao.wiki/wiki/Markov%27s_inequality)**
  $$P(X \geq a) \leq \frac{E(X)}{a}$$
  å…¶ä¸­$X$ä¸ºéè´Ÿéšæœºå˜é‡ï¼Œ$\forall a>0$
  åº”ç”¨ï¼š

  1. ç”¨äºä¼°è®¡ä¸€ä¸ªæ¦‚ç‡çš„ä¸Šç•Œï¼Œæ¯”å¦‚å‡è®¾ä½ æ‰€åœ¨å…¬å¸çš„äººå‡å·¥èµ„æ˜¯ 1 ä¸‡ï¼Œé‚£ä¹ˆéšæœºé€‰ä¸€ä¸ªä½ å¸å‘˜å·¥ï¼Œå…¶å·¥èµ„è¶…è¿‡ 10 ä¸‡çš„æ¦‚ç‡ï¼Œä¸ä¼šè¶…è¿‡ 1/10ï¼›
  2. ç”¨äºå…¶ä»–æ¦‚ç‡ä¸ç­‰å¼çš„è¯æ˜ï¼Œæ¯”å¦‚éœå¤«ä¸ä¸ç­‰å¼ï¼›

- **åˆ‡æ¯”é›ªå¤« (Chebyshev) ä¸ç­‰å¼ - [Chebyshev's inequality](https://en.jinzhao.wiki/wiki/Chebyshev%27s_inequality)**
  $$P\{|X-\mu| \geq k\} \leq \frac{\sigma^2}{k^2}$$
  å…¶ä¸­$X$ä¸ºéšæœºå˜é‡ï¼Œ$\forall k>0$, $\mu$ä¸ºå‡å€¼ï¼Œ$\sigma^2$ä¸ºæ–¹å·®
  ï¼ˆè¯æ˜å¯ä»¥åˆ©ç”¨é©¬å°”å¯å¤«ä¸ç­‰å¼ï¼Œè§æ¦‚ç‡è®ºåŸºç¡€æ•™ç¨‹ 313 é¡µï¼‰

- **éœå¤«ä¸ä¸ç­‰å¼ - [Hoeffding's inequality](https://en.jinzhao.wiki/wiki/Hoeffding%27s_inequality)**
  åº”ç”¨ï¼š
  1. [Machine Learning Foundationsï¼ˆæœºå™¨å­¦ä¹ åŸºçŸ³ï¼‰- feasibility of learning,12,13,18 é¡µ](https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/04_handout.pdf)
  2. ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼Œ26 é¡µï¼Œè¯æ˜æ³›åŒ–è¯¯å·®ä¸Šç•Œï¼ˆåœ¨[æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„å‡ ä¸ªæ¦‚ç‡ä¸ç­‰å¼åŠè¯æ˜](https://zhuanlan.zhihu.com/p/392348396)ä¸­ä¹Ÿæœ‰æåˆ°ï¼‰

å‚è€ƒï¼š[åˆç­‰æ•°å­¦å­¦ä¹ ç¬”è®°](https://github.com/zhcosin/elementary-math/blob/master/elementary-math-note.pdf)

### å‚è€ƒæ–‡çŒ®

[1-1] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[1-2] Bishop M. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). Springer,2006

[1-3] [Probabilistic Graphical Models: Principles and Techniques](https://djsaunde.github.io/read/books/pdfs/probabilistic%20graphical%20models.pdf) by Daphne Koller, Nir Friedman from The MIT Press

[1-4] [Deep Learning](https://raw.fastgit.org/Zhenye-Na/machine-learning-uiuc/master/docs/Deep%20Learning.pdf) (Ian Goodfellow, Yoshua Bengio, Aaron Courville)

[1-5] Tom M Michelle. [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html). McGraw-Hill Companies,Inc. 1997ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬ï¼šæœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003ï¼‰

[1-6] [Bayesian Reasoning and Machine Learning by David Barber 2007â€“2020](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf) ,[other version](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/)

[1-7] [Reinforcement Learning:An Introduction (second edition 2020) by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020trimmed.pdf) ,[other version](http://incompleteideas.net/book/)

[1-8] å‘¨å¿—åï¼Œ[æœºå™¨å­¦ä¹ ](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)ï¼Œæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ ([æ‰‹æ¨ç¬”è®°](https://github.com/Sophia-11/Machine-Learning-Notes) ä»¥åŠ [å…¬å¼æ¨å¯¼è§£æ](https://github.com/datawhalechina/pumpkin-book))

[1-9] [Lecture Notes in MACHINE LEARNING](https://news.vidyaacademy.ac.in/wp-content/uploads/2018/10/NotesOnMachineLearningForBTech-1.pdf) Dr V N Krishnachandran

## ç¬¬ 2 ç«  æ„ŸçŸ¥æœº

åˆ¤åˆ«æ¨¡å‹

æ„ŸçŸ¥æœº[Perceptron](https://en.jinzhao.wiki/wiki/Perceptron)æ˜¯**ç¥ç»ç½‘ç»œ**å’Œ**æ”¯æŒå‘é‡æœº**çš„åŸºç¡€ã€‚æœ€æ—©åœ¨ 1957 å¹´ç”± Rosenblatt æå‡º$^{å‚è€ƒæ–‡çŒ®[2-1]}$ã€‚Novikoff$^{å‚è€ƒæ–‡çŒ®[2-2]}$ï¼ŒMinsky ä¸ Papert$^{å‚è€ƒæ–‡çŒ®[2-3]}$ç­‰äººå¯¹æ„ŸçŸ¥æœºè¿›è¡Œäº†ä¸€ç³»åˆ—ç†è®ºç ”ç©¶ã€‚æ„ŸçŸ¥æœºçš„æ‰©å±•å­¦ä¹ æ–¹æ³•åŒ…æ‹¬å£è¢‹ç®—æ³•(pocket algorithm)$^{å‚è€ƒæ–‡çŒ®[2-4]}$ã€è¡¨å†³æ„ŸçŸ¥æœº(voted perceptron)$^{å‚è€ƒæ–‡çŒ®[2-5]}$ã€å¸¦è¾¹ç¼˜æ„ŸçŸ¥æœº(perceptron with margin)$^{å‚è€ƒæ–‡çŒ®[2-6]}$ç­‰ã€‚
[Brief History of Machine Learning](https://erogol.com/brief-history-machine-learning/)

è¦æ±‚ï¼šæ•°æ®é›†çº¿æ€§å¯åˆ†(linearly separable data set)

- **æ¨¡å‹**ï¼š
  $$f(x) = sign(w.x + b)$$
  å…¶ä¸­$x,w \in \mathbb{R}^n ,b \in \mathbb{R}$,$w$å«ä½œæƒå€¼ï¼ˆweightï¼‰æˆ–æƒå€¼å‘é‡ï¼ˆweight vectorï¼‰ï¼Œ$b$å«ä½œåç½®ï¼ˆbiasï¼‰ï¼Œsign æ˜¯ç¬¦å·å‡½æ•°
  $$
  sign(x) = \begin{cases}
     +1 & x \geq 0 \\
     -1 & x<0
  \end{cases}
  $$

æ„ŸçŸ¥æœºæ˜¯ä¸€ç§çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼Œå±äºåˆ¤åˆ«æ¨¡å‹ã€‚æ„ŸçŸ¥æœºæ¨¡å‹çš„å‡è®¾ç©ºé—´æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©º é—´ä¸­çš„æ‰€æœ‰çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼ˆlinear classification modelï¼‰æˆ–çº¿æ€§åˆ†ç±»å™¨(linear classifier)ï¼Œå³ å‡½æ•°é›†åˆ$\{f|f(x)ï¼wÂ·x+b\}$

è¶…å¹³é¢ Sï¼š$w.x+b = 0$,å…¶ä¸­$w$æ˜¯ S çš„æ³•å‘é‡ï¼Œ$b$æ˜¯ S çš„æˆªè·ï¼Œè¶…å¹³é¢ S ç§°ä¸ºåˆ†ç¦»è¶…å¹³é¢ï¼ˆseparating hyperplaneï¼‰

- **ç­–ç•¥**ï¼š
  $$L(w,b) = -\sum_{x_i \in M}{y_i(w.x_i + b)}$$
  å…¶ä¸­$M$ä¸ºè¯¯åˆ†ç±»ç‚¹çš„é›†åˆã€‚è¯¯åˆ†ç±»æ•°æ®$M = \{ (x_i,y_i)|-y_i(w.x_i +b) > 0\}$

å‡½æ•°é—´éš”ï¼š$y(w.x + b)$
å‡ ä½•é—´éš”ï¼š$\frac{1}{||w||}|w.x + b|$ (åœ¨ä¸Šé¢çš„ loss function ä¸­æ²¡æœ‰è€ƒè™‘$\frac{1}{||w||}$)

- **ç®—æ³•**ï¼š
  $$\min_{w,b} L(w,b) = -\sum_{x_i \in M}{y_i(w.x_i + b)}$$
  ä½¿ç”¨**éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆstochastic gradientï¼‰**:

1. åˆå§‹åŒ–å‚æ•°(éšæœºæ³•)ï¼š$w_0,b_0$
2. é€‰å–æ•°æ®$(x_i,y_i)$
3. å¦‚æœ$(x_i,y_i)$æ˜¯è¯¯åˆ†ç±»ç‚¹ï¼Œä¹Ÿå°±æ˜¯$y_i(w.x_i + b) \leq 0$ï¼Œåˆ™å¯¹$w,b$è¿›è¡Œæ›´æ–°
   $$åœ¨(x_i,y_i)ç‚¹å¤„æ¢¯åº¦ä¸ºï¼š\\ \nabla_wL(w,b) = -y_ix_i \\ \nabla_bL(w,b) = -y_i\\ æ›´æ–°wï¼šw_{k+1} \gets w_{k}+\eta y_ix_i \\ æ›´æ–°bï¼šb_{k+1} \gets b_{k}+\eta y_i \\å…¶ä¸­å­¦ä¹ ç‡\eta \in (0,1]$$
4. å¾ªç¯ 2-3ï¼Œç›´åˆ°è®­ç»ƒé›†ä¸­æ²¡æœ‰è¯¯åˆ†ç±»ç‚¹ã€‚

- ä¸Šè¿°**ç®—æ³•çš„æ”¶æ•›æ€§**ï¼š

Novikoff å®šç†ï¼š
è®¾è®­ç»ƒé›†$T = \{(x_1,y_1),...,(x_N,y_N)\}$æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œ

1. è®¾å®Œç¾è¶…å¹³é¢$\hat{w}_{opt}.\hat{x} = 0 , ||\hat{w}_{opt}||=1$ å°†è®­ç»ƒé›†å®Œå…¨æ­£ç¡®åˆ†å¼€ï¼ˆç®€åŒ–èµ·è§ $\hat{w}_{opt}.\hat{x} = w_{opt}.x +b$ï¼‰ï¼Œå­˜åœ¨$\gamma >0$ ,å¯¹æ‰€æœ‰ç‚¹æœ‰$y_i(\hat{w}_{opt}.\hat{x_i}) \geq \gamma$ï¼›

2. ä»¤$R = \max_{1\leq i\leq N}||\hat{x_i}||$,åˆ™ç®—æ³•ä¼šåœ¨æœ‰é™æ­¥ k æ»¡è¶³ä¸ç­‰å¼$k \leq (\frac{R}{\gamma})^2$

è¯æ˜(æ³¨æ„ï¼šå¸¦ hat çš„è¡¨ç¤ºæ‰©å……å‘é‡)ï¼š

1. å› ä¸ºæ•°æ®çº¿æ€§å¯åˆ†ï¼Œå¯¹äºæ‰€æœ‰ç‚¹$y_i(\hat{w}_{opt}.\hat{x_i}) > 0$,æ‰€ä»¥å­˜åœ¨
   $$\gamma = \min_i{y_i(\hat{w}_{opt}.\hat{x_i})} \leq {y_i(\hat{w}_{opt}.\hat{x_i})} \label{2-1}\tag{2-1}$$
   æ‰€ä»¥è¿™é‡Œçš„$\gamma$ä»£è¡¨äº†æ‰€æœ‰ç‚¹ç¦»å®Œç¾è¶…å¹³é¢çš„æœ€å°è·ç¦»ï¼›

2. ä¸ºäº†æ–¹ä¾¿è®¡ç®— è®¾ æ‰©å……å‘é‡$\hat{w} = (w^T,b)^T$ï¼Œ æœ‰
   $$\hat{w}_{k} = \hat{w}_{k-1}+\eta y_i\hat{x_i} \label{2-2}\tag{2-2}$$

3. æ¨å¯¼ä¸ç­‰å¼
   $$\hat{w}_{k}.\hat{w}_{opt} \geq k\eta\gamma \label{2-3}\tag{2-3}$$

ç”±$\eqref{2-1}$å’Œ$\eqref{2-2}$
$$\hat{w}_{k}.\hat{w}_{opt} = \hat{w}_{k-1}.\hat{w}_{opt} + \eta{y_i}\hat{w}_{opt}.\hat{x_i} \\ \geq \hat{w}_{k-1}.\hat{w}_{opt} + \eta\gamma \\ \geq \hat{w}_{k-2}.\hat{w}_{opt} + 2\eta\gamma \\ \geq k\eta\gamma$$

4. æ¨å¯¼ä¸ç­‰å¼
   $$||\hat{w}_{k}||^2 \leq k\eta^2R^2 \label{2-4}\tag{2-4}$$
   ç”±$\eqref{2-2}$
   $$||\hat{w}_{k}||^2=||\hat{w}_{k-1}+\eta y_i\hat{x_i}||^2 = ||\hat{w}_{k-1}||^2 + 2\eta{y_i}\hat{w}_{k-1}.\hat{x}_{i} + \eta^2||\hat{x}_{i}||^2$$
   å‡è®¾ k æ¬¡å®Œå…¨åˆ†å¯¹ï¼Œé‚£ä¹ˆ k-1 æ¬¡æœ‰è¯¯åˆ†ç±»ç‚¹ï¼Œåˆ™${y_i}\hat{w}_{k-1}.\hat{x}_{i} \leq 0$
   æ‰€ä»¥
   $$||\hat{w}_{k}||^2 =||\hat{w}_{k-1}||^2 + 2\eta{y_i}\hat{w}_{k-1}.\hat{x}_{i} + \eta^2||\hat{x}_{i}||^2 \\ \leq ||\hat{w}_{k-1}||^2 +  \eta^2||\hat{x}_{i}||^2 \\ \leq ||\hat{w}_{k-1}||^2 +  \eta^2R^2  \\ \leq ||\hat{w}_{k-2}||^2 +  2\eta^2R^2 \leq ... \\ \leq k\eta^2R^2$$

5. ç”±$\eqref{2-3}$å’Œ$\eqref{2-4}$

$$k\eta\gamma \leq \underbrace{\hat{w}_{k}.\hat{w}_{opt} \leq ||\hat{w}_{k}||.\underbrace{||\hat{w}_{opt}||}_{=1} }_{\text{æŸ¯è¥¿-æ–½ç“¦èŒ¨ (Cauchyâ€“Schwarz) ä¸ç­‰å¼}} \leq \sqrt{k} \eta R \\ \; \\ \Rightarrow k^2\gamma^2 \leq kR^2 \\ \Rightarrow k \leq (\frac{R}{\gamma})^2$$

ä¹Ÿå°±æ˜¯è¯´ k æ˜¯æœ‰ä¸Šç•Œçš„ã€‚

> ä¹¦ä¸­è¿˜ä»‹ç»äº†åŸå½¢å¼çš„**å¯¹å¶å½¢å¼**,ä¹Ÿå°±æ˜¯ç­‰ä»·å½¢å¼ï¼ˆSVM ä¸­ 7.2.2 èŠ‚ 127 é¡µä¹Ÿæ˜¯ç­‰ä»·çš„æ„æ€ï¼ŒåŒºåˆ«äºæ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼‰ï¼Œè¿™ä¸¤ä¸ªåœ°æ–¹çš„ç­‰ä»·éƒ½æ˜¯ç»è¿‡åŸºæœ¬æ¨å¯¼ï¼Œæ±‚å‡º w å‚æ•°ï¼Œç„¶åå¯¹åŸé—®é¢˜è¿›è¡Œäº†æ›¿æ¢ã€‚

### å‚è€ƒæ–‡çŒ®

[2-1] Rosenblatt, F. (1958). [The perceptron: A probabilistic model for information storage and organization in the brain](http://homepages.math.uic.edu/~lreyzin/papers/rosenblatt58.pdf). Psychological Review, 65(6), 386â€“408.

[2-2] Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.

[2-3] Minsky M L and Papert S A 1969 Perceptrons (Cambridge, MA: MIT Press)

[2-4] Gallant, S. I. (1990). Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179-191.

[2-5] Freund, Y. and Schapire, R. E. 1998. Large margin classification using the perceptron algorithm. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT' 98). ACM Press.

[2-6] Li YY,Zaragoza H,Herbrich R,Shawe-Taylor J,Kandola J. The Perceptron algorithmwith uneven margins. In: Proceedings of the 19th International Conference on MachineLearning. 2002,379â€“386

[2-7] [Widrow, B.](https://en.jinzhao.wiki/wiki/Bernard_Widrow), Lehr, M.A., "[30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation,](http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/widrow.pdf)" Proc. IEEE, vol 78, no 9, pp. 1415-1442, (1990)ã€‚

[2-8] Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines and OtherKernelbased Learning Methods. Cambridge University Press,2000

## ç¬¬ 3 ç«  k è¿‘é‚»æ³•

åˆ¤åˆ«æ¨¡å‹

k è¿‘é‚»æ³•ï¼ˆ[k-nearest neighborï¼Œk-NN](https://en.jinzhao.wiki/wiki/K-nearest_neighbors_algorithm)ï¼‰1968 å¹´ç”± Cover å’Œ Hart æå‡ºï¼Œæ˜¯ä¸€ç§åŸºæœ¬åˆ†ç±»ä¸å›å½’æ–¹æ³•ã€‚æœ¬ä¹¦åªè®¨è®ºåˆ†ç±»é—®é¢˜ä¸­çš„ k è¿‘é‚»æ³•ã€‚
k å€¼çš„é€‰æ‹©ã€è·ç¦»åº¦é‡åŠåˆ†ç±»å†³ç­–è§„åˆ™æ˜¯ k è¿‘é‚»æ³•çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ ã€‚
æœ€åè®²è¿° k è¿‘é‚»æ³•çš„ä¸€ä¸ªå®ç°æ–¹æ³•â€”â€”kd æ ‘ï¼Œä»‹ç»æ„é€  kd æ ‘å’Œæœç´¢ kd æ ‘çš„ç®—æ³•

**k è¿‘é‚»æ³•çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ **ï¼š
k å€¼çš„é€‰æ‹©ï¼šè¶…å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯æ³•æ¥é€‰å–æœ€ä¼˜ k å€¼
è·ç¦»åº¦é‡ï¼š$L_2$è·ç¦»/æ¬§æ°è·ç¦»ï¼Œ$L_p$è·ç¦»/Minkowski è·ç¦»
åˆ†ç±»å†³ç­–è§„åˆ™ï¼šå¤šæ•°è¡¨å†³ï¼ˆ0-1 æŸå¤±ä¹Ÿå°±æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼‰

- **æ¨¡å‹**ï¼š
  k è¿‘é‚»æ³•æ²¡æœ‰æ˜¾å¼çš„å­¦ä¹ è¿‡ç¨‹ï¼ˆä¸å­¦ä¹ ä¹Ÿèƒ½é¢„æµ‹ï¼‰ï¼Œå®ƒæœ¬èº«å¹¶æ²¡æœ‰å¯¹æ•°æ®è¿›è¡Œç†è®ºå»ºæ¨¡çš„è¿‡ç¨‹ï¼Œè€Œæ˜¯åˆ©ç”¨è®­ç»ƒæ•°æ®å¯¹ç‰¹å¾å‘é‡ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶å°†å…¶åˆ’åˆ†çš„ç»“æœä½œä¸ºå…¶æœ€ç»ˆçš„ç®—æ³•æ¨¡å‹ã€‚è¿™å°±å¥½æ¯”ï¼Œåœ¨ç°å®ä¸–ç•Œçš„ç»´åº¦ä¸­ï¼Œç»å¸¸æ¸¸èµ°äºç”·å•æ‰€çš„æˆ‘ä»¬å½’ä¸ºç”·æ€§ï¼Œè€Œç»å¸¸åœ¨å¥³å•æ‰€å‡ºæ²¡çš„äººæˆ‘ä»¬å½’ä¸ºå¥³æ€§æˆ–è€…æ˜¯å˜æ€ã€‚

- **ç­–ç•¥**ï¼š
  $$y = \argmin_{c_j} \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1- \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j) $$
  æœ€å¤§åŒ–ç±»åˆ«å±äº$c_j$ç±»çš„æ¦‚ç‡$\frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j)$
  æœ€å°åŒ–è¯¯åˆ†ç±»ç‡$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j)$
  $N_k(x)$è¡¨ç¤ºæ¶µç›– k ä¸ªç‚¹çš„ x çš„é‚»åŸŸ
- **ç®—æ³•**ï¼š
  ç›´æ¥è®¡ç®—ï¼ˆçº¿æ€§æ‰«æ linear scanï¼‰,å½“è®­ç»ƒé›†å¾ˆå¤§æ—¶ï¼Œè®¡ç®—å¾ˆè€—æ—¶ï¼ˆæ¯æ¬¡éƒ½è¦è®¡ç®—æ‰€æœ‰è·ç¦»ï¼Œç„¶åæ‰¾åˆ° k ä¸ªæœ€è¿‘è·ç¦»çš„ç‚¹ï¼‰ï¼Œå› ä¸ºæ²¡æœ‰å­¦ä¹ ã€‚
  ä¸ºäº†æé«˜ k è¿‘é‚»æœç´¢çš„æ•ˆç‡ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç‰¹æ®Šçš„ç»“æ„å­˜å‚¨è®­ç»ƒæ•°æ®ï¼Œä»¥å‡å°‘è®¡ç®—è·ç¦»çš„æ¬¡æ•°ã€‚
  å…·ä½“æ–¹æ³•å¾ˆå¤šï¼Œå¦‚ï¼š[kd_tree](https://en.jinzhao.wiki/wiki/K-d_tree)ï¼Œ[ball_tree](https://arxiv.org/pdf/1511.00628.pdf)ï¼Œbrute(è›®åŠ›å®ç°,ä¸ç®—ä¼˜åŒ–ï¼Œåªæ˜¯æŠŠ sklearn ä¸­çš„å‚æ•°æ‹¿è¿‡æ¥)ï¼Œä»¥åŠå…¶å®ƒ[æ ‘ç»“æ„](<https://en.jinzhao.wiki/wiki/Category:Trees_(data_structures)>)
  ä¸ºäº†æ”¹è¿› KDtree çš„äºŒå‰æ ‘æ ‘å½¢ç»“æ„ï¼Œå¹¶ä¸”æ²¿ç€ç¬›å¡å°”åæ ‡è¿›è¡Œåˆ’åˆ†çš„ä½æ•ˆç‡ï¼Œball tree å°†åœ¨ä¸€ç³»åˆ—åµŒå¥—çš„è¶…çƒä½“ä¸Šåˆ†å‰²æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼šä½¿ç”¨è¶…çƒé¢è€Œä¸æ˜¯è¶…çŸ©å½¢åˆ’åˆ†åŒºåŸŸã€‚è™½ç„¶åœ¨æ„å»ºæ•°æ®ç»“æ„çš„èŠ±è´¹ä¸Šå¤§è¿‡äº KDtreeï¼Œä½†æ˜¯åœ¨é«˜ç»´ç”šè‡³å¾ˆé«˜ç»´çš„æ•°æ®ä¸Šéƒ½è¡¨ç°çš„å¾ˆé«˜æ•ˆã€‚

  ä¸‹é¢ä»‹ç»å…¶ä¸­çš„ kd æ ‘ï¼ˆkd tree æ˜¯ä¸€ä¸ªäºŒå‰æ ‘ï¼‰æ–¹æ³•ï¼ˆkd æ ‘æ˜¯å­˜å‚¨ k ç»´ç©ºé—´æ•°æ®çš„æ ‘ç»“æ„ï¼Œè¿™é‡Œçš„ k ä¸ k è¿‘é‚»æ³•çš„ k æ„ä¹‰ä¸åŒï¼‰ã€‚
  æ•°æ®é›†$T = \{x_1,...,x_N\}$ï¼Œå…¶ä¸­$x_i$æ˜¯ k ç»´å‘é‡$x_i = (x_i^{(1)},...,x_i^{(k)})^T$

  - **æ„é€  kd æ ‘**ï¼š

  ```
  function kdtree (list of points pointList, int depth)
  {
      // Select axis based on depth so that axis cycles through all valid values
      var int axis := depth mod k;

      // Sort point list and choose median as pivot element
      select median by axis from pointList;

      // Create node and construct subtree
      node.location := median;
      node.leftChild := kdtree(points in pointList before median, depth+1);
      node.rightChild := kdtree(points in pointList after median, depth+1);
      return node;
  }
  ```

  1. æ ¹æ®ç¬¬(depth mod k)ç»´æŸ¥æ‰¾ä¸­ä½æ•°ï¼ˆä¸­ä½æ•°æ‰€åœ¨çš„ç‚¹ä½œä¸ºèŠ‚ç‚¹ï¼Œç¬¬ä¸€æ¬¡å°±æ˜¯ root èŠ‚ç‚¹ï¼‰ï¼Œå°†æ•°æ®åˆ’åˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸï¼Œå°äºä¸­ä½æ•°çš„åˆ’åˆ†åœ¨å·¦è¾¹ï¼Œå¤§äºä¸­ä½æ•°çš„åˆ’åˆ†åœ¨å³è¾¹
  2. é‡å¤ 1ï¼Œdepth++

  - **æœç´¢ kd æ ‘**ï¼š

  1. åœ¨ kd æ ‘ä¸­æ‰¾å‡ºåŒ…å«ç›®æ ‡ç‚¹ x çš„å¶ç»“ç‚¹ï¼šä»æ ¹ç»“ç‚¹å‡ºå‘ï¼Œé€’å½’åœ°å‘ä¸‹è®¿é—® kd æ ‘ã€‚è‹¥ç›®æ ‡ç‚¹ x å½“å‰ç»´çš„åæ ‡å°äºåˆ‡åˆ†ç‚¹çš„åæ ‡ï¼Œåˆ™ç§»åŠ¨åˆ°å·¦å­ç»“ç‚¹ï¼Œå¦åˆ™ç§»åŠ¨åˆ°å³å­ç»“ç‚¹ã€‚ç›´åˆ°å­ç»“ç‚¹ä¸ºå¶ç»“ç‚¹ä¸ºæ­¢ã€‚
  2. ä»¥æ­¤å¶ç»“ç‚¹ä¸ºâ€œå½“å‰æœ€è¿‘ç‚¹â€ã€‚
  3. é€’å½’åœ°å‘ä¸Šå›é€€ï¼Œåœ¨æ¯ä¸ªç»“ç‚¹è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
     a. å¦‚æœè¯¥ç»“ç‚¹ä¿å­˜çš„å®ä¾‹ç‚¹æ¯”å½“å‰æœ€è¿‘ç‚¹è·ç¦»ç›®æ ‡ç‚¹æ›´è¿‘ï¼Œåˆ™ä»¥è¯¥å®ä¾‹ç‚¹ä¸ºâ€œå½“å‰æœ€è¿‘ç‚¹â€ã€‚
     b. å½“å‰æœ€è¿‘ç‚¹ä¸€å®šå­˜åœ¨äºè¯¥ç»“ç‚¹ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸã€‚æ£€æŸ¥è¯¥å­ç»“ç‚¹çš„çˆ¶ç»“ç‚¹çš„å¦ä¸€å­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸæ˜¯å¦æœ‰æ›´è¿‘çš„ç‚¹ã€‚å…·ä½“åœ°ï¼Œæ£€æŸ¥å¦ä¸€å­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸæ˜¯å¦ä¸ä»¥ç›®æ ‡ç‚¹ä¸ºçƒå¿ƒã€ä»¥ç›®æ ‡ç‚¹ä¸â€œå½“å‰æœ€è¿‘ç‚¹â€é—´çš„è·ç¦»ä¸ºåŠå¾„çš„è¶…çƒä½“ç›¸äº¤ã€‚
     å¦‚æœç›¸äº¤ï¼Œå¯èƒ½åœ¨å¦ä¸€ä¸ªå­ç»“ç‚¹å¯¹åº”çš„åŒºåŸŸå†…å­˜åœ¨è·ç›®æ ‡ç‚¹æ›´è¿‘çš„ç‚¹ï¼Œç§»åŠ¨åˆ°å¦ä¸€ä¸ªå­ç»“ç‚¹ã€‚æ¥ç€ï¼Œé€’å½’åœ°è¿›è¡Œæœ€è¿‘é‚»æœç´¢ï¼›
     å¦‚æœä¸ç›¸äº¤ï¼Œå‘ä¸Šå›é€€ã€‚
  4. å½“å›é€€åˆ°æ ¹ç»“ç‚¹æ—¶ï¼Œæœç´¢ç»“æŸã€‚æœ€åçš„â€œå½“å‰æœ€è¿‘ç‚¹â€å³ä¸º x çš„æœ€è¿‘é‚»ç‚¹ã€‚
     å¦‚æœå®ä¾‹ç‚¹æ˜¯éšæœºåˆ†å¸ƒçš„ï¼Œkd æ ‘æœç´¢çš„å¹³å‡è®¡ç®—å¤æ‚åº¦æ˜¯ O(logN)ï¼Œè¿™é‡Œ N æ˜¯è®­ç»ƒå®ä¾‹æ•°ã€‚kd æ ‘æ›´é€‚ç”¨äºè®­ç»ƒå®ä¾‹æ•°è¿œå¤§äºç©ºé—´ç»´æ•°æ—¶çš„ k è¿‘é‚»æœç´¢ã€‚å½“ç©ºé—´ç»´æ•°æ¥è¿‘è®­ç»ƒå®ä¾‹æ•°æ—¶ï¼Œå®ƒçš„æ•ˆç‡ä¼šè¿…é€Ÿä¸‹é™ï¼Œå‡ ä¹æ¥è¿‘çº¿æ€§æ‰«æã€‚

  | ç®—æ³• | å¹³å‡        | æœ€å·®çš„æƒ…å†µ |
  | ---- | ----------- | ---------- |
  | ç©ºé—´ | $O(n)$      | $O(n)$     |
  | æœç´¢ | $O(\log n)$ | $O(n)$     |
  | æ’å…¥ | $O(\log n)$ | $O(n)$     |
  | åˆ é™¤ | $O(\log n)$ | $O(n)$     |

### é™„åŠ çŸ¥è¯†

#### è·ç¦»åº¦é‡

[Distance](https://en.jinzhao.wiki/wiki/Category:Distance)

[sklearn.neighbors.DistanceMetric](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)

[Distance computations(scipy.spatial.distance)](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)

[24 ç§è·ç¦»åº¦é‡å°ç»“](https://blog.csdn.net/weixin_43840403/article/details/89075759)

> å…ˆäº†è§£åº¦é‡ç©ºé—´å’Œèµ‹èŒƒç©ºé—´

å®å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Euclidean(æ¬§å‡ é‡Œå¾—è·ç¦»ä¹Ÿç§°æ¬§å¼è·ç¦») ${||u-v||}_2$ or $\sqrt{\sum_i{(u_i - v_i)^2}}$
- SEuclidean(æ ‡å‡†åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»)
- SqEuclidean(å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»)
- Mahalanobis(é©¬æ°è·ç¦») $\sqrt{ (u-v) \Sigma^{-1} (u-v)^T }$
- Manhattan/cityblock(åŸå¸‚è¡—åŒºï¼ˆæ›¼å“ˆé¡¿ï¼‰è·ç¦») $\sum_i{|u_i-v_i|}$
- Chebyshev(åˆ‡æ¯”é›ªå¤«è·ç¦») $L_\infty$åº¦é‡ $\max_i{|u_i-v_i|}$
- Minkowski(é—µå¯å¤«æ–¯åŸºè·ç¦») æ¬§å¼è·ç¦»çš„æ¨å¹¿ï¼Œp=1 æ—¶ç­‰ä»·äºæ›¼å“ˆé¡¿è·ç¦»ï¼Œp=2 æ—¶ç­‰ä»·äºæ¬§æ°è·ç¦»ï¼Œp=âˆ æ—¶ç­‰ä»·äºåˆ‡æ¯”é›ªå¤«è·ç¦»;$\sqrt[p]{\sum_i{(u_i - v_i)^p}}$
- WMinkowski(åŠ æƒ Minkowski)

å®å€¼å‘é‡ç©ºé—´çš„åº¦é‡(scipy)ï¼š

- Correlation(çš®å°”é€Šç›¸å…³ç³»æ•°(Pearson Correlation))
- Cosine(ä½™å¼¦è·ç¦»)
- JensenShannon(JS æ•£åº¦ä¹Ÿç§° JS è·ç¦»ï¼Œæ˜¯ KL æ•£åº¦çš„ä¸€ç§å˜å½¢)

æ•´æ•°å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Hamming(æ±‰æ˜è·ç¦»)
- Canberra(å ªåŸ¹æ‹‰è·ç¦»)
- BrayCurtis(å¸ƒé›·æŸ¯è’‚æ–¯è·ç¦»)

å¸ƒå°”å€¼å‘é‡ç©ºé—´çš„åº¦é‡ï¼š

- Jaccard(Jaccard-Needham ä¸ç›¸ä¼¼åº¦)
- Matching(Hamming åŒä¹‰è¯)
- Dice(Dice ç³»æ•°)
- Kulsinski(Kulsinski ç›¸å¼‚åº¦)
- RogersTanimoto(Rogers-Tanimoto ç›¸å¼‚åº¦)
- RussellRao(Russell-Rao ç›¸å¼‚æ€§)
- SokalMichener(Sokal-Michener ç›¸å¼‚æ€§)
- SokalSneath(Sokal-Sneath ç›¸å¼‚æ€§)
- Yuleï¼ˆscipy ä¸­çš„ Yule ç›¸å¼‚åº¦ï¼‰

ç»çº¬åº¦è·ç¦»ï¼š

- Haversine(sklearn ä¸­çš„åŠæ­£çŸ¢è·ç¦»)

å…¶å®ƒï¼š

- ç›¸å¯¹ç†µåˆç§° KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰[scipy.special.kl_div](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.kl_div.html)
- äº¤å‰ç†µï¼ˆCross Entropyï¼‰ [scipy.stats.entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)

### å‚è€ƒæ–‡çŒ®

[3-1] Cover T,Hart P. Nearest neighbor pattern classification. IEEE Transactions onInformation Theory,1967

[3-2] Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[3-3] Friedman J. Flexible metric nearest neighbor classification. Technical Report,1994

[3-4] Weinberger KQ,Blitzer J,Saul LK. Distance metric learning for large margin nearestneighbor classification. In: Proceedings of the NIPS. 2005

[3-5] Samet H. The Design and Analysis of Spatial Data Structures. Reading,MA: Addison-Wesley,1990

## ç¬¬ 4 ç«  æœ´ç´ è´å¶æ–¯æ³•

æœ´ç´ è´å¶æ–¯ï¼ˆ[NaÃ¯ve Bayes](https://en.jinzhao.wiki/wiki/Naive_Bayes_classifier)ï¼‰æ³•æ˜¯åŸºäº**è´å¶æ–¯å®šç†**ä¸**ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼ˆNaive å¤©çœŸçš„ï¼‰çš„åˆ†ç±»æ–¹æ³•ã€‚
å¯¹äºç»™å®šçš„è®­ç»ƒæ•°æ®é›†ï¼Œé¦–å…ˆåŸºäºç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å­¦ä¹ è¾“å…¥/è¾“å‡ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼›ç„¶ååŸºäºæ­¤æ¨¡å‹ï¼Œå¯¹ç»™å®šçš„è¾“å…¥ xï¼Œåˆ©ç”¨è´å¶æ–¯å®šç†æ±‚å‡ºåéªŒæ¦‚ç‡æœ€å¤§çš„è¾“å‡º yã€‚
æœ´ç´ è´å¶æ–¯æ³•å®ç°ç®€å•ï¼Œå­¦ä¹ ä¸é¢„æµ‹çš„æ•ˆç‡éƒ½å¾ˆé«˜ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•ã€‚å¹¶ä¸”æ”¯æŒ online learningï¼ˆæœ‰ partial_fit æ–¹æ³•ï¼‰ã€‚

æœ´ç´ è´å¶æ–¯æ³•æ˜¯å…¸å‹çš„**ç”Ÿæˆå­¦ä¹ æ–¹æ³•**ã€‚ç”Ÿæˆæ–¹æ³•ç”±è®­ç»ƒæ•°æ®å­¦ä¹ è”åˆæ¦‚ç‡åˆ†å¸ƒ P(X,Y)ï¼Œç„¶åæ±‚å¾—åéªŒæ¦‚ç‡åˆ†å¸ƒ P(Y|X)ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®å­¦ä¹  P(X|Y)å’Œ P(Y)çš„ä¼°è®¡ï¼Œå¾—åˆ°è”åˆæ¦‚ç‡åˆ†å¸ƒï¼šP(X,Y)ï¼ P(Y)P(X|Y) ï¼›æ¦‚ç‡ä¼°è®¡æ–¹æ³•å¯ä»¥æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡æˆ–è´å¶æ–¯ä¼°è®¡ã€‚

**[è´å¶æ–¯å®šç†(Bayes' theorem)](https://en.jinzhao.wiki/wiki/Bayes%27_theorem)**ï¼š
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

- $P(A|B)$ æ˜¯æ¡ä»¶æ¦‚ç‡[conditional probability](https://en.jinzhao.wiki/wiki/Conditional_probability)ï¼šæ˜¯å·²çŸ¥ B å‘ç”Ÿåï¼ŒA çš„æ¦‚ç‡ï¼Œä¹Ÿè¢«ç§°ä¸º å·²çŸ¥ B çš„æƒ…å†µä¸‹ A çš„åéªŒæ¦‚ç‡[posterior probability](https://en.jinzhao.wiki/wiki/Posterior_probability)

- $P(B|A)$ ä¹Ÿæ˜¯ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ï¼šå·²çŸ¥ A æ—¶ï¼ŒB çš„ä¼¼ç„¶æ€§/å¯èƒ½æ€§([likelihood](https://en.jinzhao.wiki/wiki/Likelihood_function)), ä¸ºä»€ä¹ˆå« likelihoodï¼Ÿå› ä¸º$P(B|A) = L(A|B) ^{å‚è§ï¼šé™„åŠ çŸ¥è¯†-å‚æ•°ä¼°è®¡-æå¤§ä¼¼ç„¶ä¼°è®¡}$

- $P(A)$ å« A çš„è¾¹é™…æ¦‚ç‡([marginal probability](https://en.jinzhao.wiki/wiki/Marginal_probability))æˆ–å…ˆéªŒæ¦‚ç‡([prior probability](https://en.jinzhao.wiki/wiki/Prior_probability))

- $P(B)$ å« B çš„è¾¹é™…æ¦‚ç‡æˆ–å…ˆéªŒæ¦‚ç‡ï¼Œä¹Ÿç§°ä¸º evidence è¯æ®

**[ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾](https://en.jinzhao.wiki/wiki/Conditional_independence)**ï¼š
**æ¡ä»¶ç‹¬ç«‹**
$$(A\perp B|C) \iff P(A|B,C) = P(A|C) \\ (A\perp B|C) \iff P(A,B|C) = P(A|C)P(B|C)$$

ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å°±æ˜¯å·²çŸ¥ y çš„æƒ…å†µä¸‹ï¼Œx ä¸­æ¯ä¸ªç‰¹å¾ç›¸äº’ç‹¬ç«‹ã€‚

æ•°æ®é›†$T = \{(x_1,y_1),...,(x_N,y_N)\}$ï¼Œ$K$ä¸ºç±»åˆ«ä¸ªæ•°,å…¶ä¸­$x_i$æ˜¯ n ç»´å‘é‡$x_i = (x_i^{(1)},...,x_i^{(n)})^T$

- **æ¨¡å‹**ï¼š
  $$\underbrace{P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}}_{\text{è´å¶æ–¯å®šç†}} \varpropto P(Y=c_k) \underbrace{\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}}_{\text{ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾}}$$

  å…¶ä¸­
  $$P(X=x) = \sum_k{P(X=x|Y=c_k)P(Y=c_k)} = \sum_k{P(Y=c_k) \prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}}$$
  P(X)æ˜¯ evidence(å·²çŸ¥çš„,è§‚å¯Ÿå˜é‡)ï¼Œå¯çœ‹åšå¸¸æ•°(ä¹Ÿå¯ä»¥è¯´å¯¹$c_k$æ¥è¯´åˆ†æ¯ P(X)æ˜¯ç›¸åŒçš„ï¼Œæ±‚æœ€å¤§æœ€å°æ—¶å¯ä»¥å»æ‰)ï¼Œåˆ™ï¼š
  $$P(Y=c_k|X=x) \varpropto {P(X=x|Y=c_k)P(Y=c_k)}$$

- **ç­–ç•¥**ï¼š
  åéªŒæœ€å¤§åŒ–ï¼ˆç­‰ä»· 0-1 æŸå¤±ï¼‰ï¼š
  $$y = \argmax_{c_k} P(Y=c_k|X=x)= \argmax_{c_k}P(Y=c_k)\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}$$
  åŸç†ï¼ˆè¯æ˜ï¼‰ï¼šä½¿ç”¨ 0-1 æŸå¤±
  $$\argmin_{y}\sum_{k=1}^K P(y \neq c_k|X=x) \\= \argmin_{y}(1- P(y = c_k|X=x)) \\= \argmax_y P(y = c_k|X=x)$$

- **ç®—æ³•**ï¼šå‚æ•°ä¼°è®¡
  æˆ‘ä»¬éœ€è¦çŸ¥é“$P(Y=c_k)$ä»¥åŠ$\prod_j{P(X^{(j)}=x^{(j)}|Y=c_k)}$
  **æå¤§ä¼¼ç„¶ä¼°è®¡**ï¼š

  1. å…ˆéªŒ$P(Y=c_k)$çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ï¼š
     $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k)}{N}$$
  2. ç¬¬$j$ä¸ªç‰¹å¾$x^{(j)}$çš„å–å€¼é›†åˆæ˜¯$\{a_{j1},...,a_{jS_j}\}$,([æ³¨æ„è¿™é‡Œç”¨çš„éƒ½æ˜¯é¢‘ç‡è®¡æ•°ï¼Œä¹Ÿå°±æ˜¯ç¦»æ•£ç‰¹å¾ï¼Œå¦‚æœæ˜¯è¿ç»­ç‰¹å¾å˜é‡ï¼Œåˆ™ä½¿ç”¨é«˜æ–¯æœ´ç´ è´å¶æ–¯](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes))
     æ¡ä»¶æ¦‚ç‡(likelihood)$P(X^{(j)}=x^{(j)}|Y=c_k)$çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ï¼š
     $$P(X^{(j)} = a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} =a_{jl} , y_i = c_k)}{\sum_{i=1}^N I(y_i=c_k)}$$
     å…¶ä¸­$j = 1,2,...N; \quad l=1,2...S_j ;\quad k=1,2,...K$ï¼Œ$x_i^{(j)}$æ˜¯ç¬¬$i$ä¸ªæ ·æœ¬çš„ç¬¬$j$ä¸ªç‰¹å¾ï¼›$a_{jl}$æ˜¯ç¬¬$j$ä¸ªç‰¹å¾å¯èƒ½å–å€¼çš„ç¬¬$l$ä¸ªå€¼ã€‚

  **è´å¶æ–¯ä¼°è®¡**ï¼ˆsmoothed version of maximum likelihoodï¼‰ï¼š
  æå¤§ä¼¼ç„¶ä¼°è®¡æœ‰ä¸€ä¸ªé—®é¢˜å°±æ˜¯æ¡ä»¶æ¦‚ç‡$P(X^{(j)}=x^{(j)}|Y=c_k)$æœ‰ä¸€ä¸ªä¸º 0ï¼Œå°±ä¼šå‡ºç°æ— æ³•ä¼°è®¡çš„æƒ…å†µ(å°±æ˜¯æ¦‚ç‡ä¸º 0)ï¼Œä¹Ÿå°±æ˜¯ç»™å®šè¦é¢„æµ‹çš„ç‰¹å¾å‘é‡çš„ä¸€ä¸ªç‰¹å¾å‡ºç°äº†æ–°çš„ç±»åˆ«ï¼ˆå¦‚ï¼šç¬¬$j$ä¸ªç‰¹å¾$x^{(j)} = a_{jS_j+1}$ï¼‰ï¼Œé‚£ä¹ˆå°±ä¼šå¯¼è‡´æ¦‚ç‡ä¸º 0ï¼Œè¿™æ˜¯è¦ä¹ˆå¢åŠ æ ·æœ¬æ•°é‡ï¼Œè¦ä¹ˆä½¿ç”¨è´å¶æ–¯ä¼°è®¡

  > æ³¨æ„ï¼šæœ´ç´ è´å¶æ–¯æ³•ä¸è´å¶æ–¯ä¼°è®¡ï¼ˆBayesian estimationï¼‰æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚

  1. å…ˆéªŒ$P(Y=c_k)$çš„è´å¶æ–¯ä¼°è®¡æ˜¯ï¼š
     $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$$
  2. æ¡ä»¶æ¦‚ç‡(likelihood)çš„è´å¶æ–¯ä¼°è®¡æ˜¯ï¼š
     $$P(X^{(j)} = a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl} , y_i = c_k) + \lambda}{\sum_{i=1}^N I(y_i=c_k) + S_j\lambda}$$

  å…¶ä¸­$\lambda \geq 0$,å½“$\lambda = 0$æ—¶å°±ç­‰ä»·äºæå¤§ä¼¼ç„¶ä¼°è®¡ï¼›å½“$\lambda = 1$æ—¶ï¼Œç§°ä¸ºæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆ[Laplacian smoothing](https://en.jinzhao.wiki/wiki/Laplacian_smoothing)ï¼‰ï¼›å½“$\lambda < 1$æ—¶ä¸º Lidstone å¹³æ»‘

  > é«˜æ–¯æœ´ç´ è´å¶æ–¯:æ¡ä»¶æ¦‚ç‡(likelihood)
  > $$P(X^{(j)} = x^{(j)}|Y=c_k) = \frac{1}{\sqrt{2\pi\sigma_{j,k}^2}} exp\bigg(-\frac{(x^{(j)}-\mu_{j,k})^2}{2\sigma_{j,k}^2}\bigg) $$
  > å…¶ä¸­$\mu_{j,k}$ä¸ºæ ·æœ¬ä¸­ç±»åˆ«ä¸º$c_k$çš„ æ‰€æœ‰$x^{(j)}$çš„å‡å€¼ï¼›$\sigma_{j,k}^2$ä¸ºæ ·æœ¬ä¸­ç±»åˆ«ä¸º$c_k$çš„ æ‰€æœ‰$x^{(j)}$çš„æ–¹å·®ï¼ˆå…¶å®å°±æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡å‡å€¼å’Œæ–¹å·®ï¼‰ã€‚
  > sklearn ä¸­ GaussianNB ç±»çš„ä¸»è¦å‚æ•°ä»…æœ‰ä¸€ä¸ªï¼Œå³å…ˆéªŒæ¦‚ç‡ priors ï¼Œå¯¹åº” Y çš„å„ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡$P(Y=c_k)$ã€‚è¿™ä¸ªå€¼é»˜è®¤ä¸ç»™å‡ºï¼Œå¦‚æœä¸ç»™å‡ºæ­¤æ—¶$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$ã€‚å¦‚æœç»™å‡ºçš„è¯å°±ä»¥ priors ä¸ºå‡†ã€‚

### é™„åŠ çŸ¥è¯†

#### å‚æ•°ä¼°è®¡

å‚æ•°ä¼°è®¡([Parameter Estimation](https://en.jinzhao.wiki/wiki/Estimation_theory)) æœ‰ç‚¹ä¼°è®¡ï¼ˆ[point estimation](https://en.jinzhao.wiki/wiki/Point_estimation)ï¼‰å’ŒåŒºé—´ä¼°è®¡ï¼ˆ[interval estimation](https://en.jinzhao.wiki/wiki/Interval_estimation)ï¼‰ä¸¤ç§

**ç‚¹ä¼°è®¡æ³•ï¼š**

- **æå¤§ä¼¼ç„¶ä¼°è®¡([Maximum likelihood estimation, MLE](https://en.jinzhao.wiki/wiki/Maximum_likelihood_estimation))**
  æå¤§ä¼¼ç„¶ä¼°è®¡æ˜¯å…¸å‹çš„**é¢‘ç‡å­¦æ´¾**è§‚ç‚¹ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå¾…ä¼°è®¡å‚æ•°$\theta$ æ˜¯å®¢è§‚å­˜åœ¨çš„ï¼Œåªæ˜¯æœªçŸ¥è€Œå·²
  $$L(\theta|x) = f(x|\theta) = P(X|\theta) \\ \hat{\theta}_{MLE} = \argmax_{\theta} L(\theta|x)$$
  è¿™é‡Œç”¨ | å’Œ ; æ˜¯ç­‰ä»·çš„; è¦æœ€å¤§åŒ– Lï¼Œå¯¹ L æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º 0 å³å¯æ±‚è§£ã€‚
  $P(X|\theta)$å°±æ˜¯è´å¶æ–¯å…¬å¼ä¸­çš„ likelihoodï¼Œ$\theta$å°±æ˜¯$c_k$
  log-likelihood:$\ell(\theta|x) = \log L(\theta|x)$ï¼ˆlog å‡½æ•°å¹¶ä¸å½±å“å‡½æ•°çš„å‡¹å‡¸æ€§ï¼‰

- **æœ€å¤§åéªŒä¼°è®¡([maximum a posteriori estimation, MAP](https://en.jinzhao.wiki/wiki/Maximum_a_posteriori_estimation))**
  è´å¶æ–¯å®šç†ï¼š
  $$f(\theta|x) = \frac{f(x|\theta)g(\theta)}{\int_\vartheta f(x|\vartheta)g(\vartheta)d\vartheta}$$
  $g$ æ˜¯$\theta $çš„å¯†åº¦å‡½æ•°ï¼ˆdensity functionï¼‰
  $$\hat{\theta}_{MAP} = \argmax_{\theta} f(\theta|x) \\= \argmax_{\theta} \frac{f(x|\theta)g(\theta)}{\int_\vartheta f(x|\vartheta)g(\vartheta)d\vartheta} \\= \argmax_{\theta}f(x|\theta)g(\theta)$$
  è¿™é‡Œåˆ†æ¯ä¸$\theta$æ— å…³ï¼Œå¯ä»¥çœç•¥
  æˆ‘ä»¬å°†likelihoodå˜æˆlog-likelihoodï¼š
  $$\hat{\theta}_{MAP} =  \argmax_{\theta}\log{f(x|\theta)g(\theta)} =  \argmax_{\theta} (\log{f(x|\theta)} + \log{g(\theta)})$$
  è¿™æ ·æˆ‘ä»¬å¯ä»¥å°†$\log{g(\theta)}$çœ‹ä½œæœºå™¨å­¦ä¹ ç»“æ„é£é™©ä¸­çš„**æ­£åˆ™åŒ–é¡¹**ï¼Œé‚£ä¹ˆå¸¦æœ‰æ­£åˆ™åŒ–é¡¹çš„æœ€å¤§ä¼¼ç„¶å­¦ä¹ å°±å¯ä»¥è¢«è§£é‡Šä¸º MAPï¼ˆå¦‚ï¼š[Ridge å›å½’å’Œ Lasso å›å½’](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)ï¼‰ã€‚
  å½“ç„¶ï¼Œè¿™å¹¶ä¸æ˜¯æ€»æ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚ï¼Œæœ‰äº›æ­£åˆ™åŒ–é¡¹å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å¯¹æ•°ï¼Œè¿˜æœ‰äº›æ­£åˆ™åŒ–é¡¹ä¾èµ–äºæ•°æ®ï¼Œå½“ç„¶ä¹Ÿä¸ä¼šæ˜¯ä¸€ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒã€‚ä¸è¿‡ï¼ŒMAP æä¾›äº†ä¸€ä¸ªç›´è§‚çš„æ–¹æ³•æ¥è®¾è®¡å¤æ‚ä½†å¯è§£é‡Šçš„æ­£åˆ™åŒ–é¡¹ï¼Œä¾‹å¦‚ï¼Œæ›´å¤æ‚çš„æƒ©ç½šé¡¹å¯ä»¥é€šè¿‡æ··åˆé«˜æ–¯åˆ†å¸ƒä½œä¸ºå…ˆéªŒå¾—åˆ°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„é«˜æ–¯åˆ†å¸ƒã€‚

  > æœ€å¤§åéªŒä¼°è®¡å°±æ˜¯**è€ƒè™‘åéªŒåˆ†å¸ƒæå¤§åŒ–è€Œæ±‚è§£å‚æ•°**çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼›MAP = æœ€å¤§ä¼¼ç„¶ä¼°è®¡ + æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ­£åˆ™åŒ–ã€‚
  > è¦æœ€å¤§åŒ– Lï¼Œå¯¹ L æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º 0 å³å¯æ±‚è§£ã€‚

- **è´å¶æ–¯ä¼°è®¡([Bayes estimation](https://en.jinzhao.wiki/wiki/Bayes_estimator))**
  è´å¶æ–¯ä¼°è®¡æ˜¯å…¸å‹çš„**è´å¶æ–¯å­¦æ´¾**è§‚ç‚¹ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå¾…ä¼°è®¡å‚æ•° $\theta$ ä¹Ÿæ˜¯éšæœºå˜é‡ï¼Œå› æ­¤éœ€è¦æ ¹æ®è§‚æµ‹æ ·æœ¬ä¼°è®¡å‚æ•° $\theta$ çš„åˆ†å¸ƒã€‚**è´å¶æ–¯ä¼°è®¡éœ€è¦è¦è®¡ç®—æ•´ä¸ªåéªŒæ¦‚ç‡çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆè€Œ MAP å€¼éœ€è¦æ±‚è§£åéªŒåˆ†å¸ƒæå¤§åŒ–æ—¶çš„å‚æ•°$\theta$ï¼‰**ã€‚

  è´å¶æ–¯ä¼°è®¡å’Œ MAP æŒºåƒçš„ï¼Œéƒ½æ˜¯ä»¥æœ€å¤§åŒ–åéªŒæ¦‚ç‡ä¸ºç›®çš„ã€‚åŒºåˆ«åœ¨äºï¼š

  1.  MLE å’Œ MAP éƒ½æ˜¯åªè¿”å›äº†çš„é¢„ä¼°å€¼
  2.  MAP åœ¨è®¡ç®—åéªŒæ¦‚ç‡çš„æ—¶å€™ï¼ŒæŠŠåˆ†æ¯ p(X)ç»™å¿½ç•¥äº†ï¼Œåœ¨è¿›è¡Œè´å¶æ–¯ä¼°è®¡çš„æ—¶å€™åˆ™ä¸èƒ½å¿½ç•¥
  3.  è´å¶æ–¯ä¼°è®¡è¦è®¡ç®—æ•´ä¸ªåéªŒæ¦‚ç‡çš„æ¦‚ç‡åˆ†å¸ƒ

> **å…±è½­å…ˆéªŒï¼ˆ[Conjugate prior](https://en.jinzhao.wiki/wiki/Conjugate_prior)ï¼‰**ï¼šå¦‚æœå…ˆéªŒåˆ†å¸ƒ prior å’ŒåéªŒåˆ†å¸ƒ posterior å±äºåŒä¸€åˆ†å¸ƒç°‡ï¼Œåˆ™ prior ç§°ä¸º likehood çš„å…±è½­å…ˆéªŒ
> likehood ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œprior ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸ºé«˜æ–¯åˆ†å¸ƒã€‚
> likehood ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹å¼åˆ†å¸ƒï¼‰ï¼Œprior ä¸º beta åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸º beta åˆ†å¸ƒã€‚
> likehood ä¸ºå¤šé¡¹å¼åˆ†å¸ƒï¼Œprior ä¸º Dirichlet åˆ†å¸ƒï¼ˆbeta åˆ†å¸ƒçš„ä¸€ä¸ªæ‰©å±•ï¼‰ï¼Œåˆ™ posterior ä¹Ÿä¸º Dirichletï¼ˆç‹„åˆ©å…‹é›·ï¼‰åˆ†å¸ƒã€‚beta åˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯ dirichlet åˆ†å¸ƒçš„ç‰¹æ®Šæƒ…å†µã€‚

æœ€å°äºŒä¹˜ä¼°è®¡([Least squares estimation, LSE](https://en.jinzhao.wiki/wiki/Least_squares))

çŸ©ä¼°è®¡(Method of moments estimators)

**åŒºé—´ä¼°è®¡æ³•ï¼š**
åŒºé—´ä¼°è®¡æœ€æµè¡Œçš„å½¢å¼æ˜¯ç½®ä¿¡åŒºé—´ [confidence intervals](https://en.jinzhao.wiki/wiki/Confidence_interval) ï¼ˆä¸€ç§[é¢‘ç‡è®ºæ–¹æ³•](https://en.jinzhao.wiki/wiki/Frequentism)ï¼‰å’Œ å¯ä¿¡åŒºé—´ [credible intervals](https://en.jinzhao.wiki/wiki/Credible_interval)ï¼ˆä¸€ç§[è´å¶æ–¯æ–¹æ³•](https://en.jinzhao.wiki/wiki/Bayesian_method)ï¼‰ï¼Œæ­¤å¤–è¿˜æœ‰é¢„æµ‹åŒºé—´ï¼ˆ[Prediction interval](https://en.jinzhao.wiki/wiki/Prediction_interval)ï¼‰ç­‰

**é‡‡æ ·æ³•ï¼š** è´å¶æ–¯ä¼°è®¡ï¼Œè¿‘ä¼¼æ¨æ–­
é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³• [Markov chain Monte Carlo, MCMC](https://en.jinzhao.wiki/wiki/Markov_chain_Monte_Carlo)

### å‚è€ƒæ–‡çŒ®

[4-1] Mitchell TM. Chapter 1: [Generative and discriminative classifiers: NaÃ¯ve Bayes andlogistic regression. In: Machine Learning.](http://www.cs.cmu.edu/~tom/mlbook/NBayeslogReg.pdf) Draft,2005.

[4-2] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning. DataMining,Inference,and Prediction. ](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf) Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[4-3] Bishop C. [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf),Springer,2006

## ç¬¬ 5 ç«  å†³ç­–æ ‘

åˆ¤åˆ«æ¨¡å‹

å†³ç­–æ ‘ï¼ˆ[decision tree](https://en.jinzhao.wiki/wiki/Decision_tree_learning)ï¼‰æ˜¯ä¸€ç§åŸºæœ¬çš„åˆ†ç±»ä¸å›å½’æ–¹æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§(å¯è§†åŒ–)ï¼Œé€šå¸¸åŒ…æ‹¬ 3 ä¸ªæ­¥éª¤ï¼šç‰¹å¾é€‰æ‹©ã€å†³ç­–æ ‘çš„ç”Ÿæˆå’Œå†³ç­–æ ‘çš„ä¿®å‰ª
![](https://scikit-learn.org/stable/_images/iris.png)
**ç‰¹å¾é€‰æ‹©**ï¼š
ç‰¹å¾é€‰æ‹©åœ¨äºé€‰å–å¯¹è®­ç»ƒæ•°æ®å…·æœ‰åˆ†ç±»èƒ½åŠ›çš„ç‰¹å¾ã€‚ï¼ˆsklearn ä¸­å¯ä»¥è¿”å› `feature_importances_`ç‰¹å¾é‡è¦æ€§ï¼Œå±æ€§è¶Šé‡è¦ï¼Œç‰¹å¾ç©ºé—´åˆ’åˆ†çš„é¢ç§¯è¶Šå¤§ï¼‰

ä¹Ÿå°±æ˜¯è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ï¼ˆä¿¡æ¯å¢ç›Šï¼ŒåŸºå°¼æŒ‡æ•°ï¼‰æ¥é€‰æ‹©ç‰¹å¾ï¼ˆä½œä¸ºæ ¹èŠ‚ç‚¹ï¼‰è¿›è¡Œç‰¹å¾ç©ºé—´åˆ’åˆ†ï¼Œæ³¨æ„ï¼šåˆ’åˆ†åå†æ¬¡è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ï¼ˆä¿¡æ¯å¢ç›Šï¼ŒåŸºå°¼æŒ‡æ•°ï¼‰ï¼Œé™¤éè¯¥ç‰¹å¾æ‰€åœ¨çš„ç©ºé—´å°±åªæœ‰ä¸€ç±»äº†ï¼ˆä¹Ÿå°±æ˜¯è¯¥ç‰¹å¾ä¸å¯åˆ†äº†ï¼Œé‚£ä¹ˆå°±ç›´æ¥ç”Ÿæˆå¶å­èŠ‚ç‚¹ï¼‰ï¼›

ç‰¹å¾é€‰æ‹©çš„å‡†åˆ™ï¼š

- ä¿¡æ¯å¢ç›Š([information gain](https://en.jinzhao.wiki/wiki/Mutual_information))ï¼ˆID3ï¼‰ï¼Œè¶Šå¤§è¶Šå¥½
- ä¿¡æ¯å¢ç›Šæ¯”([information gain ratio](https://en.jinzhao.wiki/wiki/Information_gain_ratio)) ï¼ˆC4.5ï¼‰ï¼Œè¶Šå¤§è¶Šå¥½
- åŸºå°¼æŒ‡æ•°([Gini coefficient](https://en.jinzhao.wiki/wiki/Gini_coefficient) or Gini index or Gini ratio)ï¼ˆCARTï¼‰ï¼Œè¶Šå°è¶Šå¥½

**å†³ç­–æ ‘çš„ç”Ÿæˆ**ï¼š
å¸¸è§ç®—æ³•ï¼ˆå‚è§ï¼š[Decision tree learning](https://en.jinzhao.wiki/wiki/Decision_tree_learning)ä»¥åŠ[Tree algorithms](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)ï¼‰ï¼š

- [ID3](https://en.jinzhao.wiki/wiki/ID3_algorithm) (Iterative Dichotomiser 3)
- [C4.5](https://en.jinzhao.wiki/wiki/C4.5_algorithm) (successor of ID3)
- [CART](https://en.jinzhao.wiki/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) (Classification And Regression Tree)

**[å†³ç­–æ ‘çš„ä¿®å‰ª Decision tree pruning](https://en.jinzhao.wiki/wiki/Decision_tree_pruning)**ï¼š
ä¿®å‰ªæ˜¯æœºå™¨å­¦ä¹ å’Œæœç´¢ç®—æ³•ä¸­çš„ä¸€ç§æ•°æ®å‹ç¼©æŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ é™¤æ ‘ä¸­å¯¹åˆ†ç±»å®ä¾‹ä¸é‡è¦å’Œå†—ä½™çš„éƒ¨åˆ†æ¥å‡å°å†³ç­–æ ‘çš„å¤§å°ã€‚å‰ªæé™ä½äº†æœ€ç»ˆåˆ†ç±»å™¨çš„å¤æ‚åº¦ï¼Œä»è€Œé€šè¿‡å‡å°‘è¿‡æ‹Ÿåˆæ¥æé«˜é¢„æµ‹ç²¾åº¦ã€‚

- é¢„å‰ªæï¼ˆPre-pruningï¼ŒTop-down pruningï¼‰ï¼š

  - max_depth
    é™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œè¶…è¿‡è®¾å®šæ·±åº¦çš„æ ‘æå…¨éƒ¨å‰ªæ‰

  - min_samples_leaf
    min_samples_leaf é™å®šï¼Œä¸€ä¸ªèŠ‚ç‚¹åœ¨åˆ†æåçš„æ¯ä¸ªå­èŠ‚ç‚¹éƒ½å¿…é¡»åŒ…å«è‡³å°‘ min_samples_leaf ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¦åˆ™åˆ†æå°±ä¸ä¼šå‘ç”Ÿï¼Œæˆ–è€…ï¼Œåˆ†æä¼šæœç€æ»¡è¶³æ¯ä¸ªå­èŠ‚ç‚¹éƒ½åŒ…å« min_samples_leaf ä¸ªæ ·æœ¬çš„æ–¹å‘å»å‘ç”Ÿ

  - min_samples_split
    min_samples_split é™å®šï¼Œä¸€ä¸ªèŠ‚ç‚¹å¿…é¡»è¦åŒ…å«è‡³å°‘ min_samples_split ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿™ä¸ªèŠ‚ç‚¹æ‰å…è®¸è¢«åˆ†æï¼Œå¦åˆ™åˆ†æå°±ä¸ä¼šå‘ç”Ÿã€‚

  - max_features
    ä¸€èˆ¬ max_depth ä½¿ç”¨ï¼Œç”¨ä½œæ ‘çš„â€ç²¾ä¿®â€œ
    max_features é™åˆ¶åˆ†ææ—¶è€ƒè™‘çš„ç‰¹å¾ä¸ªæ•°ï¼Œè¶…è¿‡é™åˆ¶ä¸ªæ•°çš„ç‰¹å¾éƒ½ä¼šè¢«èˆå¼ƒã€‚å’Œ max_depth å¼‚æ›²åŒå·¥ï¼Œmax_features æ˜¯ç”¨æ¥é™åˆ¶é«˜ç»´åº¦æ•°æ®çš„è¿‡æ‹Ÿåˆçš„å‰ªæå‚æ•°ï¼Œä½†å…¶æ–¹æ³•æ¯”è¾ƒæš´åŠ›ï¼Œæ˜¯ç›´æ¥é™åˆ¶å¯ä»¥ä½¿ç”¨çš„ç‰¹å¾æ•°é‡è€Œå¼ºè¡Œä½¿å†³ç­–æ ‘åœä¸‹çš„å‚æ•°ï¼Œåœ¨ä¸çŸ¥é“å†³ç­–æ ‘ä¸­çš„å„ä¸ªç‰¹å¾çš„é‡è¦æ€§çš„æƒ…å†µä¸‹ï¼Œå¼ºè¡Œè®¾å®šè¿™ä¸ªå‚æ•°å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ ä¸è¶³ã€‚å¦‚æœå¸Œæœ›é€šè¿‡é™ç»´çš„æ–¹å¼é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå»ºè®®ä½¿ç”¨ PCAï¼ŒICA æˆ–è€…ç‰¹å¾é€‰æ‹©æ¨¡å—ä¸­çš„é™ç»´ç®—æ³•ã€‚

  - min_impurity_decrease
    min_impurity_decrease é™åˆ¶ä¿¡æ¯å¢ç›Šçš„å¤§å°ï¼Œä¿¡æ¯å¢ç›Šå°äºè®¾å®šæ•°å€¼çš„åˆ†æä¸ä¼šå‘ç”Ÿã€‚è¿™æ˜¯åœ¨ 0.19 ç‰ˆæœ¬ç§æ›´æ–°çš„åŠŸèƒ½ï¼Œåœ¨ 0.19 ç‰ˆæœ¬ä¹‹å‰æ—¶ä½¿ç”¨ min_impurity_splitã€‚
  - min_weight_fraction_leaf
    åŸºäºæƒé‡çš„å‰ªæå‚æ•°

- åå‰ªæï¼ˆPost-pruningï¼ŒBottom-up pruningï¼‰ï¼š

  - ccp_alphaï¼šCCP(Cost Complexity Pruning)-[ccp_alpha å‚æ•°å¦‚ä½•è°ƒä¼˜](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py)
    [7 å¤§åå‰ªæç®—æ³•](https://blog.csdn.net/appleyuchi/article/details/83692381)
    [7 å¤§åå‰ªæç®—æ³• - æºç ](https://github.com/appleyuchi/Decision_Tree_Prune)

ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸‰è¦ç´ ï¼š

- **æ¨¡å‹**ï¼š
  å†³ç­–æ ‘æ¨¡å‹çš„å…³é”®æ˜¯é€šè¿‡ä¸€ç³»åˆ— if then å†³ç­–è§„åˆ™çš„é›†åˆï¼Œå°†ç‰¹å¾ç©ºé—´åˆ’åˆ†æˆä¸ç›¸äº¤çš„å­åŒºåŸŸï¼Œè½åœ¨ç›¸åŒå­åŒºåŸŸçš„æ ·æœ¬å…·æœ‰ç›¸åŒçš„é¢„æµ‹å€¼ã€‚
  ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80V2dJTEhCd1ZIOTR6dFI3QURGUUp3a2N2b2Z3MFNpY21CUTdzb2FScHplekRCMzZCUHB2WUx2Y054aWFGdGlhZm5nOTk5b0taUkpaU3loMnlINlZ2cGpMdy82NDA_d3hfZm10PXBuZw)
- **ç­–ç•¥**ï¼š
  ç­–ç•¥ä¸€èˆ¬åŒ…æ‹¬ä¸¤ä¸ªæ–¹é¢ï¼šç¬¬ä¸€ä¸ªæ˜¯ååº”å†³ç­–æ ‘å¯¹æ ·æœ¬æ•°æ®ç‚¹æ‹Ÿåˆå‡†ç¡®åº¦çš„æŸå¤±é¡¹ï¼Œç¬¬äºŒä¸ªæ˜¯ååº”å†³ç­–æ ‘æ¨¡å‹å¤æ‚ç¨‹åº¦çš„æ­£åˆ™åŒ–é¡¹ã€‚
  å¯¹äºæŸå¤±é¡¹ï¼Œå¦‚æœæ˜¯å›å½’é—®é¢˜ï¼ŒæŸå¤±é¡¹å¯ä»¥å–å¹³æ–¹æŸå¤±ï¼Œå¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸çº¯åº¦æ¥ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼ˆä¿¡æ¯ç†µï¼ŒåŸºå°¼ä¸çº¯åº¦ï¼Œä»¥åŠåˆ†ç±»è¯¯å·®ç‡ï¼‰ã€‚
  æ­£åˆ™åŒ–é¡¹å¯ä»¥å–æ¨¡å‹çš„å¶å­èŠ‚ç‚¹çš„æ•°é‡ã€‚å³å†³ç­–æ ‘æ¨¡å‹åˆ’åˆ†å¾—åˆ°çš„ä¸ç›¸äº¤å­åŒºåŸŸè¶Šå¤šï¼Œæˆ‘ä»¬è®¤ä¸ºæ¨¡å‹è¶Šå¤æ‚ã€‚

- **ç®—æ³•**ï¼š
  ä¼˜åŒ–ç®—æ³•ï¼ˆå¯å‘å¼ç®—æ³•ï¼‰åŒ…æ‹¬æ ‘çš„ç”Ÿæˆç­–ç•¥å’Œæ ‘çš„å‰ªæç­–ç•¥ã€‚
  æ ‘çš„ç”Ÿæˆç­–ç•¥ä¸€èˆ¬é‡‡ç”¨è´ªå¿ƒçš„æ€æƒ³ä¸æ–­é€‰æ‹©ç‰¹å¾å¯¹ç‰¹å¾ç©ºé—´è¿›è¡Œåˆ‡åˆ†ã€‚
  æ ‘çš„å‰ªæç­–ç•¥ä¸€èˆ¬åˆ†ä¸ºé¢„å‰ªæå’Œåå‰ªæç­–ç•¥ã€‚ä¸€èˆ¬æ¥è¯´åå‰ªæç­–ç•¥ç”Ÿæˆçš„å†³ç­–æ ‘æ•ˆæœè¾ƒå¥½ï¼Œä½†å…¶è®¡ç®—æˆæœ¬ä¹Ÿæ›´é«˜ã€‚

[Overview of Decision Trees](http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/dtrees/4_dtrees1.html)

[Decision Tree](https://webdocs.cs.ualberta.ca/~rgreiner/C-466/SLIDES/14.4-DecisionTree.pdf)

[Decision Trees](https://courses.cs.washington.edu/courses/csep546/16sp/slides/dtrees.pdf)

### é™„åŠ çŸ¥è¯†

#### ä¿¡æ¯è®ºï¼ˆ[Information Theory](https://en.jinzhao.wiki/wiki/Information_theory)ï¼‰

[Entropy, Relative Entropy, Cross Entropy](https://iitg.ac.in/cseweb/osint/slides/Anasua_Entropy.pdf)

##### ç†µï¼ˆ[Entropy](<https://en.jinzhao.wiki/wiki/Entropy_(information_theory)>)ï¼‰

åœ¨ä¿¡æ¯è®ºä¸­ï¼Œç†µç”¨æ¥è¡¡é‡ä¸€ä¸ªéšæœºäº‹ä»¶çš„**ä¸ç¡®å®šæ€§**ã€‚ä¹Ÿå«é¦™å†œç†µ Shannon'sï¼ˆäººåï¼‰ entropyã€‚
$$H(X) = E_{p(x)}[I(X)] = E_{p(x)}[-\log {p(x)}] \\= -\sum_{i=1}^n {p(x_i)} \log {p(x_i)} \\= -\int_{X} {p(x)} \log {p(x)} dx$$
å…¶ä¸­$I(X) = -\log {p(x)}$ ç§°ä¸º**è‡ªä¿¡æ¯**ï¼ˆ[Self Information](https://en.jinzhao.wiki/wiki/Information_content)ï¼‰ï¼Œæ˜¯ä¸€ä¸ªéšæœºäº‹ä»¶æ‰€åŒ…å«çš„ä¿¡æ¯é‡ã€‚ä¸€ä¸ªéšæœºäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šé«˜ï¼Œå…¶è‡ªä¿¡æ¯è¶Šä½ã€‚å¦‚æœä¸€ä¸ªäº‹ä»¶å¿…ç„¶å‘ç”Ÿï¼Œå…¶è‡ªä¿¡æ¯ä¸º 0ã€‚
åœ¨è‡ªä¿¡æ¯çš„å®šä¹‰ä¸­ï¼Œå¯¹æ•°çš„åº•å¯ä»¥ä½¿ç”¨ 2ã€è‡ªç„¶å¸¸æ•° ğ‘’ æˆ–æ˜¯ 10ã€‚å½“åº•ä¸º 2 æ—¶ï¼Œè‡ªä¿¡æ¯çš„å•ä½ä¸º bitï¼›å½“åº•ä¸º ğ‘’ æ—¶ï¼Œè‡ªä¿¡æ¯çš„å•ä½ä¸º natã€‚

ç†µè¶Šé«˜ï¼Œåˆ™éšæœºå˜é‡çš„ä¿¡æ¯è¶Šå¤šï¼ˆä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œç³»ç»Ÿè¶Šå¤æ‚ï¼‰ï¼›ç†µè¶Šä½ï¼Œåˆ™éšæœºå˜é‡çš„ä¿¡æ¯è¶Šå°‘ã€‚

---
æ±‚æœ€å¤§ç†µï¼šå‡è®¾æ¦‚ç‡åˆ†å¸ƒ

| X    | 1   | 2   | ... | n   |
| ---- | --- | --- | --- | --- |
| p(x) | pâ‚  | pâ‚‚  | ... | pâ¿  |

$$\max H(p) = \max -\sum_{i=1}^n p_i \log p_i \\ s.t. \sum_{i=1}^n p_i = 1$$

ç”±æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•(Lagrange Multiplier)ï¼Œæœ€å¤§å˜æœ€å°æ—¶å»æ‰è´Ÿå·
$$\mathcal L(p,\lambda) = \sum_{i=1}^n p_i \log p_i + \lambda(1-\sum_{i=1}^n p_i) \\åå¯¼ï¼š\frac{\partial\mathcal L}{\partial p_i} = \log p_i + p_i.\frac{1}{p_i} - \lambda \\ ä»¤åå¯¼ä¸º0å¾—ï¼šp_i^*=exp(\lambda-1)$$

å› ä¸º$\lambda$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼ˆå¸¸æ•°ï¼‰ï¼Œæ‰€ä»¥$p_i^*$æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæ‰€ä»¥ $p_1^*=p_2^*=...=p_n^*=\frac{1}{n}$

æ‰€ä»¥**æ¦‚ç‡åˆ†å¸ƒä¸ºä¸€ä¸ªå‡åŒ€åˆ†å¸ƒï¼Œåˆ™ç†µæœ€å¤§**ï¼Œç”±æ­¤æ€§è´¨æˆ‘ä»¬æ¥è¯æ˜ç†µçš„å–å€¼èŒƒå›´ï¼šè®¾ p æ˜¯ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ$p = \frac{1}{n}$
$$H(p) = -\sum_{i=1}^n \frac{1}{n} \log \frac{1}{n} \\= -\sum_{i=1}^n \frac{1}{n} \log n^{-1} \\= \sum_{i=1}^n \frac{1}{n} \log n \\= \log n$$
æ‰€ä»¥ï¼š$$0 \leq H(p) \leq \log n$$

---
å·²çŸ¥è¿ç»­éšæœºå˜é‡çš„å‡å€¼ä¸º$\mu$ï¼Œæ–¹å·®ä¸º$\sigma^2$ï¼Œæ±‚ç†µæœ€å¤§å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒï¼š
$$\argmax_{p(x)} -\int p(x)\log p(x)dx \\ s.t. \int p(x)dx =1 \\ \int xp(x)dx = \mu \\ \int (x-\mu)^2p(x)dx=\sigma^2$$
æ‹‰æ ¼æœ—æ—¥å‡½æ•°
$$L(p(x),\lambda_1,\lambda_2,\lambda_3) = -\int p(x)\log p(x)dx +\lambda_1(\int p(x)dx - 1)+\lambda_2(\int xp(x)dx - \mu) +\lambda_3(\int (x-\mu)^2p(x)dx - \sigma^2)$$
ä»¤$F(p)=(-\log p(x) + \lambda_{1} +\lambda_{2}x+ \lambda_{3}(x-\mu)^{2})p(x)$
æ±‚åå¯¼å¹¶ä»¤å…¶ä¸º0ï¼ˆå¯ä»¥æŠŠæ±‚ç§¯åˆ†å½“åšæ±‚å’Œï¼Œè¿™æ ·æ±‚åå¯¼å°±å®¹æ˜“æƒ³è±¡äº†ï¼‰
$$\frac{\partial L}{\partial p(x)} = -[\log p(x)+1]+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2$$
å¾—
$$p(x) = \exp\{\lambda_1-1+\lambda_2x+\lambda_3(x-\mu)^2\}$$
æŠŠè·Ÿxæœ‰å…³çš„ä¿ç•™ï¼Œå…¶å®ƒçš„è®¾ä¸ºå¸¸æ•°ï¼Œæœ‰
$$p(x) = \exp\{\lambda_1-1+\lambda_2x+\lambda_3(x-\mu)^2\}\\ =e^{-1+\lambda_{1}}\cdot e^{\lambda_{2}x+ \lambda_{3}(x-\mu)^{2}}=C e^{\lambda_{2}x+ \lambda_{3}(x-\mu)^{2}} \\ = Ce^{\lambda_{3}(x^{2} -2(\mu-\frac{\lambda_{2}}{2\lambda_{3}})x+ u^{2})} = C e^{\lambda_{3}(x -\mu+ \frac{\lambda_{2}}{2\lambda_{3}})^{2}} \\= C.\exp\{\lambda_3(x-\mu+\frac{\lambda_2}{2\lambda_3})^2\}$$

æ ¹æ®$(x-\mu+\frac{\lambda_2}{2\lambda_3})^2$å¾—åˆ°$p(x)$å…³äº$\mu - \frac{\lambda_{2}}{2\lambda_{3}}$å¯¹ç§°(å¶å‡½æ•°å…³äºx=0å¯¹ç§°$p(x) = p(-x)$)ï¼Œæ‰€ä»¥$E[p(x)] = \mu - \frac{\lambda_{2}}{2\lambda_{3}} = \mu$ï¼Œå¾—$\lambda_{2} = 0$

é‚£ä¹ˆ
$$p(x)= C e^{\lambda_{3}(x -\mu)^{2}} $$
å› ä¸º $p(x)>0$ ï¼Œæ‰€ä»¥ $C>0,\lambda_{3}<0$
ä»¤$\lambda = -\lambda_3$
æ ¹æ®ç§¯åˆ†ä¸º1çš„çº¦æŸ ä»¥åŠ$\int e^{-\frac{x^2}{2}}dx = \sqrt{2\pi}$ï¼Œå¾—ï¼š
$$\int p(x)dx = 1 = C\int e^{-\lambda(x -\mu)^{2}} dx = C\sqrt{\frac{\pi}{\lambda}}$$
å¾—åˆ°$C = \sqrt{\frac{\lambda}{\pi}}$
æ ¹æ®æ–¹å·®çš„çº¦æŸï¼Œå¾—ï¼š
$$\int (x-\mu)^2p(x)dx=\sigma^2 = \int (x-\mu)^2e^{-\lambda(x -\mu)^{2}}dx = C\sqrt{\frac{\pi}{\lambda}}.\frac{1}{2\lambda} = \frac{1}{2\lambda}$$
å¾—åˆ°$\lambda_3 = -\frac{1}{2\sigma^2}$ä»¥åŠ$C = \sqrt{\frac{\lambda}{\pi}} = \sqrt{\frac{1}{2\pi\sigma^2}}$
æ‰€ä»¥
$$p(x) = \sqrt{\frac{1}{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$


---

$X$å’Œ$Y$çš„**è”åˆç†µ**ï¼ˆ[Joint Entropy](https://en.jinzhao.wiki/wiki/Joint_entropy)ï¼‰ä¸ºï¼š
$${\displaystyle \mathrm {H} (X,Y)=-\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}P(x,y)\log _{2}[P(x,y)]} \\=\mathbb {E} _{X,Y}[-\log p(x,y)]=-\sum _{x,y}p(x,y)\log p(x ,y)\,$$

ç§¯åˆ†å½¢å¼ï¼ˆè¿ç»­ï¼‰ï¼š
$${\displaystyle h(X,Y)=-\int _{{\mathcal {X}},{\mathcal {Y}}}f(x,y)\log f(x,y)\,dxdy}$$

å¤šä¸ªéšæœºå˜é‡ï¼š
$${\displaystyle \mathrm {H} (X_{1},...,X_{n})=-\sum _{x_{1}\in {\mathcal {X}}_{1}}...\sum _{x_{n}\in {\mathcal {X}}_{n}}P(x_{1},...,x_{n})\log _{2}[P(x_{1},...,x_{n})]}$$
å¤šä¸ªéšæœºå˜é‡çš„ç§¯åˆ†å½¢å¼ï¼ˆè¿ç»­ï¼‰ï¼š
$${\displaystyle h(X_{1},\ldots ,X_{n})=-\int f(x_{1},\ldots ,x_{n})\log f(x_{1},\ldots ,x_{n})\,dx_{1}\ldots dx_{n}}$$

$X$å’Œ$Y$çš„**æ¡ä»¶ç†µ**ï¼ˆ[Conditional Entropy](https://en.jinzhao.wiki/wiki/Conditional_entropy)ï¼‰ä¸ºï¼š

$${\displaystyle \mathrm {H} (Y|X)\ =-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)}}}$$

è¯æ˜ï¼š
$${\displaystyle {\begin{aligned}\mathrm {H} (Y|X)\ &\equiv \sum _{x\in {\mathcal {X}}}\,p(x)\,\mathrm {H} (Y|X=x)\\&=-\sum _{x\in {\mathcal {X}}}p(x)\sum _{y\in {\mathcal {Y}}}\,p(y|x)\,\log \,p(y|x)\\&=-\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}\,p(x,y)\,\log \,p(y|x)\\&=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log \,p(y|x)\\&=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)}}.\\&=\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x)}{p(x,y)}}.\\\end{aligned}}}$$

ç§¯åˆ†å½¢å¼ï¼ˆè¿ç»­ï¼‰ï¼š
$${\displaystyle h(X|Y)=-\int _{{\mathcal {X}},{\mathcal {Y}}}f(x,y)\log f(x|y)\,dxdy}$$

æ ¹æ®å®šä¹‰å†™ä½œï¼š
$${\displaystyle \mathrm {H} (Y|X)\,=\,\mathrm {H} (X,Y)-\mathrm {H} (X)}$$
ä¸€èˆ¬å½¢å¼ï¼š
$${\displaystyle \mathrm {H} (X_{1},X_{2},\ldots ,X_{n})=\sum _{i=1}^{n}\mathrm {H} (X_{i}|X_{1},\ldots ,X_{i-1})}$$

è¯æ˜ï¼š
$${\displaystyle {\begin{aligned}\mathrm {H} (Y|X)&=\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log \left({\frac {p(x)}{p(x,y)}}\right)\\[4pt]&=\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)(\log(p(x))-\log(p(x,y)))\\[4pt]&=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log(p(x,y))+\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}{p(x,y)\log(p(x))}\\[4pt]&=\mathrm {H} (X,Y)+\sum _{x\in {\mathcal {X}}}p(x)\log(p(x))\\[4pt]&=\mathrm {H} (X,Y)-\mathrm {H} (X).\end{aligned}}}$$

##### äº’ä¿¡æ¯ï¼ˆ[Mutual information](https://en.jinzhao.wiki/wiki/Mutual_information)ï¼‰

å¦‚æœå˜é‡ ğ‘‹ å’Œ ğ‘Œ äº’ç›¸ç‹¬ç«‹ï¼Œå®ƒä»¬çš„äº’ä¿¡æ¯ä¸ºé›¶ï¼

$$I(X;Y)=\mathbb {E} _{X,Y}[SI(x,y)]=\sum _{x,y}p(x,y)\log {\frac {p(x,y)}{p(x)\,p(y)}}$$
å…¶ä¸­ SIï¼ˆSpecific mutual Informationï¼‰æ˜¯[pointwise mutual information](https://en.jinzhao.wiki/wiki/Pointwise_mutual_information)

åŸºæœ¬æ€§è´¨ï¼š
$$I(X;Y)=H(X)-H(X|Y) =H(Y)- H(Y|X).\,$$
å¯¹ç§°æ€§ï¼š
$$I(X;Y)=I(Y;X)=H(X)+H(Y)-H(X,Y).\,$$

äº’ä¿¡æ¯å¯ä»¥è¡¨ç¤ºä¸ºç»™å®š Y å€¼çš„ X çš„åéªŒæ¦‚ç‡åˆ†å¸ƒ ä¸ X çš„å…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„å¹³å‡ Kullback-Leibler æ•£åº¦:
$$I(X;Y)=\mathbb {E} _{p(y)}[D_{\mathrm {KL} }(p(X|Y=y)\|p(X))].$$
or
$$I(X;Y)=D_{\mathrm {KL} }(p(X,Y)\|p(X)p(Y)).$$

> ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­è®²åˆ° ä¿¡æ¯å¢ç›Šç­‰ä»·äº’ä¿¡æ¯ï¼ˆ74 é¡µï¼‰ï¼Œè€Œç»´åŸºç™¾ç§‘ä¸­ä¿¡æ¯å¢ç›Šæ˜¯[Kullback-Leibler æ•£åº¦](https://en.jinzhao.wiki/wiki/Information_gain)

##### äº¤å‰ç†µï¼ˆ[Cross Entropy](https://en.jinzhao.wiki/wiki/Cross_entropy)ï¼‰

åœ¨ç»™å®š åˆ†å¸ƒ ğ‘ çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ åˆ†å¸ƒ ğ‘ å’Œ åˆ†å¸ƒ ğ‘ è¶Šæ¥è¿‘ï¼Œäº¤å‰ç†µè¶Šå°ï¼›å¦‚æœ åˆ†å¸ƒ ğ‘ å’Œ åˆ†å¸ƒ ğ‘ è¶Šè¿œï¼Œäº¤å‰
ç†µå°±è¶Šå¤§ï¼
$${\displaystyle H(p,q)=-\operatorname {E} _{p}[\log q]} =-\sum _{x\in {\mathcal {X}}}p(x)\,\log q(x) = H(p)+D_{\mathrm {KL} }(p\|q)$$

> æ³¨æ„ä¸è”åˆç†µ${H} (X,Y)$çš„åŒºåˆ«ï¼Œè”åˆç†µæè¿°ä¸€å¯¹éšæœºå˜é‡å¹³å‡æ‰€éœ€çš„ä¿¡æ¯é‡ï¼Œäº¤å‰ç†µæè¿°ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚
> ä¹Ÿæœ‰è¯´äº¤å‰ç†µ$H(p,q)$æ˜¯ä¸æ ‡å‡†çš„å†™æ³•ï¼Œåº”è¯¥æ˜¯$H_q(p)$ (äº¤å‰ç†µä¸æ˜¯å¯¹ç§°çš„ï¼Œè€Œè”åˆç†µæ˜¯å¯¹ç§°çš„)ï¼Œå‚è§ [Difference of notation between cross entropy and joint entropy](https://stats.stackexchange.com/questions/373098/difference-of-notation-between-cross-entropy-and-joint-entropy) ä»¥åŠ[Relation between cross entropy and joint entropy](https://math.stackexchange.com/questions/2505015/relation-between-cross-entropy-and-joint-entropy)
> åº”ç”¨ï¼šä¸€èˆ¬åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¦‚ï¼šç¥ç»ç½‘ç»œï¼Œé€»è¾‘å›å½’

##### KL æ•£åº¦ï¼ˆ[Kullbackâ€“Leibler divergence](https://en.jinzhao.wiki/wiki/Kullback%E2%80%93Leibler_divergence)ï¼‰

KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰ï¼Œä¹Ÿå« KL è·ç¦»æˆ–ç›¸å¯¹ç†µ(Relative Entropy)ï¼Œæ˜¯ç”¨æ¦‚ç‡åˆ†å¸ƒ ğ‘ æ¥è¿‘ä¼¼ ğ‘ æ—¶æ‰€é€ æˆçš„ä¿¡æ¯æŸå¤±é‡ï¼ŒKL æ•£åº¦æ€»æ˜¯å¤§äºç­‰äº 0 çš„ã€‚**å¯ä»¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»**ï¼KL æ•£åº¦åªæœ‰å½“ ğ‘ = ğ‘ æ—¶ï¼ŒKL(ğ‘, ğ‘) = 0ï¼å¦‚æœä¸¤ä¸ªåˆ†å¸ƒè¶Šæ¥è¿‘ï¼ŒKL æ•£åº¦è¶Šå°ï¼›å¦‚æœä¸¤ä¸ªåˆ†å¸ƒè¶Šè¿œï¼ŒKL æ•£åº¦å°±è¶Šå¤§ï¼ä½† KL æ•£åº¦å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸæ­£çš„åº¦é‡æˆ–è·ç¦»ï¼Œä¸€æ˜¯ KL æ•£åº¦ä¸æ»¡è¶³è·ç¦»çš„å¯¹ç§°æ€§ï¼ŒäºŒæ˜¯ KL æ•£åº¦ä¸æ»¡è¶³è·ç¦»çš„ä¸‰è§’ä¸ç­‰å¼æ€§è´¨ï¼

$$D_{\mathrm {KL} }(p(X)\|q(X))=\sum _{x\in X}-p(x)\log {q(x)}\,-\,\sum _{x\in X}-p(x)\log {p(x)} \\=\sum _{x\in X}p(x)\log {\frac {p(x)}{q(x)}} = -\sum _{x\in X}p(x)\log {\frac {q(x)}{p(x)}}.$$
ä¹Ÿæœ‰å†™ä½œï¼š
$$KL(p,q)  , KL(p|q) , KL(p\|q) , D_{KL}(p,q)$$

> åº”ç”¨ï¼šå¦‚ï¼šå˜åˆ†æ¨æ–­

##### JS æ•£åº¦ï¼ˆ[Jensen-Shannon divergence](https://en.jinzhao.wiki/wiki/Jensen%E2%80%93Shannon_divergence)ï¼‰

JS æ•£åº¦ï¼ˆJensen-Shannon Divergenceï¼‰æ˜¯ä¸€ç§å¯¹ç§°çš„è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒç›¸ä¼¼åº¦çš„åº¦é‡æ–¹å¼ï¼Œæ˜¯ KL æ•£åº¦ä¸€ç§æ”¹è¿›ï¼ä½†ä¸¤ç§æ•£åº¦éƒ½å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå³å¦‚æœä¸¤ä¸ªåˆ†å¸ƒ ğ‘, ğ‘ æ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘æ—¶ï¼ŒKL æ•£åº¦å’Œ JS æ•£åº¦éƒ½å¾ˆéš¾è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»ï¼

$${\rm {D}_{JS}}(P\parallel Q)={\frac  {1}{2}}D_{KL}(P\parallel M)+{\frac  {1}{2}}D_{KL}(Q\parallel M)$$
å…¶ä¸­$M={\frac  {1}{2}}(P+Q)$, JS æ•£åº¦ä¹Ÿæœ‰å†™ä½œ$JSD(P\|Q), JS(P\|Q) ,JS(P,Q)$ç­‰ã€‚

å±äºä¸€ç§ç»Ÿè®¡è·ç¦»ï¼ˆ[Statistical distance](https://en.jinzhao.wiki/wiki/Category:Statistical_distance)ï¼‰

ç»Ÿè®¡è·ç¦»è¿˜æœ‰**Wasserstein è·ç¦»**[Wasserstein distance](https://en.jinzhao.wiki/wiki/Wasserstein_metric)ï¼Œä¹Ÿç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·
ç¦»ï¼å¯¹äºä¸¤ä¸ªåˆ†å¸ƒ$\mu ,\nuï¼Œp^{th}-Wasserstein$è·ç¦»å®šä¹‰ä¸º

$${\displaystyle W_{p}(\mu ,\nu ):=\left(\inf _{\gamma \in \Gamma (\mu ,\nu )}\int _{M\times M}d(x,y)^{p}\,\mathrm {d} \gamma (x,y)\right)^{1/p},}$$

Wasserstein è·ç¦»ç›¸æ¯” KL æ•£åº¦å’Œ JS æ•£åº¦çš„ä¼˜åŠ¿åœ¨äºï¼šå³ä½¿ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘ï¼ŒWasserstein è·ç¦»ä»ç„¶èƒ½åæ˜ ä¸¤ä¸ªåˆ†å¸ƒçš„è¿œè¿‘ï¼å‚è§[pdf443 é¡µé™„å½•](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)

### å‚è€ƒæ–‡çŒ®

[5-1] Olshen R A, Quinlan J R. [Induction of decision trees](https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf). Machine Learning,1986,1(1): 81â€“106

[5-2] Olshen R A, Quinlan J R. [C4. 5: Programs for Machine Learning](https://link.springer.com/content/pdf/10.1007/BF00993309.pdf). Morgan Kaufmann,1992

[5-3] Olshen R A, Breiman L,Friedman J,Stone C. Classification and Regression Trees. Wadsworth,1984

[5-4] Ripley B. Pattern Recognition and Neural Networks. Cambridge UniversityPress,1996

[5-5] Liu B. Web Data Mining: Exploring Hyperlinks,Contents and Usage Data. Springer-Verlag,2006

[5-6] Hyafil L,Rivest R L. Constructing Optimal Binary Decision Trees is NP-complete.Information Processing Letters,1976,5(1): 15â€“17

[5-7] Hastie T,Tibshirani R,Friedman JH. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](http://www.web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). New York: Springer-Verlag,2001

[5-8] Yamanishi K. A learning criterion for stochastic rules. Machine Learning,1992

[5-9] Li H,Yamanishi K. Text classification using ESC-based stochastic decision lists.Information Processing & Management,2002,38(3): 343â€“361

## ç¬¬ 6 ç«  é€»è¾‘æ–¯è°›å›å½’ä¸æœ€å¤§ç†µæ¨¡å‹

**é€»è¾‘æ–¯è°›å›å½’**ï¼ˆ[logistic regression](https://en.jinzhao.wiki/wiki/Logistic_regression)ï¼‰ï¼ˆä¹Ÿæœ‰ç§° å¯¹æ•°å‡ ç‡å›å½’ï¼‰æ˜¯ç»Ÿè®¡å­¦ä¹ ä¸­çš„ç»å…¸åˆ†ç±»æ–¹æ³•ã€‚æœ€å¤§ç†µæ˜¯æ¦‚ç‡æ¨¡å‹å­¦ä¹ çš„ä¸€ä¸ªå‡†åˆ™ï¼Œå°†å…¶æ¨å¹¿åˆ°åˆ†ç±»é—®é¢˜å¾—åˆ°**æœ€å¤§ç†µæ¨¡å‹**ï¼ˆ[maximum entropy model](https://en.jinzhao.wiki/wiki/Principle_of_maximum_entropy)ï¼‰ã€‚é€»è¾‘æ–¯è°›å›å½’æ¨¡å‹ä¸æœ€å¤§ç†µæ¨¡å‹éƒ½å±äº**å¯¹æ•°çº¿æ€§æ¨¡å‹**ï¼ˆä¹Ÿæœ‰ç§°æœ€å¤§ç†µåˆ†ç±»æˆ–å¯¹æ•°çº¿æ€§åˆ†ç±»ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ¨¡å‹éƒ½æ˜¯åˆ†ç±»æ¨¡å‹ï¼‰ã€‚

### é€»è¾‘æ–¯è°›å›å½’

ä¸€ä¸ªäº‹ä»¶çš„å‡ ç‡ï¼ˆoddsï¼‰æ˜¯æŒ‡è¯¥äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ä¸è¯¥äº‹ä»¶ä¸å‘ç”Ÿçš„æ¦‚ç‡çš„æ¯”å€¼ã€‚å¦‚æœäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡æ˜¯ pï¼Œé‚£ä¹ˆè¯¥äº‹ä»¶çš„å‡ ç‡æ˜¯$\frac{p}{1-p}$ï¼Œè¯¥äº‹ä»¶çš„**å¯¹æ•°å‡ ç‡**ï¼ˆlog oddsï¼‰æˆ– logit å‡½æ•°æ˜¯ï¼š
$$logit(p) = \log\frac{p}{1-p} \label{6-1}\tag{6-1}$$

- **æ¨¡å‹**ï¼š
  äºŒé¡¹é€»è¾‘æ–¯è°›å›å½’çš„æ¨¡å‹å¦‚ä¸‹(w å’Œ x æ˜¯å¢å¹¿å‘é‡ï¼Œw.x ä½œä¸º Sigmoid çš„è¾“å…¥,yâˆˆ{0,1})ï¼š
  $$P(Y=1|x) = \frac{\exp{(w.x)}}{1+\exp{(w.x)}} = \sigma{(w.x)} \\ P(Y=0|x) = \frac{1}{1+\exp{(w.x)}} = 1 - \sigma{(w.x)} \label{6-2}\tag{6-2}$$

  è¯¥äº‹ä»¶çš„å¯¹æ•°å‡ ç‡ï¼š
  $$\log\frac{P(Y=1|x)}{1-P(Y=1|x)} = w.x$$
  æ‰€ä»¥åˆå«å¯¹æ•°å‡ ç‡å›å½’ã€‚

- **ç­–ç•¥**ï¼š
  æŸå¤±å‡½æ•°:è´Ÿå¯¹æ•°ä¼¼ç„¶,negative log likelihood(NLL), è´Ÿçš„ log ä¼¼ç„¶
  æ•°æ®é›†$T=\{(x_1,y_1),...,(x_N,y_N)\}  , x_i \in \mathbb{R}^n , y_i \in \{0,1\}$
  likelihood(6-2 çš„ä¸¤ä¸ªå¼å­åˆèµ·æ¥å°±æ˜¯$[\sigma{(w.x_i)}]^{y_i}[1-\sigma{(w.x_i)}]^{1-y_i}$)ï¼š
  $$L(w|y;x) = P(Y|X;w) = \prod_{i=1}^N P(y_i|x_i;w)= \prod_{i=1}^N [\sigma{(w.x_i)}]^{y_i}[1-\sigma{(w.x_i)}]^{1-y_i}$$

  log likelihoodï¼ˆmaximizedï¼‰ï¼š
  $$\log {L(w|y;x)} = \sum_{i=1}^N[y_i\log\sigma{(w.x_i)} + (1-y_i)\log(1-\sigma{(w.x_i)})]$$

  negative log likelihoodï¼ˆminimizeï¼‰ï¼š
  $$-\log {L(w|y;x)}$$
  è¿™ä¸å°±æ˜¯äº¤å‰ç†µçš„å®šä¹‰çš„å—ã€‚

- **ç®—æ³•**ï¼š
  1. æå¤§ä¼¼ç„¶ä¼°è®¡ MLE(Maximum Likelihood Estimation)
     $$w^* = \argmin_w -\log {L(w|y;x)}$$
  2. ç„¶åä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆStochastic Gradient Descentï¼‰æ±‚æœ€ä¼˜å€¼å¤„çš„å‚æ•°
     -log æ˜¯ä¸€ä¸ªè¿ç»­çš„å‡¸å‡½æ•°

**[sklearn ä¸­ä»£ä»·å‡½æ•°](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)**ï¼š$y \in \{-1,+1\}$
$$P(Y=+1|x) = \frac{\exp{(w.x)}}{1+\exp{(w.x)}} = \sigma{(w.x)} \\ P(Y=-1|x) = \frac{1}{1+\exp{(w.x)}} = 1 - \sigma{(w.x)} = \sigma{(-w.x)}$$
ä¸¤ä¸ªå¼å­åˆèµ·æ¥å°±æ˜¯ï¼š$\sigma{(y_i.w.x_i)}$
negative log likelihoodï¼š

$$
-\log\prod_{i=1}^N \sigma{(y_i.w.x_i)} = \sum_{i=1}^N-\log\sigma{(y_i.w.x_i)}= \sum_{i=1}^N \log\frac{1}{\sigma{(y_i.w.x_i)}} \\=
\sum_{i=1}^N \log\frac{1}{\frac{\exp{(y_i.w.x_i)}}{1+\exp{(y_i.w.x_i)}}}\\= \sum_{i=1}^N \log(1+\frac{1}{\exp{(y_i.w.x_i)}})\\= \sum_{i=1}^N \log(1+\exp{(-y_i.w.x_i)})
$$

å½“ç„¶ sklearn ä¸­åŠ å…¥çš„æ­£åˆ™é¡¹ã€‚

> Softmax å›å½’æ˜¯ Logistic å›å½’çš„å¤šåˆ†ç±»æƒ…å†µã€‚
> LogisticRegression å°±æ˜¯ä¸€ä¸ªè¢« logistic æ–¹ç¨‹å½’ä¸€åŒ–åçš„çº¿æ€§å›å½’ã€‚å°†é¢„æµ‹çš„è¾“å‡ºæ˜ å°„åˆ° 0,1 ä¹‹é—´ã€‚

> é€»è¾‘æ–¯è’‚å›å½’æ¨¡å‹çš„æ€æƒ³è·Ÿçº¿æ€§å›å½’æ¨¡å‹æ€æƒ³ä¸ä¸€æ ·ï¼Œçº¿æ€§å›å½’æ¨¡å‹æ€æƒ³æ˜¯æœ€å°åŒ–çœŸå®å€¼ä¸æ¨¡å‹é¢„æµ‹å€¼çš„è¯¯å·®ï¼Œè€Œé€»è¾‘æ–¯è’‚å›å½’æ¨¡å‹æ€æƒ³å°±æ¯”è¾ƒç‹ äº†ï¼Œé¢„æµ‹å€¼é¢„æµ‹å¯¹äº†æŸå¤±å‡½æ•°å°±æ˜¯ 0ï¼Œé”™äº†æŸå¤±å°±æ˜¯æ— ç©·å¤§ï¼Œæˆ‘ä¸ªäººçš„ç†è§£(ä¸€èˆ¬é‡‡ç”¨çš„æ˜¯-log(h(x)) è¿™æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°,åˆšå¥½æ»¡è¶³è¦æ±‚)

### æœ€å¤§ç†µæ¨¡å‹

ç†µçš„æ¦‚å¿µåœ¨ç»Ÿè®¡å­¦ä¹ ä¸æœºå™¨å­¦ä¹ ä¸­çœŸæ˜¯å¾ˆé‡è¦ï¼Œæœ€å¤§ç†µæ¨¡å‹ï¼ˆ[maximum entropy model](https://en.jinzhao.wiki/wiki/Principle_of_maximum_entropy)ï¼‰æ˜¯æ¦‚ç‡æ¨¡å‹å­¦ä¹ ä¸­ä¸€ä¸ªå‡†åˆ™ï¼Œå…¶æ€æƒ³ä¸ºï¼šåœ¨å­¦ä¹ æ¦‚ç‡æ¨¡å‹æ—¶ï¼Œæ‰€æœ‰å¯èƒ½çš„æ¨¡å‹ä¸­ç†µæœ€å¤§çš„æ¨¡å‹æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼›è‹¥æ¦‚ç‡æ¨¡å‹éœ€è¦æ»¡è¶³ä¸€äº›çº¦æŸï¼Œåˆ™æœ€å¤§ç†µåŸç†ï¼ˆPrinciple of maximum entropyï¼‰å°±æ˜¯åœ¨æ»¡è¶³å·²çŸ¥çº¦æŸçš„æ¡ä»¶é›†åˆä¸­é€‰æ‹©ç†µæœ€å¤§æ¨¡å‹ã€‚
æœ€å¤§ç†µåŸç†æŒ‡å‡ºï¼Œå¯¹ä¸€ä¸ªéšæœºäº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé¢„æµ‹æ—¶ï¼Œé¢„æµ‹åº”å½“æ»¡è¶³å…¨éƒ¨å·²çŸ¥çš„çº¦æŸï¼Œè€Œå¯¹æœªçŸ¥çš„æƒ…å†µä¸è¦åšä»»ä½•ä¸»è§‚å‡è®¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¦‚ç‡åˆ†å¸ƒæœ€å‡åŒ€ï¼Œé¢„æµ‹çš„é£é™©æœ€å°ï¼Œå› æ­¤å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒçš„ç†µæ˜¯æœ€å¤§ã€‚

> å‡å€¼å’Œæ–¹å·®ä¹Ÿè¢«ç§°ä¸ºä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©ï¼ˆäºŒè‡³å››é˜¶ä¸­å¿ƒçŸ©è¢«å®šä¹‰ä¸ºæ–¹å·®ï¼ˆvarianceï¼‰ã€ååº¦ï¼ˆskewnessï¼‰å’Œå³°åº¦ï¼ˆkurtosisï¼‰ï¼‰
> å¯¹äºè¿ç»­åˆ†å¸ƒï¼šç»™å®šå‡å€¼å’Œæ–¹å·®ï¼Œé«˜æ–¯åˆ†å¸ƒçš„ç†µæœ€å¤§ï¼ˆä¹Ÿå¯ä»¥è¯´å·²çŸ¥å‡å€¼å’Œæ–¹å·®æ—¶ï¼Œé«˜æ–¯åˆ†å¸ƒéšæœºæ€§æœ€å¤§ [è¯æ˜](#ç†µentropyhttpsenjinzhaowikiwikientropy_information_theory)ï¼‰
> å¯¹äºè¿ç»­åˆ†å¸ƒï¼šå·²çŸ¥åŒºé—´ï¼Œè¿ç»­å‡åŒ€åˆ†å¸ƒçš„ç†µæœ€å¤§
> å¯¹äºè¿ç»­åˆ†å¸ƒï¼šå·²çŸ¥å‡å€¼ï¼ˆä¸€é˜¶çŸ©ï¼‰ï¼ŒæŒ‡æ•°åˆ†å¸ƒçš„ç†µæœ€å¤§
> å¯¹äºç¦»æ•£åˆ†å¸ƒï¼šç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„ç†µæœ€å¤§ï¼ˆè¿™é‡Œåœ¨å°†ç†µæ—¶æœ‰[è¯æ˜](#ç†µentropyhttpsenjinzhaowikiwikientropy_information_theory)è¿‡ï¼‰


- **æ¨¡å‹**ï¼š
è®¾ Xâˆ¼p(x) æ˜¯ä¸€ä¸ªè¿ç»­å‹éšæœºå˜é‡ï¼Œå…¶å¾®åˆ†ç†µå®šä¹‰ä¸º
$$h(X) = - \int p(x)\log p(x) dx$$
å…¶ä¸­ï¼Œlog ä¸€èˆ¬å–è‡ªç„¶å¯¹æ•° ln, å•ä½ä¸º å¥ˆç‰¹ï¼ˆnatsï¼‰

- **ç­–ç•¥**ï¼š
è€ƒè™‘å¦‚ä¸‹ä¼˜åŒ–é—®é¢˜ï¼š
$$\begin{array}{ll}
&\underset{p}{\text{Maximize}} & \displaystyle h(p) = - \int_S p(x)\log p(x) dx \\
&\text{Subject to} &\displaystyle \int_S p(x) dx = 1 \\[2pt]
&~ & p(x) \ge 0 \\[2pt]
&~ & \displaystyle \int_S p(x) f_i(x) dx = \alpha_i, ~i=1,2,3,\dots,n
\end{array}$$
å…¶ä¸­ï¼Œé›†åˆ S æ˜¯éšæœºå˜é‡çš„supportï¼Œå³å…¶æ‰€æœ‰å¯èƒ½çš„å–å€¼ã€‚æˆ‘ä»¬æ„å›¾æ‰¾åˆ°è¿™æ ·çš„æ¦‚ç‡åˆ†å¸ƒ p, ä»–æ»¡è¶³æ‰€æœ‰çš„çº¦æŸï¼ˆå‰ä¸¤æ¡æ˜¯æ¦‚ç‡å…¬ç†çš„çº¦æŸï¼Œæœ€åä¸€æ¡å«åš**çŸ©çº¦æŸ**ï¼ˆå‡å€¼å’Œæ–¹å·®ä¹Ÿè¢«ç§°ä¸ºä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©ï¼‰ï¼Œåœ¨æ¨¡å‹ä¸­æœ‰æ—¶ä¼šå‡è®¾éšæœºå˜é‡çš„çŸ©ä¸ºå¸¸æ•°ï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿å¾—ç†µæœ€å¤§ã€‚å°†ä¸Šè¿°ä¼˜åŒ–é—®é¢˜å†™æˆæ ‡å‡†å½¢å¼ï¼š
$$\begin{array}{ll}
&\underset{p}{\text{Minimize}} & \displaystyle  \int_S p(x)\log p(x) dx \\
&\text{Subject to} &-p(x) \le 0 \\[2pt]
&~ &\displaystyle \int_S p(x) dx = 1 \\ 
&~ & \displaystyle \int_S p(x) f_i(x) dx = \alpha_i, ~i=1,2,3,\dots,n
\end{array}$$

- **ç®—æ³•**ï¼š
ä½¿ç”¨Lagrangeä¹˜æ•°æ³•å¾—åˆ°å…¶Lagrangianå‡½æ•°
$$L(p,\boldsymbol{\lambda}) = \int_S p\log p ~dx - \mu_{-1}p + \mu_0 \left(\int_S p ~dx - 1\right) + \sum_{j=1}^n \lambda_j \left(\int_S pf_j~dx - \alpha_j\right)$$
æ ¹æ®KKTæ¡ä»¶å¯¹Lagrangianæ±‚å¯¼ä»¤ä¸º0ï¼Œå¯å¾—æœ€ä¼˜è§£ã€‚
$$\begin{gathered}
\frac{\partial L}{\partial p} = \ln p + 1 - \mu_{-1} + \mu_0 + \sum_{j=1}^n \lambda_jf_j := 0 \\
\implies p = \exp\left(-1 + \mu_{-1} - \mu_0 - \sum_{j=1}^n \lambda_j f_j \right) =\displaystyle c^* e^{-\sum_{j=1}^n\lambda_j^* f_j(x)} := p^*
\end{gathered}$$
å…¶ä¸­ï¼Œæˆ‘ä»¬è¦é€‰æ‹© câˆ—,Î»âˆ— ä½¿å¾— p(x) æ»¡è¶³çº¦æŸã€‚åˆ°è¿™é‡Œæˆ‘ä»¬çŸ¥é“ï¼Œåœ¨æ‰€æœ‰æ»¡è¶³çº¦æŸçš„æ¦‚ç‡åˆ†å¸ƒå½“ä¸­ï¼Œpâˆ— æ˜¯ä½¿å¾—ç†µè¾¾åˆ°æœ€å¤§çš„é‚£ä¸€ä¸ªï¼

### å‚è€ƒèµ„æ–™

[é€»è¾‘å›å½’ï¼ˆéå¸¸è¯¦ç»†ï¼‰](https://zhuanlan.zhihu.com/p/74874291)
[æœºå™¨å­¦ä¹ å®ç°ä¸åˆ†æä¹‹å››ï¼ˆå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼‰](http://blog.sina.com.cn/s/blog_13ec1876a0102xb47.html)

[é€»è¾‘å›å½’â€”â€”Logistic çš„èµ·æº](https://www.bilibili.com/video/BV1W3411z71D)
[Logistic å›å½’çš„èµ·æºï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/146206709)
[Logistic å›å½’çš„èµ·æºï¼ˆä¸­ï¼‰](https://zhuanlan.zhihu.com/p/147708076)
[Logistic å›å½’çš„èµ·æºï¼ˆä¸‹ï¼‰](https://zhuanlan.zhihu.com/p/155027693)

[6.2 Logistic Regression and the Cross Entropy Cost - Logistic regression - y å±äº 0 æˆ– 1](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_2_Cross_entropy.html)

[6.3 Logistic Regression and the Softmax Cost-Logistic regression ](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_3_Softmax.html) sklearn ä¸­çš„ä»£ä»·å‡½æ•°ï¼Œè¿™é‡Œçš„ [y å±äº-1 æˆ– 1](https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/notes/6_Linear_twoclass_classification/6_3_Softmax.ipynb)

### é™„åŠ çŸ¥è¯†

#### Generalized Linear Models å¹¿ä¹‰çº¿æ€§æ¨¡å‹

[Generalized Linear Models (GLM)](https://en.jinzhao.wiki/wiki/Generalized_linear_model)

[Generalized Linear Models](https://www.statsmodels.org/devel/glm.html)

[Generalized Linear Models Explained with Examples](https://vitalflux.com/generalized-linear-models-explained-with-examples/)

[Generalized Linear Model Theory - æ¨è](https://data.princeton.edu/wws509/notes/a2.pdf)

[Generalized Linear Models](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf)

åœ¨çº¿æ€§å›å½’æ¨¡å‹ä¸­çš„å‡è®¾ä¸­ï¼Œæœ‰ä¸¤ç‚¹éœ€è¦æå‡ºï¼š

1. å‡è®¾å› å˜é‡æœä»é«˜æ–¯åˆ†å¸ƒï¼š$Y={{\theta }^{T}}x+\xi$ï¼Œå…¶ä¸­è¯¯å·®é¡¹$\xi \sim N(0,{{\sigma }^{2}})$ï¼Œé‚£ä¹ˆå› å˜é‡$Y\sim N({{\theta }^{T}}x,{{\sigma }^{2}})$
2. æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºä¸º$E[Y]$ï¼Œæ ¹æ®$Y={{\theta }^{T}}x+\xi$ï¼Œ$E[Y]=E[{{\theta }^{T}}x+\xi ]={{\theta }^{T}}x$,è®°$\eta ={{\theta }^{T}}x$ï¼Œåˆ™$\eta =E[Y]$

å¹¿ä¹‰çº¿æ€§æ¨¡å‹å¯ä»¥è®¤ä¸ºåœ¨ä»¥ä¸Šä¸¤ç‚¹å‡è®¾åšäº†æ‰©å±•ï¼š

1. å› å˜é‡åˆ†å¸ƒä¸ä¸€å®šæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œæœä»ä¸€ä¸ªæŒ‡æ•°åˆ†å¸ƒæ—ï¼ˆ[Exponential family](https://en.jinzhao.wiki/wiki/Exponential_family)ï¼‰å³å¯ã€‚
2. æ¨¡å‹é¢„æµ‹è¾“å‡ºä»ç„¶å¯ä»¥è®¤ä¸ºæ˜¯$E[Y]$ï¼ˆå®é™…ä¸Šæ˜¯$E[T(Y)]$ï¼Œè®¸å¤šæƒ…å†µä¸‹$T(Y)=Y$ï¼‰ï¼Œä½†æ˜¯$Y$çš„åˆ†å¸ƒä¸ä¸€å®šæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œ$E[Y]$å’Œ$\eta ={{\theta }^{T}}x$ä¹Ÿä¸ä¸€å®šæ˜¯ç®€å•çš„ç›¸ç­‰å…³ç³»ï¼Œå®ƒä»¬çš„å…³ç³»ç”¨$\eta =g(E[Y])$æè¿°ï¼Œç§°ä¸ºè¿æ¥å‡½æ•°(link function)ï¼Œå…¶ä¸­$\eta$ç§°ä¸ºè‡ªç„¶å‚æ•°ã€‚

ç”±äºä»¥ä¸Šä¸¤ç‚¹çš„æ‰©å±•ï¼Œå¹¿ä¹‰çº¿æ€§æ¨¡å‹çš„åº”ç”¨æ¯”åŸºæœ¬çº¿æ€§æ¨¡å‹å¹¿æ³›è®¸å¤šã€‚å¯¹äºå¹¿ä¹‰çº¿æ€§è¿™ä¸ªæœ¯è¯­ï¼Œå¯ä»¥ç†è§£ä¸ºå¹¿ä¹‰ä½“ç°åœ¨å› å˜é‡çš„åˆ†å¸ƒå½¢å¼æ¯”è¾ƒå¹¿ï¼Œåªè¦æ˜¯ä¸€æŒ‡æ•°åˆ†å¸ƒæ—å³å¯ï¼Œè€Œçº¿æ€§åˆ™ä½“ç°åœ¨è‡ªç„¶å‚æ•°$\eta ={{\theta }^{T}}x$æ˜¯$\theta$çš„çº¿æ€§å‡½æ•°ã€‚

> è¿™ä¸€å®¶æ—ä¸­çš„æ¨¡å‹å½¢å¼åŸºæœ¬ä¸Šéƒ½å·®ä¸å¤šï¼Œä¸åŒçš„å°±æ˜¯å› å˜é‡(Y)ä¸åŒï¼Œå¦‚æœæ˜¯è¿ç»­çš„ï¼Œå°±æ˜¯å¤šé‡çº¿æ€§å›å½’ï¼Œå¦‚æœæ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå°±æ˜¯ logistic å›å½’ï¼Œå¦‚æœæ˜¯ poisson åˆ†å¸ƒï¼Œå°±æ˜¯ poisson å›å½’ï¼Œå¦‚æœæ˜¯è´ŸäºŒé¡¹åˆ†å¸ƒï¼Œå°±æ˜¯è´ŸäºŒé¡¹å›å½’ï¼Œç­‰ç­‰ã€‚åªè¦æ³¨æ„åŒºåˆ†å®ƒä»¬çš„å› å˜é‡å°±å¯ä»¥äº†ã€‚logistic å›å½’çš„å› å˜é‡å¯ä»¥æ˜¯äºŒåˆ†ç±»çš„(äºŒé¡¹é€»è¾‘å›å½’)ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤šåˆ†ç±»çš„ï¼ˆå¤šé¡¹é€»è¾‘å›å½’æˆ–è€… softmax å›å½’ï¼‰ï¼Œä½†æ˜¯äºŒåˆ†ç±»çš„æ›´ä¸ºå¸¸ç”¨ï¼Œä¹Ÿæ›´åŠ å®¹æ˜“è§£é‡Šã€‚æ‰€ä»¥å®é™…ä¸­æœ€ä¸ºå¸¸ç”¨çš„å°±æ˜¯äºŒåˆ†ç±»çš„ logistic å›å½’ã€‚

æ ¹æ®[sklearn ä¸­çš„å¹¿ä¹‰çº¿æ€§å›å½’ Generalized Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression)çš„ç¬¬äºŒç§æ–¹å¼[exponential dispersion model (EDM)](https://en.jinzhao.wiki/wiki/Exponential_dispersion_model)ï¼š

å…¶å®å°±æ˜¯è¦è®©çœŸå® y ä¸é¢„æµ‹ y ä¹‹é—´çš„å·®å¼‚è¶Šå°è¶Šå¥½ï¼š
$$\min_{w} \frac{1}{2 n_{\text{samples}}} \sum_i d(y_i, \hat{y}_i) + \frac{\alpha}{2} \|w\|_2$$

å‡è®¾ y åˆ†åˆ«ç¬¦åˆä¸‹åˆ—åˆ†å¸ƒï¼Œæ±‚çœŸå® y ä¸é¢„æµ‹ y ä¹‹é—´çš„å·®å¼‚ï¼ˆDevianceï¼‰ï¼ˆlog ç›¸å‡ä¸å°±æ˜¯ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„æ¯”å—ï¼Ÿä¸å°±æ˜¯å¯¹æ•°å‡ ç‡ï¼ˆlog oddsï¼‰å—ï¼Ÿå¯¹æ•°å‡ ç‡ä¸º 0 æ—¶ä¸å°±æ˜¯æ¦‚ç‡æ¯”ä¸º 1 å—ï¼Ÿä¸å°±æ˜¯å·®å¼‚æœ€å°ä¹ˆï¼ï¼‰ï¼š

- **Normalï¼ˆGaussianï¼‰**ï¼š
  å°±ç›¸å½“äºæ™®é€šçš„çº¿æ€§å›å½’ï¼ˆåŠ ä¸Šæ­£åˆ™å°±æ˜¯ Ridge, ElasticNet ç­‰ï¼‰
  $$f(y;\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-\mu)^2}{2\sigma^2})$$
  $$\log f(y;\mu,\sigma) = -\log\sqrt{2\pi\sigma^2} - \frac{y^2-2y\mu+\mu^2}{2\sigma^2}= -\log\sqrt{2\pi\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{-2y\mu+\mu^2}{2\sigma^2}$$
  Deviance(log-likelihood ratio)(é¢„æµ‹$\hat{y}$å°±æ˜¯é¢„æµ‹çš„å‡å€¼(å³æœŸæœ›$\mu$))ï¼š
  $$\log f(y;y,\sigma) - \log f(y;\hat{y},\sigma) = - \frac{-2y.y+y^2}{2\sigma^2} - (- \frac{-2y.\hat{y}+\hat{y}^2}{2\sigma^2}) \\= \frac{y^2 -2y.\hat{y}+\hat{y}^2}{2\sigma^2} \\= \frac{(y-\hat{y})^2}{2\sigma^2} \\= \frac{D(y,\hat{y})}{2\sigma^2}$$

- **Poisson**ï¼š
  å°±ç›¸å½“äº PoissonRegressor
  $$f(y;\mu) = \frac{\mu^y e^{-\mu}}{y!}$$
  $$\log f(y;\mu) = y\log\mu -\mu -\log(y!)$$
  Deviance(log-likelihood ratio)(é¢„æµ‹$\hat{y}$å°±æ˜¯é¢„æµ‹çš„å‡å€¼(å³æœŸæœ›$\mu$))ï¼š
  $$\log f(y;y) -\log f(y;\hat{y}) = y\log\frac{y}{\hat{y}} - y + \hat{y}$$

- **Binomial(sklearn ä¸­æ²¡æœ‰)**ï¼š
  å°±ç›¸å½“äº Logistic Regression
  $$f(y;n,p) = \binom{n}{y} p^y (1-p)^{n-y}$$
  $$\log f(y;n,p) =y\log p+(n-y)\log(1-p) + \log(\binom{n}{y})$$
  Deviance(log-likelihood ratio)(é¢„æµ‹$\hat{y}$å°±æ˜¯é¢„æµ‹çš„å‡å€¼(å³æœŸæœ›$\hat{y} = \mu = np$))ï¼š
  $$\log f(y;n,\frac{y}{n}) - \log f(y;n,\frac{\hat{y}}{n})= y\log\frac{y}{\hat{y}} + (n-y)\log\frac{1-\frac{y}{n}}{1-\frac{\hat{y}}{n}} = y\log\frac{y}{\hat{y}} + (n-y)\log\frac{n-y}{n-\hat{y}}$$
  Binomial distribution(B(n,p))ï¼Œè€Œ Bernoulli distribution ä¸­ n=1

> ä¸Šè¿°è®¡ç®—éƒ½æœ‰ä¸€ä¸ª 2 å€ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ„æ€æ‰€ä»¥æ²¡æœ‰å†™å‡ºæ¥ã€‚
> è¿˜æœ‰ç”¨ link function è§£é‡Šçš„ï¼Œç›®å‰ä¸æ˜¯å¾ˆæ˜ç™½-å‚è€ƒ[å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰](https://www.cnblogs.com/dreamvibe/p/4259460.html)ã€‚

#### S å‹å‡½æ•°ï¼ˆLogistic & Sigmoid å‡½æ•°ï¼‰

**Logistic å‡½æ•°**ï¼ˆ[Logistic function](https://en.jinzhao.wiki/wiki/Logistic_function)ï¼‰çš„å…¬å¼å®šä¹‰ï¼š

$${\displaystyle f(x)={\frac {L}{1+e^{-k(x-x_{0})}}}}$$
å…¶ä¸­$L$æ˜¯æœ€å¤§å€¼ï¼Œ$x_0$æ˜¯ä¸­å¿ƒç‚¹(ä½ç½®å‚æ•°)ï¼Œ$K$æ˜¯æ›²çº¿çš„å€¾æ–œåº¦ï¼ˆå½¢çŠ¶å‚æ•°ï¼Œ|K|>0 è¶Šå¤§ï¼Œæ›²çº¿åœ¨ä¸­å¿ƒç‚¹é™„è¿‘å¢é•¿è¶Šå¿«ï¼‰ã€‚

**é€»è¾‘æ–¯è°›åˆ†å¸ƒ**[Logistic distribution](https://en.jinzhao.wiki/wiki/Logistic_distribution)ï¼š

- åˆ†å¸ƒå‡½æ•° CDF(Cumulative distribution function)ï¼š
  $${\displaystyle F(x;\mu ,s)={\frac {1}{1+e^{-(x-\mu )/s}}}={\frac {1}{2}}+{\frac {1}{2}}\operatorname {tanh} \left({\frac {x-\mu }{2s}}\right).}$$
- å¯†åº¦å‡½æ•° PDF(Probability density function)ï¼š
  $${\displaystyle {\begin{aligned}f(x;\mu ,s)&={\frac {e^{-(x-\mu )/s}}{s\left(1+e^{-(x-\mu )/s}\right)^{2}}}\\[4pt]&={\frac {1}{s\left(e^{(x-\mu )/(2s)}+e^{-(x-\mu )/(2s)}\right)^{2}}}\\[4pt]&={\frac {1}{4s}}\operatorname {sech} ^{2}\left({\frac {x-\mu }{2s}}\right).\end{aligned}}}$$

> $\mu$ä¸ºå‡å€¼ï¼Œs æ˜¯ä¸€ä¸ªä¸æ ‡å‡†å·®ï¼ˆ[standard deviation](https://en.jinzhao.wiki/wiki/Standard_deviation)ï¼‰æˆæ¯”ä¾‹çš„å‚æ•°

**Sigmoid å‡½æ•°**ï¼ˆ[Sigmoid function](https://en.jinzhao.wiki/wiki/Sigmoid_function)ï¼‰çš„å…¬å¼å®šä¹‰ï¼š
$${\displaystyle S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}=1-S(-x).}$$

Sigmoid å‡½æ•°æ˜¯ä¸€ä¸ªæœ‰ç•Œã€å¯å¾®çš„å®å‡½æ•°ï¼Œå¯ä»¥å°†è¾“å…¥å‹ç¼©åˆ°(0,1)åŒºé—´ï¼Œç»å¸¸ç”¨ä½œæ¿€æ´»å‡½æ•°å’Œæ¦‚ç‡è¾“å‡ºã€‚
Sigmoid å‡½æ•°å¯¹äºå°äº 0 çš„å€¼æ˜¯å‡¸çš„ï¼Œå¯¹äºå¤§äº 0 çš„å€¼æ˜¯å‡¹çš„ã€‚
Sigmoid å‡½æ•°æ˜¯ä¸€ä¸ªæ ‡å‡† Logistic å‡½æ•°ï¼ˆstandard logistic function(ä¸€èˆ¬ç”¨$\sigma(x)$)ï¼š$K=1,x_0=0,L=1$ï¼‰

å¯¼æ•°ï¼š
$$ S'(x)= S(x)(1-S(x))$$
æ¨å¯¼ï¼š
$$ S(x) = {\frac {1}{1+e^{-x}}} = (1+e^{-x})^{-1} \\ S'(x)=(-1)*(1+e^{-x})^{-2}*e^{-x}*(-1) \\= (1+e^{-x})^{-2}*e^{-x} \\= \frac{e^{-x}}{(1+e^{-x})^{2}} \\= \frac{1+e^{-x}-1}{(1+e^{-x})^{2}} \\= \frac{1+e^{-x}}{(1+e^{-x})^{2}} - \frac{1}{(1+e^{-x})^{2}} \\=\frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^{2}} \\= \frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})$$

ä¸**åŒæ›²æ­£åˆ‡å‡½æ•°**ï¼ˆ [hyperbolic tangent function](https://en.jinzhao.wiki/wiki/Hyperbolic_tangent)ï¼‰ï¼š
$${\displaystyle f(x)={\frac {1}{2}}+{\frac {1}{2}}\tanh \left({\frac {x}{2}}\right),} \\ {\displaystyle \tanh(x)=2f(2x)-1.}$$

$${\displaystyle {\begin{aligned}\tanh(x)&={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}={\frac {e^{x}\cdot \left(1-e^{-2x}\right)}{e^{x}\cdot \left(1+e^{-2x}\right)}}\\&=f(2x)-{\frac {e^{-2x}}{1+e^{-2x}}}=f(2x)-{\frac {e^{-2x}+1-1}{1+e^{-2x}}}=2f(2x)-1.\end{aligned}}}$$

#### å½’ä¸€åŒ–æŒ‡æ•°å‡½æ•°ï¼ˆSoftmax å‡½æ•°ï¼‰

**Softmax å‡½æ•°**ï¼ˆ[Softmax function](https://en.jinzhao.wiki/wiki/Softmax_function)ï¼Œä¹Ÿç§°ä¸ºå½’ä¸€åŒ–æŒ‡æ•°å‡½æ•° normalized exponential functionï¼‰çš„å…¬å¼å®šä¹‰ï¼š
standard (unit) softmax function${\displaystyle \sigma :\mathbb {R} ^{K}\to [0,1]^{K}}$

$${\displaystyle \sigma (\mathbf {z} )_{i}={\frac {e^{z_{i}}}{\sum _{j=1}^{K}e^{z_{j}}}}\ \ \ \ {\text{ for }}i=1,\dotsc ,K{\text{ and }}\mathbf {z} =(z_{1},\dotsc ,z_{K})\in \mathbb {R} ^{K}.}$$

ä¸€èˆ¬å½¢å¼ï¼š
$${\displaystyle \sigma (\mathbf {z} )_{i}={\frac {e^{\beta z_{i}}}{\sum _{j=1}^{K}e^{\beta z_{j}}}}{\text{ or }}\sigma (\mathbf {z} )_{i}={\frac {e^{-\beta z_{i}}}{\sum _{j=1}^{K}e^{-\beta z_{j}}}}{\text{ for }}i=1,\dotsc ,K.}$$

æ€§è´¨ï¼š
$${\displaystyle \sigma (\mathbf {z} +\mathbf {c} )_{j}={\frac {e^{z_{j}+c}}{\sum _{k=1}^{K}e^{z_{k}+c}}}={\frac {e^{z_{j}}\cdot e^{c}}{\sum _{k=1}^{K}e^{z_{k}}\cdot e^{c}}}=\sigma (\mathbf {z} )_{j}.}$$

ç­‰å¼å·¦è¾¹çš„${\displaystyle \mathbf {c} =(c,\dots ,c)}$

> å¦‚æœ$z_i$éƒ½ç­‰äºä¸€ä¸ªå‚æ•° C æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿä»ç†è®ºä¸Šè¾“å‡ºä¸º$(\frac{1}{K},...,\frac{1}{K}) \in \mathbb{R}^K$ï¼Œä½†æ˜¯ä»æ•°å€¼è®¡ç®—ä¸Šè¯´ï¼Œå½“ C å¾ˆå¤§æ—¶$e^C$ä¼šå‘ç”Ÿä¸Šæº¢ï¼Œå½“ C å¾ˆå°æ—¶$\sum _{k=1}^{K}e^C$ä¼šå‘ç”Ÿä¸‹æº¢ï¼Œè¿™æ—¶æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ä¸Šè¿°æ€§è´¨ï¼Œå°†$\mathbf {z}$å‡å»$\max_i {z_i}$,é‚£ä¹ˆæœ€å¤§å€¼å°±æ˜¯ 0ï¼Œæ’é™¤äº†ä¸Šæº¢çš„å¯èƒ½ï¼ŒåŒæ ·çš„åˆ†æ¯è‡³å°‘æœ‰ä¸€ä¸ªä¸º 1 çš„é¡¹ï¼Œæ’é™¤äº†å› ä¸ºåˆ†æ¯ä¸‹æº¢è€Œå¯¼è‡´è¢« 0 é™¤çš„å¯èƒ½æ€§ã€‚
> $$\sigma (\mathbf {z} )_{i}={\frac {e^{z_{i}}}{\sum _{j=1}^{K}e^{z_{j}}}} = {\frac {e^{(z_{i}-z_{max})}}{\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}}  = {\frac {e^{(z_{i}-z_{max})}}{1+\sum _{j=1,j\neq max}^{K}e^{(z_{j}-z_{max})}}}$$

**log softmax**å‡½æ•°åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¹Ÿç»å¸¸é‡è§ï¼Œå…¶å®å°±æ˜¯æ±‚å®Œ softmaxï¼Œå†å¯¹å…¶æ±‚ logï¼Œå¦‚æœç›´æ¥è®¡ç®—å¯èƒ½ä¼šå‡ºç°é—®é¢˜ï¼ˆå½“ softmax å¾ˆå°æ—¶ï¼Œlog ä¼šå¾—åˆ°$-\infty$ï¼‰,è¿™æ—¶æˆ‘å°±è¦æ¨å¯¼å‡º log softmax çš„è¡¨è¾¾å¼ï¼š
$$\log(\sigma (\mathbf {z} ))_{i}=\log{\frac {e^{(z_{i}-z_{max})}}{\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}} \\= \log e^{(z_{i}-z_{max})} - \log {\sum _{j=1}^{K}e^{(z_{j}-z_{max})}} \\= (z_{i}-z_{max})- \log {\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}$$

è€Œ${\sum _{j=1}^{K}e^{(z_{j}-z_{max})}}$æ˜¯å¤§äºç­‰äº 1 çš„ï¼Œå¹¶ä¸”ä¸ä¼šå¤§çš„ç¦»è°±ï¼Œæ‰€ä»¥ä¸ä¼šå‡ºé—®é¢˜ã€‚

**negative log-likelihood**ï¼ˆNLLï¼‰ï¼Œlikelihood æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼ˆsoftmax ä¹Ÿæ˜¯æ¦‚ç‡ï¼‰ï¼Œæ‰€ä»¥ log-likelihood å°äº 0ï¼Œnegative log-likelihood åˆ™å¤§äº 0ï¼Œè¿™æ ·å°±å¯ä»¥æœ€å°åŒ– negative log-likelihood äº†

### å‚è€ƒæ–‡çŒ®

[6-1] Berger A,Della Pietra SD,Pietra VD. A maximum entropy approach to naturallanguage processing. Computational Linguistics,1996,22(1),39â€“71

[6-2] Berger A. The Improved Iterative Scaling Algorithm: A Gentle Introduction.http://www.cs.cmu.edu/ afs/cs/user/aberger/www/ps/scaling.ps

[6-3] Hastie T,Tibshirani Rï¼ŒFriedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction. Springer-Verlag. 2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬:ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[6-4] Mitchell TM. Machine Learning. McGraw-Hill Companies,Inc. 1997ï¼ˆä¸­è¯‘æœ¬ï¼šæœºå™¨å­¦ä¹ ã€‚åŒ—äº¬:æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2003ï¼‰

[6-5] Collins M,Schapire RE,Singer Y. Logistic Regression,AdaBoost and BregmanDistances. Machine Learning Journal,2004

[6-6] Canu S,Smola AJ. Kernel method and exponential family. Neurocomputing,2005,69:714â€“720

## ç¬¬ 7 ç«  æ”¯æŒå‘é‡æœº

**æ”¯æŒå‘é‡æœº**ï¼ˆ[support vector machineï¼ŒSVM](https://en.jinzhao.wiki/wiki/Support-vector_machine)ï¼‰æ˜¯ä¸€ç§äºŒç±»åˆ†ç±»æ¨¡å‹ã€‚å®ƒçš„åŸºæœ¬æ¨¡å‹æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„é—´éš”æœ€å¤§çš„çº¿æ€§åˆ†ç±»å™¨ï¼Œé—´éš”æœ€å¤§ä½¿å®ƒæœ‰åˆ«äºæ„ŸçŸ¥æœºï¼›æ”¯æŒå‘é‡æœºè¿˜åŒ…æ‹¬**æ ¸æŠ€å·§**ï¼Œè¿™ä½¿å®ƒæˆä¸ºå®è´¨ä¸Šçš„éçº¿æ€§åˆ†ç±»å™¨ã€‚æ”¯æŒå‘é‡æœºçš„å­¦ä¹ ç­–ç•¥å°±æ˜¯é—´éš”æœ€å¤§åŒ–ï¼Œå¯å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ±‚è§£**å‡¸äºŒæ¬¡è§„åˆ’**ï¼ˆconvex quadratic programmingï¼‰çš„é—®é¢˜ï¼Œä¹Ÿç­‰ä»·äºæ­£åˆ™åŒ–çš„**åˆé¡µæŸå¤±å‡½æ•°**çš„æœ€å°åŒ–é—®é¢˜ã€‚æ”¯æŒå‘é‡æœºçš„å­¦ä¹ ç®—æ³•æ˜¯æ±‚è§£å‡¸äºŒæ¬¡è§„åˆ’çš„æœ€ä¼˜åŒ–ç®—æ³•ã€‚

æ”¯æŒå‘é‡æœºå­¦ä¹ æ–¹æ³•åŒ…å«æ„å»ºç”±ç®€è‡³ç¹çš„æ¨¡å‹ï¼š**çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœº**ï¼ˆlinear supportvector machine in linearly separable caseï¼‰ã€**çº¿æ€§æ”¯æŒå‘é‡æœº**ï¼ˆlinear support vectormachineï¼‰åŠ**éçº¿æ€§æ”¯æŒå‘é‡æœº**ï¼ˆnon-linear support vector machineï¼‰ã€‚ç®€å•æ¨¡å‹æ˜¯å¤æ‚æ¨¡å‹çš„åŸºç¡€ï¼Œä¹Ÿæ˜¯å¤æ‚æ¨¡å‹çš„ç‰¹æ®Šæƒ…å†µã€‚å½“è®­ç»ƒæ•°æ®çº¿æ€§å¯åˆ†æ—¶ï¼Œé€šè¿‡**ç¡¬é—´éš”æœ€å¤§åŒ–**ï¼ˆhardmargin maximizationï¼‰ï¼Œå­¦ä¹ ä¸€ä¸ªçº¿æ€§çš„åˆ†ç±»å™¨ï¼Œå³çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœºï¼Œåˆç§°ä¸º**ç¡¬é—´éš”æ”¯æŒå‘é‡æœº**ï¼›å½“è®­ç»ƒæ•°æ®è¿‘ä¼¼çº¿æ€§å¯åˆ†æ—¶ï¼Œé€šè¿‡**è½¯é—´éš”æœ€å¤§åŒ–**ï¼ˆsoft marginmaximizationï¼‰ï¼Œä¹Ÿå­¦ä¹ ä¸€ä¸ªçº¿æ€§çš„åˆ†ç±»å™¨ï¼Œå³çº¿æ€§æ”¯æŒå‘é‡æœºï¼Œåˆç§°ä¸º**è½¯é—´éš”æ”¯æŒå‘é‡æœº**ï¼›å½“è®­ç»ƒæ•°æ®çº¿æ€§ä¸å¯åˆ†æ—¶ï¼Œé€šè¿‡ä½¿ç”¨**æ ¸æŠ€å·§ï¼ˆkernel trickï¼‰åŠè½¯é—´éš”æœ€å¤§åŒ–ï¼Œå­¦ä¹ éçº¿æ€§æ”¯æŒå‘é‡æœº**ã€‚

å½“è¾“å…¥ç©ºé—´ä¸ºæ¬§æ°ç©ºé—´æˆ–ç¦»æ•£é›†åˆã€ç‰¹å¾ç©ºé—´ä¸ºå¸Œå°”ä¼¯ç‰¹ç©ºé—´æ—¶ï¼Œæ ¸å‡½æ•°ï¼ˆkernelfunctionï¼‰è¡¨ç¤ºå°†è¾“å…¥ä»è¾“å…¥ç©ºé—´æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´å¾—åˆ°çš„ç‰¹å¾å‘é‡ä¹‹é—´çš„å†…ç§¯ã€‚é€šè¿‡ä½¿ç”¨æ ¸å‡½æ•°å¯ä»¥å­¦ä¹ éçº¿æ€§æ”¯æŒå‘é‡æœºï¼Œç­‰ä»·äºéšå¼åœ°åœ¨é«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­å­¦ä¹ çº¿æ€§æ”¯æŒå‘é‡æœºã€‚è¿™æ ·çš„æ–¹æ³•ç§°ä¸ºæ ¸æŠ€å·§ã€‚æ ¸æ–¹æ³•ï¼ˆkernel methodï¼‰æ˜¯æ¯”æ”¯æŒå‘é‡æœºæ›´ä¸ºä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

> SVM æœ‰ä¸‰å®ï¼šé—´éš”ã€å¯¹å¶ã€æ ¸æŠ€å·§

å‡½æ•°é—´éš”ï¼š$\hat\gamma = y(w.x + b), y\in \{-1,+1\}$
å‡ ä½•é—´éš”ï¼š$\gamma = \frac{1}{\|w\|}|w.x + b| = \frac{\hat\gamma}{\|w\|}$

- **æ¨¡å‹**ï¼š

$$w.x + b=0$$

- **ç­–ç•¥**ï¼š

1. æœ€å¤§é—´éš”(ç¡¬é—´éš”åŸå§‹é—®é¢˜-å‡¸äºŒæ¬¡è§„åˆ’)
   $$\begin{aligned} &\max_{w,b} &\frac{\hat\gamma}{\|w\|} \\ &\text{s.t.} &y_i(w.x_i+b) \geq \hat\gamma\end{aligned}$$
   å‡½æ•°é—´éš”$\hat\gamma$çš„å¤§å°æ˜¯å¯ä»¥å˜çš„ï¼Œæˆ‘ä»¬è®©å…¶ç­‰äº 1ï¼Œé‚£ä¹ˆå°†ä¸Šè¿°é—®é¢˜æ”¹å†™ä¸‹ï¼š
   $$\begin{aligned} &\min_{w,b} &\frac{1}{2}\|w\|^2 \\ &\text{s.t.} &y_i(w.x_i+b) \geq 1\end{aligned}$$
   è¿™ä¸å°±æ˜¯ä¸€ä¸ªæ ‡å‡†çš„å‡¸äºŒæ¬¡è§„åˆ’é—®é¢˜ä¹ˆï¼ï¼ˆ$\frac{1}{\|w\|}$åœ¨$\|w\| = 0$å¤„ä¸å¯å¾®ï¼Œ$\argmax \frac{1}{\|w\|} ä¸ \argmin \frac{1}{2}\|w\|^2$ç­‰ä»·ï¼‰
   å¦‚æœæ•°æ®é›†çº¿æ€§å¯åˆ†ï¼Œé‚£ä¹ˆæœ€å¤§é—´éš”åˆ†ç¦»è¶…å¹³é¢**å­˜åœ¨ä¸”å”¯ä¸€**ï¼Œå…·ä½“è¯æ˜å°±ä¸è¯äº†ï¼ˆè§ç»Ÿè®¡å­¦ä¹ æ–¹æ³• 117 é¡µï¼‰ã€‚
   åœ¨çº¿æ€§å¯åˆ†çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒé›†ä¸­çš„æ ·æœ¬ç‚¹ä¸åˆ†ç¦»è¶…å¹³é¢è·ç¦»æœ€è¿‘çš„ç‚¹ç§°ä¸º**æ”¯æŒå‘é‡**ï¼Œä¹Ÿå°±æ˜¯æ»¡è¶³$y_i(w.x_i +b ) =1$çš„ç‚¹ã€‚
   è€Œè¶…å¹³é¢$w.x_i +b = +1,w.x_i +b = -1$ç§°ä¸º**é—´éš”è¾¹ç•Œ**ï¼Œä¸¤ä¸ªé—´éš”è¾¹ç•Œä¹‹é—´çš„è·ç¦»ç§°ä¸º**é—´éš”**ï¼ˆmarginï¼‰ï¼Œé—´éš”å¤§å°ä¸º$\frac{2}{\|w\|}$ã€‚
   åœ¨å†³å®šåˆ†ç¦»è¶…å¹³é¢æ—¶**åªæœ‰æ”¯æŒå‘é‡èµ·ä½œç”¨**ï¼Œè€Œå…¶å®ƒæ ·æœ¬ç‚¹å¹¶ä¸èµ·ä½œç”¨ï¼Œæ‰€ä»¥è¯¥æ¨¡å‹å«åšæ”¯æŒå‘é‡æœºã€‚

   - æ‹‰æ ¼æœ—æ—¥å‡½æ•°(æ±‚æœ€å°)
     $$L(w,b,\alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^N \alpha_i(1-y_i(w.x_i+b))$$
     $$\min_{w,b} \max_{\alpha} L(w,b,\alpha)$$

   - æ‹‰æ ¼æœ—æ—¥å¯¹å¶å‡½æ•°ï¼ˆæ±‚æœ€å¤§ï¼‰
     $$\max_{\alpha} g(\alpha) = \max_{\alpha} \inf_{w,b} L(w,b,\alpha)$$

2. å¸¦æ­£åˆ™é¡¹çš„åˆé¡µæŸå¤±å‡½æ•°(è½¯é—´éš”åŸå§‹é—®é¢˜-å‡¸äºŒæ¬¡è§„åˆ’)
   $$\min_{w,b} \underbrace{\sum_{i=1}^N\max(0,1-y_i(w.x_i+b))}_{\text{hinge loss function}} + \underbrace{\lambda\|w\|^2}_{\text{æ­£åˆ™åŒ–é¡¹}}$$
   ç­‰ä»·**è½¯é—´éš”**æœ€å¤§åŒ–çš„ä¼˜åŒ–é—®é¢˜ï¼š
   $$\begin{aligned} &\min_{w,b,\xi} & \sum _{i=1}^{n}\xi _{i}+\lambda \|\mathbf {w} \|^{2} \\ &\displaystyle {\text{subject to }} & y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}+b)\geq 1-\xi _{i}\\ &&\xi _{i}\geq 0,\,{\text{for all }}i.\end{aligned}$$
   å…¶ä¸­${\displaystyle \xi _{i}=\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}+b)\right)}$æ˜¯æ¾å¼›å˜é‡ï¼ˆæ¾å¼›æ–¹æ³•[Relaxation](<https://en.jinzhao.wiki/wiki/Category:Relaxation_(approximation)>)æœ‰å¾ˆå¤šï¼Œä¸‹é¢åªæ˜¯ä¸€ç§è€Œå·²ï¼‰ï¼ˆè·Ÿä¸‹é¢æ¾å¼›å˜é‡å®šä¹‰ä¸åŒï¼Œåªæœ‰ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­æœ‰è¯´åˆ°ï¼Œå…¶å®ƒåœ°æ–¹æ²¡æœ‰æ‰¾åˆ°ï¼‰ã€‚
   $y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}+b)\geq 1-\xi _{i}$ç›¸å½“äºåˆ†ç±»ç‚¹å¯ä»¥å¤„äºé—´éš”ä¸­(ä¸å¤ªå‡†ç¡®ï¼Œå¤„äºé—´éš”è¾¹ç•Œçš„ä¸€ä¾§)ï¼Œå¯¹äºè½¯é—´éš”æ”¯æŒå‘é‡æœºä¸­çš„æ”¯æŒå‘é‡åŒ…å«äº†é—´éš”ä¸­çš„å‘é‡ã€‚

   - æ‹‰æ ¼æœ—æ—¥å‡½æ•°(æ±‚æœ€å°)
     $$L(w,b,\xi,\alpha,\mu) = \lambda\|w\|^2 + \sum_{i=1}^N \xi_{i} + \sum_{i=1}^N \alpha_i(1-\xi_{i}-y_i(w.x_i+b)) + \sum_{i=1}^N(\mu_i*(-\xi_{i}))$$
     $$\min_{w,b,\xi} \max_{\alpha,\mu} L(w,b,\xi,\alpha,\mu)$$

   - æ‹‰æ ¼æœ—æ—¥å¯¹å¶å‡½æ•°ï¼ˆæ±‚æœ€å¤§ï¼‰
     $$\max_{\alpha,\mu} g(\alpha,\mu) = \max_{\alpha,\mu} \inf_{w,b,\xi} L(w,b,\xi,\alpha,\mu)$$

- **ç®—æ³•**ï¼š
  ç›´æ¥æ±‚å¯¼ï¼Œä»¤å…¶ç­‰äº 0ï¼ˆå…¶å®å°±æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰

**æ ¸æŠ€å·§**ï¼š
é¦–å…ˆå†™å‡ºäº†åŸå½¢å¼çš„å¯¹å¶å½¢å¼ï¼Œç„¶åæŠŠ$xç”¨\phi (x)$ä»£æ›¿ï¼Œæœ€ç»ˆå‘ç°æ ¹æœ¬ä¸éœ€è¦çŸ¥é“$\phi (x)$ï¼Œåªéœ€è¦æ ¸å‡½æ•°å°±è¡Œäº†ï¼Œå…·ä½“è¯æ˜å°±ä¸è¯äº†ï¼Œå¾ˆç®€å•ï¼Œä¸Šé¢ä¹Ÿæœ‰ä»‹ç»äº†æ ¸æŠ€å·§çŸ¥è¯†ã€‚

> ä¹¦ä¸­è¿˜ä»‹ç»äº†åŸå½¢å¼çš„**å¯¹å¶å½¢å¼**ï¼ˆåŒºåˆ«äºæ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼‰,ä¹Ÿå°±æ˜¯ç­‰ä»·å½¢å¼ï¼ˆæ„ŸçŸ¥æœºä¸­ 2.3.3 èŠ‚ 44 é¡µ ä¹Ÿæ˜¯ç­‰ä»·çš„æ„æ€ï¼‰ï¼Œè¿™ä¸¤ä¸ªåœ°æ–¹çš„ç­‰ä»·éƒ½æ˜¯ç»è¿‡åŸºæœ¬æ¨å¯¼ï¼Œæ±‚å‡º w å‚æ•°ï¼Œç„¶åå¯¹åŸé—®é¢˜è¿›è¡Œäº†æ›¿æ¢ã€‚

### é™„åŠ çŸ¥è¯†

#### æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•

[æœ€ä¼˜åŒ–ï¼šå»ºæ¨¡ã€ç®—æ³•ä¸ç†è®º/æœ€ä¼˜åŒ–è®¡ç®—æ–¹æ³•](http://bicmr.pku.edu.cn/~wenzw/optbook.html)
[Stephen Boyd çš„ Convex Optimization - å‡¸ä¼˜åŒ–](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
[Nonlinear Programming by Dimitri P. Bertsekas - éçº¿æ€§è§„åˆ’](http://www.athenasc.com/nonlinbook.html)

**å‡¸ä¼˜åŒ–**ï¼ˆ[Convex optimization](https://en.jinzhao.wiki/wiki/Convex_optimization)ï¼‰ï¼š
å‡¸ä¼˜åŒ–é—®é¢˜æ˜¯ç›®æ ‡å‡½æ•°ä¸ºå‡¸å‡½æ•°ï¼Œå¯è¡Œé›†ä¸ºå‡¸é›†çš„ä¼˜åŒ–é—®é¢˜ã€‚
æ ‡å‡†å½¢å¼ï¼š
$${\displaystyle {\begin{aligned}&{\underset {\mathbf {x} }{\operatorname {minimize} }}&&f(\mathbf {x} )\\&\operatorname {subject\ to} &&g_{i}(\mathbf {x} )\leq 0,\quad i=1,\dots ,m\\&&&h_{i}(\mathbf {x} )=0,\quad i=1,\dots ,p,\end{aligned}}}$$
å…¶ä¸­$\mathbf {x} \in \mathbb {R} ^{n}$ä¸ºä¼˜åŒ–å˜é‡ï¼Œ**ç›®æ ‡å‡½æ•°**ï¼ˆobjective functionï¼‰${\displaystyle f:{\mathcal {D}}\subseteq \mathbb {R} ^{n}\to \mathbb {R} }$æ˜¯å‡¸çš„ï¼Œ**ä¸ç­‰å¼çº¦æŸ**${\displaystyle g_{i}:\mathbb {R} ^{n}\to \mathbb {R} }$ä¹Ÿæ˜¯å‡¸çš„ï¼Œ**ç­‰å¼çº¦æŸ**${\displaystyle h_{i}:\mathbb {R} ^{n}\to \mathbb {R} }$æ˜¯**ä»¿å°„**ï¼ˆ[affine](https://en.jinzhao.wiki/wiki/Affine_transformation)ï¼‰çš„

**äºŒæ¬¡çº¦æŸäºŒæ¬¡è§„åˆ’**ï¼ˆ[Quadratically constrained quadratic program](https://en.jinzhao.wiki/wiki/Quadratically_constrained_quadratic_program)ï¼‰ï¼š
$${\begin{aligned}&{\text{minimize}}&&{\tfrac  12}x^{{\mathrm  {T}}}P_{0}x+q_{0}^{{\mathrm  {T}}}x\\&{\text{subject to}}&&{\tfrac  12}x^{{\mathrm  {T}}}P_{i}x+q_{i}^{{\mathrm  {T}}}x+r_{i}\leq 0\quad {\text{for }}i=1,\dots ,m,\\&&&Ax=b,\end{aligned}}$$
å…¶ä¸­$P_0ä»¥åŠP_1,..,P_m \in \mathbb{R}^{n \times n}$,$\mathbf {x} \in \mathbb {R} ^{n}$ä¸ºä¼˜åŒ–å˜é‡
å¦‚æœ$P_0ä»¥åŠP_1,..,P_m \in \mathbb{R}^{n \times n}$æ˜¯åŠæ­£å®šçŸ©é˜µï¼Œé‚£ä¹ˆé—®é¢˜æ˜¯å‡¸çš„ï¼Œå¦‚æœ$P_1,..,P_m$ä¸º 0ï¼Œé‚£ä¹ˆçº¦æŸæ˜¯çº¿æ€§çš„ï¼Œå°±æ˜¯**äºŒæ¬¡è§„åˆ’**ï¼ˆ[Quadratic programming](https://en.jinzhao.wiki/wiki/Quadratic_programming)ï¼‰,å³ç›®æ ‡å‡½æ•°æ˜¯äºŒæ¬¡çš„ï¼Œä¸ç­‰å¼ä»¥åŠç­‰å¼çº¦æŸä¹Ÿæ˜¯çº¿æ€§çš„ï¼›äºŒæ¬¡è§„åˆ’çš„å‰æä¸‹ï¼Œå¦‚æœ$P_0$æ˜¯åŠæ­£å®šçŸ©é˜µé‚£ä¹ˆå°±æ˜¯**å‡¸äºŒæ¬¡è§„åˆ’**ï¼›å¦‚æœ$P_0$ä¸º 0ï¼Œå°±æ˜¯**ä¸æ ‡å‡†çš„çº¿æ€§è§„åˆ’**ï¼ˆ[Linear programming](https://en.jinzhao.wiki/wiki/Linear_programming)ï¼‰ï¼Œå³ç›®æ ‡å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œä¸ç­‰å¼ä»¥åŠç­‰å¼çº¦æŸä¹Ÿæ˜¯çº¿æ€§çš„ã€‚
**æ ‡å‡†çš„çº¿æ€§è§„åˆ’**ï¼šå³ç›®æ ‡å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œéè´Ÿçº¦æŸï¼ˆä¼˜åŒ–å˜é‡æ˜¯éè´Ÿçš„ï¼‰ï¼Œç­‰å¼çº¦æŸä¹Ÿæ˜¯çº¿æ€§çš„ã€‚

çº¿æ€§è§„åˆ’è§£æ³•æœ‰[å•çº¯å½¢æ³•](https://en.jinzhao.wiki/wiki/Simplex_algorithm)ç­‰ã€‚å…¶å®ƒè§„åˆ’çš„[ä¼˜åŒ–ç®—æ³•çœ‹è¿™é‡Œ](https://en.jinzhao.wiki/wiki/Category:Optimization_algorithms_and_methods):å†…ç‚¹æ³•ï¼Œå•çº¯å½¢æ³•ç­‰ï¼›æœ‰çº¿æ€§è§„åˆ’è‡ªç„¶ä¹Ÿæœ‰**åŠ¨æ€è§„åˆ’**ï¼ˆ[Dynamic programming](https://en.jinzhao.wiki/wiki/Dynamic_programming)ï¼‰

**æœ€å°äºŒä¹˜ä¸å°±æ˜¯å‡¸äºŒæ¬¡è§„åˆ’ä¹ˆ**ï¼ˆ$\|y-f(x)\|^2,f(x) = Ax+c$ï¼‰ï¼š
$$\text{minimize} f(x) =\frac{1}{2} \|Ax-b\|^2 = \frac{1}{2}(Ax-b)^T(Ax-b) \\= \frac{1}{2}(x^TA^TAx - x^TA^Tb -b^TAx + b^tb) \\= \frac{1}{2}(x^TA^TAx - 2x^TA^Tb + b^tb)$$
å…¶ä¸­$A \in \mathbb{R}^{m \times n}ï¼Œx \in \mathbb{R}^{n \times 1}ï¼Œb \in \mathbb{R}^{m \times 1} $ï¼Œæ‰€ä»¥$x^TA^Tb å’Œ b^TAx$éƒ½æ˜¯ç›¸ç­‰çš„å®æ•°ï¼Œ$b^tb$ä¹Ÿæ˜¯å®æ•°$A^TA$æ˜¯åŠæ­£å®šçŸ©é˜µ

**L1 å’Œ L2 å›å½’ä¹Ÿèƒ½è½¬åŒ–ç§°ç›¸åº”çš„ä¼˜åŒ–é—®é¢˜**ï¼Œå‚è€ƒï¼š[L1L2 æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–](../å›¾è§£æ•°å­¦/L1L2æ­£åˆ™åŒ–å’Œå‡¸ä¼˜åŒ–.md)

**å…±è½­å‡½æ•°**ï¼ˆ[conjugate function](https://en.jinzhao.wiki/wiki/Convex_conjugate)ï¼‰ï¼š
è®¾å‡½æ•°$f:\mathbb{R}^n \to \mathbb{R}$ï¼Œå®šä¹‰ f çš„å…±è½­å‡½æ•°$f^*:\mathbb{R}^n \to \mathbb{R}$ä¸ºï¼š
$$f^*(y) = \sup_{x \in \mathrm{dom} f} (y^Tx - f(x))$$
å…±è½­å‡½æ•°ä¸€å®šæ˜¯å‡¸çš„ï¼Œsupremum ä¸ºä¸Šç•Œï¼Œ infimum ä¸ºä¸‹ç•Œã€‚

**æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•**ï¼ˆ[Lagrange multiplier](https://en.jinzhao.wiki/wiki/Lagrange_multiplier)ï¼‰ï¼š
æ ¹æ®ä¸Šé¢æ ‡å‡†å½¢å¼çš„ä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æ¥æ„é€ ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š
$$L(x,\lambda,\nu) = f(x) +\sum _{i=1}^{m}\lambda _{i}g_{i}(x)+\sum _{ i=1}^{p}\nu _{i}h_{i}(x)$$
å…¶ä¸­$\lambda _{i},\nu _{i}$åˆ†åˆ«æ˜¯ä¸ç­‰å¼å’Œç­‰å¼å¯¹åº”çš„ Lagrange ä¹˜å­ï¼Œå½“ç„¶å¦‚æœç”¨å‘é‡$\lambda,\nu$è¡¨ç¤ºï¼Œç§°ä¸ºåŸé—®é¢˜çš„ **Lagrange ä¹˜å­å‘é‡**æˆ–**å¯¹å¶å˜é‡**ã€‚

**å¯¹å¶å‡½æ•°**ï¼ˆ[Dual function](<https://en.jinzhao.wiki/wiki/Duality_(optimization)>)ï¼‰ï¼š
$$g(\lambda,\nu) = \inf_{x} L(x,\lambda,\nu)$$
å¯¹å¶å‡½æ•°ä¸€å®šæ˜¯å‡¹çš„ï¼Œåˆç§°**Lagrange å¯¹å¶å‡½æ•°**ã€‚å¯¹å¶å‡½æ•°$g(\lambda,\nu) \leq p^{\star}$, $p^{\star}$æ˜¯åŸé—®é¢˜çš„æœ€ä¼˜å€¼ã€‚Lagrange å¯¹å¶é—®é¢˜çš„æœ€ä¼˜å€¼ç”¨$d^{\star}$è¡¨ç¤ºï¼Œåˆ™$d^{\star}\leq p^{\star}$ï¼Œè¿™ä¸ªæ€§è´¨ç§°ä¸º**å¼±å¯¹å¶æ€§**ï¼ˆ[Weak Duality](https://en.jinzhao.wiki/wiki/Weak_duality)ï¼‰ï¼Œ$p^{\star}- d^{\star}$ç§°ä¸ºåŸé—®é¢˜çš„**æœ€ä¼˜å¯¹å¶é—´éš™**ï¼ˆ[Duality gap](https://en.jinzhao.wiki/wiki/Duality_gap)ï¼‰ï¼Œå½“$d^{\star}= p^{\star}$æ—¶ç§°ä¸º**å¼ºå¯¹å¶æ€§**ï¼ˆ[Strong Duality](https://en.jinzhao.wiki/wiki/Strong_duality)ï¼‰ã€‚

æ‰€ä»¥å½“å¼ºå¯¹å¶æ€§æˆç«‹æ—¶ï¼Œæ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„æœ€å°ç­‰ä»·äºå¯¹å¶å‡½æ•°çš„æœ€å¤§å€¼$\min_{x} \max_{\lambda,\nu}L(x,\lambda,\nu) \iff \max_{\lambda,\nu} g(\lambda,\nu)$ï¼Œ**æˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥å‡½æ•°æ±‚æœ€å°åŒ–æ—¶çš„å‚æ•°$x$ï¼Œç„¶åå¸¦å…¥å…¶ä¸­ï¼Œåˆ©ç”¨å¯¹å¶å‡½æ•°æ±‚å…¶æœ€å¤§æ—¶çš„ Lagrange ä¹˜å­ï¼Œå³$\lambda,\nu$ã€‚**
è¿™é‡Œçš„$\max_{\lambda,\nu}L(x,\lambda,\nu)$æ˜¯ä¸ºäº†è®©çº¦æŸèµ·ä½œç”¨(æè¿°ä¸æ˜¯å¾ˆå‡†ç¡®ï¼Œå…¶å®å°±æ˜¯ç›®æ ‡å‡½æ•°å’Œçº¦æŸçš„äº¤ç‚¹ï¼Œå‚è§[é™„å½• C.3.2](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf))ã€‚

å½“å¼ºå¯¹å¶æ€§æˆç«‹æ—¶ï¼Œé‚£ä¹ˆ$x^{\star},\lambda^{\star},\nu^{\star}$åˆ†åˆ«æ˜¯åŸé—®é¢˜å’Œå¯¹å¶é—®é¢˜çš„æœ€ä¼˜è§£çš„å……åˆ†å¿…è¦æ¡ä»¶æ˜¯æ»¡è¶³ä¸‹é¢çš„**KKT æ¡ä»¶**ï¼ˆ[Karushâ€“Kuhnâ€“Tucker conditions](https://en.jinzhao.wiki/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)ï¼‰ï¼š

$$\nabla_x L(x^{\star},\lambda^{\star},\nu^{\star}) = 0 \\ \lambda_i^{\star}g_i(x^{\star}) = 0,i=1,2,...,m \quad (\text{Complementary slackness})\\ \lambda_i^{\star} \geq 0,i=1,2,...,m \quad (\text{Dual feasibility})\\ g_i(x^{\star}) \leq 0,i=1,2,...,m \quad (\text{Primal feasibility})\\ h_i(x^{\star}) = 0,i=1,2,...,p \quad (\text{Primal feasibility})$$

> å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åªå¯¹ x å‚æ•°æ±‚å¯¼ï¼Œæ²¡æœ‰å¯¹ Lagrange ä¹˜å­æ±‚å¯¼ï¼›

å…¶ä¸­**äº’è¡¥æ¾å¼›**ï¼ˆComplementary slacknessï¼‰æ¡ä»¶ç”¨$\sum_{i=0}^m\lambda_i^{\star}g_i(x^{\star})=0$å¯èƒ½å¾ˆåˆç†ï¼Œå¦‚æœæœ€ä¼˜è§£$x^{\star}$å‡ºç°åœ¨ä¸ç­‰å¼çº¦æŸçš„è¾¹ç•Œä¸Š$g_i(x) = 0$ï¼Œåˆ™$\lambda_i^{\star} > 0$ï¼›å¦‚æœæœ€ä¼˜è§£$x^{\star}$å‡ºç°åœ¨ä¸ç­‰å¼çº¦æŸçš„å†…éƒ¨$g_i(x) < 0$ï¼Œåˆ™$\lambda_i^{\star} = 0$ï¼›**äº’è¡¥æ¾å¼›æ¡ä»¶è¯´æ˜å½“æœ€ä¼˜è§£å‡ºç°åœ¨ä¸ç­‰å¼çº¦æŸçš„å†…éƒ¨ï¼Œåˆ™çº¦æŸå¤±æ•ˆ**ï¼Œæ‰€ä»¥$\lambda_i^{\star} \geq 0,i=1,2,...,m$è¡¨ç¤ºå¯¹å¶å¯è¡Œæ€§ï¼ˆDual feasibilityï¼‰ã€‚

å¦‚ä½•å°†ä¸æ ‡å‡†çš„ä¼˜åŒ–é—®é¢˜è½¬æ¢ç§°æ ‡å‡†çš„ä¼˜åŒ–é—®é¢˜ï¼ˆçº¿æ€§è§„åˆ’ï¼‰ï¼šå‚è€ƒ[çº¿æ€§è§„åˆ’é—®é¢˜](https://www.bilibili.com/video/BV1TK4y1t74p)
A. å¦‚ä½•å°†ä¸ç­‰å¼çº¦æŸå˜æˆç­‰å¼çº¦æŸï¼š

1. $a^Tx \leq b$
   åªéœ€è¦åŠ ä¸Šæ¾å¼›å˜é‡ï¼ˆ[Slack variable](https://en.jinzhao.wiki/wiki/Slack_variable)ï¼‰ï¼Œæ¾å¼›å˜é‡æ˜¯æ·»åŠ åˆ°ä¸ç­‰å¼çº¦æŸä»¥å°†å…¶è½¬æ¢ä¸ºç­‰å¼çš„å˜é‡ï¼Œæ¾å¼›å˜é‡ç‰¹åˆ«ç”¨äºçº¿æ€§è§„åˆ’ã€‚æ¾å¼›å˜é‡ä¸èƒ½å–è´Ÿå€¼ï¼Œå› ä¸ºå•çº¯å½¢ç®—æ³•è¦æ±‚å®ƒä»¬ä¸ºæ­£å€¼æˆ–é›¶ã€‚
   $$a^Tx +s = b \\  s \geq 0$$

2. $a^Tx \geq b$
   åªéœ€è¦å‡å»å‰©ä½™å˜é‡ï¼ˆsurplus variableï¼‰ï¼Œå‰©ä½™å˜é‡ä¸èƒ½å–è´Ÿå€¼ã€‚
   $$a^Tx - e = b \\  e \geq 0$$

B. æ— çº¦æŸå˜é‡å˜æˆæœ‰éè´Ÿçº¦æŸå˜é‡ï¼š
$${\begin{aligned}&z_{1}=z_{1}^{+}-z_{1}^{-} \\& |z_{1}| = z_{1}^{+}+\,z_{1}^{-} \\&\braket{z_{1}^{+},z_{1}^{-}} = 0 \\&z_{1}^{+},\,z_{1}^{-} \geq 0\end{aligned}}$$

å¦‚ï¼š
a. åˆ©ç”¨ä¸Šè¿°ç¬¬ä¸€æ¡æ€§è´¨
$$5 = 5-0 \\ -5 = 0-5$$
or

$$
\begin{pmatrix}
   1 \\
   2 \\
   -3 \\
   -4
\end{pmatrix} =
\begin{pmatrix}
   1 \\
   2 \\
   0 \\
   0
\end{pmatrix} -
\begin{pmatrix}
   0 \\
   0 \\
   3 \\
   4
\end{pmatrix}
$$

b. ç›®æ ‡å‡½æ•°æœ‰å¸¦ç»å¯¹å€¼çš„(ç¬¬äºŒæ¡æ€§è´¨)
$$\begin{aligned} \max |x| \\ ç­‰ä»·ï¼š\max x^+ + x^- \\ s.t. \quad x^+ \geq 0 \\ x^- \geq 0\end{aligned}$$

### å‚è€ƒæ–‡çŒ®

[7-1] Cortes C,Vapnik V. Support-vector networks. Machine Learning,1995,20

[7-2] Boser BE,Guyon IM,Vapnik VN. A training algorithm for optimal margin classifiers.In: Haussler D,ed. Proc of the 5th Annual ACM Workshop on COLT. Pittsburgh,PA,1992,144â€“152

[7-3] Drucker H,Burges CJC,Kaufman L,Smola A,Vapnik V. Support vector regressionmachines. In: Advances in Neural Information Processing Systems 9,NIPS 1996. MITPress,155â€“161

[7-4] Vapnik Vladimir N. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag,1995ï¼ˆä¸­è¯‘æœ¬ï¼šå¼ å­¦å·¥ï¼Œè¯‘ã€‚ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„æœ¬è´¨ã€‚åŒ—äº¬ï¼šæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ2000ï¼‰

[7-5] Platt JC. Fast training of support vector machines using sequential minimaloptimization. [Microsoft Research](https://www.microsoft.com/en-us/research/project/support-vector-machines/)

[7-6] Weston JAE,Watkins C. Support vector machines for multi-class pattern recognition. In: Proceedings of the 7th European Symposium on Articial Neural Networks. 1999

[7-7] Crammer K,Singer Y. On the algorithmic implementation of multiclass kernel-basedmachines. Journal of Machine Learning Research,2001,2(Dec): 265â€“292

[7-8] Tsochantaridis I,Joachims T,Hofmann T,Altun Y. Large margin methods forstructured and interdependent output variables. JMLR,2005,6: 1453â€“1484

[7-9] Burges JC. A tutorial on support vector machines for pattern recognition. BellLaboratories,Lucent Technologies. 1997

[7-10] Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines andOthre KernerBased Learning Methods. Cambridge University Pressï¼Œ2000ï¼ˆä¸­è¯‘æœ¬ï¼šæå›½æ­£ï¼Œç­‰è¯‘ã€‚æ”¯æŒå‘é‡æœºå¯¼è®ºã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[7-11] é‚“ä¹ƒæ‰¬ï¼Œç”°è‹±æ°ã€‚æ•°æ®æŒ–æ˜ä¸­çš„æ–°æ–¹æ³•â€”â€”æ”¯æŒå‘é‡æœºã€‚åŒ—äº¬ï¼šç§‘å­¦å‡ºç‰ˆç¤¾ï¼Œ2004

[7-12] é‚“ä¹ƒæ‰¬ï¼Œç”°è‹±æ°ã€‚æ”¯æŒå‘é‡æœºâ€”â€”ç†è®ºï¼Œç®—æ³•ä¸æ‹“å±•ã€‚åŒ—äº¬ï¼šç§‘å­¦å‡ºç‰ˆç¤¾ï¼Œ2009

[7-13] Scholkpf B,Smola AJ. Learning with Kernels: Support VectorMachines,Regularization,Optimization,and Beyond. MIT Press,2002

[7-14] Herbrich R. Learning Kernel Classifiers,Theory and Algorithms. The MITPress,2002

[7-15] Hofmann T,Scholkopf B,Smola AJ. Kernel methods in machine learning. The Annalsof Statistics,2008,36(3): 1171â€“1220

## ç¬¬ 8 ç«  æå‡æ–¹æ³•

**é›†æˆå­¦ä¹ **ï¼ˆ[Ensemble Learning](https://en.jinzhao.wiki/wiki/Ensemble_learning)ï¼‰ä¹Ÿå«é›†æˆæ–¹æ³•ï¼ˆEnsemble methodsï¼‰æ˜¯ä¸€ç§å°†å¤šç§å­¦ä¹ ç®—æ³•ç»„åˆåœ¨ä¸€èµ·ä»¥å–å¾—æ›´å¥½è¡¨ç°çš„ä¸€ç§æ–¹æ³•ã€‚

åˆ©ç”¨æˆå‘˜æ¨¡å‹çš„å¤šæ ·æ€§æ¥çº æ­£æŸäº›æˆå‘˜æ¨¡å‹é”™è¯¯çš„èƒ½åŠ›ï¼Œå¸¸ç”¨çš„**å¤šæ ·æ€§æŠ€æœ¯**æœ‰ï¼š

- æœ€æµè¡Œçš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†æ¥è®­ç»ƒæ¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¿™äº›æ•°æ®é›†é€šè¿‡ä»æ€»ä½“æ•°æ®é›†ä¸­æœ‰æ”¾å›çš„éšæœºé‡‡æ ·è·å¾—ï¼Œä¾‹å¦‚ bootstrapping æˆ– bagging æŠ€æœ¯
- åœ¨åˆ†ç±»çš„åœºæ™¯ä¸­ï¼Œå¯ä½¿ç”¨**å¼±åˆ†ç±»å™¨**æˆ–è€…ä¸ç¨³å®šæ¨¡å‹ï¼ˆunstable modelï¼‰ä½œä¸ºæˆå‘˜æ¨¡å‹æ¥æé«˜å¤šæ ·æ€§ï¼Œå› ä¸ºå³ä½¿å¯¹è®­ç»ƒå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿä¼šå¾—åˆ°å®Œå…¨ä¸åŒçš„å†³ç­–è¾¹ç•Œï¼›
- ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒç±»å‹çš„åˆ†ç±»å™¨ï¼Œå¦‚å†³ç­–æ ‘ï¼Œæœ€è¿‘é‚»ï¼Œæ”¯æŒå‘é‡æœºç­‰æ··åˆåˆ°ä¸€èµ·æ¥å¢åŠ å¤šæ ·æ€§ï¼›

> å¼±åˆ†ç±»å™¨:æ¯”éšæœºçŒœæµ‹ç•¥å¥½ï¼Œå¦‚äºŒåˆ†ç±»ä¸­ï¼Œå‡†ç¡®ç‡å¤§äº 0.5 å°±å¯ä»¥äº†ï¼ˆå¦‚ 0.51ï¼‰ã€‚

å‚è§çš„é›†æˆå­¦ä¹ ç±»å‹ï¼š

- Bayes optimal classifier
- [Bootstrap aggregating](https://en.jinzhao.wiki/wiki/Bootstrap_aggregating) (ä¹Ÿç§°ä¸º bagging æ¥è‡ª **b**ootstrap **agg**regat**ing**)
  Bagging å°±æ˜¯é‡‡ç”¨æœ‰æ”¾å›çš„æ–¹å¼è¿›è¡ŒæŠ½æ ·ï¼Œç”¨æŠ½æ ·çš„æ ·æœ¬å»ºç«‹å­æ¨¡å‹,å¯¹å­æ¨¡å‹ï¼ˆå¹¶è¡Œè®­ç»ƒï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸ªè¿‡ç¨‹é‡å¤å¤šæ¬¡ï¼Œæœ€åè¿›è¡Œèåˆã€‚
- [Boosting](<https://en.jinzhao.wiki/wiki/Boosting_(meta-algorithm)>)
  Boosting çš„æ€æƒ³æ˜¯ä¸€ç§è¿­ä»£çš„æ–¹æ³•(ä¸²è¡Œ)ï¼Œæ¯ä¸€æ¬¡è®­ç»ƒçš„æ—¶å€™éƒ½æ›´åŠ å…³å¿ƒåˆ†ç±»é”™è¯¯çš„æ ·ä¾‹ï¼Œç»™è¿™äº›åˆ†ç±»é”™è¯¯çš„æ ·ä¾‹å¢åŠ æ›´å¤§çš„æƒé‡ï¼Œä¸‹ä¸€æ¬¡è¿­ä»£çš„ç›®æ ‡å°±æ˜¯èƒ½å¤Ÿæ›´å®¹æ˜“è¾¨åˆ«å‡ºä¸Šä¸€è½®åˆ†ç±»é”™è¯¯çš„æ ·ä¾‹ã€‚æœ€ç»ˆå°†è¿™äº›å¼±åˆ†ç±»å™¨è¿›è¡ŒåŠ æƒç›¸åŠ ã€‚
- Bayesian model averaging
- Bayesian model combination
- Bucket of models
- Stacking

æ¨¡å‹çš„ç»„åˆ(blending)æ–¹æ³•ï¼š

- çº¿æ€§ç»„åˆ(å¹³å‡æ³•-åŠ æƒå¹³å‡æ³•) - ç”¨äºå›å½’å’Œåˆ†ç±»é—®é¢˜
  å…¶ä¸­ x è¾“å…¥å‘é‡ï¼Œä¼°è®¡ç±»åˆ« y çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆæ¨¡å‹é›†åˆæ•´ä½“å¯¹ç±»åˆ« y çš„æ¦‚ç‡ä¼°è®¡ä¸ºï¼š
  $$\overline{f}(y|x) = \sum_{t=1}^T w_t f_t(y|x)$$
  æƒé‡ç¡®å®šæ¯”è¾ƒå›°éš¾ï¼Œè¿˜å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå› æ­¤å¹³å‡æƒé‡æ˜¯æœ€å¸¸ç”¨çš„ï¼ˆ$w_t = \frac{1}{T}$å³ç®€å•å¹³å‡æ³•ï¼‰ã€‚

- æŠ•ç¥¨ç»„åˆ - ç”¨äºåˆ†ç±»é—®é¢˜
  $$H(\textbf{x}) = sign( \sum_{t=1}^{T}w_t h_t(y|\textbf{x} ) )$$
  å¦‚äºŒåˆ†ç±»é—®é¢˜ï¼š$h_t(y|\textbf{x} ) \in \{-1,+1\}$,åŒæ ·ï¼Œæƒé‡å¯ä»¥æ˜¯å‡åŒ€çš„(ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•)ï¼Œä¹Ÿå¯ä»¥ä¸å‡åŒ€(åŠ æƒæŠ•ç¥¨æ³•)ã€‚

- ä¹˜ç§¯ç»„åˆ
  $$\overline f(y|\textbf{x}) = \frac{1}{Z}\prod_{t=1}^{T} f_t(y | \textbf{x})^{w_t}$$
  åœ¨å„æ¨¡å‹çš„ç±»åˆ«æ¡ä»¶æ¦‚ç‡ä¼°è®¡ç›¸äº’ç‹¬ç«‹çš„å‡è®¾ä¸‹ï¼Œä¹˜ç§¯ç»„åˆåœ¨ç†è®ºä¸Šæ˜¯æœ€å¥½çš„ç»„åˆç­–ç•¥ï¼Œä½†æ˜¯åœ¨å®é™…ä¸­ï¼Œè¿™ç§å‡è®¾å¾ˆéš¾æˆç«‹ï¼ŒåŒæ—¶æƒé‡ä¸çº¿æ€§ç»„åˆä¸€æ ·ä¸å¥½ç¡®å®šã€‚

- å­¦ä¹ ç»„åˆ
  å½“è®­ç»ƒæ•°æ®å¾ˆå¤šæ—¶ï¼Œä¸€ç§æ›´ä¸ºå¼ºå¤§çš„ç»„åˆç­–ç•¥å«â€œå­¦ä¹ æ³•â€ï¼Œå³é€šè¿‡ä¸€ä¸ªå­¦ä¹ å™¨æ¥è¿›è¡Œç»„åˆï¼Œè¿™ç§æ–¹æ³•å« Stackingã€‚
  è¿™é‡ŒæŠŠåŸºå­¦ä¹ å™¨ç§°ä¸ºåˆçº§å­¦ä¹ å™¨ï¼ŒæŠŠç”¨æ¥ç»„åˆçš„å­¦ä¹ å™¨ç§°ä¸ºæ¬¡çº§å­¦ä¹ å™¨ã€‚
  Stacking å…ˆä»åˆå§‹æ•°æ®é›†è®­ç»ƒå‡º**åˆçº§å­¦ä¹ å™¨**ï¼Œå†æŠŠ**åˆçº§å­¦ä¹ å™¨çš„è¾“å‡º**ç»„åˆæˆæ–°çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒ**æ¬¡çº§å­¦ä¹ å™¨**ã€‚å¯¹äºæµ‹è¯•é›†ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨åˆçº§å­¦ä¹ å™¨é¢„æµ‹ä¸€æ¬¡ï¼Œå¾—åˆ°æ¬¡çº§å­¦ä¹ å™¨çš„è¾“å…¥æ ·æœ¬ï¼Œå†ç”¨æ¬¡çº§å­¦ä¹ å™¨é¢„æµ‹ä¸€æ¬¡ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

| aggregation type | blending         | learning      |
| ---------------- | ---------------- | ------------- |
| uniform          | voting/averaging | Bagging       |
| non-uniform      | linear           | AdaBoost      |
| conditional      | stacking         | Decision Tree |

> [å‚è€ƒ](https://github.com/openjw/penter/blob/master/scikit-learn/api/ensemble.ipynb)

[AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) ï¼š[AdaBoost](https://en.jinzhao.wiki/wiki/AdaBoost)æ˜¯ Adaptive Boosting çš„ç¼©å†™

è®­ç»ƒé›†$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}, x_i \in \mathbb{R}^n,y_i\in \{-1,+1\}$

- **æ¨¡å‹**ï¼š

  1. é¦–å…ˆåˆå§‹åŒ–æ•°æ®é›†çš„æƒå€¼åˆ†å¸ƒï¼š$D_1 = (w_{11},...w_{1i},...,w_{iN}), w_{1i} = \frac{1}{N}$
  2. å¯¹$m=1,2...M$ä½¿ç”¨å…·æœ‰æƒå€¼åˆ†å¸ƒ$D_m$çš„è®­ç»ƒæ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œå¾—åˆ°åŸºæœ¬åˆ†ç±»å™¨$G_m(x):\mathbb{R}^n \to \{-1,+1\}$  
     a. è®¡ç®—$\alpha_m = \frac{1}{2}\log\frac{1-e_m}{e_m}$æ˜¯$G_m(x)$çš„ç³»æ•°ï¼ˆåŸºæœ¬\ä¸ªä½“åˆ†ç±»å™¨çš„æƒå€¼ï¼ŒåŒºåˆ«äº w æ˜¯è®­ç»ƒæ•°æ®çš„æƒé‡ï¼‰,$e$è¶Šå¤§é”™çš„è¶Šå¤š,é‚£ä¹ˆ$\alpha$å°±è¶Šå°ï¼ˆå°äº 0ï¼Œæ¥è¿‘è´Ÿæ— ç©·ï¼‰ï¼›
     b. è®¡ç®—$e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i)= \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$æ˜¯$G_m(x)$åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç‡ï¼ˆè¶Šå¤§é”™çš„è¶Šå¤šï¼‰ï¼›
     c. æ›´æ–°æƒå€¼åˆ†å¸ƒ$D_{m+1} = (w_{m+1,1},...w_{m+1,i},...,w_{m+1,N})$,å…¶ä¸­
     $$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i))$$
     $\alpha$è¶Šå°ï¼ˆå°äº 0ï¼Œæ¥è¿‘è´Ÿæ— ç©·ï¼‰$w$å°±è¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯è¢«åˆ†é”™çš„æ ·æœ¬æƒé‡å¤§ã€‚
     $Z_m = \sum_{i=1}^N w_{mi}\exp(-\alpha_m y_i G_m(x_i))$æ˜¯å½’ä¸€åŒ–å› å­ï¼ˆå› ä¸º D æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼‰
  3. æ„å»ºåŸºæœ¬åˆ†ç±»å™¨çš„çº¿æ€§ç»„åˆï¼Œå¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨
     $$G(x) =\mathrm{sign} ( \sum_{m=1}^M \alpha_m G_m(x))$$

- **ç­–ç•¥**ï¼š
  ä½¿æ¯æ¬¡è¿­ä»£çš„$e$ï¼ˆåˆ†ç±»è¯¯å·®ç‡ï¼‰è¶Šå°è¶Šå¥½
  $$e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i)= \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i) \\ æ¨å¯¼å‡º e_m = \sum_{G_m(x_i) \neq y_i} w_{mi}$$
- **ç®—æ³•**ï¼š

[Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)ï¼šæ¢¯åº¦æå‡ï¼ˆ[Gradient boosting](https://en.jinzhao.wiki/wiki/Gradient_boosting)ï¼‰æ˜¯ä¸€ç§ç”¨äºå›å½’ã€åˆ†ç±»å’Œå…¶ä»–ä»»åŠ¡çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒä»¥å¼±é¢„æµ‹æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰çš„é›†åˆå½¢å¼ç”Ÿæˆé¢„æµ‹æ¨¡å‹ã€‚å½“å†³ç­–æ ‘æ˜¯å¼±å­¦ä¹ å™¨æ—¶ï¼Œäº§ç”Ÿçš„ç®—æ³•ç§°ä¸º**æ¢¯åº¦æå‡æ ‘**(Gradient Tree Boosting or Gradient boosted trees or Gradient Boosted Decision Trees(GBDT))ï¼Œé€šå¸¸ä¼˜äºéšæœºæ£®æ—ï¼ˆ[Random forest](https://en.jinzhao.wiki/wiki/Random_forest)å®ƒæ˜¯ Bagging ç®—æ³•çš„è¿›åŒ–ç‰ˆï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒçš„æ€æƒ³ä»ç„¶æ˜¯ bagging,ä½†æ˜¯è¿›è¡Œäº†ç‹¬æœ‰çš„æ”¹è¿›ã€‚ï¼‰

- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š

éšæœºæ£®æ—ä¸ä¸€èˆ¬çš„ bagging ç›¸æ¯”ï¼šï¼ˆå‚è€ƒï¼š[Bagging ä¸éšæœºæ£®æ—ç®—æ³•åŸç†å°ç»“](https://www.cnblogs.com/pinard/p/6156009.html)å’Œ[æœºå™¨å­¦ä¹ ç®—æ³•ç³»åˆ—ï¼ˆäº”ï¼‰ï¼šbagging ä¸éšæœºæ£®æ—å¯¹æ¯”åŠéšæœºæ£®æ—æ¨¡å‹å‚æ•°ä»‹ç»](https://blog.csdn.net/qq_20106375/article/details/94383076)ï¼‰

- bagging æ–¹æ³•çš„çš„éšæœºæ€§ä»…ä»…æ¥è‡ªæ ·æœ¬æ‰°åŠ¨ï¼Œéšæœºæ—æ¨¡å‹ä¸­å¼•å…¥äº†å±æ€§æ‰°åŠ¨ï¼Œè¿™æ ·ä½¿å¾—æœ€ç»ˆæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å¯ä»¥é€šè¿‡ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´çš„å·®å¼‚åº¦çš„å¢åŠ è€Œè¿›ä¸€æ­¥æå‡ã€‚

- å’Œ bagging ç›¸æ¯”ï¼Œéšæœºæ£®æ—çš„èµ·å§‹æ€§èƒ½å¾€å¾€æ¯”è¾ƒå·®ï¼Œç„¶è€Œéšç€ä¸ªä½“å­¦ä¹ å™¨æ•°ç›®çš„å¢åŠ ï¼Œéšæœºæ£®æ—ä¼šæ”¶æ•›åˆ°æ›´å°çš„è¯¯å·®ã€‚

- éšæœºæ£®æ—çš„è®­ç»ƒæ•ˆç‡ä¼˜äº baggingï¼Œå› ä¸º bagging ä¸­çš„æ¯æ£µæ ‘æ˜¯å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œè€ƒå¯Ÿï¼Œè€Œéšæœºæ£®æ—ä»…ä»…è€ƒè™‘ä¸€ä¸ªç‰¹å¾å­é›†ï¼ˆmax_featuresï¼šéšæœºæ£®æ—å…è®¸å•ä¸ªå†³ç­–æ ‘ä½¿ç”¨ç‰¹å¾çš„æœ€å¤§æ•°é‡ã€‚ï¼‰ã€‚

å› ä¸º Bagging ä½¿ç”¨çš„æœ‰æ”¾å›é‡‡æ ·ï¼Œæ‰€ä»¥ BaggingClassifier or RandomForestClassifier éƒ½å…·æœ‰ oob*score*å±æ€§ï¼šçº¦æœ‰ 37%ï¼ˆ$\lim_{n \to -\infty}(1-1/n)^n=1/e$ï¼‰çš„æ ·æœ¬æ²¡æœ‰ç”¨æ¥è®­ç»ƒ,è¿™ä¸€éƒ¨åˆ†ç§°ä¸º out-of-bag(oob),å› ä¸ºæ¨¡å‹æ²¡æœ‰è§è¿‡è¿™éƒ¨åˆ†æ ·æœ¬ï¼Œæ‰€ä»¥å¯ä»¥æ‹¿æ¥å½“éªŒè¯é›†åˆï¼Œè€Œä¸éœ€è¦å†åˆ’åˆ†éªŒè¯é›†æˆ–è€…äº¤å‰éªŒè¯äº†ã€‚ æ¯”å¦‚æˆ‘ä»¬è®¡ç®— accuracy*score æ—¶ï¼Œä¹Ÿå¯ä»¥çœ‹ä¸‹ oob_score*çš„æƒ…å†µ

### å‚è€ƒæ–‡çŒ®

[8-1] Freund Yï¼ŒSchapire RE. [A short introduction to boosting](http://www.cs.columbia.edu/~jebara/6772/papers/IntroToBoosting.pdf). Journal of Japanese Societyfor Artificial Intelligence,1999,14(5): 771â€“780

[8-2] Hastie T,Tibshirani R,Friedman J. [The Elements of Statistical Learning: DataMining,Inference,and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf). Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ï¼Œç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[8-3] Valiant LG. [A theory of the learnable](http://web.mit.edu/6.435/www/Valiant84.pdf). Communications of the ACM,1984,27(11):1134â€“1142

[8-4] Schapire R. [The strength of weak learnability](https://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf). Machine Learning,1990,5(2): 197â€“227

[8-5] Freund Y,Schapire RE. [A decision-theoretic generalization of on-line learning and anapplication to boosting](https://www.ee.columbia.edu/~sfchang/course/spr/papers/freund95decisiontheoretic-adaboost.pdf). Computational Learning Theory. Lecture Notes in ComputerScience,Vol. 904,1995,23â€“37 ï¼ˆ[55, 119-139 (1997)](http://www.cim.mcgill.ca/~dmeger/mrlRead/papers/Boosting/AdaOrig.pdf)ï¼‰

[8-6] Friedman J,Hastie T,Tibshirani R. [Additive logistic regression: a statistical view ofboosting(with discussions)](https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf). Annals of Statistics,2000,28: 337â€“407

[8-7] Friedman J. [Greedy function approximation: a gradient boosting machine](http://biostat.jhsph.edu/~mmccall/articles/friedman_1999.pdf). Annals ofStatistics,2001,29(5)

[8-8] Schapire RE,Singer Y. [Improved boosting algorithms using confidence-ratedpredictions](<https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1999-ML-Improved%20boosting%20algorithms%20using%20confidence-rated%20predictions%20(Schapire%20y%20Singer).pdf>). Machine Learning,1999,37(3): 297â€“336

[8-9] Collins M,Schapire R E,Singer Y. [Logistic regression,AdaBoost and Bregmandistances](https://link.springer.com/content/pdf/10.1023%2FA%3A1013912006537.pdf). Machine Learning Journal,2004

## ç¬¬ 9 ç«  EM ç®—æ³•åŠå…¶æ¨å¹¿

EM ç®—æ³•ï¼ˆ[Expectationâ€“maximization algorithm](https://en.jinzhao.wiki/wiki/Expectation%E2%80%93maximization_algorithm)ï¼‰æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œç”¨äºå«æœ‰**éšå˜é‡**ï¼ˆhiddenï¼ˆunseen or unmeasurableï¼‰ variable or Latent variableï¼‰çš„æ¦‚ç‡æ¨¡å‹å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæˆ–æå¤§åéªŒæ¦‚ç‡ä¼°è®¡ã€‚EM ç®—æ³•çš„æ¯æ¬¡è¿­ä»£ç”±ä¸¤æ­¥ç»„æˆï¼šE æ­¥ï¼Œæ±‚æœŸæœ›ï¼ˆexpectationï¼‰ï¼›M æ­¥ï¼Œæ±‚æå¤§ï¼ˆmaximizationï¼‰ã€‚æ‰€ä»¥è¿™ä¸€ç®—æ³•ç§°ä¸ºæœŸæœ›æå¤§ç®—æ³•ï¼ˆexpectation maximization algorithmï¼‰ï¼Œç®€ç§° EM ç®—æ³•ã€‚

æ¦‚ç‡æ¨¡å‹æœ‰æ—¶æ—¢å«æœ‰è§‚æµ‹å˜é‡ï¼ˆobservable variableï¼‰ï¼Œåˆå«æœ‰éšå˜é‡æˆ–æ½œåœ¨å˜é‡ï¼ˆlatent variableï¼‰ã€‚å¦‚æœæ¦‚ç‡æ¨¡å‹çš„å˜é‡éƒ½æ˜¯è§‚æµ‹å˜é‡ï¼Œé‚£ä¹ˆç»™å®šæ•°æ®ï¼Œå¯ä»¥ç›´æ¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ³•ï¼Œæˆ–è´å¶æ–¯ä¼°è®¡æ³•ä¼°è®¡æ¨¡å‹å‚æ•°ã€‚ä½†æ˜¯ï¼Œå½“æ¨¡å‹å«æœ‰éšå˜é‡æ—¶ï¼Œå°±ä¸èƒ½ç®€å•åœ°ä½¿ç”¨è¿™äº›ä¼°è®¡æ–¹æ³•ã€‚EM ç®—æ³•å°±æ˜¯å«æœ‰éšå˜é‡çš„æ¦‚ç‡æ¨¡å‹å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡æ³•ï¼Œæˆ–æå¤§åéªŒæ¦‚ç‡ä¼°è®¡æ³•ã€‚

æœ¬ç« é¦–å…ˆå™è¿° EM ç®—æ³•ï¼Œç„¶åè®¨è®º EM ç®—æ³•çš„æ”¶æ•›æ€§ï¼›ä½œä¸º EM ç®—æ³•çš„åº”ç”¨ï¼Œä»‹ç»é«˜æ–¯æ··åˆæ¨¡å‹çš„å­¦ä¹ ï¼›æœ€åå™è¿° EM ç®—æ³•çš„æ¨å¹¿â€”â€”GEM ç®—æ³•ï¼ˆgeneralized expectation maximization (GEM) algorithmï¼Œå¹¿ä¹‰ EM ç®—æ³•ï¼‰ã€‚

> EM ç®—æ³•æ˜¯ä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œä¸æ˜¯ä¸€ä¸ªç»Ÿè®¡å­¦ä¹ æ¨¡å‹ã€‚
> EM ç®—æ³•ä¼˜ç‚¹:ä¸éœ€è¦è°ƒå‚æ•°ï¼Œæ²¡æœ‰è¶…å‚æ•°ï¼›ç¼–ç¨‹ç®€å•ï¼Œåªéœ€è¦è¿­ä»£ï¼›ç†è®ºä¼˜ç¾ï¼Œæ”¶æ•›æ€§ã€‚
> EM ç®—æ³•çš„æ¨å¹¿:F å‡½æ•°ï¼ˆF functionï¼‰ çš„æå¤§-æå¤§ç®—æ³• F-MM or MMï¼ˆmaximization maximization algorithmï¼‰ï¼ŒMCEMï¼ŒVBEM or VEMï¼ŒGEM

è§‚æµ‹æ•°æ®$X=\{x_i\}_{i=1}^N$ ï¼Œå¯¹åº”çš„éšå«(éšè—)æ•°æ®$Z=\{z_i\}_{i=1}^N$ï¼Œæ¨¡å‹å‚æ•°$\theta$ï¼Œå®Œå…¨æ•°æ®$T=\{(x_1,z_1),...,(x_N,z_N)\}$

> æ³¨æ„ä¸‹åˆ—å…¬å¼ä¸­$|$ä¹Ÿæœ‰ç”¨$;$è¡¨ç¤ºçš„ï¼Œæ˜¯ä¸€ä¸ªæ„æ€ï¼Œå› ä¸º$\theta$æ˜¯ä¸€ä¸ªæœªçŸ¥çš„å‚æ•°ï¼Œæœ€å¼€å§‹æ—¶ä¼šåˆå§‹åŒ–$\theta^{(0)}$
> å…¬å¼ä¸­çš„å¤§ P å’Œå° p ä»¥åŠå¤§ X å’Œå° x æ²¡æœ‰ç»Ÿä¸€ï¼Œä¸æ˜¯å¾ˆä¸¥è°¨

å¦‚æœä¸è€ƒè™‘éšè—æ•°æ®ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ä¼°è®¡å‡ºå‚æ•° $\theta$ :
$$\theta_{MLE} = \arg\max\limits_\theta\log p(x|\theta) = \arg\max\limits_\theta\sum_i^N\log p(x_i|\theta) $$

ä½†ç”±äºéšè—æ•°æ®çš„å­˜åœ¨ï¼Œ æˆ‘ä»¬æœ‰ x çš„è¾¹é™…ä¼¼ç„¶å‡½æ•°ï¼ˆ[Marginal Likelihood](https://en.jinzhao.wiki/wiki/Marginal_likelihood)ï¼‰,åœ¨è´å¶æ–¯ç»Ÿè®¡çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¾¹é™…ä¼¼ç„¶ä¹Ÿç§°ä¸ºè¯æ®ï¼ˆEvidenceï¼‰
$$p(x|\theta) = \sum_{z} p(x, z|\theta) = \int_Z p(x, z|\theta)dz$$
å°† x çš„è¾¹é™…ä¼¼ç„¶å‡½æ•°å¸¦å…¥æå¤§ä¼¼ç„¶ä¼°è®¡ä¸­ï¼Œåœ¨ log é‡Œé¢ä¼šå‡ºç°ç§¯åˆ†(æ±‚å’Œ)ç¬¦å·ï¼Œå¯¼è‡´å¯¹ä¼¼ç„¶å‡½æ•°çš„æ±‚å¯¼å˜å¾—å›°éš¾ï¼Œæ— æ³•æ±‚è§£ã€‚å¯¹äºè¿™ç§æ— æ³•ç›´æ¥æ±‚è§£çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé‡‡ç”¨è¿­ä»£æ±‚è§£çš„ç­–ç•¥ï¼Œä¸€æ­¥ä¸€æ­¥é€¼è¿‘æœ€ç»ˆçš„ç»“æœï¼Œåœ¨ EM ç®—æ³•ä¸­å°±æ˜¯ E æ­¥å’Œ M æ­¥çš„äº¤æ›¿è¿›è¡Œï¼Œç›´è‡³æ”¶æ•›ã€‚

ä¸‹é¢æ¥ä»‹ç»**EM ç®—æ³•**ï¼š

1. éšæœºåŒ–å‚æ•°$\theta^{(0)}$çš„åˆå§‹å€¼ï¼›
2. å‡è®¾åœ¨ç¬¬ $t$ æ¬¡è¿­ä»£åï¼Œå‚æ•°çš„ä¼°è®¡å€¼ä¸º $\theta^{(t)}$ ï¼Œå¯¹äºç¬¬ $t+1$ æ¬¡è¿­ä»£ï¼Œå…·ä½“åˆ†ä¸ºä¸¤æ­¥ï¼š
   a. E-stepï¼šæ±‚æœŸæœ›
   Q å‡½æ•°çš„å®šä¹‰ï¼š
   $$Q(\theta,\theta^{(t)}) = \int_Z P(Z|X,\theta^{(t)}) \log P(X,Z|\theta) dZ \\ =\mathbb{E}_{Z|X,\theta^{(t)}}[\log P(X,Z|\theta)]$$
   **å…¨æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°$\log p(x, z|\theta)$å…³äºåœ¨ç»™å®šè§‚æµ‹æ•°æ®$X$å’Œå½“å‰å‚æ•°$\theta^{(t)}$ä¸‹å¯¹æœªè§‚æµ‹æ•°æ®$Z$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$p(z|x,\theta^{(t)})$çš„æœŸæœ›ç§°ä¸º Q å‡½æ•°**

   > æ³¨æ„ Q å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è‡ªå˜é‡ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯å·²çŸ¥çš„ï¼ˆä¸Šä¸€æ­¥æ±‚å¾—çš„ï¼‰

   b. M-step: æœ€å¤§åŒ–$Q(\theta,\theta^{(t)})$ å¹¶æ±‚è§£$\theta^{(t+1)}$
   $$\theta^{(t+1)} = \arg\max\limits_\theta Q(\theta, \theta^{(t)}) $$

3. é‡å¤ 2ï¼Œç›´åˆ°æ”¶æ•›ã€‚
   ä¸€èˆ¬åˆ¤æ–­æ”¶æ•›æœ‰ä¸¤ç§æ–¹æ³•ï¼š1. åˆ¤æ–­å‚æ•°æ˜¯å¦æ”¶æ•›$\theta^{(t+1)}-\theta^{(t)} \leq \varepsilon$ï¼›2. åˆ¤æ–­å‡½æ•°å€¼æ˜¯å¦æ”¶æ•›$Q(\theta^{(t+1)},\theta^{(t)})-Q(\theta^{(t)},\theta^{(t)}) \leq \varepsilon$ã€‚

**EM ç®—æ³•çš„å¯¼å‡º**ï¼š
ä¸ºä»€ä¹ˆ EM ç®—æ³•èƒ½è¿‘ä¼¼å®ç°å¯¹è§‚æµ‹æ•°æ®çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Ÿ
ä¹¦ä¸­çš„æ¨å¯¼æˆ‘è¿™é‡Œå°±ä¸é‡å¤äº†(ä¹¦ä¸­ç”¨çš„ Jensen ä¸ç­‰å¼æ¨å¯¼)ï¼Œè¿™é‡Œä»‹ç»å˜åˆ†æ³•/ELBO+KL
$$P(X|\theta) = \frac{P(X,Z|\theta)}{P(Z|X, \theta)} \implies \log P(X|\theta) = \log P(X,Z|\theta) - \log P(Z|X,\theta) $$
æ ¹æ®å˜åˆ†æ¨æ–­çš„æ€æƒ³ï¼šå¯»æ‰¾ä¸€ä¸ªç®€å•åˆ†å¸ƒ$q(z)$æ¥è¿‘ä¼¼æ¡ä»¶æ¦‚ç‡å¯†åº¦$p(z|x)$
$$\log P(X|\theta) = \log \frac{P(X,Z|\theta)}{q(Z)} - \log \frac{P(Z|X,\theta)}{q(Z)} $$
ç„¶åä¸¤è¾¹åŒæ—¶æ±‚å…³äºå˜é‡ $Z$ çš„æœŸæœ›
$$\mathbb{E}_Z[\log P(X|\theta)] = \mathbb{E}\_Z[\log \frac{P(X,Z|\theta)}{q(Z)}] - \mathbb{E}\_Z[\log \frac{P(Z|X,\theta)}{q(Z)}]$$
å°†æœŸæœ›å†™æˆç§¯åˆ†çš„å½¢å¼
$$\int_Z q(Z)\log P(X|\theta)dZ = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ - \int_Zq(Z)\log \frac{P(Z|X,\theta)}{q(Z)}dZ$$
ç­‰å¼å·¦è¾¹å’Œ$Z$æ— å…³ï¼ˆ$\int_Zq(Z)dZ = 1$ï¼‰ï¼Œæ‰€ä»¥
$$\log P(X|\theta) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ - \int_Zq(Z)\log \frac{P(Z|X,\theta)}{q(Z)}dZ \\ = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ + D_{KL}(q(Z)||P(Z|X,\theta)) \\= ELBO + D*{KL}(q(Z)||P(Z|X,\theta))$$
æˆ‘ä»¬ç›´æ¥ä»¤$D*{KL} = 0, å³ q(Z)=P(Z|X,\theta^{(t)})$ï¼Œç„¶åæœ€å¤§åŒ–ELBO
$$\hat{\theta} = \argmax_{\theta}\int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ \\ = \argmax_{\theta}\int_Z P(Z|X,\theta^{(t)})\log \frac{P(X,Z|\theta)}{P(Z|X,\theta^{(t)})}dZ$$
$\theta^{(t)}$æ˜¯ä¸Šä¸€æ­¥æ±‚å‡ºçš„ï¼Œå¯ä»¥çœ‹ä½œå·²çŸ¥çš„å‚æ•°
$$\hat{\theta} = \argmax_{\theta} \int_Z P(Z|X,\theta^{(t)})\log P(X,Z|\theta)dZ - \int_Z P(Z|X,\theta^{(t)})\log {P(Z|X,\theta^{(t)})}dZ \\  = \argmax_{\theta} \int_Z P(Z|X,\theta^{(t)})\log P(X,Z|\theta)dZ - C \\  = \argmax_{\theta} \int_Z P(Z|X,\theta^{(t)})\log P(X,Z|\theta)dZ \\= \argmax_{\theta} Q(\theta,\theta^{(t)})$$

> å‚è€ƒ[EM ç®—æ³• 11.2.2.1 èŠ‚](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)
> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•ï¼ˆELBO+KL å½¢å¼ï¼‰](https://zhuanlan.zhihu.com/p/365641813)
> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•-Jensen ä¸ç­‰å¼](https://zhuanlan.zhihu.com/p/366365408)

**EM ç®—æ³•çš„æ”¶æ•›æ€§**ï¼š
è¯æ˜$p(\mathbf {X} \mid {\boldsymbol {\theta }})$æ˜¯å•è°ƒé€’å¢çš„ï¼ˆæ¦‚ç‡å¤§äºç­‰äº 0ï¼Œå°äºç­‰äº 1ï¼Œå•è°ƒå¢ä¸€å®šèƒ½æ”¶æ•›ï¼‰ï¼Œå°±æ˜¯è¯æ˜$\log p(\mathbf {X} \mid {\boldsymbol {\theta }})$æ˜¯å•è°ƒé€’å¢çš„ï¼Œå³ï¼š
$$\log p(\mathbf {X} \mid {\boldsymbol {\theta^{(t+1)} }}) \geq \log p(\mathbf {X} \mid {\boldsymbol {\theta^{(t)} }})$$

- æ–¹æ³•ä¸€ï¼šæ ¹æ®ç»´åŸºç™¾ç§‘
  $${\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})=\log p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})-\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}).}$$
  $${\displaystyle {\begin{aligned}\log p(\mathbf {X} \mid {\boldsymbol {\theta }})&=\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})-\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})\\&=Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}),\end{aligned}}}$$
  $${\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})-\log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)}) =Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-H({\boldsymbol {\theta } }^{(t)}\mid {\boldsymbol {\theta }}^{(t)}),}$$
  ç”±**å‰å¸ƒæ–¯ä¸ç­‰å¼(Gibbs' inequality)**${\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})\geq H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})}$å¾—ï¼š
  $${\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})-\log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)})\geq Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)}).}$$
  å› ä¸º M-step: æœ€å¤§åŒ–$Q(\theta,\theta^{(t)})$ï¼Œæ‰€ä»¥$Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}) \geq Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})$
  æ‰€ä»¥$\log p(\mathbf {X} \mid {\boldsymbol {\theta }})$æ˜¯å•è°ƒå¢å‡½æ•°

- æ–¹æ³•äºŒï¼šæ ¹æ®ç»Ÿè®¡å­¦ä¹ æ–¹æ³•
  ä¸åŒçš„æ˜¯ç”¨çš„**Jensen ä¸ç­‰å¼**
  $$H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}) -H({\boldsymbol {\theta } }^{(t)}\mid {\boldsymbol {\theta }}^{(t)}) = -\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}) + \sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}}) \\ = -\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log \frac{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}})} \\ \geq -\log \bigg( \sum _{\mathbf {Z} } \frac{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}})} p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)}) \bigg) \\= -\log \bigg( \sum _{\mathbf {Z} } p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}) \bigg) = -\log(1) = 0$$
  å› ä¸º$\sum _{\mathbf {Z} } p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}}) = 1$,log æ˜¯å•è°ƒå¢å‡½æ•°ï¼Œæ‰€ä»¥ Jensen ä¸ç­‰å¼æˆç«‹ã€‚

- æ–¹æ³•ä¸‰ï¼šæ ¹æ® **KL divergence çš„å®šä¹‰**ï¼Œå¹¶ä¸”å¤§äºç­‰äº 0
  $$H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}) -H({\boldsymbol {\theta } }^{(t)}\mid {\boldsymbol {\theta }}^{(t)}) = -\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log \frac{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}{p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }^{(t)}})} \\ = D_{KL}(p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)}) || p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})) \geq 0$$

> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•-æ”¶æ•›æ€§è¯æ˜](https://zhuanlan.zhihu.com/p/367072875)

**å¹¿ä¹‰ EM**ï¼ˆgeneralized expectation maximization (GEM) algorithmï¼‰
æˆ‘ä»¬ä»¥ EM ç®—æ³•çš„ ELBO+KL å½¢å¼ä¸ºä¾‹(ä¹¦ä¸­çš„å½¢å¼è‡ªå·±äº†è§£ï¼Œè¿™é‡Œä¸åšä»‹ç»)
$$\log P(X|\theta) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ + D_{KL}(q(Z)||P(Z|X,\theta))$$
ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä»¤
$$\mathcal{L}(q, \theta) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ$$
å…¶å®å°±æ˜¯å˜åˆ†å‡½æ•°ï¼ˆæ³›å‡½çš„æå€¼é—®é¢˜ï¼Œè¾“å…¥æ˜¯å‡½æ•° qï¼‰
åœ¨å‰é¢ï¼Œæˆ‘ä»¬ä¸€ç›´æ˜¯è®©$D_{KL} = 0$ï¼Œç„¶åæœ€å¤§åŒ– $\mathcal{L}(q,\theta)$ ä»¥å¢å¤§ $\log P(X|\theta)$ ã€‚ä½†æ˜¯æˆ‘ä»¬ç°åœ¨æ— æ³•ç›´æ¥è®©$q(Z) = P(Z|X,\theta)$ ï¼Œå› ä¸ºåéªŒæœ¬èº«ä¹Ÿæ¯”è¾ƒéš¾æ±‚ï¼Œæ‰€ä»¥æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ±‚å‡ºæŸä¸ª $q(Z)$ ï¼Œèƒ½å¤Ÿä½¿å¾—åœ¨å›ºå®š $\theta$ æ—¶ $D_{KL}$ å°½å¯èƒ½å°ï¼Œå°½å¯èƒ½ç­‰äº 0ï¼š
$$q = \arg\min\limits_q D_{KL}(q(Z)||P(Z|X,\theta))= \arg\max\limits_q \mathcal{L}(q, \theta)$$

- E-stepï¼šå›ºå®š$\theta = \theta^{(t)}$
  $$q^{(t)} = \arg\max\limits_q \mathcal{L}(q, \theta^{(t)})$$
- M-stepï¼šå›ºå®š$q = q^{(t)}$
  $$\theta^{(t+1)} = \arg\max\limits_\theta \mathcal{L}(q^{(t)}, \theta)$$

> å‚è€ƒ[æ·±å…¥ç†è§£ EM ç®—æ³•-å¹¿ä¹‰ EM](https://zhuanlan.zhihu.com/p/367076459)

**é«˜æ–¯æ··åˆæ¨¡å‹**ï¼ˆ[Gaussian mixture model](https://en.jinzhao.wiki/wiki/Mixture_model#Gaussian_mixture_model)ï¼‰ï¼š

æœ‰æ ·æœ¬(è§‚æµ‹æ•°æ®)$Data = \{x_1,...,x_N\}$ï¼Œç”Ÿæˆæ¨¡å‹ï¼Œå¯¹å®Œå…¨æ•°æ®$T = \{(x_1,z_1),...,(x_N,z_N)\}$å»ºæ¨¡ï¼Œç»™å®šè¾“å…¥$x_i$é¢„æµ‹ï¼š$\text{arg max}_{k \in \{1, \dots, K \}} P(Z_i = k \mid \boldsymbol{x}_i ; \hat{\Theta})$ï¼Œæƒ³è¦é¢„æµ‹æ¨¡å‹ï¼Œå°±éœ€è¦æ±‚å‡ºæ¨¡å‹çš„å‚æ•°$\hat{\Theta}$ï¼Œå…¶ä¸­
$$\begin{align*} P(Z_i = k \mid \boldsymbol{x}_i ; \hat{\Theta}) &= \frac{p(\boldsymbol{x}_i \mid Z_i = k ; \hat{\Theta})P(Z_i = k; \hat{\Theta})}{\sum_{h=1}^K p(\boldsymbol{x}_i \mid Z_i = h ; \hat{\Theta})P(Z_i = h; \hat{\Theta})} \\ &= \frac{\phi(\boldsymbol{x}_i \mid \hat{\boldsymbol{\mu}}_k, \hat{\boldsymbol{\Sigma}}_k) \hat{\alpha}_k}{\sum_{h=1}^K \phi(\boldsymbol{x}_i \mid \hat{\boldsymbol{\mu}_h}, \hat{\boldsymbol{\Sigma}}_h) \hat{\alpha}_h} \end{align*}$$

- **æ¨¡å‹**ï¼š
  $$P(x;\theta) = \sum_{k=1}^K P(x,z=k;\theta) = \sum_{k=1}^K \underbrace{P(x|z=k;\theta)}_{\text{æœä»é«˜æ–¯åˆ†å¸ƒ}} \underbrace{P(z=k;\theta)}_{\text{å±äºkç±»çš„æ¦‚ç‡}} \\ = \sum_{k=1}^K \alpha_k \phi(x;\theta_k)$$
  å…¶ä¸­$\alpha_k \geq 0, \sum_{k=1}^K \alpha_k = 1$æ˜¯ç³»æ•°,$\theta_k = (\mu_k,\sigma_k^2)$,
  $$\phi(x;\theta_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}\exp(-\frac{(x-\mu_k)^2}{2\sigma_k^2})$$

- **ç­–ç•¥**ï¼š
  æ±‚å–å‚æ•°$\hat{\Theta} $ï¼Œä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡
  $$\hat{\Theta} := \argmax_{\Theta} \prod_{i=1}^N p(\boldsymbol{x}_i ; \Theta) \\ = \argmax_{\Theta} \prod_{i=1}^N \sum_{k=1}^K \alpha_k \phi(x_i;\theta_k)$$
  log æ±‚è§£ï¼Œä¼šå‘ç° log ä¸­æœ‰æ±‚å’Œï¼Œæ±‚å¯¼å¾ˆéš¾è§£å‡ºæ¥

- **ç®—æ³•**ï¼š
  æ ¹æ® EM ç®—æ³•ç¡®å®š Q å‡½æ•°
  $$Q(\theta,\theta^{(t)}) = \sum_{Z \in \{1,2...K\}} P(Z|X,\theta^{(t)}) \log P(X,Z|\theta) \\ = \sum_{Z_1,Z_2,...,Z_N} \bigg(\prod_{i=1}^N P(z_i|x_i,\theta^{(t)})\log \prod_{i=1}^N P(x_i,z_i|\theta) \bigg) \\ = \sum_{Z_1,Z_2,...,Z_N} \bigg(\prod_{i=1}^N P(z_i|x_i,\theta^{(t)}) \sum_{i=1}^N \log P(x_i,z_i|\theta) \bigg)$$
  å–ç¬¬ä¸€é¡¹åˆ†æï¼š
  $$\begin{equation*} \begin{split} &\sum_{z_1,z_2,\cdots,z_N}\log P(x_1,z_1|\theta)\cdot \prod_{i=1}^N P(z_i|x_i,\theta^{(t)})\\ =&\sum_{z_1,z_2,\cdots,z_N} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) \cdot \prod_{i=2}^N P(z_i|x_i,\theta^{(t)})\\ =& \sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) \sum_{z_2,\cdots,z_N}\prod_{i=2}^N P(z_i|x_i,\theta^{(t)})\\ =& \sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) \sum_{z_2} P(z_2|x_2,\theta^{(t)})\sum_{z_3} P(z_3|x_3,\theta^{(t)}) \cdots \sum_{z_N} P(z_N|x_N,\theta^{(t)}) \end{split}\end{equation*}$$
  ä¸Šå¼ä¸­ $\sum_{z_i} P(z_i|x_i,\theta^{(t)}) =1$ï¼Œå› æ­¤å¯ç®€åŒ–ä¸ºï¼š
  $$\sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)})$$
  å°†å…¶å¸¦å…¥åŸå¼ï¼š
  $$\begin{equation*}\begin{split} Q(\theta, \theta^{(t)})&= \sum_{z_1} \log P(x_1,z_1|\theta)\cdot P(z_1|x_1,\theta^{(t)}) +\cdots + \sum_{z_N} \log P(x_N,z_N|\theta)\cdot P(z_N|x_N,\theta^{(t)}) \\ &=\sum_{i=1}^N \sum_{z_i} \log P(x_i,z_i|\theta)\cdot P(z_i|x_i,\theta^{(t)})  \end{split}\end{equation*}$$

å…·ä½“ç®—æ³•æ±‚è§£å‚è€ƒï¼š[Gaussian mixture models](https://mbernste.github.io/posts/gmm_em/) ä»¥åŠ[æœºå™¨å­¦ä¹ -ç™½æ¿æ¨å¯¼ç³»åˆ—(åä¸€)-é«˜æ–¯æ··åˆæ¨¡å‹ GMMï¼ˆGaussian Mixture Modelï¼‰](https://www.bilibili.com/video/BV13b411w7Xj?p=3)
[æå¤§ä¼¼ç„¶ä¼°è®¡ã€EM ç®—æ³•åŠé«˜æ–¯æ··åˆæ¨¡å‹](https://blog.csdn.net/chris_xy/article/details/88970322)
[EM ç®—æ³•ä¸ GMMï¼ˆé«˜æ–¯æ··åˆèšç±»ï¼‰Jensen ä¸ç­‰å¼å’Œå˜åˆ†æ³•ä¸¤ç§æ¨å¯¼](https://zhuanlan.zhihu.com/p/50686800)
[Expectation-maximization algorithm EM ç®—æ³•](https://encyclopedia.thefreedictionary.com/Expectation-maximization+algorithm)
[Mixture model](https://encyclopedia.thefreedictionary.com/Mixture+model)
[Gaussian Mixture Model](https://brilliant.org/wiki/gaussian-mixture-model/)

### é™„åŠ çŸ¥è¯†

#### å˜åˆ†æ¨æ–­

**å˜åˆ†æ¨æ–­**ï¼ˆ[Variational Inference](https://en.jinzhao.wiki/wiki/Variational_Bayesian_methods)ï¼‰ä¹Ÿç§°ä¸ºå˜åˆ†è´å¶æ–¯ï¼ˆVariational Bayesianï¼‰ï¼Œè€Œå˜åˆ†æ³•ä¸»è¦æ˜¯ç ”ç©¶å˜åˆ†é—®é¢˜ï¼Œå³æ³›å‡½çš„æå€¼é—®é¢˜ï¼ˆå‡½æ•°çš„è¾“å…¥ä¹Ÿæ˜¯å‡½æ•°ï¼‰ï¼Œæ ¹æ®è´å¶æ–¯å…¬å¼ï¼ŒåéªŒæ¦‚ç‡
$${\displaystyle P(\mathbf {Z} \mid \mathbf {X} )={\frac {P(\mathbf {X} \mid \mathbf {Z} )P(\mathbf {Z} )}{P(\mathbf {X} )}}={\frac {P(\mathbf {X} \mid \mathbf {Z} )P(\mathbf {Z} )}{\int _{\mathbf {Z} }P(\mathbf {X} ,\mathbf {Z} ')\,d\mathbf {Z} '}}}$$

ä¸Šé¢å…¬å¼ä¸­çš„ç§¯åˆ†å¯¹äºå¾ˆå¤šæƒ…å†µä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼ˆæ‰€ä»¥æœ‰äº›æ¨¡å‹å¿½ç•¥äº† P(x)ï¼‰ï¼Œè¦ä¹ˆç§¯åˆ†æ²¡æœ‰é—­å¼è§£ï¼Œè¦ä¹ˆæ˜¯æŒ‡æ•°çº§åˆ«çš„è®¡ç®—å¤æ‚åº¦ï¼Œæ‰€ä»¥å¾ˆéš¾æ±‚å‡ºåéªŒæ¦‚ç‡ï¼Œè¿™æ—¶æˆ‘ä»¬éœ€è¦å¯»æ‰¾ä¸€ä¸ªç®€å•åˆ†å¸ƒ${\displaystyle q(\mathbf {Z} )\approx P(\mathbf {Z} \mid \mathbf {X} )}$ï¼Œè¿™æ ·æ¨æ–­é—®é¢˜è½¬åŒ–æˆä¸€ä¸ªæ³›å‡½ä¼˜åŒ–é—®é¢˜ï¼š
$$\hat{q(Z)} = \argmin_{q(Z) \in å€™é€‰çš„æ¦‚ç‡åˆ†å¸ƒæ—Q} KL(q(Z)|P(Z|X))$$

æˆ‘ä»¬æœ‰ä¸Šé¢**EM ç®—æ³•çš„å¯¼å‡º**å¾—åˆ°
$${\displaystyle \log P(\mathbf {X} )=D_{\mathrm {KL} }(q\parallel P)+{\mathcal {L}}(q)}$$
KL-divergence å¤§äºç­‰äº 0ï¼Œæ‰€ä»¥$\log P(\mathbf {X} ) \geq {\mathcal {L}}(q)$ï¼Œæ‰€ä»¥${\mathcal {L}}(q)$ç§°ä¸ºè¯æ®ä¸‹ç•Œï¼ˆ[Evidence Lower BOund,ELBO](https://en.jinzhao.wiki/wiki/Evidence_lower_bound)ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„å˜åˆ†å‡½æ•°ã€‚

KL-divergence ä¸­æœ‰åéªŒæ¦‚ç‡ P(Z|X)ï¼Œæœ¬èº«å°±æ˜¯éš¾ä»¥è®¡ç®—ï¼Œæ‰æƒ³æ‰¾ä¸ªç®€å•åˆ†å¸ƒ q(z)æ¥è¿‘ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¼˜åŒ– KL-divergenceï¼Œè¿›è€Œè½¬åŒ–æˆä¼˜åŒ– ELBO
$$\hat{q(Z)} = \argmax_{q(z) \in Q} ELBO(q,x)$$

åˆ†å¸ƒæ— Q ä¸€èˆ¬é€‰æ‹©æ˜¯**å¹³å‡åœº**ï¼ˆMean fieldï¼‰åˆ†å¸ƒæ—ï¼Œå³å¯ä»¥å°† Z æ‹†åˆ†ä¸ºå¤šç»„ç›¸äº’ç‹¬ç«‹çš„å˜é‡ï¼Œé‚£ä¹ˆ
$$q({\mathbf  {Z}})=\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m})$$

é‚£ä¹ˆ
$$ELBO(q,x) = \int_Zq(Z)\log \frac{P(X,Z|\theta)}{q(Z)}dZ \\ = \int_Z (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \log P(X,Z|\theta)dZ - \int_Z (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \log \prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m})dZ$$

å‡è®¾æˆ‘ä»¬åªå…³å¿ƒå…¶ä¸­ä¸€ä¸ªå­é›†(åˆ†é‡)$Z_j$çš„è¿‘ä¼¼åˆ†å¸ƒ$q_j(Z_j)$ï¼ˆå…ˆæ±‚ä¸€ä¸ªå­é›†ï¼Œå…¶å®ƒå­é›†ä¹Ÿå°±æ±‚å‡ºæ¥äº†ï¼‰
å…ˆçœ‹å‡å·åé¢çš„é¡¹(è¿›è¡Œå±•å¼€)ï¼š
$$\int_Z (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \sum_{m=1}^M \log q_{m}({\mathbf  {Z}}_{m})dZ \\ = \int_Z q_1(Z_1).q_2(Z_2)...q_M(Z_M).[\log q_1(Z_1)+...+\log q_M(Z_M)]dZ$$

åªçœ‹å…¶ä¸­ä¸€é¡¹
$$\int_{Z_1Z_2...Z_M} q_1(Z_1).q_2(Z_2)...q_M(Z_M).\log q_1(Z_1){dZ_1dZ_2...dZ_M} = \int_{Z_1} q_1(Z_1).\log q_1(Z_1)dZ_1.\int_{Z_2}q_2(Z_2)dZ_2....\int_{Z_M}q_M(Z_M)dZ_M$$

é‚£ä¹ˆæœ€ç»ˆæˆ‘ä»¬å¾—åˆ°å‡å·åé¢çš„é¡¹
$$\sum_{m=1}^M \int_{Z_m} q_m(Z_m).\log q_m(Z_m)dZ_m$$

å‰é¢è¯´äº†ï¼Œæˆ‘ä»¬åªå…³æ³¨å…¶ä¸­ä¸€ä¸ªå­é›†$Z_j$ï¼Œå…¶å®ƒ$m \neq j$çš„å¯¹å…¶æ¥è¯´å¯ä»¥çœ‹ä½œå¸¸æ•°é¡¹ï¼Œå¾—åˆ°
$$\int_{Z_j} q_j(Z_j).\log q_j(Z_j)dZ_j + C$$

å†çœ‹å‡å·å‰é¢çš„é¡¹
$$\int_{Z_1Z_2...Z_M} (\prod _{{m=1}}^{M}q_{m}({\mathbf  {Z}}_{m}) ) \log P(X,Z|\theta){dZ_1dZ_2...dZ_M} \\= \int_{Z_j}q_j(Z_j) \bigg(\int_{Z_i} \prod_{i \neq j}^M q_i(Z_i) \log P(X,Z|\theta) dZ_i \bigg)dZ_j \\= \int_{Z_j}q_j(Z_j) \bigg(E_{\prod_{i \neq j}^M q_i(Z_i)}[\log P(X,Z|\theta)] \bigg)dZ_j \\= \int_{Z_j}q_j(Z_j) \bigg(\log \tilde{p}(X,Z_j) \bigg)dZ_j$$

æœ€ç»ˆï¼š
$$ELBO(q,x) =\int_{Z_j}q_j(Z_j) \bigg(\log \tilde{p}(X,Z_j) \bigg)dZ_j - \int_{Z_j} q_j(Z_j).\log q_j(Z_j)dZ_j - C \\= -KL(q_j(Z_j) \| \tilde{p}(X,Z_j)) -C$$
æ‰€ä»¥ï¼š
$$\argmax ELBO(q,x) = \argmin KL(q_j(Z_j) \| \tilde{p}(X,Z_j))$$
å½“$q_j(Z_j) = \tilde{p}(X,Z_j)$å– KL æœ€å°å€¼

å…·ä½“æ±‚æœ€ä¼˜ç®—æ³•è¿™é‡Œä¸åšä»‹ç»ï¼Œå¯ä»¥å‚è€ƒ[å˜åˆ†æ¨æ–­ï¼ˆäºŒï¼‰â€”â€” è¿›é˜¶](https://www.cnblogs.com/kai-nutshell/p/13156319.html) ä»¥åŠ [ã€ä¸€æ–‡å­¦ä¼šã€‘å˜åˆ†æ¨æ–­åŠå…¶æ±‚è§£æ–¹æ³•](https://blog.csdn.net/weixin_40255337/article/details/83088786)

> å‚è€ƒ[11.4 èŠ‚å˜åˆ†æ¨æ–­](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)

### å‚è€ƒæ–‡çŒ®

[9-1] Dempster AP,Laird NM,Rubin DB. Maximum-likelihood from incomplete data via theEM algorithm. J. Royal Statist. Soc. Ser. B.,1977ï¼Œ39

[9-2] Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: DataMining,Inference,and Prediction. Springer-Verlag,2001ï¼ˆä¸­è¯‘æœ¬ï¼šç»Ÿè®¡å­¦ä¹ åŸºç¡€â€”â€”æ•°æ®æŒ–æ˜ã€æ¨ç†ä¸é¢„æµ‹ã€‚èŒƒæ˜ï¼ŒæŸ´ç‰æ¢…ï¼Œæ˜çº¢è‹±ç­‰è¯‘ã€‚åŒ—äº¬ï¼šç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2004ï¼‰

[9-3] McLachlan G,Krishnan T. The EM Algorithm and Extensions. New York: John Wiley& Sons,1996

[9-4] èŒ†è¯—æ¾ï¼Œç‹é™é¾™ï¼Œæ¿®æ™“é¾™ã€‚é«˜ç­‰æ•°ç†ç»Ÿè®¡ã€‚åŒ—äº¬ï¼šé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼›æµ·ç™»å ¡ï¼šæ–¯æ™®æ—æ ¼å‡ºç‰ˆç¤¾ï¼Œ1998

[9-5] Wu CFJ. On the convergence properties of the EM algorithm. The Annals ofStatistics,1983,11: 95â€“103

[9-6] Radford N,Geoffrey H,Jordan MI. A view of the EM algorithm that justifiesincremental,sparse,and other variants. In: Learning in Graphical Models. Cambridge,MA: MITPress,1999,355â€“368


