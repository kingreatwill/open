## ç¬¬ 19 ç«  é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•

**é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—**ï¼ˆ[Markov Chain Monte Carlo, MCMC](https://en.jinzhao.wiki/wiki/Markov_chain_Monte_Carlo)ï¼‰ç”±ä¸¤ä¸ªMCç»„æˆï¼Œå³**è’™ç‰¹å¡ç½—æ–¹æ³•**ï¼ˆ[Monte Carlo Simulation, MC](https://en.jinzhao.wiki/wiki/Monte_Carlo_method)ï¼‰å’Œ**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov Chain, MC](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚

è¦å¼„æ‡‚MCMCçš„åŸç†æˆ‘ä»¬é¦–å…ˆå¾—ææ¸…æ¥šè’™ç‰¹å¡ç½—æ–¹æ³•å’Œé©¬å°”å¯å¤«é“¾çš„åŸç†ã€‚

é©¬å°”å¯å¤«é“¾åœ¨å‰é¢çš„ç« èŠ‚æœ‰è®²åˆ°ï¼Œå†ç»“åˆä¹¦ä¸­çš„å†…å®¹ã€‚è¿™é‡Œè¡¥å……ä¸‹å‡ ä¸ªçŸ¥è¯†ï¼š
**é©¬å°”å¯å¤«é“¾çš„éå†å®šç†**ï¼š
ä¹¦ä¸­å•°å—¦äº†å¾ˆå¤šï¼Œæˆ‘çš„ç†è§£æ˜¯éå†è¶³å¤Ÿå¤šèƒ½è¾¾åˆ°å¹³ç¨³åˆ†å¸ƒçš„é©¬å°”å¯å¤«é“¾ã€‚å¹¶ä¸”è¾¾åˆ°ä»»ä½•ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ä¸èƒ½ä¸º0ï¼›

**å¯é€†é©¬å°”å¯å¤«é“¾**ï¼ˆreversible Markov chainï¼‰ï¼š
è®¾æœ‰é©¬å°”å¯å¤«é“¾$X=\{X_0,X_1,...,X_t,...\}$ï¼ŒçŠ¶æ€ç©ºé—´Sï¼Œè½¬ç§»çŸ©é˜µPï¼Œå¦‚æœæœ‰çŠ¶æ€åˆ†å¸ƒ$\pi = (\pi_1,\pi_2,...)^T$ï¼Œå¯¹ä»»æ„çŠ¶æ€$i,j \in S$ï¼Œå¯¹ä»»æ„æ—¶åˆ»tæ»¡è¶³
$$P(X_t=i|X_{t-1}=j)\pi_j = P(X_{t-1}=j|X_t=i)\pi_i  ,\quad i,j =1,2,...$$
æˆ–è€…ç®€å†™
$$p_{ij}\pi_j = p_{ji}\pi_i ,\quad i,j =1,2,...$$
åˆ™ç§°æ­¤é©¬å°”å¯å¤«é“¾ä¸ºå¯é€†é©¬å°”å¯å¤«é“¾ï¼›ç®€å†™çš„ç­‰å¼ç§°ä¸º**ç»†è‡´å¹³è¡¡æ–¹ç¨‹**ï¼ˆdetailed balance equationï¼‰ï¼Œå¹¶ä¸”æ»¡è¶³ç»†è‡´å¹³è¡¡æ–¹ç¨‹çš„çŠ¶æ€åˆ†å¸ƒ$\pi$å°±æ˜¯è¯¥é©¬å°”å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒï¼ˆå¹¶ä¸æ˜¯æ‰€æœ‰çš„é©¬å°”å¯å¤«é“¾éƒ½æ˜¯å¯é€†çš„ï¼‰ã€‚
å¯é€†é©¬å°”å¯å¤«é“¾æ»¡è¶³éå†å®šç†ã€‚




**é‡‡æ ·æ³•**ï¼ˆ[Sampling Method](https://en.jinzhao.wiki/wiki/Sampling_(statistics))ï¼‰ä¹Ÿç§°ä¸ºè’™ç‰¹å¡ç½—æ–¹æ³•ï¼ˆ[Monte Carlo Method, MC](https://en.jinzhao.wiki/wiki/Monte_Carlo_method)ï¼‰æˆ–ç»Ÿè®¡æ¨¡æ‹Ÿæ–¹æ³•ï¼ˆStatistical Simulation  Methodï¼‰

è’™ç‰¹å¡ç½—æ–¹æ³•è¯ç”Ÿäº20 ä¸–çºª 40 å¹´ä»£ç¾å›½çš„â€œæ›¼å“ˆé¡¿è®¡åˆ’â€ï¼Œå…¶åå­—æ¥æºäºæ‘©çº³å“¥çš„ä¸€ä¸ªä»¥èµŒåšä¸šé—»åçš„åŸå¸‚è’™ç‰¹å¡ç½—ï¼Œè±¡å¾æ¦‚ç‡ï¼
è’™ç‰¹å¡ç½—æ–¹æ³•æ˜¯ä¸€ç§é€šè¿‡éšæœºé‡‡æ ·æ¥è¿‘ä¼¼ä¼°è®¡ä¸€äº›è®¡ç®—é—®é¢˜**æ•°å€¼è§£**ï¼ˆNumerical solutionä¸å…¶å¯¹åº”çš„æ˜¯é—­å¼è§£Closed-form solutionæˆ–è§£æè§£Analytical solutionï¼‰çš„æ–¹æ³•ï¼

æœ€æ—©çš„è’™ç‰¹å¡ç½—æ–¹æ³•éƒ½æ˜¯ä¸ºäº†æ±‚è§£ä¸€äº›ä¸å¤ªå¥½æ±‚è§£çš„æ±‚å’Œæˆ–è€…ç§¯åˆ†é—®é¢˜ã€‚æ¯”å¦‚ç§¯åˆ†ï¼š$\theta = \int_a^b f(x)dx$ æˆ–è€…ä¼°è®¡$\pi$å€¼æˆ–åœ†çš„é¢ç§¯ï¼ˆç§¯åˆ†ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è’™ç‰¹å¡ç½—æ–¹æ³•æ¥æ¨¡æ‹Ÿæ±‚è§£è¿‘ä¼¼å€¼ã€‚å¦‚ä½•æ¨¡æ‹Ÿå‘¢ï¼Ÿ

**éšæœºé‡‡æ ·**æŒ‡ä»ç»™å®šæ¦‚ç‡å¯†åº¦å‡½æ•° ğ‘(ğ‘¥) ä¸­æŠ½å–å‡ºç¬¦åˆå…¶æ¦‚ç‡åˆ†å¸ƒçš„æ ·æœ¬ï¼

éšæœºé‡‡æ · é‡‡æ ·æ³•çš„éš¾ç‚¹æ˜¯å¦‚ä½•è¿›è¡Œéšæœºé‡‡æ ·ï¼Œå³å¦‚ä½•è®©è®¡ç®—æœºç”Ÿæˆæ»¡è¶³æ¦‚ç‡å¯†åº¦å‡½æ•° ğ‘(ğ‘¥) çš„æ ·æœ¬ï¼æˆ‘ä»¬çŸ¥é“ï¼Œè®¡ç®—æœºå¯ä»¥æ¯”è¾ƒå®¹æ˜“åœ°éšæœºç”Ÿæˆä¸€ä¸ªåœ¨ [0, 1]åŒºé—´ä¸Šå‡å¸ƒåˆ†å¸ƒçš„æ ·æœ¬ ğœ‰ï¼å¦‚æœè¦éšæœºç”Ÿæˆæœä»æŸä¸ªéå‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬ï¼Œå°±éœ€è¦ä¸€äº›é—´æ¥çš„é‡‡æ ·æ–¹æ³•ï¼
å¦‚æœä¸€ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º ğ‘(ğ‘¥)ï¼Œå…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•° cdf(ğ‘¥) ä¸ºè¿ç»­çš„ä¸¥æ ¼å¢å‡½æ•°ï¼Œä¸”å­˜åœ¨é€†å‡½æ•°$cdf^{âˆ’1}(ğ‘¦), ğ‘¦ âˆˆ [0, 1]$ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åˆ©ç”¨**ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†å‡½æ•°**ï¼ˆinverse CDFï¼‰æ¥ç”Ÿæˆæœä»è¯¥éšæœºåˆ†å¸ƒçš„æ ·æœ¬ï¼å‡è®¾ ğœ‰ æ˜¯ [0, 1] åŒºé—´ä¸Šå‡åŒ€åˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œåˆ™ $cdf^{âˆ’1}(\xi)$ æœä»æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºğ‘(ğ‘¥)çš„åˆ†å¸ƒï¼ä½†å½“ ğ‘(ğ‘¥) éå¸¸å¤æ‚ï¼Œå…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†å‡½æ•°éš¾ä»¥è®¡ç®—ï¼Œæˆ–è€…ä¸çŸ¥é“ ğ‘(ğ‘¥)çš„ç²¾ç¡®å€¼ï¼ŒåªçŸ¥é“æœªå½’ä¸€åŒ–çš„åˆ†å¸ƒ Ì‚ğ‘(ğ‘¥)æ—¶ï¼Œå°±éš¾ä»¥ç›´æ¥å¯¹ğ‘(ğ‘¥)è¿›è¡Œé‡‡æ ·ï¼Œå¾€å¾€éœ€è¦ä½¿ç”¨ä¸€äº›é—´æ¥çš„é‡‡æ ·ç­–ç•¥ï¼Œæ¯”å¦‚**æ‹’ç»é‡‡æ ·ã€é‡è¦æ€§é‡‡æ ·ã€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—é‡‡æ ·**ç­‰ï¼è¿™äº›æ–¹æ³•ä¸€èˆ¬æ˜¯å…ˆæ ¹æ®ä¸€ä¸ªæ¯”è¾ƒå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œç„¶åé€šè¿‡ä¸€äº›ç­–ç•¥æ¥é—´æ¥å¾—åˆ°ç¬¦åˆğ‘(ğ‘¥)åˆ†å¸ƒçš„æ ·æœ¬ï¼

> rejection sampling, inverse CDF, Box-Muller,  Ziggurat algorithm

**æ‹’ç»é‡‡æ ·**ï¼ˆRejection Samplingï¼‰æ˜¯ä¸€ç§é—´æ¥é‡‡æ ·æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºæ¥å—-æ‹’ç»é‡‡æ ·ï¼ˆAcceptance-Rejection Samplingï¼‰ï¼
å‡è®¾åŸå§‹åˆ†å¸ƒğ‘(ğ‘¥)éš¾ä»¥ç›´æ¥é‡‡æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒğ‘(ğ‘¥)ï¼Œä¸€èˆ¬ç§°ä¸ºæè®®åˆ†å¸ƒï¼ˆProposal Distributionï¼‰ï¼Œç„¶åä»¥æŸä¸ªæ ‡å‡†æ¥æ‹’ç»ä¸€éƒ¨åˆ†çš„æ ·æœ¬ä½¿å¾—æœ€ç»ˆé‡‡é›†çš„æ ·æœ¬æœä»åˆ†å¸ƒ ğ‘(ğ‘¥)ã€‚æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæè®®åˆ†å¸ƒ ğ‘(ğ‘¥) å’Œä¸€ä¸ªå¸¸æ•° ğ‘˜ï¼Œä½¿å¾— ğ‘˜ğ‘(ğ‘¥) å¯ä»¥è¦†ç›–å‡½æ•°ğ‘(ğ‘¥)ï¼Œå³ğ‘˜ğ‘(ğ‘¥) â‰¥ ğ‘(ğ‘¥),
![](https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170327143755811-993574578.png)
å¯¹äºæ¯æ¬¡æŠ½å–çš„æ ·æœ¬ $\^{x}$ è®¡ç®—æ¥å—æ¦‚ç‡ï¼ˆAcceptance Probabilityï¼‰ï¼š$\alpha(\^{x}) = \frac{p(\^{x})}{kq(\^{x})}$ï¼Œå¹¶ä»¥æ¦‚ç‡$\alpha(\^{x})$)æ¥æ¥å—æ ·æœ¬ $\^{x}$

æ‹’ç»é‡‡æ ·çš„é‡‡æ ·è¿‡ç¨‹
```
è¾“å…¥: æè®®åˆ†å¸ƒğ‘(ğ‘¥)ï¼Œå¸¸æ•°ğ‘˜ï¼Œæ ·æœ¬é›†åˆğ’± = âˆ…;
1 repeat
2   æ ¹æ®ğ‘(ğ‘¥)éšæœºç”Ÿæˆä¸€ä¸ªæ ·æœ¬ ;Ì‚ğ‘¥
3   è®¡ç®—æ¥å—æ¦‚ç‡ğ›¼( Ì‚ğ‘¥);
4   ä»(0, 1)çš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºç”Ÿæˆä¸€ä¸ªå€¼ğ‘§;
5   if ğ‘§ â‰¤ ğ›¼( Ì‚ğ‘¥) then // ä»¥ğ›¼(ğ‘¥)Ì‚ çš„æ¦‚ç‡æ¥å—ğ’™Ì‚
6       ğ’± = ğ’± âˆª { Ì‚ğ‘¥};
7   end
8 until è·å¾— ğ‘ ä¸ªæ ·æœ¬(|ğ’±| = ğ‘);
è¾“å‡º: æ ·æœ¬é›†åˆğ’±
```

åˆ¤æ–­ä¸€ä¸ªæ‹’ç»é‡‡æ ·æ–¹æ³•çš„å¥½åå°±æ˜¯çœ‹å…¶é‡‡æ ·æ•ˆç‡ï¼Œå³æ€»ä½“çš„æ¥å—ç‡ï¼å¦‚æœå‡½æ•°ğ‘˜ğ‘(ğ‘¥)è¿œå¤§äºåŸå§‹åˆ†å¸ƒå‡½æ•° Ì‚ğ‘(ğ‘¥)ï¼Œæ‹’ç»ç‡ä¼šæ¯”è¾ƒé«˜ï¼Œé‡‡æ ·æ•ˆç‡ä¼šéå¸¸ä¸ç†æƒ³ï¼ä½†è¦æ‰¾åˆ°ä¸€ä¸ªå’Œ Ì‚ğ‘(ğ‘¥)æ¯”è¾ƒæ¥è¿‘çš„æè®®åˆ†å¸ƒå¾€å¾€æ¯”è¾ƒå›°éš¾ï¼ç‰¹åˆ«æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œå…¶é‡‡æ ·ç‡ä¼šéå¸¸ä½ï¼Œå¯¼è‡´å¾ˆéš¾åº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­ï¼

**é‡è¦æ€§é‡‡æ ·**ï¼ˆImportance samplingï¼‰
å¦‚æœé‡‡æ ·çš„ç›®çš„æ˜¯è®¡ç®—åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹å‡½æ•°ğ‘“(ğ‘¥)çš„æœŸæœ›ï¼Œé‚£ä¹ˆå®é™…ä¸ŠæŠ½å–çš„æ ·æœ¬ä¸éœ€è¦ä¸¥æ ¼æœä»åˆ†å¸ƒ ğ‘(ğ‘¥)ï¼ä¹Ÿå¯ä»¥é€šè¿‡å¦ä¸€ä¸ªåˆ†å¸ƒï¼Œå³æè®®åˆ†å¸ƒ ğ‘(ğ‘¥)ï¼Œç›´æ¥é‡‡æ ·å¹¶ä¼°è®¡ğ”¼ğ‘[ğ‘“(ğ‘¥)]ï¼
å‡½æ•°ğ‘“(ğ‘¥)åœ¨åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹çš„æœŸæœ›å¯ä»¥å†™ä¸º
$$E_p[f(x)] = \int_x f(x)p(x)dx = \int_x f(x)\frac{p(x)}{q(x)}q(x)dx = \int_x f(x)w(x)q(x)dx = E_q[f(x)w(x)]$$
å…¶ä¸­ğ‘¤(ğ‘¥)ç§°ä¸ºé‡è¦æ€§æƒé‡ï¼

é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Samplingï¼‰æ˜¯é€šè¿‡å¼•å…¥é‡è¦æ€§æƒé‡ï¼Œå°†åˆ†å¸ƒ ğ‘(ğ‘¥)ä¸‹ğ‘“(ğ‘¥)çš„æœŸæœ›å˜ä¸ºåœ¨åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹ğ‘“(ğ‘¥)ğ‘¤(ğ‘¥)çš„æœŸæœ›ï¼Œä»è€Œå¯ä»¥è¿‘ä¼¼ä¸º
$$E_p[f(x)] = E_q[f(x)w(x)] =\frac{1}{N} \sum_{i=1}^N f(x_i)w(x_i) =\frac{1}{N} \sum_{i=1}^N f(x_i)\frac{p(x_i)}{q(x_i)}$$
å…¶ä¸­$\{x_1,...,x_N\}$æ˜¯ç‹¬ç«‹ä»$q(x)$ä¸­éšæœºé‡‡æ ·æ¥çš„ï¼ˆp(x)æ˜¯å·²çŸ¥çš„ï¼Œåªæ˜¯ä¸å¥½é‡‡æ ·ï¼Œä½†æ˜¯èƒ½è®¡ç®—ï¼‰ã€‚

**é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³•**
åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ‹’ç»é‡‡æ ·å’Œé‡è¦æ€§é‡‡æ ·çš„æ•ˆç‡éšç©ºé—´ç»´æ•°çš„å¢åŠ è€ŒæŒ‡æ•°é™ä½ï¼é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ï¼ˆMarkov Chain Monte Carloï¼ŒMCMCï¼‰æ–¹æ³•æ˜¯ä¸€ç§æ›´å¥½çš„é‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å¯¹é«˜ç»´å˜é‡è¿›è¡Œé‡‡æ ·ï¼

å‡è®¾å¤šå…ƒéšæœºå˜é‡$x$,æ»¡è¶³$x \in \mathcal{X}$,å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º$p(x)$ï¼›$f(x)$ä¸ºå®šä¹‰åœ¨$x \in \mathcal{X}$ä¸Šçš„å‡½æ•°ï¼Œç›®æ ‡æ˜¯è·å¾—æ¦‚ç‡åˆ†å¸ƒ$p(x)$çš„æ ·æœ¬é›†åˆä»¥åŠæ±‚å‡½æ•°$f(x)$çš„æ•°å­¦æœŸæœ›$E_{p(x)}[f(x)]$ã€‚

åº”ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•è§£å†³è¿™ä¸ªé—®é¢˜çš„**åŸºæœ¬æƒ³æ³•**æ˜¯ï¼š
åœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\mathcal{S}$ä¸Šå®šä¹‰ä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å°”å¯å¤«é“¾$X=\{X_0,X_1,...,X_t,...\}$,ä½¿å…¶å¹³ç¨³åˆ†å¸ƒå°±æ˜¯æŠ½æ ·çš„çš„ç›®æ ‡åˆ†å¸ƒ$p(x)$ï¼ˆè®¾è®¡ä¸€ä¸ªéšæœºçŸ©é˜µï¼ˆè¿ç»­éšæœºå˜é‡å°±æ˜¯è½¬ç§»æ ¸å‡½æ•°ï¼‰ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒç­‰äºç›®æ ‡åˆ†å¸ƒï¼‰ã€‚ç„¶ååœ¨è¿™ä¸ªé©¬å°”å¯å¤«é“¾ä¸Šè¿›è¡Œéšæœºæ¸¸èµ°ï¼Œæ¯ä¸ªæ—¶åˆ»å¾—åˆ°ä¸€ä¸ªæ ·æœ¬ã€‚æ ¹æ®éå†å®šç†ï¼Œå½“æ—¶é—´è¶‹äºæ— ç©·æ—¶ï¼Œæ ·æœ¬çš„åˆ†å¸ƒè¶‹è¿‘å¹³ç¨³åˆ†å¸ƒï¼Œæ ·æœ¬çš„å‡½æ•°å‡å€¼è¶‹è¿‘å‡½æ•°çš„æ•°å­¦æœŸæœ›ã€‚æ‰€ä»¥ï¼Œå½“æ—¶é—´è¶³å¤Ÿé•¿æ—¶ï¼ˆæ—¶åˆ»å¤§äºæŸä¸ªæ­£æ•´æ•°mï¼‰ï¼Œåœ¨ä¹‹åçš„æ—¶é—´ï¼ˆæ—¶åˆ»å°äºç­‰äºæŸä¸ªæ­£æ•´æ•°n,n>mï¼‰é‡Œéšæœºæ¸¸èµ°å¾—åˆ°çš„æ ·æœ¬é›†åˆ$\left\{x_{m+1}, x_{m+2}, \cdots, x_{n}\right\}$å°±æ˜¯ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒçš„æŠ½æ ·ç»“æœï¼Œå¾—åˆ°çš„å‡½æ•°å‡å€¼å°±æ˜¯è¦è®¡ç®—çš„æ•°å­¦æœŸæœ›å€¼ï¼š
$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left(x_{i}\right)$$
ä»æ—¶åˆ»0åˆ°æ—¶åˆ»mä¸ºæ­¢çš„è¿™æ®µæ—¶é—´ç§°ä¸ºç‡ƒçƒ§æœŸï¼ˆBurn-in Periodï¼‰ã€‚ç‡ƒçƒ§æœŸçš„æ ·æœ¬ä¹Ÿæ˜¯è¦è¢«ä¸¢å¼ƒçš„ã€‚

**åŸºæœ¬æ­¥éª¤**
1. é¦–å…ˆï¼Œåœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\mathcal{S}$ä¸Šæ„é€ ä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å°”å¯å¤«é“¾ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒä¸ºç›®æ ‡åˆ†å¸ƒ$p(x)$;
1. ä»çŠ¶æ€ç©ºé—´çš„æŸä¸€ç‚¹$x_0$å‡ºå‘ï¼Œç”¨æ„é€ çš„é©¬å°”å¯å¤«é“¾è¿›è¡Œéšæœºæ¸¸èµ°ï¼Œäº§ç”Ÿæ ·æœ¬åºåˆ—$x_0,x_1,...,x_t,...$ã€‚
1. åº”ç”¨é©¬å°”å¯å¤«é“¾çš„éå†åŸç†ï¼Œç¡®å®šæ­£æ•´æ•°må’Œnï¼ˆm < nï¼‰ï¼Œå¾—åˆ°æ ·æœ¬é›†åˆ$\{x_{m+1},x_{m+2},...,x_{n}\}$ï¼Œæ±‚å¾—å‡½æ•°$f(x)$çš„å‡å€¼ï¼ˆéå†å‡å€¼ï¼‰
$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left(x_{i}\right)$$

è¿™é‡Œæœ‰å‡ ä¸ª**é‡è¦é—®é¢˜**
1. å¦‚ä½•ç¬¬ä¸€é©¬å°”å¯å¤«é“¾ï¼Œä¿è¯é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•çš„æ¡ä»¶æˆç«‹ã€‚
1. å¦‚ä½•ç¡®å®šæ”¶æ•›æ­¥æ•°$m$ï¼Œä¿è¯æ ·æœ¬æŠ½æ ·çš„æ— åæ€§ã€‚
1. å¦‚ä½•ç¡®å®šè¿­ä»£æ­¥æ•°$n$ï¼Œä¿è¯éå†å‡å€¼è®¡ç®—çš„ç²¾åº¦ã€‚

**MCMCé‡‡æ ·**
ç”±äºä¸€èˆ¬æƒ…å†µä¸‹ï¼Œç›®æ ‡å¹³ç¨³åˆ†å¸ƒ $\pi(x)$ å’ŒæŸä¸€ä¸ªé©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ ä¸æ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼Œå³ï¼š

$$\pi(i)Q(i,j)\neq\pi(j)Q(j,i)$$

ä»¥ä¸‹è®°å·è¡¨è¾¾åŒä¸€ä¸ªæ„æ€ï¼š $Q(i,j)\Leftrightarrow Q(j|i)\Leftrightarrow Q(i\rightarrow j)$ ï¼ˆçŠ¶æ€è½¬ç§»åˆ†å¸ƒæˆ–æè®®åˆ†å¸ƒï¼‰

æˆ‘ä»¬å¼•å…¥ä¸€ä¸ª $\alpha(i,j)$ ,ä½¿ä¸Šå¼å¯ä»¥å–ç­‰å·ã€‚

$$\pi(i)Q(i,j)\alpha(i,j)=\pi(j)Q(j,i)\alpha(j,i)$$

æ€æ ·æ‰èƒ½æˆç«‹å‘¢ï¼Œå…¶å®å¾ˆç®€å•ï¼ŒæŒ‰ç…§å¯¹ç§°æ€§ï¼š

$$\alpha(i,j)=\pi(j)Q(j,i); \pi(i)Q(i,j)=\alpha(j,i)$$

ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°äº†åˆ†å¸ƒ $\pi(x)$ å¯¹è®©é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $P(i,j)=Q(i,j)\alpha(i,j)$

å…¶ä¸­ $\alpha(i,j)$ ä¸€èˆ¬ç§°ä¹‹ä¸ºæ¥å—ç‡ï¼Œå–å€¼åœ¨ $[0,1]$ ä¹‹é—´ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ¦‚ç‡å€¼ã€‚è¿™å¾ˆåƒæ¥å—-æ‹’ç»é‡‡æ ·ï¼Œé‚£é‡Œæ˜¯ä»¥ä¸€ä¸ªå¸¸ç”¨åˆ†å¸ƒé€šè¿‡ä¸€å®šçš„æ¥å—-æ‹’ç»æ¦‚ç‡å¾—åˆ°ä¸€ä¸ªéå¸¸è§åˆ†å¸ƒï¼Œ è¿™é‡Œæ˜¯ä»¥ä¸€ä¸ªå¸¸è§çš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ é€šè¿‡ä¸€å®šçš„æ¥å—-æ‹’ç»æ¦‚ç‡å¾—åˆ°ç›®æ ‡è½¬ç§»çŸ©é˜µ $P$ ,ä¸¤è€…çš„è§£å†³é—®é¢˜æ€è·¯æ˜¯ç±»ä¼¼çš„ã€‚

MCMCé‡‡æ ·ç®—æ³•å¦‚ä¸‹ï¼š
1. è¾“å…¥ä»»æ„ç»™å®šçš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ ï¼Œç›®æ ‡å¹³ç¨³åˆ†å¸ƒ $\pi(x)$ ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼ $n_1$ ï¼Œéœ€è¦çš„æ ·æœ¬æ•° $n_2$;
1. ä»ä»»æ„ç®€å•æ¦‚ç‡åˆ†å¸ƒå¾—åˆ°åˆå§‹çŠ¶æ€å€¼ $x_0$ ï¼›
1. for $t=0 ~in ~n_{1}+n_{2}-1$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ $Q(x|x_{t})$ å¾—åˆ°æ ·æœ¬å€¼ $x_{*}$
    1. ä»å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ · $u\sim uniform[0,1]$
    1. å¦‚æœ $u<\alpha(x_{t},x_{*})=\pi(x_{*})Q(x_{*},x_{t})$ ï¼Œåˆ™æ¥å— $x_{t}\rightarrow x_{*}$ ï¼Œå³ $x_{t+1}= x_{*}$
    1. å¦åˆ™ä¸æ¥å—è½¬ç§»ï¼Œ å³$x_{t+1}= x_{t}$

ä¸Šè¿°è¿‡ç¨‹ä¸­ $p(x),q(x|y)$ è¯´çš„éƒ½æ˜¯ç¦»æ•£çš„æƒ…å½¢ï¼Œäº‹å®ä¸Šå³ä¾¿è¿™ä¸¤ä¸ªåˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œä»¥ä¸Šç®—æ³•ä»ç„¶æ˜¯æœ‰æ•ˆï¼Œäºæ˜¯å°±å¾—åˆ°æ›´ä¸€èˆ¬çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒ $p(x)$ çš„é‡‡æ ·ç®—æ³•ï¼Œè€Œ $q(x|y)$ å°±æ˜¯ä»»æ„ä¸€ä¸ªè¿ç»­äºŒå…ƒæ¦‚ç‡åˆ†å¸ƒå¯¹åº”çš„æ¡ä»¶åˆ†å¸ƒã€‚

ä½†æ˜¯è¿™ä¸ªé‡‡æ ·ç®—æ³•è¿˜æ˜¯æ¯”è¾ƒéš¾åœ¨å®é™…ä¸­åº”ç”¨ï¼Œå› ä¸ºåœ¨ç¬¬ä¸‰æ­¥ä¸­ï¼Œç”±äº $\alpha(x_{t},x_{*})$ å¯èƒ½éå¸¸çš„å°ï¼Œæ¯”å¦‚0.1ï¼Œå¯¼è‡´æˆ‘ä»¬å¤§éƒ¨åˆ†çš„é‡‡æ ·å€¼éƒ½è¢«æ‹’ç»è½¬ç§»ï¼Œé‡‡æ ·æ•ˆç‡å¾ˆä½ã€‚æœ‰å¯èƒ½æˆ‘ä»¬é‡‡æ ·äº†ä¸Šç™¾ä¸‡æ¬¡é©¬å°”å¯å¤«é“¾è¿˜æ²¡æœ‰æ”¶æ•›ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢è¿™ä¸ª $n_1$ è¦éå¸¸éå¸¸çš„å¤§ï¼Œè¿™è®©äººéš¾ä»¥æ¥å—ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿè¿™æ—¶å°±è½®åˆ°æˆ‘ä»¬çš„M-Hé‡‡æ ·å‡ºåœºäº†ã€‚

**Metropolis-Hastingsç®—æ³•**ï¼š
M-Hé‡‡æ ·æ˜¯Metropolis-Hastingsé‡‡æ ·çš„ç®€ç§°ï¼Œè¿™ä¸ªç®—æ³•é¦–å…ˆç”±Metropolisæå‡ºï¼Œè¢«Hastingsæ”¹è¿›ï¼Œå› æ­¤è¢«ç§°ä¹‹ä¸ºMetropolis-Hastingsé‡‡æ ·æˆ–M-Hé‡‡æ ·ã€‚

> ä¹¦ä¸­çš„ä»‹ç»æ›´è¯¦ç»†ï¼Œè¿™é‡Œç®€å•ä»‹ç»åŸç†

æˆ‘ä»¬å›åˆ°MCMCé‡‡æ ·çš„ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼š$\pi(i)Q(i,j)\alpha(i,j) = \pi(j)Q(j,i)\alpha(j,i)$
æˆ‘ä»¬é‡‡æ ·æ•ˆç‡ä½çš„åŸå› æ˜¯$\alpha(i,j)$å¤ªå°äº†ï¼Œæ¯”å¦‚ä¸º0.1ï¼Œè€Œ$\alpha(j,i)$ä¸º0.2ã€‚å³ï¼š$\pi(i)Q(i,j)\times 0.1 = \pi(j)Q(j,i)\times 0.2$
è¿™æ—¶æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœä¸¤è¾¹åŒæ—¶æ‰©å¤§äº”å€ï¼Œæ¥å—ç‡æé«˜åˆ°äº†0.5ï¼Œä½†æ˜¯ç»†è‡´å¹³ç¨³æ¡ä»¶å´ä»ç„¶æ˜¯æ»¡è¶³çš„ï¼Œå³ï¼š$\pi(i)Q(i,j)\times 0.5 = \pi(j)Q(j,i)\times 1$
è¿™æ ·æˆ‘ä»¬çš„æ¥å—ç‡å¯ä»¥åšå¦‚ä¸‹æ”¹è¿›ï¼Œå³ï¼š$\alpha(i,j) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}$

- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š
### å‚è€ƒæ–‡ç« 
[é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ç®—æ³•ï¼ˆMCMCï¼‰](https://zhuanlan.zhihu.com/p/37121528)
[MCMC(ä¸‰)MCMCé‡‡æ ·å’ŒM-Hé‡‡æ ·](https://www.cnblogs.com/pinard/p/6638955.html)

### å‚è€ƒæ–‡çŒ®
[19-1] Serfozo R. [Basics of applied stochastic processes](http://www.stat.yale.edu/~jtc5/251/readings/Basics%20of%20Applied%20Stochastic%20Processes_Serfozo.pdf). Springer, 2009.
[19-2] Metropolis N, Rosenbluth A W, Rosenbluth M N, et al. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 1953,21(6):1087-1092. 
[19-3] Geman S, Geman D. Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984,6:721-741
[19-4] Bishop C M. Pattern recognition and machine learning. Springer, 2006.
[19-5] Gilks W R, Richardson S, Spiegelhalter, DJ. Introducing Markov chain Monte Carlo. Markov Chain Monte Carlo in Practice, 1996.
[19-6] Andrieu C, De Freitas N, Doucet A, et al. An introduction to MCMC for machine learning. Machine Learning, 2003,50(1-2): 5-43.
[19-7] Hoff P. [A first course in Bayesian statistical methods](https://esl.hohoweiya.xyz/references/A_First_Course_in_Bayesian_Statistical_Methods.pdf). Springer, 2009.
[19-8] èŒ†è¯—æ¾ï¼Œç‹é™é¾™ï¼Œæ¿®æ™“é¾™. é«˜ç­‰æ•°ç†ç»Ÿè®¡. åŒ—äº¬ï¼šé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼Œ 1998.


## ç¬¬ 20 ç«  æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…
- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š
### é™„åŠ çŸ¥è¯†
### å‚è€ƒæ–‡çŒ®
[20-1] Blei D M, Ng A Y, Jordan M I. Latent Dirichlet allocation. In: Advances in Neural Information Processing Systems 14. MIT Press, 2002.
[20-2] Blei D M, Ng A Y, Jordan M I. Latent Dirichlet allocation. Journal of Machine Learning Research, 2003, 3:933-1022.
[20-3] Griffiths T L, Steyvers M. Finding scientific topics. Proceedings of the National Academy of Science, 2004, 101:5288-5235.
[20-4] Steyvers M, Griffiths T. Probabilistic topic models. In: Landauer T, McNamara D, Dennis S, et al. (eds.) Handbook of Latent Semantic Analysis, Psychology Press, 2014.
[20-5] Gregor Heinrich. Parameter estimation for text analysis. Techniacl note, 2004.
[20-6] Blei D M, Kucukelbir A, McAuliffe J D. Variational inference: a review for statisticians. Journal of the American Statistical Association, 2017, 112(518).
[20-7] Newman D, Smyth P, Welling M, Asuncion A U. Distributed inference for latent Dirichlet allocation. In: Advances in Neural Information Processing Systems, 2008: 1081-1088
[20-8] Porteous I, Newman D, Ihler A, et al. Fast collapsed Gibbs sampling for latent Dirichlet allocation. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2008: 569-577.
[20-9] Hoffiman M, Bach F R, Blei D M. Online learning for latent Dirichlet allocation. In: Advances in Neural Information Processing Systems, 2010:856-864.

## ç¬¬ 21 ç«  PageRankç®—æ³•
- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š
### é™„åŠ çŸ¥è¯†
### å‚è€ƒæ–‡çŒ®
[21-1] Page L, Brin S, Motwani R, et al. The PageRank citation ranking: bringing order to the Web. Stanford University, 1999.
[21-2] Rajaraman A, Ullman J D. Mining of massive datasets. Cambridge University Press, 2014.
[21-3] Liu B. Web data mining: exploring Hyperlinks, contents, and usage data. Springer Science & Business Media, 2007.
[21-4] Serdozo R. Basics of applied stochastic processes. Springer, 2009.
[21-5] Kleinberg J M. Authoritative sources in a hyperlinked environment. Journal of the ACM(JACM), 1999,46(5):604-632.
[21-6] Liu Y, Gao B, Liu T Y, et al. BrowseRank: letting Web users vote for page importance. Proceedings of the 31st SIGIR Conference, 2008:451-458 
[21-7] Jeh G, Widom J. Scaling Personalized Web search. Proceedings of the 12th WWW Conference, 2003: 271-279.
[21-8] Haveliwala T H. Topic-sensitive PageRank. Proceedings of the 11th WWW Conference, 2002: 517-526.
[21-9] GyÃ¶ngyi Z, Garcia-Molina H, Pedersen J. Combating Web spam with TrustRank. Proceedings of VLDB Conference, 2004:576-587.

## ç¬¬ 22 ç«  æ— ç›‘ç£å­¦ä¹ æ–¹æ³•æ€»ç»“
### é™„åŠ çŸ¥è¯†
### å‚è€ƒæ–‡çŒ®
[22-1] Singh A P, Gordon G J. A unified view of matrix factorization models. In: Daelemans W, Goethals B, Morik K,(eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2008. Lecture Notes in Computer Science, vol 5212. Berlin: Springer, 2008.
[22-2] Wang Q, Xu J, Li H, et al. Regularized latent semantic indexing:a new approach to large-scale topic modeling. ACM Transactions on Information Systems (TOIS), 2013, 31(1), 5.
