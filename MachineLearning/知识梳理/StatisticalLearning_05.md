## ç¬¬ 19 ç«  é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•

**é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—**ï¼ˆ[Markov Chain Monte Carlo, MCMC](https://en.jinzhao.wiki/wiki/Markov_chain_Monte_Carlo)ï¼‰ç”±ä¸¤ä¸ªMCç»„æˆï¼Œå³**è’™ç‰¹å¡ç½—æ–¹æ³•**ï¼ˆ[Monte Carlo Simulation, MC](https://en.jinzhao.wiki/wiki/Monte_Carlo_method)ï¼‰å’Œ**é©¬å°”å¯å¤«é“¾**ï¼ˆ[Markov Chain, MC](https://en.jinzhao.wiki/wiki/Markov_chain)ï¼‰ã€‚

è¦å¼„æ‡‚MCMCçš„åŸç†æˆ‘ä»¬é¦–å…ˆå¾—ææ¸…æ¥šè’™ç‰¹å¡ç½—æ–¹æ³•å’Œé©¬å°”å¯å¤«é“¾çš„åŸç†ã€‚

é©¬å°”å¯å¤«é“¾åœ¨å‰é¢çš„ç« èŠ‚æœ‰è®²åˆ°ï¼Œå†ç»“åˆä¹¦ä¸­çš„å†…å®¹ã€‚è¿™é‡Œè¡¥å……ä¸‹å‡ ä¸ªçŸ¥è¯†ï¼š
**é©¬å°”å¯å¤«é“¾çš„éå†å®šç†**ï¼š
ä¹¦ä¸­å•°å—¦äº†å¾ˆå¤šï¼Œæˆ‘çš„ç†è§£æ˜¯éå†è¶³å¤Ÿå¤šèƒ½è¾¾åˆ°å¹³ç¨³åˆ†å¸ƒçš„é©¬å°”å¯å¤«é“¾ã€‚å¹¶ä¸”è¾¾åˆ°ä»»ä½•ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ä¸èƒ½ä¸º0ï¼›ï¼ˆä¸å¯çº¦ï¼Œéå‘¨æœŸä¸”æ­£å¸¸è¿”ï¼‰

**å¯é€†é©¬å°”å¯å¤«é“¾**ï¼ˆreversible Markov chainï¼‰ï¼š
è®¾æœ‰é©¬å°”å¯å¤«é“¾$X=\{X_0,X_1,...,X_t,...\}$ï¼ŒçŠ¶æ€ç©ºé—´Sï¼Œè½¬ç§»çŸ©é˜µPï¼Œå¦‚æœæœ‰çŠ¶æ€åˆ†å¸ƒ$\pi = (\pi_1,\pi_2,...)^T$ï¼Œå¯¹ä»»æ„çŠ¶æ€$i,j \in S$ï¼Œå¯¹ä»»æ„æ—¶åˆ»tæ»¡è¶³
$$P(X_t=i|X_{t-1}=j)\pi_j = P(X_{t-1}=j|X_t=i)\pi_i  ,\quad i,j =1,2,...$$
æˆ–è€…ç®€å†™
$$p_{ij}\pi_j = p_{ji}\pi_i ,\quad i,j =1,2,...$$
åˆ™ç§°æ­¤é©¬å°”å¯å¤«é“¾ä¸ºå¯é€†é©¬å°”å¯å¤«é“¾ï¼›ç®€å†™çš„ç­‰å¼ç§°ä¸º**ç»†è‡´å¹³è¡¡æ–¹ç¨‹**ï¼ˆdetailed balance equationï¼‰ï¼Œå¹¶ä¸”æ»¡è¶³ç»†è‡´å¹³è¡¡æ–¹ç¨‹çš„çŠ¶æ€åˆ†å¸ƒ$\pi$å°±æ˜¯è¯¥é©¬å°”å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒï¼ˆå¹¶ä¸æ˜¯æ‰€æœ‰çš„é©¬å°”å¯å¤«é“¾éƒ½æ˜¯å¯é€†çš„ï¼‰ã€‚
å¯é€†é©¬å°”å¯å¤«é“¾æ»¡è¶³éå†å®šç†ã€‚




**é‡‡æ ·æ³•**ï¼ˆ[Sampling Method](https://en.jinzhao.wiki/wiki/Sampling_(statistics))ï¼‰ä¹Ÿç§°ä¸ºè’™ç‰¹å¡ç½—æ–¹æ³•ï¼ˆ[Monte Carlo Method, MC](https://en.jinzhao.wiki/wiki/Monte_Carlo_method)ï¼‰æˆ–ç»Ÿè®¡æ¨¡æ‹Ÿæ–¹æ³•ï¼ˆStatistical Simulation  Methodï¼‰

è’™ç‰¹å¡ç½—æ–¹æ³•è¯ç”Ÿäº20 ä¸–çºª 40 å¹´ä»£ç¾å›½çš„â€œæ›¼å“ˆé¡¿è®¡åˆ’â€ï¼Œå…¶åå­—æ¥æºäºæ‘©çº³å“¥çš„ä¸€ä¸ªä»¥èµŒåšä¸šé—»åçš„åŸå¸‚è’™ç‰¹å¡ç½—ï¼Œè±¡å¾æ¦‚ç‡ï¼
è’™ç‰¹å¡ç½—æ–¹æ³•æ˜¯ä¸€ç§é€šè¿‡éšæœºé‡‡æ ·æ¥è¿‘ä¼¼ä¼°è®¡ä¸€äº›è®¡ç®—é—®é¢˜**æ•°å€¼è§£**ï¼ˆNumerical solutionä¸å…¶å¯¹åº”çš„æ˜¯é—­å¼è§£Closed-form solutionæˆ–è§£æè§£Analytical solutionï¼‰çš„æ–¹æ³•ï¼

æœ€æ—©çš„è’™ç‰¹å¡ç½—æ–¹æ³•éƒ½æ˜¯ä¸ºäº†æ±‚è§£ä¸€äº›ä¸å¤ªå¥½æ±‚è§£çš„æ±‚å’Œæˆ–è€…ç§¯åˆ†é—®é¢˜ã€‚æ¯”å¦‚ç§¯åˆ†ï¼š$\theta = \int_a^b f(x)dx$ æˆ–è€…ä¼°è®¡$\pi$å€¼æˆ–åœ†çš„é¢ç§¯ï¼ˆç§¯åˆ†ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è’™ç‰¹å¡ç½—æ–¹æ³•æ¥æ¨¡æ‹Ÿæ±‚è§£è¿‘ä¼¼å€¼ã€‚å¦‚ä½•æ¨¡æ‹Ÿå‘¢ï¼Ÿ

**éšæœºé‡‡æ ·**æŒ‡ä»ç»™å®šæ¦‚ç‡å¯†åº¦å‡½æ•° ğ‘(ğ‘¥) ä¸­æŠ½å–å‡ºç¬¦åˆå…¶æ¦‚ç‡åˆ†å¸ƒçš„æ ·æœ¬ï¼

éšæœºé‡‡æ · é‡‡æ ·æ³•çš„éš¾ç‚¹æ˜¯å¦‚ä½•è¿›è¡Œéšæœºé‡‡æ ·ï¼Œå³å¦‚ä½•è®©è®¡ç®—æœºç”Ÿæˆæ»¡è¶³æ¦‚ç‡å¯†åº¦å‡½æ•° ğ‘(ğ‘¥) çš„æ ·æœ¬ï¼æˆ‘ä»¬çŸ¥é“ï¼Œè®¡ç®—æœºå¯ä»¥æ¯”è¾ƒå®¹æ˜“åœ°éšæœºç”Ÿæˆä¸€ä¸ªåœ¨ [0, 1]åŒºé—´ä¸Šå‡å¸ƒåˆ†å¸ƒçš„æ ·æœ¬ ğœ‰ï¼å¦‚æœè¦éšæœºç”Ÿæˆæœä»æŸä¸ªéå‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬ï¼Œå°±éœ€è¦ä¸€äº›é—´æ¥çš„é‡‡æ ·æ–¹æ³•ï¼
å¦‚æœä¸€ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º ğ‘(ğ‘¥)ï¼Œå…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•° cdf(ğ‘¥) ä¸ºè¿ç»­çš„ä¸¥æ ¼å¢å‡½æ•°ï¼Œä¸”å­˜åœ¨é€†å‡½æ•°$cdf^{âˆ’1}(ğ‘¦), ğ‘¦ âˆˆ [0, 1]$ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åˆ©ç”¨**ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†å‡½æ•°**ï¼ˆinverse CDFï¼‰æ¥ç”Ÿæˆæœä»è¯¥éšæœºåˆ†å¸ƒçš„æ ·æœ¬ï¼å‡è®¾ ğœ‰ æ˜¯ [0, 1] åŒºé—´ä¸Šå‡åŒ€åˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œåˆ™ $cdf^{âˆ’1}(\xi)$ æœä»æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºğ‘(ğ‘¥)çš„åˆ†å¸ƒï¼ä½†å½“ ğ‘(ğ‘¥) éå¸¸å¤æ‚ï¼Œå…¶ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†å‡½æ•°éš¾ä»¥è®¡ç®—ï¼Œæˆ–è€…ä¸çŸ¥é“ ğ‘(ğ‘¥)çš„ç²¾ç¡®å€¼ï¼ŒåªçŸ¥é“æœªå½’ä¸€åŒ–çš„åˆ†å¸ƒ Ì‚ğ‘(ğ‘¥)æ—¶ï¼Œå°±éš¾ä»¥ç›´æ¥å¯¹ğ‘(ğ‘¥)è¿›è¡Œé‡‡æ ·ï¼Œå¾€å¾€éœ€è¦ä½¿ç”¨ä¸€äº›é—´æ¥çš„é‡‡æ ·ç­–ç•¥ï¼Œæ¯”å¦‚**æ‹’ç»é‡‡æ ·ã€é‡è¦æ€§é‡‡æ ·ã€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—é‡‡æ ·**ç­‰ï¼è¿™äº›æ–¹æ³•ä¸€èˆ¬æ˜¯å…ˆæ ¹æ®ä¸€ä¸ªæ¯”è¾ƒå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œç„¶åé€šè¿‡ä¸€äº›ç­–ç•¥æ¥é—´æ¥å¾—åˆ°ç¬¦åˆğ‘(ğ‘¥)åˆ†å¸ƒçš„æ ·æœ¬ï¼

> rejection sampling, inverse CDF, Box-Muller,  Ziggurat algorithm

**æ‹’ç»é‡‡æ ·**ï¼ˆRejection Samplingï¼‰æ˜¯ä¸€ç§é—´æ¥é‡‡æ ·æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºæ¥å—-æ‹’ç»é‡‡æ ·ï¼ˆAcceptance-Rejection Samplingï¼‰ï¼
å‡è®¾åŸå§‹åˆ†å¸ƒğ‘(ğ‘¥)éš¾ä»¥ç›´æ¥é‡‡æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒğ‘(ğ‘¥)ï¼Œä¸€èˆ¬ç§°ä¸ºæè®®åˆ†å¸ƒï¼ˆProposal Distributionï¼‰ï¼Œç„¶åä»¥æŸä¸ªæ ‡å‡†æ¥æ‹’ç»ä¸€éƒ¨åˆ†çš„æ ·æœ¬ä½¿å¾—æœ€ç»ˆé‡‡é›†çš„æ ·æœ¬æœä»åˆ†å¸ƒ ğ‘(ğ‘¥)ã€‚æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæè®®åˆ†å¸ƒ ğ‘(ğ‘¥) å’Œä¸€ä¸ªå¸¸æ•° ğ‘˜ï¼Œä½¿å¾— ğ‘˜ğ‘(ğ‘¥) å¯ä»¥è¦†ç›–å‡½æ•°ğ‘(ğ‘¥)ï¼Œå³ğ‘˜ğ‘(ğ‘¥) â‰¥ ğ‘(ğ‘¥),
![](./img/1042406-20170327143755811-993574578.png)
å¯¹äºæ¯æ¬¡æŠ½å–çš„æ ·æœ¬ $\^{x}$ è®¡ç®—æ¥å—æ¦‚ç‡ï¼ˆAcceptance Probabilityï¼‰ï¼š$\alpha(\^{x}) = \frac{p(\^{x})}{kq(\^{x})}$ï¼Œå¹¶ä»¥æ¦‚ç‡$\alpha(\^{x})$)æ¥æ¥å—æ ·æœ¬ $\^{x}$

æ‹’ç»é‡‡æ ·çš„é‡‡æ ·è¿‡ç¨‹
```
è¾“å…¥: æè®®åˆ†å¸ƒğ‘(ğ‘¥)ï¼Œå¸¸æ•°ğ‘˜ï¼Œæ ·æœ¬é›†åˆğ’± = âˆ…;
1 repeat
2   æ ¹æ®ğ‘(ğ‘¥)éšæœºç”Ÿæˆä¸€ä¸ªæ ·æœ¬ ;Ì‚ğ‘¥
3   è®¡ç®—æ¥å—æ¦‚ç‡ğ›¼( Ì‚ğ‘¥);
4   ä»(0, 1)çš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºç”Ÿæˆä¸€ä¸ªå€¼ğ‘§;
5   if ğ‘§ â‰¤ ğ›¼( Ì‚ğ‘¥) then // ä»¥ğ›¼(ğ‘¥)Ì‚ çš„æ¦‚ç‡æ¥å—ğ’™Ì‚
6       ğ’± = ğ’± âˆª { Ì‚ğ‘¥};
7   end
8 until è·å¾— ğ‘ ä¸ªæ ·æœ¬(|ğ’±| = ğ‘);
è¾“å‡º: æ ·æœ¬é›†åˆğ’±
```

åˆ¤æ–­ä¸€ä¸ªæ‹’ç»é‡‡æ ·æ–¹æ³•çš„å¥½åå°±æ˜¯çœ‹å…¶é‡‡æ ·æ•ˆç‡ï¼Œå³æ€»ä½“çš„æ¥å—ç‡ï¼å¦‚æœå‡½æ•°ğ‘˜ğ‘(ğ‘¥)è¿œå¤§äºåŸå§‹åˆ†å¸ƒå‡½æ•° Ì‚ğ‘(ğ‘¥)ï¼Œæ‹’ç»ç‡ä¼šæ¯”è¾ƒé«˜ï¼Œé‡‡æ ·æ•ˆç‡ä¼šéå¸¸ä¸ç†æƒ³ï¼ä½†è¦æ‰¾åˆ°ä¸€ä¸ªå’Œ Ì‚ğ‘(ğ‘¥)æ¯”è¾ƒæ¥è¿‘çš„æè®®åˆ†å¸ƒå¾€å¾€æ¯”è¾ƒå›°éš¾ï¼ç‰¹åˆ«æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œå…¶é‡‡æ ·ç‡ä¼šéå¸¸ä½ï¼Œå¯¼è‡´å¾ˆéš¾åº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­ï¼

**é‡è¦æ€§é‡‡æ ·**ï¼ˆImportance samplingï¼‰
å¦‚æœé‡‡æ ·çš„ç›®çš„æ˜¯è®¡ç®—åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹å‡½æ•°ğ‘“(ğ‘¥)çš„æœŸæœ›ï¼Œé‚£ä¹ˆå®é™…ä¸ŠæŠ½å–çš„æ ·æœ¬ä¸éœ€è¦ä¸¥æ ¼æœä»åˆ†å¸ƒ ğ‘(ğ‘¥)ï¼ä¹Ÿå¯ä»¥é€šè¿‡å¦ä¸€ä¸ªåˆ†å¸ƒï¼Œå³æè®®åˆ†å¸ƒ ğ‘(ğ‘¥)ï¼Œç›´æ¥é‡‡æ ·å¹¶ä¼°è®¡ğ”¼ğ‘[ğ‘“(ğ‘¥)]ï¼
å‡½æ•°ğ‘“(ğ‘¥)åœ¨åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹çš„æœŸæœ›å¯ä»¥å†™ä¸º
$$E_p[f(x)] = \int_x f(x)p(x)dx = \int_x f(x)\frac{p(x)}{q(x)}q(x)dx = \int_x f(x)w(x)q(x)dx = E_q[f(x)w(x)]$$
å…¶ä¸­ğ‘¤(ğ‘¥)ç§°ä¸ºé‡è¦æ€§æƒé‡ï¼

é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Samplingï¼‰æ˜¯é€šè¿‡å¼•å…¥é‡è¦æ€§æƒé‡ï¼Œå°†åˆ†å¸ƒ ğ‘(ğ‘¥)ä¸‹ğ‘“(ğ‘¥)çš„æœŸæœ›å˜ä¸ºåœ¨åˆ†å¸ƒğ‘(ğ‘¥)ä¸‹ğ‘“(ğ‘¥)ğ‘¤(ğ‘¥)çš„æœŸæœ›ï¼Œä»è€Œå¯ä»¥è¿‘ä¼¼ä¸º
$$E_p[f(x)] = E_q[f(x)w(x)] =\frac{1}{N} \sum_{i=1}^N f(x_i)w(x_i) =\frac{1}{N} \sum_{i=1}^N f(x_i)\frac{p(x_i)}{q(x_i)}$$
å…¶ä¸­$\{x_1,...,x_N\}$æ˜¯ç‹¬ç«‹ä»$q(x)$ä¸­éšæœºé‡‡æ ·æ¥çš„ï¼ˆp(x)æ˜¯å·²çŸ¥çš„ï¼Œåªæ˜¯ä¸å¥½é‡‡æ ·ï¼Œä½†æ˜¯èƒ½è®¡ç®—ï¼‰ã€‚

**é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³•**
åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ‹’ç»é‡‡æ ·å’Œé‡è¦æ€§é‡‡æ ·çš„æ•ˆç‡éšç©ºé—´ç»´æ•°çš„å¢åŠ è€ŒæŒ‡æ•°é™ä½ï¼é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ï¼ˆMarkov Chain Monte Carloï¼ŒMCMCï¼‰æ–¹æ³•æ˜¯ä¸€ç§æ›´å¥½çš„é‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°å¯¹é«˜ç»´å˜é‡è¿›è¡Œé‡‡æ ·ï¼

å‡è®¾å¤šå…ƒéšæœºå˜é‡$x$,æ»¡è¶³$x \in \mathcal{X}$,å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º$p(x)$ï¼›$f(x)$ä¸ºå®šä¹‰åœ¨$x \in \mathcal{X}$ä¸Šçš„å‡½æ•°ï¼Œç›®æ ‡æ˜¯è·å¾—æ¦‚ç‡åˆ†å¸ƒ$p(x)$çš„æ ·æœ¬é›†åˆä»¥åŠæ±‚å‡½æ•°$f(x)$çš„æ•°å­¦æœŸæœ›$E_{p(x)}[f(x)]$ã€‚

åº”ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•è§£å†³è¿™ä¸ªé—®é¢˜çš„**åŸºæœ¬æƒ³æ³•**æ˜¯ï¼š
åœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\mathcal{S}$ä¸Šå®šä¹‰ä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å°”å¯å¤«é“¾$X=\{X_0,X_1,...,X_t,...\}$,ä½¿å…¶å¹³ç¨³åˆ†å¸ƒå°±æ˜¯æŠ½æ ·çš„çš„ç›®æ ‡åˆ†å¸ƒ$p(x)$ï¼ˆè®¾è®¡ä¸€ä¸ªéšæœºçŸ©é˜µï¼ˆè¿ç»­éšæœºå˜é‡å°±æ˜¯è½¬ç§»æ ¸å‡½æ•°ï¼‰ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒç­‰äºç›®æ ‡åˆ†å¸ƒï¼‰ã€‚ç„¶ååœ¨è¿™ä¸ªé©¬å°”å¯å¤«é“¾ä¸Šè¿›è¡Œéšæœºæ¸¸èµ°ï¼Œæ¯ä¸ªæ—¶åˆ»å¾—åˆ°ä¸€ä¸ªæ ·æœ¬ã€‚æ ¹æ®éå†å®šç†ï¼Œå½“æ—¶é—´è¶‹äºæ— ç©·æ—¶ï¼Œæ ·æœ¬çš„åˆ†å¸ƒè¶‹è¿‘å¹³ç¨³åˆ†å¸ƒï¼Œæ ·æœ¬çš„å‡½æ•°å‡å€¼è¶‹è¿‘å‡½æ•°çš„æ•°å­¦æœŸæœ›ã€‚æ‰€ä»¥ï¼Œå½“æ—¶é—´è¶³å¤Ÿé•¿æ—¶ï¼ˆæ—¶åˆ»å¤§äºæŸä¸ªæ­£æ•´æ•°mï¼‰ï¼Œåœ¨ä¹‹åçš„æ—¶é—´ï¼ˆæ—¶åˆ»å°äºç­‰äºæŸä¸ªæ­£æ•´æ•°n,n>mï¼‰é‡Œéšæœºæ¸¸èµ°å¾—åˆ°çš„æ ·æœ¬é›†åˆ$\left\{x_{m+1}, x_{m+2}, \cdots, x_{n}\right\}$å°±æ˜¯ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒçš„æŠ½æ ·ç»“æœï¼Œå¾—åˆ°çš„å‡½æ•°å‡å€¼å°±æ˜¯è¦è®¡ç®—çš„æ•°å­¦æœŸæœ›å€¼ï¼š
$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left(x_{i}\right)$$
ä»æ—¶åˆ»0åˆ°æ—¶åˆ»mä¸ºæ­¢çš„è¿™æ®µæ—¶é—´ç§°ä¸ºç‡ƒçƒ§æœŸï¼ˆBurn-in Periodï¼‰ã€‚ç‡ƒçƒ§æœŸçš„æ ·æœ¬ä¹Ÿæ˜¯è¦è¢«ä¸¢å¼ƒçš„ã€‚

**åŸºæœ¬æ­¥éª¤**
1. é¦–å…ˆï¼Œåœ¨éšæœºå˜é‡$x$çš„çŠ¶æ€ç©ºé—´$\mathcal{S}$ä¸Šæ„é€ ä¸€ä¸ªæ»¡è¶³éå†å®šç†çš„é©¬å°”å¯å¤«é“¾ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒä¸ºç›®æ ‡åˆ†å¸ƒ$p(x)$;
1. ä»çŠ¶æ€ç©ºé—´çš„æŸä¸€ç‚¹$x_0$å‡ºå‘ï¼Œç”¨æ„é€ çš„é©¬å°”å¯å¤«é“¾è¿›è¡Œéšæœºæ¸¸èµ°ï¼Œäº§ç”Ÿæ ·æœ¬åºåˆ—$x_0,x_1,...,x_t,...$ã€‚
1. åº”ç”¨é©¬å°”å¯å¤«é“¾çš„éå†åŸç†ï¼Œç¡®å®šæ­£æ•´æ•°må’Œnï¼ˆm < nï¼‰ï¼Œå¾—åˆ°æ ·æœ¬é›†åˆ$\{x_{m+1},x_{m+2},...,x_{n}\}$ï¼Œæ±‚å¾—å‡½æ•°$f(x)$çš„å‡å€¼ï¼ˆéå†å‡å€¼ï¼‰
$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left(x_{i}\right)$$

è¿™é‡Œæœ‰å‡ ä¸ª**é‡è¦é—®é¢˜**
1. å¦‚ä½•ç¬¬ä¸€é©¬å°”å¯å¤«é“¾ï¼Œä¿è¯é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ³•çš„æ¡ä»¶æˆç«‹ã€‚
1. å¦‚ä½•ç¡®å®šæ”¶æ•›æ­¥æ•°$m$ï¼Œä¿è¯æ ·æœ¬æŠ½æ ·çš„æ— åæ€§ã€‚
1. å¦‚ä½•ç¡®å®šè¿­ä»£æ­¥æ•°$n$ï¼Œä¿è¯éå†å‡å€¼è®¡ç®—çš„ç²¾åº¦ã€‚

**MCMCé‡‡æ ·**
ç”±äºä¸€èˆ¬æƒ…å†µä¸‹ï¼Œç›®æ ‡å¹³ç¨³åˆ†å¸ƒ $\pi(x)$ å’ŒæŸä¸€ä¸ªé©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ ä¸æ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼Œå³ï¼š

$$\pi(i)Q(i,j)\neq\pi(j)Q(j,i)$$

**è¿™äº›è®°å·è¡¨è¾¾åŒä¸€ä¸ªæ„æ€**ï¼š $Q(i,j)\Leftrightarrow Q(j|i)\Leftrightarrow Q(i\rightarrow j)$ ï¼ˆçŠ¶æ€è½¬ç§»åˆ†å¸ƒæˆ–æè®®åˆ†å¸ƒï¼‰

æˆ‘ä»¬å¼•å…¥ä¸€ä¸ª $\alpha(i,j)$ ,ä½¿ä¸Šå¼å¯ä»¥å–ç­‰å·ã€‚

$$\pi(i)Q(i,j)\alpha(i,j)=\pi(j)Q(j,i)\alpha(j,i)$$

æ€æ ·æ‰èƒ½æˆç«‹å‘¢ï¼Œå…¶å®å¾ˆç®€å•ï¼ŒæŒ‰ç…§å¯¹ç§°æ€§ï¼š

$$\alpha(i,j)=\pi(j)Q(j,i); \pi(i)Q(i,j)=\alpha(j,i)$$

ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°äº†åˆ†å¸ƒ $\pi(x)$ å¯¹è®©é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $P(i,j)=Q(i,j)\alpha(i,j)$

å…¶ä¸­ $\alpha(i,j)$ ä¸€èˆ¬ç§°ä¹‹ä¸ºæ¥å—ç‡ï¼Œå–å€¼åœ¨ $[0,1]$ ä¹‹é—´ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ¦‚ç‡å€¼ã€‚è¿™å¾ˆåƒæ¥å—-æ‹’ç»é‡‡æ ·ï¼Œé‚£é‡Œæ˜¯ä»¥ä¸€ä¸ªå¸¸ç”¨åˆ†å¸ƒé€šè¿‡ä¸€å®šçš„æ¥å—-æ‹’ç»æ¦‚ç‡å¾—åˆ°ä¸€ä¸ªéå¸¸è§åˆ†å¸ƒï¼Œ è¿™é‡Œæ˜¯ä»¥ä¸€ä¸ªå¸¸è§çš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ é€šè¿‡ä¸€å®šçš„æ¥å—-æ‹’ç»æ¦‚ç‡å¾—åˆ°ç›®æ ‡è½¬ç§»çŸ©é˜µ $P$ ,ä¸¤è€…çš„è§£å†³é—®é¢˜æ€è·¯æ˜¯ç±»ä¼¼çš„ã€‚

MCMCé‡‡æ ·ç®—æ³•å¦‚ä¸‹ï¼š
1. è¾“å…¥ä»»æ„ç»™å®šçš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ $Q$ ï¼Œç›®æ ‡å¹³ç¨³åˆ†å¸ƒ $\pi(x)$ ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼ $n_1$ ï¼Œéœ€è¦çš„æ ·æœ¬æ•° $n_2$;
1. ä»ä»»æ„ç®€å•æ¦‚ç‡åˆ†å¸ƒå¾—åˆ°åˆå§‹çŠ¶æ€å€¼ $x_0$ ï¼›
1. for $t=0 ~in ~n_{1}+n_{2}-1$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ $Q(x|x_{t})$ å¾—åˆ°æ ·æœ¬å€¼ $x_{*}$
    1. ä»å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ · $u\sim uniform[0,1]$
    1. å¦‚æœ $u<\alpha(x_{t},x_{*})=\pi(x_{*})Q(x_{*},x_{t})$ ï¼Œåˆ™æ¥å— $x_{t}\rightarrow x_{*}$ ï¼Œå³ $x_{t+1}= x_{*}$
    1. å¦åˆ™ä¸æ¥å—è½¬ç§»ï¼Œ å³$x_{t+1}= x_{t}$

ä¸Šè¿°è¿‡ç¨‹ä¸­ $p(x),q(x|y)$ è¯´çš„éƒ½æ˜¯ç¦»æ•£çš„æƒ…å½¢ï¼Œäº‹å®ä¸Šå³ä¾¿è¿™ä¸¤ä¸ªåˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œä»¥ä¸Šç®—æ³•ä»ç„¶æ˜¯æœ‰æ•ˆï¼Œäºæ˜¯å°±å¾—åˆ°æ›´ä¸€èˆ¬çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒ $p(x)$ çš„é‡‡æ ·ç®—æ³•ï¼Œè€Œ $q(x|y)$ å°±æ˜¯ä»»æ„ä¸€ä¸ªè¿ç»­äºŒå…ƒæ¦‚ç‡åˆ†å¸ƒå¯¹åº”çš„æ¡ä»¶åˆ†å¸ƒã€‚

ä½†æ˜¯è¿™ä¸ªé‡‡æ ·ç®—æ³•è¿˜æ˜¯æ¯”è¾ƒéš¾åœ¨å®é™…ä¸­åº”ç”¨ï¼Œå› ä¸ºåœ¨ç¬¬ä¸‰æ­¥ä¸­ï¼Œç”±äº $\alpha(x_{t},x_{*})$ å¯èƒ½éå¸¸çš„å°ï¼Œæ¯”å¦‚0.1ï¼Œå¯¼è‡´æˆ‘ä»¬å¤§éƒ¨åˆ†çš„é‡‡æ ·å€¼éƒ½è¢«æ‹’ç»è½¬ç§»ï¼Œé‡‡æ ·æ•ˆç‡å¾ˆä½ã€‚æœ‰å¯èƒ½æˆ‘ä»¬é‡‡æ ·äº†ä¸Šç™¾ä¸‡æ¬¡é©¬å°”å¯å¤«é“¾è¿˜æ²¡æœ‰æ”¶æ•›ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢è¿™ä¸ª $n_1$ è¦éå¸¸éå¸¸çš„å¤§ï¼Œè¿™è®©äººéš¾ä»¥æ¥å—ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿè¿™æ—¶å°±è½®åˆ°æˆ‘ä»¬çš„M-Hé‡‡æ ·å‡ºåœºäº†ã€‚

**Metropolis-Hastingsç®—æ³•**ï¼š
M-Hé‡‡æ ·æ˜¯Metropolis-Hastingsé‡‡æ ·çš„ç®€ç§°ï¼Œè¿™ä¸ªç®—æ³•é¦–å…ˆç”±Metropolisæå‡ºï¼Œè¢«Hastingsæ”¹è¿›ï¼Œå› æ­¤è¢«ç§°ä¹‹ä¸ºMetropolis-Hastingsé‡‡æ ·æˆ–M-Hé‡‡æ ·ã€‚

> ä¹¦ä¸­çš„ä»‹ç»æ›´è¯¦ç»†ï¼Œè¿™é‡Œç®€å•ä»‹ç»åŸç†

æˆ‘ä»¬å›åˆ°MCMCé‡‡æ ·çš„ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼š$\pi(i)Q(i,j)\alpha(i,j) = \pi(j)Q(j,i)\alpha(j,i)$
æˆ‘ä»¬é‡‡æ ·æ•ˆç‡ä½çš„åŸå› æ˜¯$\alpha(i,j)$å¤ªå°äº†ï¼Œæ¯”å¦‚ä¸º0.1ï¼Œè€Œ$\alpha(j,i)$ä¸º0.2ã€‚å³ï¼š$\pi(i)Q(i,j)\times 0.1 = \pi(j)Q(j,i)\times 0.2$
è¿™æ—¶æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœä¸¤è¾¹åŒæ—¶æ‰©å¤§äº”å€ï¼Œæ¥å—ç‡æé«˜åˆ°äº†0.5ï¼Œä½†æ˜¯ç»†è‡´å¹³ç¨³æ¡ä»¶å´ä»ç„¶æ˜¯æ»¡è¶³çš„ï¼Œå³ï¼š$\pi(i)Q(i,j)\times 0.5 = \pi(j)Q(j,i)\times 1$
è¿™æ ·æˆ‘ä»¬çš„æ¥å—ç‡å¯ä»¥åšå¦‚ä¸‹æ”¹è¿›ï¼Œå³ï¼š$\alpha(i,j) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}$

æ‰€ä»¥ä¿®æ­£è½¬ç§»æ¦‚ç‡çš„$\hat{Q}(i,j) = Q(i,j)\alpha(i,j)$ï¼Œå¹¶ä¸”å¹³ç¨³åˆ†å¸ƒå°±æ˜¯$\pi(x)$

**Metropolisç®—æ³•**ä¸­çš„æè®®åˆ†å¸ƒæ˜¯å¯¹ç§°çš„(å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬é€‰æ‹©å¯¹ç§°çš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ$Q$)ï¼Œå³$Q(i,j)=Q(j,i)$ï¼Œæ‰€ä»¥æ¥å—æ¦‚ç‡$\alpha(i,j) = min\{ \frac{\pi(j)}{\pi(i)},1\}$

M-Hé‡‡æ ·ç®—æ³•è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è¾“å…¥æˆ‘ä»¬ä»»æ„é€‰å®šçš„é©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ$Q$ï¼Œå¹³ç¨³åˆ†å¸ƒ$\pi(x)$ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼$n_1$ï¼Œéœ€è¦çš„æ ·æœ¬ä¸ªæ•°$n_2$
1. ä»ä»»æ„ç®€å•æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·å¾—åˆ°åˆå§‹çŠ¶æ€å€¼$x_0$
1. for $t=0 ~to ~n_1+n_2âˆ’1$: 
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$Q(x|x_t)$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_âˆ—$
    1. ä»å‡åŒ€åˆ†å¸ƒé‡‡æ ·$uâˆ¼uniform[0,1]$
    1. å¦‚æœ$u < \alpha(x_t,x_{*}) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}$, åˆ™æ¥å—è½¬ç§»$x_t \to x_{*}$ï¼Œå³$x_{t+1}=x_âˆ—$
    1. å¦åˆ™ä¸æ¥å—è½¬ç§»ï¼Œå³$x_{t+1}=x_t$


æ ·æœ¬é›†$(x_{n_1}, x_{n_1+1},..., x_{n_1+n_2-1})$å³ä¸ºæˆ‘ä»¬éœ€è¦çš„å¹³ç¨³åˆ†å¸ƒå¯¹åº”çš„æ ·æœ¬é›†ã€‚


M-Hé‡‡æ ·å®Œæ•´è§£å†³äº†ä½¿ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•éœ€è¦çš„ä»»æ„æ¦‚ç‡åˆ†å¸ƒæ ·æœ¬é›†çš„é—®é¢˜ï¼Œå› æ­¤åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚
ä½†æ˜¯åœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œ**M-Hé‡‡æ ·é¢ä¸´ç€ä¸¤å¤§éš¾é¢˜**ï¼š
1. æˆ‘ä»¬çš„æ•°æ®ç‰¹å¾éå¸¸çš„å¤šï¼ŒM-Hé‡‡æ ·ç”±äºæ¥å—ç‡è®¡ç®—å¼$\frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)}$çš„å­˜åœ¨ï¼Œåœ¨é«˜ç»´æ—¶éœ€è¦çš„è®¡ç®—æ—¶é—´éå¸¸çš„å¯è§‚ï¼Œç®—æ³•æ•ˆç‡å¾ˆä½ã€‚åŒæ—¶$\alpha(i,j)$ä¸€èˆ¬å°äº1ï¼Œæœ‰æ—¶å€™è¾›è‹¦è®¡ç®—å‡ºæ¥å´è¢«æ‹’ç»äº†ã€‚èƒ½ä¸èƒ½åšåˆ°ä¸æ‹’ç»è½¬ç§»å‘¢ï¼Ÿ
1. ç”±äºç‰¹å¾ç»´åº¦å¤§ï¼Œå¾ˆå¤šæ—¶å€™æˆ‘ä»¬ç”šè‡³å¾ˆéš¾æ±‚å‡ºç›®æ ‡çš„å„ç‰¹å¾ç»´åº¦è”åˆåˆ†å¸ƒï¼Œä½†æ˜¯å¯ä»¥æ–¹ä¾¿æ±‚å‡ºå„ä¸ªç‰¹å¾ä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚è¿™æ—¶å€™æˆ‘ä»¬èƒ½ä¸èƒ½åªæœ‰å„ç»´åº¦ä¹‹é—´æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„æƒ…å†µä¸‹æ–¹ä¾¿çš„é‡‡æ ·å‘¢ï¼Ÿ

**å‰å¸ƒæ–¯é‡‡æ ·**ï¼ˆGibbs Samplingï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆåœ°å¯¹é«˜ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·çš„ MCMC æ–¹æ³•ï¼Œå¯ä»¥çœ‹ä½œ Metropolis-Hastings ç®—æ³•çš„ç‰¹ä¾‹ï¼å‰å¸ƒæ–¯é‡‡æ ·ä½¿ç”¨å…¨æ¡ä»¶æ¦‚ç‡ï¼ˆFull Conditional Probabilityï¼‰ä½œä¸ºæè®®åˆ†å¸ƒæ¥ä¾æ¬¡å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œé‡‡æ ·ï¼Œå¹¶è®¾ç½®æ¥å—ç‡ä¸º$\alpha = 1$ï¼

å‰é¢è®²åˆ°äº†ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼šå¦‚æœéå‘¨æœŸé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$P$å’Œæ¦‚ç‡åˆ†å¸ƒ$\pi(x)$å¯¹äºæ‰€æœ‰çš„$i,j$æ»¡è¶³ï¼š$\pi(i)P(i,j) = \pi(j)P(j,i)$ï¼Œåˆ™ç§°æ¦‚ç‡åˆ†å¸ƒ$\pi(x)$æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µ$P$çš„å¹³ç¨³åˆ†å¸ƒã€‚

åœ¨M-Hé‡‡æ ·ä¸­æˆ‘ä»¬é€šè¿‡å¼•å…¥æ¥å—ç‡ä½¿ç»†è‡´å¹³ç¨³æ¡ä»¶æ»¡è¶³ã€‚ç°åœ¨æˆ‘ä»¬æ¢ä¸€ä¸ªæ€è·¯ã€‚
ä»äºŒç»´çš„æ•°æ®åˆ†å¸ƒå¼€å§‹ï¼Œå‡è®¾$\pi(x_1,x_2)$æ˜¯ä¸€ä¸ªäºŒç»´è”åˆæ•°æ®åˆ†å¸ƒï¼Œè§‚å¯Ÿç¬¬ä¸€ä¸ªç‰¹å¾ç»´åº¦ç›¸åŒçš„ä¸¤ä¸ªç‚¹$A(x_1^{(1)},x_2^{(1)})$å’Œ$B(x_1^{(1)},x_2^{(2)})$ï¼Œå®¹æ˜“å‘ç°ä¸‹é¢ä¸¤å¼æˆç«‹(å°±æ˜¯è”åˆæ¦‚ç‡å…¬å¼ï¼Œä»”ç»†çœ‹ï¼Œå¾ˆç®€å•)ï¼š
$$\pi(x_1^{(1)},x_2^{(1)}) \pi(x_2^{(2)} | x_1^{(1)}) = \pi(x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)}) \pi(x_2^{(2)} | x_1^{(1)})$$
$$\pi(x_1^{(1)},x_2^{(2)}) \pi(x_2^{(1)} | x_1^{(1)}) = \pi(x_1^{(1)}) \pi(x_2^{(2)} | x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)})$$
ç”±äºä¸¤å¼çš„å³è¾¹ç›¸ç­‰ï¼Œå› æ­¤æˆ‘ä»¬æœ‰ï¼š
$$\pi(x_1^{(1)},x_2^{(1)}) \pi(x_2^{(2)} | x_1^{(1)})  = \pi(x_1^{(1)},x_2^{(2)}) \pi(x_2^{(1)} | x_1^{(1)})$$
ä¹Ÿå°±æ˜¯ï¼š
$$\pi(A) \pi(x_2^{(2)} | x_1^{(1)})  = \pi(B) \pi(x_2^{(1)} | x_1^{(1)})$$
è§‚å¯Ÿä¸Šå¼å†è§‚å¯Ÿç»†è‡´å¹³ç¨³æ¡ä»¶çš„å…¬å¼ï¼Œæˆ‘ä»¬å‘ç°åœ¨$x_1 = x_1^{(1)}$è¿™æ¡ç›´çº¿ä¸Šï¼Œå¦‚æœç”¨æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$\pi(x_2| x_1^{(1)})$ä½œä¸ºé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œåˆ™ä»»æ„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è½¬ç§»æ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ï¼è¿™çœŸæ˜¯ä¸€ä¸ªå¼€å¿ƒçš„å‘ç°ï¼ŒåŒæ ·çš„é“ç†ï¼Œåœ¨åœ¨$x_2 = x_2^{(1)}$è¿™æ¡ç›´çº¿ä¸Šï¼Œå¦‚æœç”¨æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$\pi(x_1| x_2^{(1)})$ä½œä¸ºé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œåˆ™ä»»æ„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è½¬ç§»ä¹Ÿæ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶ã€‚é‚£æ˜¯å› ä¸ºå‡å¦‚æœ‰ä¸€ç‚¹$C(x_1^{(2)},x_2^{(1)})$,æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š
$$\pi(A) \pi(x_1^{(2)} | x_2^{(1)})  = \pi(C) \pi(x_1^{(1)} | x_2^{(1)})$$
åŸºäºä¸Šé¢çš„å‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æ„é€ åˆ†å¸ƒ$\pi(x_1,x_2)$çš„é©¬å°”å¯å¤«é“¾å¯¹åº”çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$P$ï¼š
$$P(A \to B) = \pi(x_2^{(B)}|x_1^{(1)})\;\; if\; x_1^{(A)} = x_1^{(B)} =x_1^{(1)} \\ P(A \to C) = \pi(x_1^{(C)}|x_2^{(1)})\;\; if\; x_2^{(A)} = x_2^{(C)} =x_2^{(1)} \\ P(A \to D) = 0\;\; else$$
äºæ˜¯è¿™ä¸ªäºŒç»´ç©ºé—´ä¸Šçš„é©¬æ°é“¾å°†æ”¶æ•›åˆ°å¹³ç¨³åˆ†å¸ƒ$\pi(x,y)$

äºŒç»´Gibbsé‡‡æ ·ï¼Œè¿™ä¸ªé‡‡æ ·éœ€è¦ä¸¤ä¸ªç»´åº¦ä¹‹é—´çš„æ¡ä»¶æ¦‚ç‡ã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è¾“å…¥å¹³ç¨³åˆ†å¸ƒ$\pi(x_1,x_2)$ï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼$n_1$ï¼Œéœ€è¦çš„æ ·æœ¬ä¸ªæ•°$n_2$
1. éšæœºåˆå§‹åŒ–åˆå§‹çŠ¶æ€å€¼$x_1^{(0)}$å’Œ$x_2^{(0)}$
1. for $t=0 ~to ~n_1 +n_2-1$: 
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_2|x_1^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_2^{t+1}$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_1|x_2^{(t+1)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_1^{t+1}$

æ ·æœ¬é›†$\{(x_1^{(n_1)}, x_2^{(n_1)}), (x_1^{(n_1+1)}, x_2^{(n_1+1)}), ...,  (x_1^{(n_1+n_2-1)}, x_2^{(n_1+n_2-1)})\}$å³ä¸ºæˆ‘ä»¬éœ€è¦çš„å¹³ç¨³åˆ†å¸ƒå¯¹åº”çš„æ ·æœ¬é›†ã€‚


æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è½®æ¢åæ ‡è½´ï¼Œé‡‡æ ·çš„è¿‡ç¨‹ä¸ºï¼š
$$(x_1^{(1)}, x_2^{(1)}) \to  (x_1^{(1)}, x_2^{(2)}) \to (x_1^{(2)}, x_2^{(2)}) \to ... \to (x_1^{(n_1+n_2-1)}, x_2^{(n_1+n_2-1)})$$

é©¬æ°é“¾æ”¶æ•›åï¼Œæœ€ç»ˆå¾—åˆ°çš„æ ·æœ¬å°±æ˜¯ $p(x_1,x_2)$çš„æ ·æœ¬ï¼Œè€Œæ”¶æ•›ä¹‹å‰çš„é˜¶æ®µç§°ä¸º burn-in periodã€‚é¢å¤–è¯´æ˜ä¸€ä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°æ•™ç§‘ä¹¦ä¸Šçš„ Gibbs Sampling ç®—æ³•å¤§éƒ½æ˜¯åæ ‡è½´è½®æ¢é‡‡æ ·çš„ï¼Œä½†æ˜¯è¿™å…¶å®æ˜¯ä¸å¼ºåˆ¶è¦æ±‚çš„ã€‚æœ€ä¸€èˆ¬çš„æƒ…å½¢å¯ä»¥æ˜¯ï¼Œåœ¨ $t$ æ—¶åˆ»ï¼Œå¯ä»¥åœ¨ $x_1$ è½´å’Œ $x_2$ è½´ä¹‹é—´éšæœºçš„é€‰ä¸€ä¸ªåæ ‡è½´ï¼Œç„¶åæŒ‰æ¡ä»¶æ¦‚ç‡åšè½¬ç§»ï¼Œé©¬æ°é“¾ä¹Ÿæ˜¯ä¸€æ ·æ”¶æ•›çš„ã€‚è½®æ¢ä¸¤ä¸ªåæ ‡è½´åªæ˜¯ä¸€ç§æ–¹ä¾¿çš„å½¢å¼ã€‚

ä¸Šé¢çš„è¿™ä¸ªç®—æ³•æ¨å¹¿åˆ°å¤šç»´çš„æ—¶å€™ä¹Ÿæ˜¯æˆç«‹çš„ã€‚æ¯”å¦‚ä¸€ä¸ª$n$ç»´çš„æ¦‚ç‡åˆ†å¸ƒ$\pi(x_1,x_2,...x_n)$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨$n$ä¸ªåæ ‡è½´ä¸Šè½®æ¢é‡‡æ ·ï¼Œæ¥å¾—åˆ°æ–°çš„æ ·æœ¬ã€‚å¯¹äºè½®æ¢åˆ°çš„ä»»æ„ä¸€ä¸ªåæ ‡è½´$x_i$ä¸Šçš„è½¬ç§»ï¼Œé©¬å°”ç§‘å¤«é“¾çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ä¸º$P(x_i|x_1,x_2,...,x_{i-1},x_{i+1},...,x_n)$ï¼Œå³å›ºå®š$nâˆ’1$ä¸ªåæ ‡è½´ï¼Œåœ¨æŸä¸€ä¸ªåæ ‡è½´ä¸Šç§»åŠ¨ã€‚

å¤šç»´Gibbsé‡‡æ ·è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è¾“å…¥å¹³ç¨³åˆ†å¸ƒ$\pi(x_1,x_2ï¼Œ...,x_n)$æˆ–è€…å¯¹åº”çš„æ‰€æœ‰ç‰¹å¾çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œè®¾å®šçŠ¶æ€è½¬ç§»æ¬¡æ•°é˜ˆå€¼$n_1$ï¼Œéœ€è¦çš„æ ·æœ¬ä¸ªæ•°$n_2$
1. éšæœºåˆå§‹åŒ–åˆå§‹çŠ¶æ€å€¼$(x_1^{(0)},x_2^{(0)},...,x_n^{(0)})$
1. for $t=0 ~to ~n_1 +n_2-1$: 
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_1|x_2^{(t)}, x_3^{(t)},...,x_n^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_1^{t+1}$
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_2|x_1^{(t+1)}, x_3^{(t)}, x_4^{(t)},...,x_n^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_2^{t+1}$
    1. ...
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_j|x_1^{(t+1)}, x_2^{(t+1)},..., x_{j-1}^{(t+1)},x_{j+1}^{(t)}...,x_n^{(t)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_j^{t+1}$
    1. ...
    1. ä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x_n|x_1^{(t+1)}, x_2^{(t+1)},...,x_{n-1}^{(t+1)})$ä¸­é‡‡æ ·å¾—åˆ°æ ·æœ¬$x_n^{t+1}$

æ ·æœ¬é›†$\{(x_1^{(n_1)}, x_2^{(n_1)},...,  x_n^{(n_1)}), ...,  (x_1^{(n_1+n_2-1)}, x_2^{(n_1+n_2-1)},...,x_n^{(n_1+n_2-1)})\}$å³ä¸ºæˆ‘ä»¬éœ€è¦çš„å¹³ç¨³åˆ†å¸ƒå¯¹åº”çš„æ ·æœ¬é›†ã€‚

æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹å’ŒLassoå›å½’çš„[åæ ‡è½´ä¸‹é™æ³•ç®—æ³•](https://www.cnblogs.com/pinard/p/6018889.html)éå¸¸ç±»ä¼¼ï¼Œåªä¸è¿‡Lassoå›å½’æ˜¯å›ºå®š$nâˆ’1$ä¸ªç‰¹å¾ï¼Œå¯¹æŸä¸€ä¸ªç‰¹å¾æ±‚æå€¼ã€‚è€ŒGibbsé‡‡æ ·æ˜¯å›ºå®š$nâˆ’1$ä¸ªç‰¹å¾åœ¨æŸä¸€ä¸ªç‰¹å¾é‡‡æ ·ã€‚

- **æ¨¡å‹**ï¼š
- **ç­–ç•¥**ï¼š
- **ç®—æ³•**ï¼š
### å‚è€ƒæ–‡ç« 
[é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ç®—æ³•ï¼ˆMCMCï¼‰](https://zhuanlan.zhihu.com/p/37121528)
[MCMC(ä¸‰)MCMCé‡‡æ ·å’ŒM-Hé‡‡æ ·](https://www.cnblogs.com/pinard/p/6638955.html)
[ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹  - é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æ–¹æ³• 296 é¡µ](https://github.com/nndl/nndl.github.io/blob/master/nndl-book.pdf)

### å‚è€ƒæ–‡çŒ®
[19-1] Serfozo R. [Basics of applied stochastic processes](http://www.stat.yale.edu/~jtc5/251/readings/Basics%20of%20Applied%20Stochastic%20Processes_Serfozo.pdf). Springer, 2009.
[19-2] Metropolis N, Rosenbluth A W, Rosenbluth M N, et al. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 1953,21(6):1087-1092. 
[19-3] Geman S, Geman D. Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984,6:721-741
[19-4] Bishop C M. Pattern recognition and machine learning. Springer, 2006.
[19-5] Gilks W R, Richardson S, Spiegelhalter, DJ. Introducing Markov chain Monte Carlo. Markov Chain Monte Carlo in Practice, 1996.
[19-6] Andrieu C, De Freitas N, Doucet A, et al. An introduction to MCMC for machine learning. Machine Learning, 2003,50(1-2): 5-43.
[19-7] Hoff P. [A first course in Bayesian statistical methods](https://esl.hohoweiya.xyz/references/A_First_Course_in_Bayesian_Statistical_Methods.pdf). Springer, 2009.
[19-8] èŒ†è¯—æ¾ï¼Œç‹é™é¾™ï¼Œæ¿®æ™“é¾™. é«˜ç­‰æ•°ç†ç»Ÿè®¡. åŒ—äº¬ï¼šé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼Œ 1998.


## ç¬¬ 20 ç«  æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…
> ç”Ÿæˆæ¦‚ç‡æ¨¡å‹

æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…ï¼ˆ[latent Dirichlet allocationï¼ŒLDA](https://en.jinzhao.wiki/wiki/Latent_Dirichlet_allocation)ï¼‰ï¼Œä½œä¸ºåŸºäº è´å¶æ–¯å­¦ä¹ çš„è¯é¢˜æ¨¡å‹ï¼Œæ˜¯æ½œåœ¨è¯­ä¹‰åˆ†æã€æ¦‚ç‡æ½œåœ¨è¯­ä¹‰åˆ†æçš„æ‰©å±•ï¼Œäº2002å¹´ç”±Bleiç­‰æå‡ºã€‚LDAåœ¨æ–‡æœ¬æ•°æ®æŒ–æ˜ã€å›¾åƒå¤„ç†ã€ç”Ÿç‰©ä¿¡æ¯å¤„ç†ç­‰é¢†åŸŸè¢«å¹¿æ³›ä½¿ç”¨ã€‚
ä¹¦ä¸­çš„æ¨¡å‹ä»¥åŠå‚æ•°ï¼ˆæ¨å¯¼å‚è€ƒ[Latent Dirichlet allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)ä»¥åŠä¹¦ä¸­çš„æ¨å¯¼ï¼‰
![](https://www.researchgate.net/publication/326140642/figure/fig1/AS:644129876873217@1530583938944/Graphical-model-of-latent-Dirichlet-allocation-LDA.png)
![](https://www.researchgate.net/profile/Diego-Buenano-Fernandez/publication/339368709/figure/fig1/AS:860489982689280@1582168207260/Schematic-of-LDA-algorithm.png)

sklearnä¸­çš„æ¨¡å‹ä»¥åŠå‚æ•°ï¼ˆä¸‹é¢çš„ä»‹ç»ä»¥æ­¤å›¾ä¸ºå‡†ï¼‰
![https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation](https://scikit-learn.org/stable/_images/lda_model_graph.png)

> ä¸Šå›¾æ¥æºä»¥åŠè§£é‡Šï¼šâ€œ[Stochastic Variational Inference](http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf)â€ M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013

> åœ¨ä»‹ç»æ¦‚ç‡å›¾æ—¶æœ‰è®²è¿‡å„ç§å›¾å½¢ä»£è¡¨çš„å«ä¹‰ï¼Œè¿™é‡Œçš„å›¾æ›´å…¨é¢ä¹Ÿå¾ˆæ ‡å‡†ï¼šå®å¿ƒåœ†ç‚¹ä»£è¡¨è¶…å‚æ•°ï¼›ç®­å¤´ä»£è¡¨å› æœå…³ç³»ï¼›æ–¹æ¡†ä»£è¡¨é‡å¤ï¼›æ–¹æ¡†å³ä¸‹è§’çš„å­—æ¯ä»£è¡¨é‡å¤æ¬¡æ•°ï¼›æ— é˜´å½±åœ†åœˆä»£è¡¨éšå˜é‡ï¼›é˜´å½±åœ†åœˆä»£è¡¨è§‚æµ‹å˜é‡ï¼›

å‡è®¾æ¯ä¸ªæ–‡æœ¬ç”±è¯é¢˜çš„ä¸€ä¸ªå¤šé¡¹åˆ†å¸ƒè¡¨ç¤ºï¼Œæ¯ä¸ªè¯é¢˜ç”±å•è¯çš„ä¸€ä¸ªå¤šé¡¹åˆ†å¸ƒè¡¨ç¤ºï¼›
ç‰¹åˆ«å‡è®¾æ–‡æœ¬çš„è¯é¢˜åˆ†å¸ƒçš„å…ˆéªŒåˆ†å¸ƒæ˜¯ç‹„åˆ©å…‹é›·åˆ†å¸ƒï¼Œè¯é¢˜çš„å•è¯åˆ†å¸ƒçš„å…ˆéªŒåˆ†å¸ƒä¹Ÿæ˜¯ç‹„åˆ©å…‹é›·åˆ†å¸ƒã€‚


$D$ä¸ªæ–‡æ¡£ç»„æˆçš„è¯­æ–™åº“ï¼ˆcorpusï¼‰ï¼Œæ¯ä¸ªæ–‡æ¡£æœ‰$N_d$ä¸ªå•è¯ï¼›
å‡è®¾æ•´ä¸ªè¯­æ–™åº“æœ‰$K$ä¸ªè¯é¢˜ï¼ˆsklearnå‚æ•°n_componentsï¼‰ï¼›æ–¹æ¡†ä»£è¡¨é‡å¤æŠ½æ ·ï¼›
$w_{d,n}$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£ä¸­çš„ç¬¬$n$ä¸ªå•è¯ï¼ˆæ³¨æ„æ•´ä¸ªé›†åˆï¼ˆåœ†åœˆï¼‰æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼›
$N_d$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£ä¸­çš„å•è¯ä¸ªæ•°ï¼›
$Z_{d,n}$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£ä¸­çš„ç¬¬$n$ä¸ªè¯é¢˜ï¼›
$\theta_d$è¡¨ç¤ºç¬¬$d$ä¸ªæ–‡æ¡£çš„è¯é¢˜åˆ†å¸ƒï¼ˆdocument topic distributionï¼‰å‚æ•°ï¼›
$\beta_k$è¡¨ç¤ºç¬¬$k$ä¸ªè¯é¢˜çš„å•è¯åˆ†å¸ƒï¼ˆtopic word distributionï¼‰å‚æ•°ï¼›
$\eta$ï¼ˆsklearnå‚æ•°topic_word_priorï¼‰
$\alpha$ï¼ˆsklearnå‚æ•°doc_topic_priorï¼‰

$${\displaystyle {\begin{aligned}{\boldsymbol {\beta }}_{k=1\dots K}&\sim \operatorname {Dirichlet} _{V}({\boldsymbol {\eta }})\\{\boldsymbol {\theta }}_{d=1\dots D}&\sim \operatorname {Dirichlet} _{K}({\boldsymbol {\alpha }})\\z_{d=1\dots D,n=1\dots N_{d}}&\sim \operatorname {Categorical} _{K}({\boldsymbol {\theta }}_{d})\\w_{d=1\dots D,n=1\dots N_{d}}&\sim \operatorname {Categorical} _{V}({\boldsymbol {\beta }}_{z_{dn}})\end{aligned}}}$$

å…¶ä¸­$V$å•è¯æ€»æ•°

æ¯ä¸ªdocçš„æ¯ä¸ªwordï¼Œéƒ½æ˜¯é€šè¿‡ä¸€å®šæ¦‚ç‡é€‰æ‹©äº†æŸä¸ªtopicå¹¶ä»è¿™ä¸ªtopicä¸­ä»¥ä¸€å®šæ¦‚ç‡é€‰æ‹©äº†æŸä¸ªwordã€‚
å…·ä½“ç”Ÿæˆï¼ˆdocï¼‰è¿‡ç¨‹ï¼š
1. æ ¹æ®è¶…å‚æ•°$\eta, K$ç”Ÿæˆ$K$ä¸ªtopicçš„wordåˆ†å¸ƒçš„å‚æ•°$\beta_k$
$\beta_k âˆ¼ Dir(\eta) \text{ for } k \in \{1,...,K\}$
1. For each doc $d \in \{1,...,D\}$
    ---$\theta_d âˆ¼ Dir(\alpha)$ æ¯ä¸ªdocçš„topicåˆ†å¸ƒ
    ---For each word $n \in \{1,...,N_d\}$
    ------$Z_{d,n} âˆ¼ Mult(\theta_d)$ ç¡®å®šä¸€ä¸ªtopic
    ------$w_{d,n} âˆ¼ Mult(\beta_{Z_{d,n}})$ æ ¹æ®topicç”Ÿæˆword($Z_{d,n} \in \{1,...,K\}$)




Var| Type |Conditional|Param|Relevant Expectations
---|---|---|---|---
$Z_{d,n}$|Multinomial|$\log \theta_{dk} + \log \beta_{k,w_{d,n}}$|$\phi_{dn}$|$E[Z^k_{dn}]=\phi_{dn}^k$
$\theta_d$|Dirichlet|$\alpha + \sum_{n=1}^{N_d} Z_{dn}$|$\gamma_d$|$E[\log \theta_{dk}]=\Psi(\gamma_{dk}) -\sum_{j=1}^K \Psi(\gamma_{dj})$
$\beta_k$|Dirichlet|$\eta + \sum_{d=1}^D\sum_{n=1}^{N_d} Z_{dn}^k w_{dn}$|$\lambda_k$|$$E[\log \beta_{kv}]=\Psi(\lambda_{kv}) -\sum_{y=1}^V \Psi(\lambda_{ky})$


- **æ¨¡å‹**ï¼š
åéªŒåˆ†å¸ƒposterior distributionï¼š
$$p(z, \theta, \beta |w, \alpha, \eta) =  \frac{p(z, \theta, \beta, w|\alpha, \eta)}{p(w|\alpha, \eta)}$$
å·²çŸ¥$w, \alpha, \eta$ï¼Œæ±‚$z, \theta, \beta$
- **ç­–ç•¥**ï¼š
å‡è®¾ä¸‰ä¸ªéšå˜é‡$(z, \theta, \beta)$åˆ†åˆ«ç”±ç‹¬ç«‹åˆ†å¸ƒ$(\phi,\gamma,\lambda)$å½¢æˆï¼Œåˆ™è”åˆçš„å˜åˆ†åˆ†å¸ƒä¸º$q(z, \theta, \beta |\phi,\gamma,\lambda)$ï¼Œå˜åˆ†æ¨æ–­çš„ç›®çš„å°±æ˜¯ç”¨$q(z, \theta, \beta |\phi,\gamma,\lambda)$æ¥è¿‘ä¼¼$p(z, \theta, \beta |w, \alpha, \eta)$
$$(\phi^*,\gamma^*,\lambda^*) = \argmin_{\phi,\gamma,\lambda} KL(q\|p)$$
ç›´æ¥æ±‚è§£ä¸å¥½æ±‚ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹è¯æ®ï¼ˆæ•°æ®ï¼‰
$$\log p(w|\alpha, \eta) = \log p(z, \theta, \beta, w|\alpha, \eta) - \log p(z, \theta, \beta |w, \alpha, \eta) \\= \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}  - \log \frac{ p(z, \theta, \beta |w, \alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}$$
ç­‰å¼ä¸¤è¾¹åŒæ—¶å¯¹$q(z, \theta, \beta |\phi,\gamma,\lambda)$æ±‚æœŸæœ›
$$LHS = E_q[\log p(w|\alpha, \eta)] = \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log p(w|\alpha, \eta) dz d\theta d\beta = \log p(w|\alpha, \eta)$$
$$RHS = E_q[\log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}  - \log \frac{ p(z, \theta, \beta |w, \alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}] \\= \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}dz d\theta d\beta - \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{ p(z, \theta, \beta |w, \alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)} dz d\theta d\beta \\= \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}dz d\theta d\beta + KL(q\|p)$$
ä»¤
$$ELBO = \int_{z, \theta, \beta} q(z, \theta, \beta |\phi,\gamma,\lambda) \log \frac{p(z, \theta, \beta, w|\alpha, \eta)}{q(z, \theta, \beta |\phi,\gamma,\lambda)}dz d\theta d\beta \\=   E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]$$

$$\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
  E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]$$
æˆ‘ä»¬å¸Œæœ›æœ€å¤§è¯æ®ï¼Œè€ŒKLæœ€å°ï¼Œæ‰€ä»¥æœ€ç»ˆæ˜¯ï¼š
$$\max ELBO$$
æ¥ç€å°±ç”¨EMç®—æ³•
- **ç®—æ³•**ï¼š
Gibbsé‡‡æ ·å’Œå˜åˆ†EMç®—æ³•

### å‚è€ƒæ–‡ç« 
[spark-ml LDA](/BigData/spark-ml-source-analysis/èšç±»/LDA/lda.md)

[å¾äº¦è¾¾æœºå™¨å­¦ä¹ ï¼šVariational Inference for LDA ç”¨å˜åˆ†æ¨æ–­åšLDAã€2015å¹´ç‰ˆ-å…¨é›†ã€‘](https://www.bilibili.com/video/BV1pp411d7US)

[ä¸»é¢˜æ¨¡å‹-æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…-Latent Dirichlet Allocation(LDA)](https://www.bilibili.com/video/BV1t54y127U8)


### é™„åŠ çŸ¥è¯†
#### Dirichlet Process ç‹„åˆ©å…‹é›·è¿‡ç¨‹
[å¾äº¦è¾¾æœºå™¨å­¦ä¹ ï¼šDirichlet Process ç‹„åˆ©å…‹é›·è¿‡ç¨‹ã€2015å¹´ç‰ˆ-å…¨é›†ã€‘](https://www.bilibili.com/video/BV1Tp411R7Sf)

#### æŒ‡æ•°æ—åˆ†å¸ƒ
å‚è€ƒï¼š[æ¦‚ç‡åˆ†å¸ƒ](../å›¾è§£æ•°å­¦/æ¦‚ç‡åˆ†å¸ƒ.md)

**ç‹„åˆ©å…‹é›·åˆ†å¸ƒ**ï¼ˆ[Dirichlet distribution](https://en.jinzhao.wiki/wiki/Dirichlet_distribution)ï¼‰

**å•çº¯å½¢**ï¼ˆ[Simplex](https://en.jinzhao.wiki/wiki/Simplex)ï¼‰

```mermaid
graph LR
    x1["ç‹„åˆ©å…‹é›·åˆ†å¸ƒ"]
    x2["å¤šé¡¹åˆ†å¸ƒ"]
    x3["ç±»åˆ«åˆ†å¸ƒ"]
    x4["è´å¡”åˆ†å¸ƒ"]
    x5["äºŒé¡¹åˆ†å¸ƒ"]
    x6["ä¼¯åŠªåˆ©åˆ†å¸ƒ"]
    x1--"å…±è½­å…ˆéªŒ"-->x2
    x1--"åŒ…å«"-->x4
    x2--"åŒ…å«"-->x3
    x2--"åŒ…å«"-->x5
    x3--"åŒ…å«"-->x6
    x4--"å…±è½­å…ˆéªŒ"-->x5
    x5--"åŒ…å«"-->x6
```

å‰é¢è®²è´å¶æ–¯ä¼°è®¡æ—¶æœ‰æåˆ°**å…±è½­å…ˆéªŒ**ï¼ˆ[Conjugate prior](https://en.jinzhao.wiki/wiki/Conjugate_prior)ï¼‰
å¦‚æœå…ˆéªŒåˆ†å¸ƒ prior å’ŒåéªŒåˆ†å¸ƒ posterior å±äºåŒä¸€åˆ†å¸ƒç°‡ï¼Œåˆ™ prior ç§°ä¸º likehood çš„å…±è½­å…ˆéªŒã€‚

$$\underbrace{p(\theta|data)}_{\text{åéªŒåˆ†å¸ƒposterior distribution}} = \frac{\overbrace{p(data|\theta)}^{\text{ä¼¼ç„¶(æ•°æ®)likelihood }}\overbrace{p(\theta)}^{\text{ å…ˆéªŒåˆ†å¸ƒprior distribution}}}{\underbrace{p(data)}_{\text{è¯æ®evidence}}}$$

å¸¸è§çš„å…±è½­å…ˆéªŒï¼š
- likehood ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œprior ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸ºé«˜æ–¯åˆ†å¸ƒã€‚
- likehood ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹å¼åˆ†å¸ƒï¼‰ï¼Œprior ä¸º beta åˆ†å¸ƒï¼Œåˆ™ posterior ä¹Ÿä¸º beta åˆ†å¸ƒã€‚
- likehood ä¸ºå¤šé¡¹å¼åˆ†å¸ƒï¼Œprior ä¸º Dirichlet åˆ†å¸ƒï¼ˆbeta åˆ†å¸ƒçš„ä¸€ä¸ªæ‰©å±•ï¼‰ï¼Œåˆ™ posterior ä¹Ÿä¸º Dirichletï¼ˆç‹„åˆ©å…‹é›·ï¼‰åˆ†å¸ƒã€‚beta åˆ†å¸ƒå¯ä»¥çœ‹ä½œæ˜¯ dirichlet åˆ†å¸ƒçš„ç‰¹æ®Šæƒ…å†µã€‚

### å‚è€ƒæ–‡çŒ®
[20-1] Blei D M, Ng A Y, Jordan M I. Latent Dirichlet allocation. In: Advances in Neural Information Processing Systems 14. MIT Press, 2002.
[20-2] Blei D M, Ng A Y, Jordan M I. [Latent Dirichlet allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). Journal of Machine Learning Research, 2003, 3:933-1022.
[20-3] Griffiths T L, Steyvers M. Finding scientific topics. Proceedings of the National Academy of Science, 2004, 101:5288-5235.
[20-4] Steyvers M, Griffiths T. Probabilistic topic models. In: Landauer T, McNamara D, Dennis S, et al. (eds.) Handbook of Latent Semantic Analysis, Psychology Press, 2014.
[20-5] Gregor Heinrich. Parameter estimation for text analysis. Techniacl note, 2004.
[20-6] Blei D M, Kucukelbir A, McAuliffe J D. Variational inference: a review for statisticians. Journal of the American Statistical Association, 2017, 112(518).
[20-7] Newman D, Smyth P, Welling M, Asuncion A U. Distributed inference for latent Dirichlet allocation. In: Advances in Neural Information Processing Systems, 2008: 1081-1088
[20-8] Porteous I, Newman D, Ihler A, et al. Fast collapsed Gibbs sampling for latent Dirichlet allocation. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2008: 569-577.
[20-9] Hoffiman M, Bach F R, Blei D M. [Online learning for latent Dirichlet allocation](https://papers.nips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf). In: Advances in Neural Information Processing Systems, 2010:856-864.

## ç¬¬ 21 ç«  PageRankç®—æ³•
[PageRank](https://en.jinzhao.wiki/wiki/PageRank)æ˜¯è¡¡é‡ç½‘ç«™é¡µé¢é‡è¦æ€§çš„ä¸€ç§æ–¹å¼ã€‚PageRank çš„å·¥ä½œåŸç†æ˜¯è®¡ç®—é¡µé¢é“¾æ¥çš„æ•°é‡å’Œè´¨é‡ï¼Œä»¥ç¡®å®šå¯¹ç½‘ç«™é‡è¦æ€§çš„ç²—ç•¥ä¼°è®¡ã€‚
ç›®å‰ï¼ŒPageRank å¹¶ä¸æ˜¯ Google ç”¨äºå¯¹æœç´¢ç»“æœè¿›è¡Œæ’åºçš„å”¯ä¸€ç®—æ³•ï¼Œä½†å®ƒæ˜¯è¯¥å…¬å¸ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªç®—æ³•ï¼Œå¹¶ä¸”æ˜¯æœ€è‘—åçš„ç®—æ³•ã€‚

PageRank æ˜¯ä¸€ç§é“¾æ¥åˆ†æç®—æ³•ï¼Œå®ƒä¸ºè¶…é“¾æ¥æ–‡æ¡£é›†ï¼ˆä¾‹å¦‚ä¸‡ç»´ç½‘ï¼‰çš„æ¯ä¸ªå…ƒç´ åˆ†é…æ•°å­—æƒé‡ï¼Œç›®çš„æ˜¯â€œè¡¡é‡â€å…¶åœ¨é›†åˆä¸­çš„ç›¸å¯¹é‡è¦æ€§ã€‚è¯¥ç®—æ³•å¯ä»¥åº”ç”¨äºå…·æœ‰ç›¸äº’å¼•ç”¨å’Œå¼•ç”¨çš„ä»»ä½•å®ä½“é›†åˆã€‚å®ƒåˆ†é…ç»™ä»»ä½•ç»™å®šå…ƒç´ $E$çš„æ•°å­—æƒé‡ç§°ä¸º$E$çš„PageRankï¼Œè¡¨ç¤ºä¸º ${\displaystyle PR(E).}$ã€‚

ä¸€ä¸ªçŠ¶æ€è½¬ç§»çŸ©é˜µçš„å¹³ç¨³åˆ†å¸ƒå°±å¯¹åº”å„ä¸ªå…ƒç´ çš„PageRank

- **æ¨¡å‹**ï¼š
$$MR = R$$
å«æœ‰nä¸ªèŠ‚ç‚¹çš„æœ‰å‘å›¾æ˜¯å¼ºè¿é€šä¸”éå‘¨æœŸçš„ï¼Œåœ¨å…¶åŸºç¡€ä¸Šå®šä¹‰éšæœºæ¸¸èµ°æ¨¡å‹ï¼ˆå³ä¸€é˜¶é©¬å°”å¯å¤«é“¾å…·æœ‰å¹³ç¨³åˆ†å¸ƒï¼‰ï¼›
$M=[m_{ij}]_{n \times n}$ æ˜¯é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œå…¶ä¸­çš„å…ƒç´  $m_{ij}$è¡¨ç¤ºèŠ‚ç‚¹$j$è·³åˆ°èŠ‚ç‚¹$i$çš„æ¦‚ç‡ï¼›$R$æ˜¯å¹³ç¨³åˆ†å¸ƒå‘é‡ï¼Œç§°ä¸ºè¿™ä¸ªæœ‰å‘å›¾çš„PageRank
- **ç­–ç•¥**ï¼š
$$\lim_{t \to \infty} M^tR_0 = R$$
- **ç®—æ³•**ï¼š
è¿­ä»£ï¼Œå¹‚æ³•ï¼Œä»£æ•°ç®—æ³•

### å‚è€ƒæ–‡çŒ®
[21-1] Page L, Brin S, Motwani R, et al. The PageRank citation ranking: bringing order to the Web. Stanford University, 1999.
[21-2] Rajaraman A, Ullman J D. Mining of massive datasets. Cambridge University Press, 2014.
[21-3] Liu B. Web data mining: exploring Hyperlinks, contents, and usage data. Springer Science & Business Media, 2007.
[21-4] Serdozo R. Basics of applied stochastic processes. Springer, 2009.
[21-5] Kleinberg J M. Authoritative sources in a hyperlinked environment. Journal of the ACM(JACM), 1999,46(5):604-632.
[21-6] Liu Y, Gao B, Liu T Y, et al. BrowseRank: letting Web users vote for page importance. Proceedings of the 31st SIGIR Conference, 2008:451-458 
[21-7] Jeh G, Widom J. Scaling Personalized Web search. Proceedings of the 12th WWW Conference, 2003: 271-279.
[21-8] Haveliwala T H. Topic-sensitive PageRank. Proceedings of the 11th WWW Conference, 2002: 517-526.
[21-9] GyÃ¶ngyi Z, Garcia-Molina H, Pedersen J. Combating Web spam with TrustRank. Proceedings of VLDB Conference, 2004:576-587.

## ç¬¬ 22 ç«  æ— ç›‘ç£å­¦ä¹ æ–¹æ³•æ€»ç»“

æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ä¹‹é—´çš„å…³ç³»
![](https://img-blog.csdnimg.cn/20200601152905315.png)

æ— ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ç‰¹ç‚¹
.|æ–¹æ³•|æ¨¡å‹|ç­–ç•¥|ç®—æ³•
---|---|---|---|---
èšç±»| å±‚æ¬¡èšç±»|èšç±»æ ‘|ç±»å†…æ ·æœ¬è·ç¦»æœ€å°|å¯å‘å¼ç®—æ³•
èšç±»|kå‡å€¼èšç±»|kä¸­å¿ƒèšç±»|æ ·æœ¬ä¸ç±»ä¸­å¿ƒè·ç¦»æœ€å°|è¿­ä»£ç®—æ³•
èšç±»|é«˜æ–¯æ··åˆæ¨¡å‹|é«˜æ–¯æ··åˆæ¨¡å‹|ä¼¼ç„¶å‡½æ•°æœ€å¤§|EMç®—æ³•
é™ç»´|PCA|ä½ç»´æ­£äº¤ç©ºé—´|æ–¹å·®æœ€å¤§|SVD
è¯é¢˜åˆ†æ|LSA|çŸ©é˜µåˆ†è§£æ¨¡å‹|å¹³æ–¹æŸå¤±æœ€å°|SVD
è¯é¢˜åˆ†æ|NMF|çŸ©é˜µåˆ†è§£æ¨¡å‹|å¹³æ–¹æŸå¤±æœ€å°|éè´ŸçŸ©é˜µåˆ†è§£
è¯é¢˜åˆ†æ|PLSA|PLSAæ¨¡å‹|ä¼¼ç„¶å‡½æ•°æœ€å¤§|EMç®—æ³•
è¯é¢˜åˆ†æ|LDA|LDAæ¨¡å‹|åéªŒæ¦‚ç‡ä¼°è®¡|å‰å¸ƒæ–¯æŠ½æ ·ï¼Œå˜åˆ†æ¨ç†
å›¾åˆ†æ|PageRank|æœ‰å‘å›¾ä¸Šçš„é©¬å°”å¯å¤«é“¾|å¹³ç¨³åˆ†å¸ƒæ±‚è§£|å¹‚æ³•

å«æœ‰éšå˜é‡æ¦‚ç‡æ¨¡å‹çš„å­¦ä¹ æ–¹æ³•çš„ç‰¹ç‚¹
ç®—æ³•|åŸºæœ¬åŸç†|æ”¶æ•›æ€§|æ”¶æ•›é€Ÿåº¦|å®ç°éš¾æ˜“åº¦|é€‚åˆé—®é¢˜
---|---|---|---|---|---
EMç®—æ³•| è¿­ä»£è®¡ç®—ã€åéªŒæ¦‚ç‡ä¼°è®¡|æ”¶æ•›äºå±€éƒ¨æœ€ä¼˜|è¾ƒå¿«|å®¹æ˜“|ç®€å•æ¨¡å‹
å˜åˆ†æ¨ç†|è¿­ä»£è®¡ç®—ã€åéªŒæ¦‚ç‡è¿‘ä¼¼ä¼°è®¡|æ”¶æ•›äºå±€éƒ¨æœ€ä¼˜|è¾ƒæ…¢|è¾ƒå¤æ‚|å¤æ‚æ¨¡å‹
å‰å¸ƒæ–¯æŠ½æ ·|éšæœºæŠ½æ ·ã€åéªŒæ¦‚ç‡ä¼°è®¡|ä»¥æ¦‚ç‡æ”¶æ•›äºå…¨å±€æœ€ä¼˜|è¾ƒæ…¢|å®¹æ˜“|å¤æ‚æ¨¡å‹


çŸ©é˜µåˆ†è§£çš„è§’åº¦çœ‹è¯é¢˜æ¨¡å‹(Bè¡¨ç¤º[Bregmanæ•£åº¦](https://en.jinzhao.wiki/wiki/Bregman_divergence))
æ–¹æ³•|ä¸€èˆ¬æŸå¤±å‡½æ•°$B(D\|UV)$|çŸ©é˜µ$U$çš„çº¦æŸæ¡ä»¶|çŸ©é˜µ$V$çš„çº¦æŸæ¡ä»¶
---|---|---|---
LSA| $\|D-UV\|^2_F$|$U^TU = I$|$VV^T=\Lambda^2$
NMF| $\|D-UV\|^2_F$|$u_{mk} \geq 0$|$u_{kn} \geq 0$
PLSA|$\sum_{mn}d_{mn} \log\frac{d_{mn}}{(UV)_{mn}}$| $U^T1=1 \\ u_{mk} \geq 0$ |$V^T1=1 \\ u_{kn} \geq 0$

### é™„åŠ çŸ¥è¯†
#### çº¿æ€§ä»£æ•°
[ã€å®Œæ•´ç‰ˆ-éº»çœç†å·¥-çº¿æ€§ä»£æ•°ã€‘å…¨34è®²+é…å¥—æ•™æ](https://www.bilibili.com/video/BV1ix411f7Yp)

[ä¸­ç§‘å¤§-å‡¸ä¼˜åŒ–](https://www.bilibili.com/video/BV1Jt411p7jE)

#### å¾äº¦è¾¾æœºå™¨å­¦ä¹ 
EMï¼Œ MCMCï¼ŒHMMï¼Œ LDA, å˜åˆ†æ¨æ–­ï¼Œ æŒ‡æ•°æ—åˆ†å¸ƒç­‰è®²çš„éƒ½éå¸¸å¥½ï¼Œ[Bç«™](https://space.bilibili.com/97068901/)
#### ã€æœºå™¨å­¦ä¹ ã€‘ç™½æ¿æ¨å¯¼ç³»åˆ—
å¼ºçƒˆå»ºè®®çœ‹å®Œï¼Œ[Bç«™](https://space.bilibili.com/327617676)

### å‚è€ƒæ–‡çŒ®
[22-1] Singh A P, Gordon G J. A unified view of matrix factorization models. In: Daelemans W, Goethals B, Morik K,(eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2008. Lecture Notes in Computer Science, vol 5212. Berlin: Springer, 2008.
[22-2] Wang Q, Xu J, Li H, et al. Regularized latent semantic indexing:a new approach to large-scale topic modeling. ACM Transactions on Information Systems (TOIS), 2013, 31(1), 5.
